# ECE Paris RAG Backend Documentation

## ğŸ¯ Vision & Principes

- RÃ©pondre aux questions des Ã©tudiantÂ·eÂ·s Ã  partir des PDF de cours, 100% local
- Code minimaliste, traÃ§able, sans hallucination : cite les documents et s'abstient si l'info manque
- MÃ©tadonnÃ©es au cÅ“ur (matiÃ¨re, sous_matiÃ¨re, enseignant, promo, semestre) pour filtrer interactivement
- Fonctionne sur un laptop Ã©tudiant (Python + Ollama), documentÃ© pour reprise rapide

## ğŸ“‹ PrÃ©requis & Installation

### Logiciels Requis
- **Python 3.10+** avec `pip`
  - macOS: `brew install python@3.10`
  - Linux: Package manager systÃ¨me
- **[Ollama](https://ollama.ai)** installÃ© localement
  - macOS: `brew install --cask ollama`
  - Linux/Windows: Installeur officiel
  - VÃ©rifier: `ollama serve`

### ModÃ¨les Ollama
```bash
# ModÃ¨le d'embedding (Ã  lancer une fois)
ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf

# ModÃ¨le de gÃ©nÃ©ration
ollama pull mistral:7b
```

### DÃ©pendances Python
```bash
# CrÃ©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

**DÃ©pendances principales** :
- `fastapi`, `uvicorn` - Serveurs API
- `pymupdf` - Traitement PDF
- `faiss-cpu` - Recherche vectorielle
- `rank-bm25` - Recherche par mots-clÃ©s
- `numpy` - Calculs numÃ©riques
- `ollama` - Client Ollama
- `jinja2` - Templates HTML
- `requests` - Appels HTTP

## ğŸ—ï¸ Architecture

```
ragmilo/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py              # Serveur principal RAG (port 8000)
â”‚   â”œâ”€â”€ file_manager_rag.py    # Gestion fichiers (port 8001)
â”‚   â”œâ”€â”€ rag_core.py            # Logique retrieval (FAISS + BM25)
â”‚   â”œâ”€â”€ output_formatter.py    # Formatage rÃ©ponses
â”‚   â”œâ”€â”€ database_manager.py    # Gestion base SQLite
â”‚   â”œâ”€â”€ enhanced_ingest.py     # Ingestion PDF + embeddings
â”‚   â”œâ”€â”€ database_schema.sql    # SchÃ©ma DB
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ file_manager.html
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ rag_database.db        # Base de donnÃ©es SQLite
â”‚   â”œâ”€â”€ vector_index.faiss     # Index FAISS
â”‚   â”œâ”€â”€ bm25_index.pkl         # Index BM25
â”‚   â”œâ”€â”€ vector_db.json         # Export JSON
â”‚   â”œâ”€â”€ index_meta.json        # MÃ©tadonnÃ©es
â”‚   â””â”€â”€ uploads/               # PDF uploadÃ©s
â”‚
â”œâ”€â”€ frontend/                   # Interface utilisateur
â”œâ”€â”€ docs/                      # Documentation
â””â”€â”€ requirements.txt
```

## ğŸ”„ Workflow

### 1. Ingestion (`enhanced_ingest.py`)
- DÃ©coupage du PDF en chunks ~500 mots (overlap 50 mots) via PyMuPDF
- Pour chaque chunk :
  1. GÃ©nÃ©ration embedding BGE via Ollama
  2. Ajout mÃ©tadonnÃ©es (matiÃ¨re, sous_matiÃ¨re, enseignant, semestre, promo)
  3. Stockage dans SQLite
- Export vers :
  - `data/vector_db.json` - Texte + mÃ©tadonnÃ©es + embeddings
  - `data/vector_index.faiss` - Index FAISS (cosinus similarity)
  - `data/bm25_index.pkl` - Index BM25
  - `data/index_meta.json` - Configuration

### 2. Retrieval Hybride (`rag_core.HybridRetriever`)
- RequÃªte vectorisÃ©e avec BGE (`vector_k` candidats FAISS, dÃ©faut 20)
- En parallÃ¨le, scoring BM25 (`bm25_k` candidats, dÃ©faut 40)
- Normalisation des scores (0-1)
- Fusion pondÃ©rÃ©e : `score = alpha Ã— score_vectoriel + (1-alpha) Ã— score_BM25`
  - `alpha = 0.65` par dÃ©faut
- SÃ©lection des `top_n` meilleurs (dÃ©faut 3)

### 3. Filtre de Confiance
- Si `max(score_vectoriel) < threshold` (0.35 dÃ©faut)
  â†’ RÃ©ponse : "Information non trouvÃ©e dans les sources"
- Ã‰vite les hallucinations

### 4. GÃ©nÃ©ration (`server.py`)
- Chunks contextualisÃ©s : `[doc_id:page:index]`
- Envoi au LLM Mistral 7b avec instruction stricte
- Format JSON : `answer`, `sources`, `confidence`, `metadata_used`, `retrieval_stats`

## ğŸš€ Utilisation

### Lancement des Serveurs

**Serveur RAG Principal** (port 8000) :
```bash
uvicorn backend.server:app --reload
```
- Interface vocale : http://localhost:8000/front_voice/index_voice.html
- Interface texte : http://localhost:8000/front_text/index_text.html
- API : http://localhost:8000/docs

**Gestionnaire de Fichiers** (port 8001) :
```bash
python3 -m backend.file_manager_rag
```
- Interface : http://localhost:8001
- Upload + classification + ingestion automatique

### Ingestion de Fichiers

**MÃ©thode 1 : Via l'interface web** (recommandÃ©)
1. Ouvrir http://localhost:8001
2. Remplir mÃ©tadonnÃ©es : matiÃ¨re, sous_matiÃ¨re, enseignant, semestre, promo
3. Upload PDF
4. Statut : `En attente` â†’ `En traitement` â†’ `TraitÃ©`

**MÃ©thode 2 : En ligne de commande**
```bash
python3 -m backend.enhanced_ingest \
  --pdf data/uploads/cours_ml.pdf \
  --matiere "Machine Learning" \
  --sous_matiere "Deep Learning" \
  --enseignant "Jean Dupont" \
  --promo 2027 \
  --semestre S1 \
  --output data/vector_db.json \
  --faiss-index data/vector_index.faiss \
  --bm25-index data/bm25_index.pkl \
  --db-path data/rag_database.db
```

### RequÃªtes

**Via l'interface web** :
- SÃ©lectionner filtres (matiÃ¨re, enseignant, etc.)
- Poser la question
- RÃ©ponse avec citations cliquables

**Via API** :
```bash
curl -X POST "http://localhost:8000/api/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Explique le perceptron",
    "matiere": "Machine Learning",
    "top_n": 3,
    "threshold": 0.35,
    "alpha": 0.65
  }'
```

## ğŸ“Š Format de Sortie JSON

```json
{
  "answer": "Le perceptron est...",
  "sources": [
    {
      "doc_id": "ml-cours1",
      "doc_label": "Cours ML",
      "page": 5,
      "chunk_index": 2,
      "chunk_id": "ml-cours1:5:2",
      "fragment": "...",
      "score": 0.85,
      "vector_score": 0.92,
      "lexical_score": 0.68,
      "matiere": "Machine Learning",
      "enseignant": "Jean Dupont"
    }
  ],
  "confidence": 0.82,
  "metadata_used": {
    "matiere": "Machine Learning"
  },
  "retrieval_stats": {
    "top1": 0.92,
    "avg_topk": 0.82,
    "threshold": 0.35,
    "vector_k": 20,
    "bm25_k": 40
  }
}
```

## ğŸ”§ Configuration

### ParamÃ¨tres RAG (ajustables via API)
- `top_n` (3) - Nombre de chunks retournÃ©s
- `threshold` (0.35) - Seuil minimum de confiance
- `alpha` (0.65) - PondÃ©ration vectoriel/BM25
- `vector_k` (20) - Candidats FAISS
- `bm25_k` (40) - Candidats BM25

### ModÃ¨les
- **Embedding** : `hf.co/CompendiumLabs/bge-base-en-v1.5-gguf`(768 dims)
- **LLM** : `mistral:7b`

## âš ï¸ Bonnes Pratiques

1. **PDF** : PrÃ©fÃ©rer des PDF structurÃ©s (titres/sections clairs)
2. **MÃ©tadonnÃ©es** : Toujours renseigner matiÃ¨re + enseignant minimum
3. **Indexes** : Se rÃ©gÃ©nÃ¨rent automatiquement aprÃ¨s upload
4. **Seuil** : Ajuster `threshold` selon le corpus (0.3-0.4 recommandÃ©)
5. **Ollama** : Toujours vÃ©rifier que `ollama serve` tourne

## ğŸ› DÃ©pannage

**"Connection refused port 8000/8001"**
â†’ VÃ©rifier qu'aucun autre service n'utilise ces ports

**"No module named 'backend'"**
â†’ ExÃ©cuter depuis la racine du projet : `cd ragmilo`

**"No module named 'fitz'"**
â†’ `pip install pymupdf`

**"Ollama connection failed"**
â†’ `ollama serve` dans un terminal sÃ©parÃ©

**"Index FAISS contient X vecteurs, mais Y documents"**
â†’ RÃ©gÃ©nÃ©rer les indexes : uploader un nouveau fichier ou relancer `enhanced_ingest.py --export-only`

## ğŸ“š Ressources

- [Documentation base de donnÃ©es](DATABASE.md)
- [Structure projet](../README.md)
- [Ollama Documentation](https://ollama.ai/docs)
- [FAISS Documentation](https://faiss.ai/)

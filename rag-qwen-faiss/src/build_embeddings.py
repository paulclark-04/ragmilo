#!/usr/bin/env python3
"""
Script de g√©n√©ration d'embeddings et d'indexation FAISS pour le pipeline RAG.
G√©n√®re des vecteurs s√©mantiques pour chaque chunk et cr√©e un index de recherche rapide.
"""

import sys
import json
import argparse
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import torch

# Imports pour embeddings et FAISS
from sentence_transformers import SentenceTransformer
import faiss


def load_chunks(chunks_json_path: str) -> Tuple[List[str], List[Dict]]:
    """
    Charge les chunks depuis le fichier JSON.
    
    Args:
        chunks_json_path (str): Chemin vers le fichier chunks.json
        
    Returns:
        Tuple[List[str], List[Dict]]: (textes des chunks, m√©tadonn√©es des chunks)
    """
    chunks_path = Path(chunks_json_path)
    
    if not chunks_path.exists():
        raise FileNotFoundError(f"Le fichier chunks '{chunks_path}' n'existe pas.")
    
    print(f"üìÑ Chargement des chunks depuis: {chunks_path.name}")
    
    with open(chunks_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    chunks = data['chunks']
    texts = [chunk['text'] for chunk in chunks]
    
    print(f"üìä {len(chunks)} chunks charg√©s")
    print(f"üìù Total mots: {sum(chunk['word_count'] for chunk in chunks):,}")
    
    return texts, chunks


def load_embedding_model(model_name: str = "all-mpnet-base-v2") -> SentenceTransformer:
    """
    Charge le mod√®le d'embedding avec gestion GPU/CPU.
    
    Args:
        model_name (str): Nom du mod√®le sentence-transformers
        
    Returns:
        SentenceTransformer: Mod√®le charg√©
    """
    print(f"ü§ñ Chargement du mod√®le d'embedding: {model_name}")
    
    try:
        model = SentenceTransformer(model_name)
        
        # Afficher les informations du mod√®le
        device = "GPU" if torch.cuda.is_available() else "CPU"
        print(f"üíª Device utilis√©: {device}")
        print(f"üìè Dimension des embeddings: {model.get_sentence_embedding_dimension()}")
        
        if torch.cuda.is_available():
            print(f"üéÆ GPU disponible: {torch.cuda.get_device_name(0)}")
            print(f"üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        return model
        
    except Exception as e:
        raise Exception(f"Erreur lors du chargement du mod√®le {model_name}: {e}")


def generate_embeddings_batch(
    model: SentenceTransformer, 
    texts: List[str], 
    batch_size: int = 32,
    show_progress: bool = True
) -> np.ndarray:
    """
    G√©n√®re les embeddings par batch avec normalisation L2.
    
    Args:
        model (SentenceTransformer): Mod√®le d'embedding
        texts (List[str]): Liste des textes √† encoder
        batch_size (int): Taille des batches
        show_progress (bool): Afficher la barre de progression
        
    Returns:
        np.ndarray: Matrice des embeddings normalis√©s
    """
    print(f"üîÑ G√©n√©ration des embeddings (batch_size={batch_size})...")
    
    # G√©n√©rer les embeddings avec sentence-transformers
    # normalize_embeddings=True pour cosine similarity
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=show_progress,
        convert_to_numpy=True,
        normalize_embeddings=True  # Normalisation L2 pour cosine similarity
    )
    
    print(f"‚úÖ {len(embeddings)} embeddings g√©n√©r√©s")
    print(f"üìè Dimensions: {embeddings.shape}")
    
    return embeddings


def create_faiss_index(embeddings: np.ndarray) -> faiss.Index:
    """
    Cr√©e un index FAISS optimis√© pour la recherche de similarit√©.
    
    Args:
        embeddings (np.ndarray): Matrice des embeddings normalis√©s
        
    Returns:
        faiss.Index: Index FAISS configur√©
    """
    print("üîç Cr√©ation de l'index FAISS...")
    
    # V√©rifier que les embeddings sont normalis√©s
    norms = np.linalg.norm(embeddings, axis=1)
    if not np.allclose(norms, 1.0, atol=1e-6):
        print("‚ö†Ô∏è  Normalisation des embeddings...")
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    
    # Cr√©er l'index FAISS avec Inner Product (cosine similarity)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner Product pour cosine similarity
    
    # Ajouter les vecteurs √† l'index
    index.add(embeddings.astype('float32'))
    
    print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs")
    print(f"üìè Dimension: {dimension}")
    print(f"üîç Type d'index: IndexFlatIP (cosine similarity)")
    
    return index


def save_index_and_metadata(
    index: faiss.Index,
    chunks: List[Dict],
    embeddings: np.ndarray,
    model_name: str,
    output_dir: str
) -> None:
    """
    Sauvegarde l'index FAISS et les m√©tadonn√©es.
    
    Args:
        index (faiss.Index): Index FAISS
        chunks (List[Dict]): M√©tadonn√©es des chunks
        embeddings (np.ndarray): Embeddings g√©n√©r√©s
        model_name (str): Nom du mod√®le utilis√©
        output_dir (str): R√©pertoire de sortie
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Sauvegarder l'index FAISS
    faiss_path = output_path / "faiss_index.bin"
    faiss.write_index(index, str(faiss_path))
    print(f"üíæ Index FAISS sauvegard√©: {faiss_path}")
    
    # Cr√©er les m√©tadonn√©es avec mapping vector_index
    metadata = {
        "model_info": {
            "model_name": model_name,
            "embedding_dimension": embeddings.shape[1],
            "total_chunks": len(chunks),
            "index_type": "IndexFlatIP",
            "normalization": True,
            "similarity_metric": "cosine"
        },
        "chunks": []
    }
    
    # Ajouter les m√©tadonn√©es des chunks avec vector_index
    for i, chunk in enumerate(chunks):
        chunk_metadata = {
            "chunk_id": chunk["chunk_id"],
            "text": chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"],  # Truncate pour lisibilit√©
            "word_count": chunk["word_count"],
            "source_page": chunk["source_page"],
            "vector_index": i,  # Index du vecteur dans FAISS
            "start_position": chunk.get("start_position", 0),
            "end_position": chunk.get("end_position", 0)
        }
        metadata["chunks"].append(chunk_metadata)
    
    # Sauvegarder les m√©tadonn√©es
    metadata_path = output_path / "chunks_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ M√©tadonn√©es sauvegard√©es: {metadata_path}")
    
    # Afficher les statistiques
    print(f"\nüìä Statistiques de l'index:")
    print(f"   üì¶ Chunks index√©s: {len(chunks)}")
    print(f"   üìè Dimension des embeddings: {embeddings.shape[1]}")
    print(f"   üíæ Taille de l'index: {faiss_path.stat().st_size / 1024 / 1024:.1f} MB")
    print(f"   üéØ Type de recherche: Cosine Similarity")


def build_embeddings_index(
    chunks_json_path: str,
    output_dir: str,
    model_name: str = "all-mpnet-base-v2",
    batch_size: int = 32
) -> Dict:
    """
    Fonction principale de g√©n√©ration d'embeddings et d'indexation.
    
    Args:
        chunks_json_path (str): Chemin vers le fichier chunks.json
        output_dir (str): R√©pertoire de sortie
        model_name (str): Nom du mod√®le d'embedding
        batch_size (int): Taille des batches
        
    Returns:
        Dict: Statistiques de l'indexation
    """
    print("üöÄ D√©marrage de la g√©n√©ration d'embeddings et d'indexation FAISS")
    print("=" * 60)
    
    # 1. Charger les chunks
    texts, chunks = load_chunks(chunks_json_path)
    
    # 2. Charger le mod√®le d'embedding
    model = load_embedding_model(model_name)
    
    # 3. G√©n√©rer les embeddings
    embeddings = generate_embeddings_batch(model, texts, batch_size)
    
    # 4. Cr√©er l'index FAISS
    index = create_faiss_index(embeddings)
    
    # 5. Sauvegarder l'index et les m√©tadonn√©es
    save_index_and_metadata(index, chunks, embeddings, model_name, output_dir)
    
    print("\n" + "=" * 60)
    print("üéâ Indexation termin√©e avec succ√®s!")
    
    return {
        "total_chunks": len(chunks),
        "embedding_dimension": embeddings.shape[1],
        "model_name": model_name,
        "index_type": "IndexFlatIP",
        "output_dir": output_dir
    }


def main():
    """Interface en ligne de commande pour le script."""
    parser = argparse.ArgumentParser(
        description="G√©n√®re des embeddings et cr√©e un index FAISS pour le pipeline RAG",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python src/build_embeddings.py data/chunks.json data/index
  python src/build_embeddings.py data/chunks.json data/index --model all-MiniLM-L6-v2 --batch-size 64
  python src/build_embeddings.py data/chunks.json data/index --model jinaai/jina-embeddings-v2-base-en
        """
    )
    
    parser.add_argument(
        'chunks_json_path',
        help='Chemin vers le fichier chunks.json'
    )
    
    parser.add_argument(
        'output_dir',
        help='R√©pertoire de sortie pour l\'index FAISS et les m√©tadonn√©es'
    )
    
    parser.add_argument(
        '--model',
        default='all-mpnet-base-v2',
        help='Nom du mod√®le sentence-transformers (d√©faut: all-mpnet-base-v2)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='Taille des batches pour la g√©n√©ration d\'embeddings (d√©faut: 32)'
    )
    
    args = parser.parse_args()
    
    try:
        stats = build_embeddings_index(
            args.chunks_json_path,
            args.output_dir,
            args.model,
            args.batch_size
        )
        
        print(f"\n‚úÖ Index cr√©√© avec succ√®s!")
        print(f"üì¶ {stats['total_chunks']} chunks index√©s")
        print(f"ü§ñ Mod√®le: {stats['model_name']}")
        print(f"üìè Dimension: {stats['embedding_dimension']}")
        print(f"üìÅ R√©pertoire: {stats['output_dir']}")
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

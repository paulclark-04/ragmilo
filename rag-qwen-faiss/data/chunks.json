{
  "metadata": {
    "source_file": "data/cours_texte.txt",
    "total_pages": 433,
    "total_chunks": 701,
    "total_words": 157281,
    "average_chunk_size": 224.37,
    "chunk_size_target": 250,
    "overlap_size": 50,
    "chunking_algorithm": "sentence-based with overlap"
  },
  "chunks": [
    {
      "chunk_id": 1,
      "text": "Understanding Machine Learning\nMachine learning is one of the fastest growing areas of computer science,\nwith far-reaching applications The aim of this textbook is to introduce\nmachine learning, and the algorithmic paradigms it offers, in a princi-\npled way The book provides an extensive theoretical account of the\nfundamental ideas underlying machine learning and the mathematical\nderivations that transform these principles into practical algorithms Fol-\nlowing a presentation of the basics of the ﬁeld, the book covers a wide\narray of central topics that have not been addressed by previous text-\nbooks These include a discussion of the computational complexity of\nlearning and the concepts of convexity and stability; important algorith-\nmic paradigms including stochastic gradient descent, neural networks,\nand structured output learning; and emerging theoretical concepts such as\nthe PAC-Bayes approach and compression-based bounds Designed for\nan advanced undergraduate or beginning graduate course, the text makes\nthe fundamentals and algorithms of machine learning accessible to stu-\ndents and nonexpert readers in statistics, computer science, mathematics,\nand engineering Shai Shalev-Shwartz is an Associate Professor at the School of Computer\nScience and Engineering at The Hebrew University, Israel Shai Ben-David is a Professor in the School of Computer Science at the\nUniversity of Waterloo, Canada.",
      "word_count": 204,
      "source_page": 3,
      "start_position": 68,
      "end_position": 271,
      "sentences_count": 8
    },
    {
      "chunk_id": 2,
      "text": "32 Avenue of the Americas, New York, NY 10013-2473, USA\nCambridge University Press is part of the University of Cambridge It furthers the University’s mission by disseminating knowledge in the pursuit of\neducation, learning and research at the highest international levels of excellence www.cambridge.org\nInformation on this title: www.cambridge.org/9781107057135\nc⃝Shai Shalev-Shwartz and Shai Ben-David 2014\nThis publication is in copyright Subject to statutory exception\nand to the provisions of relevant collective licensing agreements,\nno reproduction of any part may take place without the written\npermission of Cambridge University Press First published 2014\nPrinted in the United States of America\nA catalog record for this publication is available from the British Library\nLibrary of Congress Cataloging in Publication Data\nISBN 978-1-107-05713-5 Hardback\nCambridge University Press has no responsibility for the persistence or accuracy of\nURLs for external or third-party Internet Web sites referred to in this publication,\nand does not guarantee that any content on such Web sites is, or will remain,\naccurate or appropriate.",
      "word_count": 164,
      "source_page": 5,
      "start_position": 272,
      "end_position": 435,
      "sentences_count": 5
    },
    {
      "chunk_id": 3,
      "text": "vii\nPreface\nThe term machine learning refers to the automated detection of meaningful\npatterns in data In the past couple of decades it has become a common tool in\nalmost any task that requires information extraction from large data sets We are\nsurrounded by a machine learning based technology: search engines learn how\nto bring us the best results (while placing proﬁtable ads), anti-spam software\nlearns to ﬁlter our email messages, and credit card transactions are secured by\na software that learns how to detect frauds Digital cameras learn to detect\nfaces and intelligent personal assistance applications on smart-phones learn to\nrecognize voice commands Cars are equipped with accident prevention systems\nthat are built using machine learning algorithms Machine learning is also widely\nused in scientiﬁc applications such as bioinformatics, medicine, and astronomy One common feature of all of these applications is that, in contrast to more\ntraditional uses of computers, in these cases, due to the complexity of the patterns\nthat need to be detected, a human programmer cannot provide an explicit, ﬁne-\ndetailed speciﬁcation of how such tasks should be executed Taking example from\nintelligent beings, many of our skills are acquired or reﬁned through learning from\nour experience (rather than following explicit instructions given to us) Machine\nlearning tools are concerned with endowing programs with the ability to “learn”\nand adapt",
      "word_count": 224,
      "source_page": 7,
      "start_position": 436,
      "end_position": 659,
      "sentences_count": 9
    },
    {
      "chunk_id": 4,
      "text": "Taking example from\nintelligent beings, many of our skills are acquired or reﬁned through learning from\nour experience (rather than following explicit instructions given to us) Machine\nlearning tools are concerned with endowing programs with the ability to “learn”\nand adapt The ﬁrst goal of this book is to provide a rigorous, yet easy to follow, intro-\nduction to the main concepts underlying machine learning: What is learning How can a machine learn How do we quantify the resources needed to learn a\ngiven concept Is learning always possible Can we know if the learning process\nsucceeded or failed The second goal of this book is to present several key machine learning algo-\nrithms We chose to present algorithms that on one hand are successfully used\nin practice and on the other hand give a wide spectrum of diﬀerent learning\ntechniques Additionally, we pay speciﬁc attention to algorithms appropriate for\nlarge scale learning (a.k.a “Big Data”), since in recent years, our world has be-\ncome increasingly “digitized” and the amount of data available for learning is\ndramatically increasing As a result, in many applications data is plentiful and\ncomputation time is the main bottleneck We therefore explicitly quantify both\nthe amount of data and the amount of computation time needed to learn a given\nconcept The book is divided into four parts The ﬁrst part aims at giving an initial\nrigorous answer to the fundamental questions of learning",
      "word_count": 238,
      "source_page": 7,
      "start_position": 619,
      "end_position": 856,
      "sentences_count": 15
    },
    {
      "chunk_id": 5,
      "text": "viii\na “no-free-lunch” theorem We also discuss how much computation time is re-\nquired for learning In the second part of the book we describe various learning\nalgorithms For some of the algorithms, we ﬁrst present a more general learning\nprinciple, and then show how the algorithm follows the principle While the ﬁrst\ntwo parts of the book focus on the PAC model, the third part extends the scope\nby presenting a wider variety of learning models Finally, the last part of the\nbook is devoted to advanced theory We made an attempt to keep the book as self-contained as possible However,\nthe reader is assumed to be comfortable with basic notions of probability, linear\nalgebra, analysis, and algorithms The ﬁrst three parts of the book are intended\nfor ﬁrst year graduate students in computer science, engineering, mathematics, or\nstatistics It can also be accessible to undergraduate students with the adequate\nbackground The more advanced chapters can be used by researchers intending\nto gather a deeper theoretical understanding Acknowledgements\nThe book is based on Introduction to Machine Learning courses taught by Shai\nShalev-Shwartz at the Hebrew University and by Shai Ben-David at the Univer-\nsity of Waterloo The ﬁrst draft of the book grew out of the lecture notes for\nthe course that was taught at the Hebrew University by Shai Shalev-Shwartz\nduring 2010–2013",
      "word_count": 224,
      "source_page": 8,
      "start_position": 931,
      "end_position": 1154,
      "sentences_count": 13
    },
    {
      "chunk_id": 6,
      "text": "Acknowledgements\nThe book is based on Introduction to Machine Learning courses taught by Shai\nShalev-Shwartz at the Hebrew University and by Shai Ben-David at the Univer-\nsity of Waterloo The ﬁrst draft of the book grew out of the lecture notes for\nthe course that was taught at the Hebrew University by Shai Shalev-Shwartz\nduring 2010–2013 We greatly appreciate the help of Ohad Shamir, who served\nas a TA for the course in 2010, and of Alon Gonen, who served as a TA for the\ncourse in 2011–2013 Ohad and Alon prepared few lecture notes and many of\nthe exercises Alon, to whom we are indebted for his help throughout the entire\nmaking of the book, has also prepared a solution manual We are deeply grateful for the most valuable work of Dana Rubinstein Dana\nhas scientiﬁcally proofread and edited the manuscript, transforming it from\nlecture-based chapters into ﬂuent and coherent text Special thanks to Amit Daniely, who helped us with a careful read of the\nadvanced part of the book and also wrote the advanced chapter on multiclass\nlearnability We are also grateful for the members of a book reading club in\nJerusalem that have carefully read and constructively criticized every line of\nthe manuscript The members of the reading club are: Maya Alroy, Yossi Arje-\nvani, Aharon Birnbaum, Alon Cohen, Alon Gonen, Roi Livni, Ofer Meshi, Dan\nRosenbaum, Dana Rubinstein, Shahar Somin, Alon Vinnikov, and Yoav Wald",
      "word_count": 239,
      "source_page": 8,
      "start_position": 1099,
      "end_position": 1337,
      "sentences_count": 10
    },
    {
      "chunk_id": 7,
      "text": "Contents\nPreface\npage vii\n1\nIntroduction\n19\n1.1\nWhat Is Learning 19\n1.2\nWhen Do We Need Machine Learning 21\n1.3\nTypes of Learning\n22\n1.4\nRelations to Other Fields\n24\n1.5\nHow to Read This Book\n25\n1.5.1\nPossible Course Plans Based on This Book\n26\n1.6\nNotation\n27\nPart I\nFoundations\n31\n2\nA Gentle Start\n33\n2.1\nA Formal Model – The Statistical Learning Framework\n33\n2.2\nEmpirical Risk Minimization\n35\n2.2.1\nSomething May Go Wrong – Overﬁtting\n35\n2.3\nEmpirical Risk Minimization with Inductive Bias\n36\n2.3.1\nFinite Hypothesis Classes\n37\n2.4\nExercises\n41\n3\nA Formal Learning Model\n43\n3.1\nPAC Learning\n43\n3.2\nA More General Learning Model\n44\n3.2.1\nReleasing the Realizability Assumption – Agnostic PAC\nLearning\n45\n3.2.2\nThe Scope of Learning Problems Modeled\n47\n3.3\nSummary\n49\n3.4\nBibliographic Remarks\n50\n3.5\nExercises\n50\n4\nLearning via Uniform Convergence\n54\n4.1\nUniform Convergence Is Suﬃcient for Learnability\n54\n4.2\nFinite Classes Are Agnostic PAC Learnable\n55\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 194,
      "source_page": 9,
      "start_position": 1370,
      "end_position": 1563,
      "sentences_count": 7
    },
    {
      "chunk_id": 8,
      "text": "x\nContents\n4.3\nSummary\n58\n4.4\nBibliographic Remarks\n58\n4.5\nExercises\n58\n5\nThe Bias-Complexity Tradeoﬀ\n60\n5.1\nThe No-Free-Lunch Theorem\n61\n5.1.1\nNo-Free-Lunch and Prior Knowledge\n63\n5.2\nError Decomposition\n64\n5.3\nSummary\n65\n5.4\nBibliographic Remarks\n66\n5.5\nExercises\n66\n6\nThe VC-Dimension\n67\n6.1\nInﬁnite-Size Classes Can Be Learnable\n67\n6.2\nThe VC-Dimension\n68\n6.3\nExamples\n70\n6.3.1\nThreshold Functions\n70\n6.3.2\nIntervals\n71\n6.3.3\nAxis Aligned Rectangles\n71\n6.3.4\nFinite Classes\n72\n6.3.5\nVC-Dimension and the Number of Parameters\n72\n6.4\nThe Fundamental Theorem of PAC learning\n72\n6.5\nProof of Theorem 6.7\n73\n6.5.1\nSauer’s Lemma and the Growth Function\n73\n6.5.2\nUniform Convergence for Classes of Small Eﬀective Size\n75\n6.6\nSummary\n78\n6.7\nBibliographic remarks\n78\n6.8\nExercises\n78\n7\nNonuniform Learnability\n83\n7.1\nNonuniform Learnability\n83\n7.1.1\nCharacterizing Nonuniform Learnability\n84\n7.2\nStructural Risk Minimization\n85\n7.3\nMinimum Description Length and Occam’s Razor\n89\n7.3.1\nOccam’s Razor\n91\n7.4\nOther Notions of Learnability – Consistency\n92\n7.5\nDiscussing the Diﬀerent Notions of Learnability\n93\n7.5.1\nThe No-Free-Lunch Theorem Revisited\n95\n7.6\nSummary\n96\n7.7\nBibliographic Remarks\n97\n7.8\nExercises\n97\n8\nThe Runtime of Learning\n100\n8.1\nComputational Complexity of Learning\n101",
      "word_count": 200,
      "source_page": 10,
      "start_position": 1564,
      "end_position": 1763,
      "sentences_count": 1
    },
    {
      "chunk_id": 9,
      "text": "Contents\nxi\n8.1.1\nFormal Deﬁnition*\n102\n8.2\nImplementing the ERM Rule\n103\n8.2.1\nFinite Classes\n104\n8.2.2\nAxis Aligned Rectangles\n105\n8.2.3\nBoolean Conjunctions\n106\n8.2.4\nLearning 3-Term DNF\n107\n8.3\nEﬃciently Learnable, but Not by a Proper ERM\n107\n8.4\nHardness of Learning*\n108\n8.5\nSummary\n110\n8.6\nBibliographic Remarks\n110\n8.7\nExercises\n110\nPart II\nFrom Theory to Algorithms\n115\n9\nLinear Predictors\n117\n9.1\nHalfspaces\n118\n9.1.1\nLinear Programming for the Class of Halfspaces\n119\n9.1.2\nPerceptron for Halfspaces\n120\n9.1.3\nThe VC Dimension of Halfspaces\n122\n9.2\nLinear Regression\n123\n9.2.1\nLeast Squares\n124\n9.2.2\nLinear Regression for Polynomial Regression Tasks\n125\n9.3\nLogistic Regression\n126\n9.4\nSummary\n128\n9.5\nBibliographic Remarks\n128\n9.6\nExercises\n128\n10\nBoosting\n130\n10.1\nWeak Learnability\n131\n10.1.1\nEﬃcient Implementation of ERM for Decision Stumps\n133\n10.2\nAdaBoost\n134\n10.3\nLinear Combinations of Base Hypotheses\n137\n10.3.1\nThe VC-Dimension of L(B, T)\n139\n10.4\nAdaBoost for Face Recognition\n140\n10.5\nSummary\n141\n10.6\nBibliographic Remarks\n141\n10.7\nExercises\n142\n11\nModel Selection and Validation\n144\n11.1\nModel Selection Using SRM\n145\n11.2\nValidation\n146\n11.2.1\nHold Out Set\n146\n11.2.2\nValidation for Model Selection\n147\n11.2.3\nThe Model-Selection Curve\n148",
      "word_count": 200,
      "source_page": 11,
      "start_position": 1764,
      "end_position": 1963,
      "sentences_count": 1
    },
    {
      "chunk_id": 10,
      "text": "xii\nContents\n11.2.4\nk-Fold Cross Validation\n149\n11.2.5\nTrain-Validation-Test Split\n150\n11.3\nWhat to Do If Learning Fails\n151\n11.4\nSummary\n154\n11.5\nExercises\n154\n12\nConvex Learning Problems\n156\n12.1\nConvexity, Lipschitzness, and Smoothness\n156\n12.1.1\nConvexity\n156\n12.1.2\nLipschitzness\n160\n12.1.3\nSmoothness\n162\n12.2\nConvex Learning Problems\n163\n12.2.1\nLearnability of Convex Learning Problems\n164\n12.2.2\nConvex-Lipschitz/Smooth-Bounded Learning Problems\n166\n12.3\nSurrogate Loss Functions\n167\n12.4\nSummary\n168\n12.5\nBibliographic Remarks\n169\n12.6\nExercises\n169\n13\nRegularization and Stability\n171\n13.1\nRegularized Loss Minimization\n171\n13.1.1\nRidge Regression\n172\n13.2\nStable Rules Do Not Overﬁt\n173\n13.3\nTikhonov Regularization as a Stabilizer\n174\n13.3.1\nLipschitz Loss\n176\n13.3.2\nSmooth and Nonnegative Loss\n177\n13.4\nControlling the Fitting-Stability Tradeoﬀ\n178\n13.5\nSummary\n180\n13.6\nBibliographic Remarks\n180\n13.7\nExercises\n181\n14\nStochastic Gradient Descent\n184\n14.1\nGradient Descent\n185\n14.1.1\nAnalysis of GD for Convex-Lipschitz Functions\n186\n14.2\nSubgradients\n188\n14.2.1\nCalculating Subgradients\n189\n14.2.2\nSubgradients of Lipschitz Functions\n190\n14.2.3\nSubgradient Descent\n190\n14.3\nStochastic Gradient Descent (SGD)\n191\n14.3.1\nAnalysis of SGD for Convex-Lipschitz-Bounded Functions\n191\n14.4\nVariants\n193\n14.4.1\nAdding a Projection Step\n193\n14.4.2\nVariable Step Size\n194\n14.4.3\nOther Averaging Techniques\n195",
      "word_count": 198,
      "source_page": 12,
      "start_position": 1964,
      "end_position": 2161,
      "sentences_count": 1
    },
    {
      "chunk_id": 11,
      "text": "Contents\nxiii\n14.4.4\nStrongly Convex Functions*\n195\n14.5\nLearning with SGD\n196\n14.5.1\nSGD for Risk Minimization\n196\n14.5.2\nAnalyzing SGD for Convex-Smooth Learning Problems\n198\n14.5.3\nSGD for Regularized Loss Minimization\n199\n14.6\nSummary\n200\n14.7\nBibliographic Remarks\n200\n14.8\nExercises\n201\n15\nSupport Vector Machines\n202\n15.1\nMargin and Hard-SVM\n202\n15.1.1\nThe Homogenous Case\n205\n15.1.2\nThe Sample Complexity of Hard-SVM\n205\n15.2\nSoft-SVM and Norm Regularization\n206\n15.2.1\nThe Sample Complexity of Soft-SVM\n208\n15.2.2\nMargin and Norm-Based Bounds versus Dimension\n208\n15.2.3\nThe Ramp Loss*\n209\n15.3\nOptimality Conditions and “Support Vectors”*\n210\n15.4\nDuality*\n211\n15.5\nImplementing Soft-SVM Using SGD\n212\n15.6\nSummary\n213\n15.7\nBibliographic Remarks\n213\n15.8\nExercises\n214\n16\nKernel Methods\n215\n16.1\nEmbeddings into Feature Spaces\n215\n16.2\nThe Kernel Trick\n217\n16.2.1\nKernels as a Way to Express Prior Knowledge\n221\n16.2.2\nCharacterizing Kernel Functions*\n222\n16.3\nImplementing Soft-SVM with Kernels\n222\n16.4\nSummary\n224\n16.5\nBibliographic Remarks\n225\n16.6\nExercises\n225\n17\nMulticlass, Ranking, and Complex Prediction Problems\n227\n17.1\nOne-versus-All and All-Pairs\n227\n17.2\nLinear Multiclass Predictors\n230\n17.2.1\nHow to Construct Ψ\n230\n17.2.2\nCost-Sensitive Classiﬁcation\n232\n17.2.3\nERM\n232\n17.2.4\nGeneralized Hinge Loss\n233\n17.2.5\nMulticlass SVM and SGD\n234\n17.3\nStructured Output Prediction\n236\n17.4\nRanking\n238",
      "word_count": 213,
      "source_page": 13,
      "start_position": 2162,
      "end_position": 2374,
      "sentences_count": 1
    },
    {
      "chunk_id": 12,
      "text": "xiv\nContents\n17.4.1\nLinear Predictors for Ranking\n240\n17.5\nBipartite Ranking and Multivariate Performance Measures\n243\n17.5.1\nLinear Predictors for Bipartite Ranking\n245\n17.6\nSummary\n247\n17.7\nBibliographic Remarks\n247\n17.8\nExercises\n248\n18\nDecision Trees\n250\n18.1\nSample Complexity\n251\n18.2\nDecision Tree Algorithms\n252\n18.2.1\nImplementations of the Gain Measure\n253\n18.2.2\nPruning\n254\n18.2.3\nThreshold-Based Splitting Rules for Real-Valued Features\n255\n18.3\nRandom Forests\n255\n18.4\nSummary\n256\n18.5\nBibliographic Remarks\n256\n18.6\nExercises\n256\n19\nNearest Neighbor\n258\n19.1\nk Nearest Neighbors\n258\n19.2\nAnalysis\n259\n19.2.1\nA Generalization Bound for the 1-NN Rule\n260\n19.2.2\nThe “Curse of Dimensionality”\n263\n19.3\nEﬃcient Implementation*\n264\n19.4\nSummary\n264\n19.5\nBibliographic Remarks\n264\n19.6\nExercises\n265\n20\nNeural Networks\n268\n20.1\nFeedforward Neural Networks\n269\n20.2\nLearning Neural Networks\n270\n20.3\nThe Expressive Power of Neural Networks\n271\n20.3.1\nGeometric Intuition\n273\n20.4\nThe Sample Complexity of Neural Networks\n274\n20.5\nThe Runtime of Learning Neural Networks\n276\n20.6\nSGD and Backpropagation\n277\n20.7\nSummary\n281\n20.8\nBibliographic Remarks\n281\n20.9\nExercises\n282\nPart III\nAdditional Learning Models\n285\n21\nOnline Learning\n287\n21.1\nOnline Classiﬁcation in the Realizable Case\n288",
      "word_count": 194,
      "source_page": 14,
      "start_position": 2375,
      "end_position": 2568,
      "sentences_count": 1
    },
    {
      "chunk_id": 13,
      "text": "Contents\nxv\n21.1.1\nOnline Learnability\n290\n21.2\nOnline Classiﬁcation in the Unrealizable Case\n294\n21.2.1\nWeighted-Majority\n295\n21.3\nOnline Convex Optimization\n300\n21.4\nThe Online Perceptron Algorithm\n301\n21.5\nSummary\n304\n21.6\nBibliographic Remarks\n305\n21.7\nExercises\n305\n22\nClustering\n307\n22.1\nLinkage-Based Clustering Algorithms\n310\n22.2\nk-Means and Other Cost Minimization Clusterings\n311\n22.2.1\nThe k-Means Algorithm\n313\n22.3\nSpectral Clustering\n315\n22.3.1\nGraph Cut\n315\n22.3.2\nGraph Laplacian and Relaxed Graph Cuts\n315\n22.3.3\nUnnormalized Spectral Clustering\n317\n22.4\nInformation Bottleneck*\n317\n22.5\nA High Level View of Clustering\n318\n22.6\nSummary\n320\n22.7\nBibliographic Remarks\n320\n22.8\nExercises\n320\n23\nDimensionality Reduction\n323\n23.1\nPrincipal Component Analysis (PCA)\n324\n23.1.1\nA More Eﬃcient Solution for the Case d ≫m\n326\n23.1.2\nImplementation and Demonstration\n326\n23.2\nRandom Projections\n329\n23.3\nCompressed Sensing\n330\n23.3.1\nProofs*\n333\n23.4\nPCA or Compressed Sensing 338\n23.5\nSummary\n338\n23.6\nBibliographic Remarks\n339\n23.7\nExercises\n339\n24\nGenerative Models\n342\n24.1\nMaximum Likelihood Estimator\n343\n24.1.1\nMaximum Likelihood Estimation for Continuous Ran-\ndom Variables\n344\n24.1.2\nMaximum Likelihood and Empirical Risk Minimization\n345\n24.1.3\nGeneralization Analysis\n345\n24.2\nNaive Bayes\n347\n24.3\nLinear Discriminant Analysis\n347\n24.4\nLatent Variables and the EM Algorithm\n348",
      "word_count": 203,
      "source_page": 15,
      "start_position": 2569,
      "end_position": 2771,
      "sentences_count": 2
    },
    {
      "chunk_id": 14,
      "text": "xvi\nContents\n24.4.1\nEM as an Alternate Maximization Algorithm\n350\n24.4.2\nEM for Mixture of Gaussians (Soft k-Means)\n352\n24.5\nBayesian Reasoning\n353\n24.6\nSummary\n355\n24.7\nBibliographic Remarks\n355\n24.8\nExercises\n356\n25\nFeature Selection and Generation\n357\n25.1\nFeature Selection\n358\n25.1.1\nFilters\n359\n25.1.2\nGreedy Selection Approaches\n360\n25.1.3\nSparsity-Inducing Norms\n363\n25.2\nFeature Manipulation and Normalization\n365\n25.2.1\nExamples of Feature Transformations\n367\n25.3\nFeature Learning\n368\n25.3.1\nDictionary Learning Using Auto-Encoders\n368\n25.4\nSummary\n370\n25.5\nBibliographic Remarks\n371\n25.6\nExercises\n371\nPart IV\nAdvanced Theory\n373\n26\nRademacher Complexities\n375\n26.1\nThe Rademacher Complexity\n375\n26.1.1\nRademacher Calculus\n379\n26.2\nRademacher Complexity of Linear Classes\n382\n26.3\nGeneralization Bounds for SVM\n383\n26.4\nGeneralization Bounds for Predictors with Low ℓ1 Norm\n386\n26.5\nBibliographic Remarks\n386\n27\nCovering Numbers\n388\n27.1\nCovering\n388\n27.1.1\nProperties\n388\n27.2\nFrom Covering to Rademacher Complexity via Chaining\n389\n27.3\nBibliographic Remarks\n391\n28\nProof of the Fundamental Theorem of Learning Theory\n392\n28.1\nThe Upper Bound for the Agnostic Case\n392\n28.2\nThe Lower Bound for the Agnostic Case\n393\n28.2.1\nShowing That m(ϵ, δ) ≥0.5 log(1/(4δ))/ϵ2\n393\n28.2.2\nShowing That m(ϵ, 1/8) ≥8d/ϵ2\n395\n28.3\nThe Upper Bound for the Realizable Case\n398\n28.3.1\nFrom ϵ-Nets to PAC Learnability\n401",
      "word_count": 214,
      "source_page": 16,
      "start_position": 2772,
      "end_position": 2985,
      "sentences_count": 1
    },
    {
      "chunk_id": 15,
      "text": "1\nIntroduction\nThe subject of this book is automated learning, or, as we will more often call\nit, Machine Learning (ML) That is, we wish to program computers so that\nthey can “learn” from input available to them Roughly speaking, learning is\nthe process of converting experience into expertise or knowledge The input to\na learning algorithm is training data, representing experience, and the output\nis some expertise, which usually takes the form of another computer program\nthat can perform some task Seeking a formal-mathematical understanding of\nthis concept, we’ll have to be more explicit about what we mean by each of the\ninvolved terms: What is the training data our programs will access How can\nthe process of learning be automated How can we evaluate the success of such\na process (namely, the quality of the output of a learning program) 1.1\nWhat Is Learning Let us begin by considering a couple of examples from naturally occurring ani-\nmal learning Some of the most fundamental issues in ML arise already in that\ncontext, which we are all familiar with Bait Shyness – Rats Learning to Avoid Poisonous Baits: When rats encounter\nfood items with novel look or smell, they will ﬁrst eat very small amounts, and\nsubsequent feeding will depend on the ﬂavor of the food and its physiological\neﬀect If the food produces an ill eﬀect, the novel food will often be associated\nwith the illness, and subsequently, the rats will not eat it",
      "word_count": 246,
      "source_page": 19,
      "start_position": 3113,
      "end_position": 3358,
      "sentences_count": 12
    },
    {
      "chunk_id": 16,
      "text": "Bait Shyness – Rats Learning to Avoid Poisonous Baits: When rats encounter\nfood items with novel look or smell, they will ﬁrst eat very small amounts, and\nsubsequent feeding will depend on the ﬂavor of the food and its physiological\neﬀect If the food produces an ill eﬀect, the novel food will often be associated\nwith the illness, and subsequently, the rats will not eat it Clearly, there is a\nlearning mechanism in play here – the animal used past experience with some\nfood to acquire expertise in detecting the safety of this food If past experience\nwith the food was negatively labeled, the animal predicts that it will also have\na negative eﬀect when encountered in the future Inspired by the preceding example of successful learning, let us demonstrate a\ntypical machine learning task Suppose we would like to program a machine that\nlearns how to ﬁlter spam e-mails A naive solution would be seemingly similar\nto the way rats learn how to avoid poisonous baits The machine will simply\nmemorize all previous e-mails that had been labeled as spam e-mails by the\nhuman user When a new e-mail arrives, the machine will search for it in the set\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 229,
      "source_page": 19,
      "start_position": 3293,
      "end_position": 3521,
      "sentences_count": 13
    },
    {
      "chunk_id": 17,
      "text": "20\nIntroduction\nof previous spam e-mails If it matches one of them, it will be trashed Otherwise,\nit will be moved to the user’s inbox folder While the preceding “learning by memorization” approach is sometimes use-\nful, it lacks an important aspect of learning systems – the ability to label unseen\ne-mail messages A successful learner should be able to progress from individual\nexamples to broader generalization This is also referred to as inductive reasoning\nor inductive inference In the bait shyness example presented previously, after\nthe rats encounter an example of a certain type of food, they apply their attitude\ntoward it on new, unseen examples of food of similar smell and taste To achieve\ngeneralization in the spam ﬁltering task, the learner can scan the previously seen\ne-mails, and extract a set of words whose appearance in an e-mail message is\nindicative of spam Then, when a new e-mail arrives, the machine can check\nwhether one of the suspicious words appears in it, and predict its label accord-\ningly Such a system would potentially be able correctly to predict the label of\nunseen e-mails However, inductive reasoning might lead us to false conclusions To illustrate\nthis, let us consider again an example from animal learning Pigeon Superstition: In an experiment performed by the psychologist B F Skinner,\nhe placed a bunch of hungry pigeons in a cage",
      "word_count": 229,
      "source_page": 20,
      "start_position": 3522,
      "end_position": 3750,
      "sentences_count": 15
    },
    {
      "chunk_id": 18,
      "text": "F Skinner,\nhe placed a bunch of hungry pigeons in a cage An automatic mechanism had\nbeen attached to the cage, delivering food to the pigeons at regular intervals\nwith no reference whatsoever to the birds’ behavior The hungry pigeons went\naround the cage, and when food was ﬁrst delivered, it found each pigeon engaged\nin some activity (pecking, turning the head, etc.) The arrival of food reinforced\neach bird’s speciﬁc action, and consequently, each bird tended to spend some\nmore time doing that very same action That, in turn, increased the chance that\nthe next random food delivery would ﬁnd each bird engaged in that activity\nagain What results is a chain of events that reinforces the pigeons’ association\nof the delivery of the food with whatever chance actions they had been perform-\ning when it was ﬁrst delivered They subsequently continue to perform these\nsame actions diligently.1\nWhat distinguishes learning mechanisms that result in superstition from useful\nlearning This question is crucial to the development of automated learners While human learners can rely on common sense to ﬁlter out random meaningless\nlearning conclusions, once we export the task of learning to a machine, we must\nprovide well deﬁned crisp principles that will protect the program from reaching\nsenseless or useless conclusions The development of such principles is a central\ngoal of the theory of machine learning What, then, made the rats’ learning more successful than that of the pigeons",
      "word_count": 241,
      "source_page": 20,
      "start_position": 3739,
      "end_position": 3979,
      "sentences_count": 12
    },
    {
      "chunk_id": 19,
      "text": "1.2 When Do We Need Machine Learning 21\nrats turns out to be more complex than what one may expect In experiments\ncarried out by Garcia (Garcia & Koelling 1996), it was demonstrated that if the\nunpleasant stimulus that follows food consumption is replaced by, say, electrical\nshock (rather than nausea), then no conditioning occurs Even after repeated\ntrials in which the consumption of some food is followed by the administration of\nunpleasant electrical shock, the rats do not tend to avoid that food Similar failure\nof conditioning occurs when the characteristic of the food that implies nausea\n(such as taste or smell) is replaced by a vocal signal The rats seem to have\nsome “built in” prior knowledge telling them that, while temporal correlation\nbetween food and nausea can be causal, it is unlikely that there would be a\ncausal relationship between food consumption and electrical shocks or between\nsounds and nausea We conclude that one distinguishing feature between the bait shyness learning\nand the pigeon superstition is the incorporation of prior knowledge that biases\nthe learning mechanism This is also referred to as inductive bias The pigeons in\nthe experiment are willing to adopt any explanation for the occurrence of food However, the rats “know” that food cannot cause an electric shock and that the\nco-occurrence of noise with some food is not likely to aﬀect the nutritional value\nof that food",
      "word_count": 234,
      "source_page": 21,
      "start_position": 4028,
      "end_position": 4261,
      "sentences_count": 10
    },
    {
      "chunk_id": 20,
      "text": "The pigeons in\nthe experiment are willing to adopt any explanation for the occurrence of food However, the rats “know” that food cannot cause an electric shock and that the\nco-occurrence of noise with some food is not likely to aﬀect the nutritional value\nof that food The rats’ learning process is biased toward detecting some kind of\npatterns while ignoring other temporal correlations between events It turns out that the incorporation of prior knowledge, biasing the learning\nprocess, is inevitable for the success of learning algorithms (this is formally stated\nand proved as the “No-Free-Lunch theorem” in Chapter 5) The development of\ntools for expressing domain expertise, translating it into a learning bias, and\nquantifying the eﬀect of such a bias on the success of learning is a central theme\nof the theory of machine learning Roughly speaking, the stronger the prior\nknowledge (or prior assumptions) that one starts the learning process with, the\neasier it is to learn from further examples However, the stronger these prior\nassumptions are, the less ﬂexible the learning is – it is bound, a priori, by the\ncommitment to these assumptions We shall discuss these issues explicitly in\nChapter 5 1.2\nWhen Do We Need Machine Learning When do we need machine learning rather than directly program our computers\nto carry out the task at hand",
      "word_count": 223,
      "source_page": 21,
      "start_position": 4215,
      "end_position": 4437,
      "sentences_count": 10
    },
    {
      "chunk_id": 21,
      "text": "22\nIntroduction\ndeﬁned program Examples of such tasks include driving, speech\nrecognition, and image understanding In all of these tasks, state\nof the art machine learning programs, programs that “learn from\ntheir experience,” achieve quite satisfactory results, once exposed\nto suﬃciently many training examples • Tasks beyond Human Capabilities: Another wide family of tasks that\nbeneﬁt from machine learning techniques are related to the analy-\nsis of very large and complex data sets: astronomical data, turning\nmedical archives into medical knowledge, weather prediction, anal-\nysis of genomic data, Web search engines, and electronic commerce With more and more available digitally recorded data, it becomes\nobvious that there are treasures of meaningful information buried\nin data archives that are way too large and too complex for humans\nto make sense of Learning to detect meaningful patterns in large\nand complex data sets is a promising domain in which the combi-\nnation of programs that learn with the almost unlimited memory\ncapacity and ever increasing processing speed of computers opens\nup new horizons Adaptivity One limiting feature of programmed tools is their rigidity – once\nthe program has been written down and installed, it stays unchanged However, many tasks change over time or from one user to another Machine learning tools – programs whose behavior adapts to their input\ndata – oﬀer a solution to such issues; they are, by nature, adaptive\nto changes in the environment they interact with",
      "word_count": 238,
      "source_page": 22,
      "start_position": 4508,
      "end_position": 4745,
      "sentences_count": 10
    },
    {
      "chunk_id": 22,
      "text": "However, many tasks change over time or from one user to another Machine learning tools – programs whose behavior adapts to their input\ndata – oﬀer a solution to such issues; they are, by nature, adaptive\nto changes in the environment they interact with Typical successful\napplications of machine learning to such problems include programs that\ndecode handwritten text, where a ﬁxed program can adapt to variations\nbetween the handwriting of diﬀerent users; spam detection programs,\nadapting automatically to changes in the nature of spam e-mails; and\nspeech recognition programs 1.3\nTypes of Learning\nLearning is, of course, a very wide domain Consequently, the ﬁeld of machine\nlearning has branched into several subﬁelds dealing with diﬀerent types of learn-\ning tasks We give a rough taxonomy of learning paradigms, aiming to provide\nsome perspective of where the content of this book sits within the wide ﬁeld of\nmachine learning We describe four parameters along which learning paradigms can be classiﬁed Supervised versus Unsupervised Since learning involves an interaction be-\ntween the learner and the environment, one can divide learning tasks\naccording to the nature of that interaction The ﬁrst distinction to note\nis the diﬀerence between supervised and unsupervised learning As an",
      "word_count": 202,
      "source_page": 22,
      "start_position": 4702,
      "end_position": 4903,
      "sentences_count": 10
    },
    {
      "chunk_id": 23,
      "text": "1.3 Types of Learning\n23\nillustrative example, consider the task of learning to detect spam e-mail\nversus the task of anomaly detection For the spam detection task, we\nconsider a setting in which the learner receives training e-mails for which\nthe label spam/not-spam is provided On the basis of such training the\nlearner should ﬁgure out a rule for labeling a newly arriving e-mail mes-\nsage In contrast, for the task of anomaly detection, all the learner gets\nas training is a large body of e-mail messages (with no labels) and the\nlearner’s task is to detect “unusual” messages More abstractly, viewing learning as a process of “using experience\nto gain expertise,” supervised learning describes a scenario in which the\n“experience,” a training example, contains signiﬁcant information (say,\nthe spam/not-spam labels) that is missing in the unseen “test examples”\nto which the learned expertise is to be applied In this setting, the ac-\nquired expertise is aimed to predict that missing information for the test\ndata In such cases, we can think of the environment as a teacher that\n“supervises” the learner by providing the extra information (labels) In\nunsupervised learning, however, there is no distinction between training\nand test data The learner processes input data with the goal of coming\nup with some summary, or compressed version of that data Clustering\na data set into subsets of similar objets is a typical example of such a\ntask",
      "word_count": 238,
      "source_page": 23,
      "start_position": 4904,
      "end_position": 5141,
      "sentences_count": 10
    },
    {
      "chunk_id": 24,
      "text": "The learner processes input data with the goal of coming\nup with some summary, or compressed version of that data Clustering\na data set into subsets of similar objets is a typical example of such a\ntask There is also an intermediate learning setting in which, while the\ntraining examples contain more information than the test examples, the\nlearner is required to predict even more information for the test exam-\nples For example, one may try to learn a value function that describes for\neach setting of a chess board the degree by which White’s position is bet-\nter than the Black’s Yet, the only information available to the learner at\ntraining time is positions that occurred throughout actual chess games,\nlabeled by who eventually won that game Such learning frameworks are\nmainly investigated under the title of reinforcement learning Active versus Passive Learners Learning paradigms can vary by the role\nplayed by the learner We distinguish between “active” and “passive”\nlearners An active learner interacts with the environment at training\ntime, say, by posing queries or performing experiments, while a passive\nlearner only observes the information provided by the environment (or\nthe teacher) without inﬂuencing or directing it Note that the learner of a\nspam ﬁlter is usually passive – waiting for users to mark the e-mails com-\ning to them",
      "word_count": 222,
      "source_page": 23,
      "start_position": 5105,
      "end_position": 5326,
      "sentences_count": 10
    },
    {
      "chunk_id": 25,
      "text": "24\nIntroduction\nful for achieving the learning goal In contrast, when a scientist learns\nabout nature, the environment, playing the role of the teacher, can be\nbest thought of as passive – apples drop, stars shine, and the rain falls\nwithout regard to the needs of the learner We model such learning sce-\nnarios by postulating that the training data (or the learner’s experience)\nis generated by some random process This is the basic building block in\nthe branch of “statistical learning.” Finally, learning also occurs when\nthe learner’s input is generated by an adversarial “teacher.” This may be\nthe case in the spam ﬁltering example (if the spammer makes an eﬀort\nto mislead the spam ﬁltering designer) or in learning to detect fraud One also uses an adversarial teacher model as a worst-case scenario,\nwhen no milder setup can be safely assumed If you can learn against an\nadversarial teacher, you are guaranteed to succeed interacting any odd\nteacher Online versus Batch Learning Protocol The last parameter we mention is\nthe distinction between situations in which the learner has to respond\nonline, throughout the learning process, and settings in which the learner\nhas to engage the acquired expertise only after having a chance to process\nlarge amounts of data For example, a stockbroker has to make daily\ndecisions, based on the experience collected so far He may become an\nexpert over time, but might have made costly mistakes in the process",
      "word_count": 242,
      "source_page": 24,
      "start_position": 5397,
      "end_position": 5638,
      "sentences_count": 9
    },
    {
      "chunk_id": 26,
      "text": "For example, a stockbroker has to make daily\ndecisions, based on the experience collected so far He may become an\nexpert over time, but might have made costly mistakes in the process In\ncontrast, in many data mining settings, the learner – the data miner –\nhas large amounts of training data to play with before having to output\nconclusions In this book we shall discuss only a subset of the possible learning paradigms Our main focus is on supervised statistical batch learning with a passive learner\n(for example, trying to learn how to generate patients’ prognoses, based on large\narchives of records of patients that were independently collected and are already\nlabeled by the fate of the recorded patients) We shall also brieﬂy discuss online\nlearning and batch unsupervised learning (in particular, clustering) 1.4\nRelations to Other Fields\nAs an interdisciplinary ﬁeld, machine learning shares common threads with the\nmathematical ﬁelds of statistics, information theory, game theory, and optimiza-\ntion It is naturally a subﬁeld of computer science, as our goal is to program\nmachines so that they will learn In a sense, machine learning can be viewed as\na branch of AI (Artiﬁcial Intelligence), since, after all, the ability to turn expe-\nrience into expertise or to detect meaningful patterns in complex sensory data\nis a cornerstone of human (and animal) intelligence",
      "word_count": 224,
      "source_page": 24,
      "start_position": 5607,
      "end_position": 5830,
      "sentences_count": 9
    },
    {
      "chunk_id": 27,
      "text": "1.5 How to Read This Book\n25\nspecial abilities of computers to complement human intelligence, often perform-\ning tasks that fall way beyond human capabilities For example, the ability to\nscan and process huge databases allows machine learning programs to detect\npatterns that are outside the scope of human perception The component of experience, or training, in machine learning often refers\nto data that is randomly generated The task of the learner is to process such\nrandomly generated examples toward drawing conclusions that hold for the en-\nvironment from which these examples are picked This description of machine\nlearning highlights its close relationship with statistics Indeed there is a lot in\ncommon between the two disciplines, in terms of both the goals and techniques\nused There are, however, a few signiﬁcant diﬀerences of emphasis; if a doctor\ncomes up with the hypothesis that there is a correlation between smoking and\nheart disease, it is the statistician’s role to view samples of patients and check\nthe validity of that hypothesis (this is the common statistical task of hypothe-\nsis testing) In contrast, machine learning aims to use the data gathered from\nsamples of patients to come up with a description of the causes of heart disease The hope is that automated techniques may be able to ﬁgure out meaningful\npatterns (or hypotheses) that may have been missed by the human observer",
      "word_count": 230,
      "source_page": 25,
      "start_position": 5860,
      "end_position": 6089,
      "sentences_count": 9
    },
    {
      "chunk_id": 28,
      "text": "In contrast, machine learning aims to use the data gathered from\nsamples of patients to come up with a description of the causes of heart disease The hope is that automated techniques may be able to ﬁgure out meaningful\npatterns (or hypotheses) that may have been missed by the human observer In contrast with traditional statistics, in machine learning in general, and\nin this book in particular, algorithmic considerations play a major role Ma-\nchine learning is about the execution of learning by computers; hence algorith-\nmic issues are pivotal We develop algorithms to perform the learning tasks and\nare concerned with their computational eﬃciency Another diﬀerence is that\nwhile statistics is often interested in asymptotic behavior (like the convergence\nof sample-based statistical estimates as the sample sizes grow to inﬁnity), the\ntheory of machine learning focuses on ﬁnite sample bounds Namely, given the\nsize of available samples, machine learning theory aims to ﬁgure out the degree\nof accuracy that a learner can expect on the basis of such samples There are further diﬀerences between these two disciplines, of which we shall\nmention only one more here",
      "word_count": 187,
      "source_page": 25,
      "start_position": 6039,
      "end_position": 6225,
      "sentences_count": 8
    },
    {
      "chunk_id": 29,
      "text": "Namely, given the\nsize of available samples, machine learning theory aims to ﬁgure out the degree\nof accuracy that a learner can expect on the basis of such samples There are further diﬀerences between these two disciplines, of which we shall\nmention only one more here While in statistics it is common to work under the\nassumption of certain presubscribed data models (such as assuming the normal-\nity of data-generating distributions, or the linearity of functional dependencies),\nin machine learning the emphasis is on working under a “distribution-free” set-\nting, where the learner assumes as little as possible about the nature of the\ndata distribution and allows the learning algorithm to ﬁgure out which models\nbest approximate the data-generating process A precise discussion of this issue\nrequires some technical preliminaries, and we will come back to it later in the\nbook, and in particular in Chapter 5 1.5\nHow to Read This Book\nThe ﬁrst part of the book provides the basic theoretical principles that underlie\nmachine learning (ML) In a sense, this is the foundation upon which the rest",
      "word_count": 180,
      "source_page": 25,
      "start_position": 6180,
      "end_position": 6359,
      "sentences_count": 6
    },
    {
      "chunk_id": 30,
      "text": "26\nIntroduction\nof the book is built This part could serve as a basis for a minicourse on the\ntheoretical foundations of ML The second part of the book introduces the most commonly used algorithmic\napproaches to supervised machine learning A subset of these chapters may also\nbe used for introducing machine learning in a general AI course to computer\nscience, Math, or engineering students The third part of the book extends the scope of discussion from statistical\nclassiﬁcation to other learning models It covers online learning, unsupervised\nlearning, dimensionality reduction, generative models, and feature learning The fourth part of the book, Advanced Theory, is geared toward readers who\nhave interest in research and provides the more technical mathematical tech-\nniques that serve to analyze and drive forward the ﬁeld of theoretical machine\nlearning The Appendixes provide some technical tools used in the book In particular,\nwe list basic results from measure concentration and linear algebra A few sections are marked by an asterisk, which means they are addressed to\nmore advanced students Each chapter is concluded with a list of exercises A\nsolution manual is provided in the course Web site 1.5.1\nPossible Course Plans Based on This Book\nA 14 Week Introduction Course for Graduate Students:\n1 Chapters 2–4 2 Chapter 9 (without the VC calculation) 3 Chapters 5–6 (without proofs) 4 Chapter 10 5 Chapters 7, 11 (without proofs) 6 Chapters 12, 13 (with some of the easier proofs) 7",
      "word_count": 243,
      "source_page": 26,
      "start_position": 6360,
      "end_position": 6602,
      "sentences_count": 25
    },
    {
      "chunk_id": 31,
      "text": "1.6 Notation\n27\n6 Chapter 30 7 Chapters 12, 13 8 Chapter 14 9 Chapter 8 10 Chapter 17 11 Chapter 29 12 Chapter 19 13 Chapter 20 14 Chapter 21 1.6\nNotation\nMost of the notation we use throughout the book is either standard or deﬁned\non the spot In this section we describe our main conventions and provide a\ntable summarizing our notation (Table 1.1) The reader is encouraged to skip\nthis section and return to it if during the reading of the book some notation is\nunclear We denote scalars and abstract objects with lowercase letters (e.g x and λ) Often, we would like to emphasize that some object is a vector and then we\nuse boldface letters (e.g x and λ) The ith element of a vector x is denoted\nby xi We use uppercase letters to denote matrices, sets, and sequences The\nmeaning should be clear from the context As we will see momentarily, the input\nof a learning algorithm is a sequence of training examples We denote by z an\nabstract example and by S = z1, , zm a sequence of m examples Historically,\nS is often referred to as a training set; however, we will always assume that S is\na sequence rather than a set A sequence of m vectors is denoted by x1, , xm The ith element of xt is denoted by xt,i Throughout the book, we make use of basic notions from probability",
      "word_count": 245,
      "source_page": 27,
      "start_position": 6661,
      "end_position": 6905,
      "sentences_count": 36
    },
    {
      "chunk_id": 32,
      "text": "The ith element of xt is denoted by xt,i Throughout the book, we make use of basic notions from probability We\ndenote by D a distribution over some set,2 for example, Z We use the notation\nz ∼D to denote that z is sampled according to D Given a random variable\nf : Z →R, its expected value is denoted by Ez∼D[f(z)] We sometimes use the\nshorthand E[f] when the dependence on z is clear from the context For f : Z →\n{true, false} we also use Pz∼D[f(z)] to denote D({z : f(z) = true}) In the\nnext chapter we will also introduce the notation Dm to denote the probability\nover Zm induced by sampling (z1, , zm) where each point zi is sampled from\nD independently of the other points In general, we have made an eﬀort to avoid asymptotic notation However, we\noccasionally use it to clarify the main results In particular, given f : R →R+\nand g : R →R+ we write f = O(g) if there exist x0, α ∈R+ such that for all\nx > x0 we have f(x) ≤αg(x) We write f = o(g) if for every α > 0 there exists\n2 To be mathematically precise, D should be deﬁned over some σ-algebra of subsets of Z The user who is not familiar with measure theory can skip the few footnotes and remarks\nregarding more formal measurability deﬁnitions and assumptions.",
      "word_count": 239,
      "source_page": 27,
      "start_position": 6886,
      "end_position": 7124,
      "sentences_count": 14
    },
    {
      "chunk_id": 33,
      "text": "28\nIntroduction\nTable 1.1 Summary of notation\nsymbol\nmeaning\nR\nthe set of real numbers\nRd\nthe set of d-dimensional vectors over R\nR+\nthe set of non-negative real numbers\nN\nthe set of natural numbers\nO, o, Θ, ω, Ω, ˜O\nasymptotic notation (see text)\n1[Boolean expression]\nindicator function (equals 1 if expression is true and 0 o.w.)\n[a]+\n= max{0, a}\n[n]\nthe set {1, , n} (for n ∈N)\nx, v, w\n(column) vectors\nxi, vi, wi\nthe ith element of a vector\n⟨x, v⟩\n= Pd\ni=1 xivi (inner product)\n∥x∥2 or ∥x∥\n=\np\n⟨x, x⟩(the ℓ2 norm of x)\n∥x∥1\n= Pd\ni=1 |xi| (the ℓ1 norm of x)\n∥x∥∞\n= maxi |xi| (the ℓ∞norm of x)\n∥x∥0\nthe number of nonzero elements of x\nA ∈Rd,k\na d × k matrix over R\nA⊤\nthe transpose of A\nAi,j\nthe (i, j) element of A\nx x⊤\nthe d × d matrix A s.t Ai,j = xixj (where x ∈Rd)\nx1, , xm\na sequence of m vectors\nxi,j\nthe jth element of the ith vector in the sequence\nw(1),",
      "word_count": 187,
      "source_page": 28,
      "start_position": 7125,
      "end_position": 7311,
      "sentences_count": 4
    },
    {
      "chunk_id": 34,
      "text": "Ai,j = xixj (where x ∈Rd)\nx1, , xm\na sequence of m vectors\nxi,j\nthe jth element of the ith vector in the sequence\nw(1), , w(T )\nthe values of a vector w during an iterative algorithm\nw(t)\ni\nthe ith element of the vector w(t)\nX\ninstances domain (a set)\nY\nlabels domain (a set)\nZ\nexamples domain (a set)\nH\nhypothesis class (a set)\nℓ: H × Z →R+\nloss function\nD\na distribution over some set (usually over Z or over X)\nD(A)\nthe probability of a set A ⊆Z according to D\nz ∼D\nsampling z according to D\nS = z1, , zm\na sequence of m examples\nS ∼Dm\nsampling S = z1, , zm i.i.d according to D\nP, E\nprobability and expectation of a random variable\nPz∼D[f(z)]\n= D({z : f(z) = true}) for f : Z →{true, false}\nEz∼D[f(z)]\nexpectation of the random variable f : Z →R\nN(µ, C)\nGaussian distribution with expectation µ and covariance C\nf ′(x)\nthe derivative of a function f : R →R at x\nf ′′(x)\nthe second derivative of a function f : R →R at x\n∂f(w)\n∂wi\nthe partial derivative of a function f : Rd →R at w w.r.t",
      "word_count": 211,
      "source_page": 28,
      "start_position": 7286,
      "end_position": 7496,
      "sentences_count": 6
    },
    {
      "chunk_id": 35,
      "text": ", zm i.i.d according to D\nP, E\nprobability and expectation of a random variable\nPz∼D[f(z)]\n= D({z : f(z) = true}) for f : Z →{true, false}\nEz∼D[f(z)]\nexpectation of the random variable f : Z →R\nN(µ, C)\nGaussian distribution with expectation µ and covariance C\nf ′(x)\nthe derivative of a function f : R →R at x\nf ′′(x)\nthe second derivative of a function f : R →R at x\n∂f(w)\n∂wi\nthe partial derivative of a function f : Rd →R at w w.r.t wi\n∇f(w)\nthe gradient of a function f : Rd →R at w\n∂f(w)\nthe diﬀerential set of a function f : Rd →R at w\nminx∈C f(x)\n= min{f(x) : x ∈C} (minimal value of f over C)\nmaxx∈C f(x)\n= max{f(x) : x ∈C} (maximal value of f over C)\nargminx∈C f(x)\nthe set {x ∈C : f(x) = minz∈C f(z)}\nargmaxx∈C f(x)\nthe set {x ∈C : f(x) = maxz∈C f(z)}\nlog\nthe natural logarithm",
      "word_count": 168,
      "source_page": 28,
      "start_position": 7407,
      "end_position": 7574,
      "sentences_count": 3
    },
    {
      "chunk_id": 36,
      "text": "1.6 Notation\n29\nx0 such that for all x > x0 we have f(x) ≤αg(x) We write f = Ω(g) if there\nexist x0, α ∈R+ such that for all x > x0 we have f(x) ≥αg(x) The notation\nf = ω(g) is deﬁned analogously The notation f = Θ(g) means that f = O(g)\nand g = O(f) Finally, the notation f = ˜O(g) means that there exists k ∈N\nsuch that f(x) = O(g(x) logk(g(x))) The inner product between vectors x and w is denoted by ⟨x, w⟩ Whenever we\ndo not specify the vector space we assume that it is the d-dimensional Euclidean\nspace and then ⟨x, w⟩= Pd\ni=1 xiwi The Euclidean (or ℓ2) norm of a vector w is\n∥w∥2 =\np\n⟨w, w⟩ We omit the subscript from the ℓ2 norm when it is clear from\nthe context We also use other ℓp norms, ∥w∥p = (P\ni |wi|p)1/p, and in particular\n∥w∥1 = P\ni |wi| and ∥w∥∞= maxi |wi| We use the notation minx∈C f(x) to denote the minimum value of the set\n{f(x) : x ∈C} To be mathematically more precise, we should use infx∈C f(x)\nwhenever the minimum is not achievable However, in the context of this book\nthe distinction between inﬁmum and minimum is often of little interest Hence,\nto simplify the presentation, we sometimes use the min notation even when inf\nis more adequate An analogous remark applies to max versus sup.",
      "word_count": 244,
      "source_page": 29,
      "start_position": 7575,
      "end_position": 7818,
      "sentences_count": 15
    },
    {
      "chunk_id": 37,
      "text": "2\nA Gentle Start\nLet us begin our mathematical analysis by showing how successful learning can be\nachieved in a relatively simpliﬁed setting Imagine you have just arrived in some\nsmall Paciﬁc island You soon ﬁnd out that papayas are a signiﬁcant ingredient\nin the local diet However, you have never before tasted papayas You have to\nlearn how to predict whether a papaya you see in the market is tasty or not First, you need to decide which features of a papaya your prediction should be\nbased on On the basis of your previous experience with other fruits, you decide\nto use two features: the papaya’s color, ranging from dark green, through orange\nand red to dark brown, and the papaya’s softness, ranging from rock hard to\nmushy Your input for ﬁguring out your prediction rule is a sample of papayas\nthat you have examined for color and softness and then tasted and found out\nwhether they were tasty or not Let us analyze this task as a demonstration of\nthe considerations involved in learning problems Our ﬁrst step is to describe a formal model aimed to capture such learning\ntasks 2.1\nA Formal Model – The Statistical Learning Framework\n• The learner’s input: In the basic statistical learning setting, the learner has\naccess to the following:\n– Domain set: An arbitrary set, X This is the set of objects that we\nmay wish to label",
      "word_count": 237,
      "source_page": 33,
      "start_position": 7819,
      "end_position": 8055,
      "sentences_count": 12
    },
    {
      "chunk_id": 38,
      "text": "2.1\nA Formal Model – The Statistical Learning Framework\n• The learner’s input: In the basic statistical learning setting, the learner has\naccess to the following:\n– Domain set: An arbitrary set, X This is the set of objects that we\nmay wish to label For example, in the papaya learning problem men-\ntioned before, the domain set will be the set of all papayas Usually,\nthese domain points will be represented by a vector of features (like\nthe papaya’s color and softness) We also refer to domain points as\ninstances and to X as instance space – Label set: For our current discussion, we will restrict the label set to\nbe a two-element set, usually {0, 1} or {−1, +1} Let Y denote our\nset of possible labels For our papayas example, let Y be {0, 1}, where\n1 represents being tasty and 0 stands for being not-tasty – Training data: S = ((x1, y1) (xm, ym)) is a ﬁnite sequence of pairs in\nX ×Y: that is, a sequence of labeled domain points This is the input\nthat the learner has access to (like a set of papayas that have been\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 222,
      "source_page": 33,
      "start_position": 8011,
      "end_position": 8232,
      "sentences_count": 15
    },
    {
      "chunk_id": 39,
      "text": "34\nA Gentle Start\ntasted and their color, softness, and tastiness) Such labeled examples\nare often called training examples We sometimes also refer to S as a\ntraining set.1\n• The learner’s output: The learner is requested to output a prediction rule,\nh : X →Y This function is also called a predictor, a hypothesis, or a clas-\nsiﬁer The predictor can be used to predict the label of new domain points In our papayas example, it is a rule that our learner will employ to predict\nwhether future papayas he examines in the farmers’ market are going to\nbe tasty or not We use the notation A(S) to denote the hypothesis that a\nlearning algorithm, A, returns upon receiving the training sequence S • A simple data-generation model We now explain how the training data is\ngenerated First, we assume that the instances (the papayas we encounter)\nare generated by some probability distribution (in this case, representing\nthe environment) Let us denote that probability distribution over X by\nD It is important to note that we do not assume that the learner knows\nanything about this distribution For the type of learning tasks we discuss,\nthis could be any arbitrary probability distribution As to the labels, in the\ncurrent discussion we assume that there is some “correct” labeling function,\nf : X →Y, and that yi = f(xi) for all i This assumption will be relaxed in\nthe next chapter The labeling function is unknown to the learner",
      "word_count": 249,
      "source_page": 34,
      "start_position": 8233,
      "end_position": 8481,
      "sentences_count": 15
    },
    {
      "chunk_id": 40,
      "text": "This assumption will be relaxed in\nthe next chapter The labeling function is unknown to the learner In fact,\nthis is just what the learner is trying to ﬁgure out In summary, each pair\nin the training data S is generated by ﬁrst sampling a point xi according\nto D and then labeling it by f • Measures of success: We deﬁne the error of a classiﬁer to be the probability\nthat it does not predict the correct label on a random data point generated\nby the aforementioned underlying distribution That is, the error of h is\nthe probability to draw a random instance x, according to the distribution\nD, such that h(x) does not equal f(x) Formally, given a domain subset,2 A ⊂X, the probability distribution,\nD, assigns a number, D(A), which determines how likely it is to observe a\npoint x ∈A In many cases, we refer to A as an event and express it using\na function π : X →{0, 1}, namely, A = {x ∈X : π(x) = 1} In that case,\nwe also use the notation Px∼D[π(x)] to express D(A) We deﬁne the error of a prediction rule, h : X →Y, to be\nLD,f(h)\ndef\n=\nP\nx∼D[h(x) ̸= f(x)]\ndef\n= D({x : h(x) ̸= f(x)}) (2.1)\nThat is, the error of such h is the probability of randomly choosing an\nexample x for which h(x) ̸= f(x)",
      "word_count": 236,
      "source_page": 34,
      "start_position": 8465,
      "end_position": 8700,
      "sentences_count": 11
    },
    {
      "chunk_id": 41,
      "text": "2.2 Empirical Risk Minimization\n35\ncorrect labeling function f We omit this subscript when it is clear from\nthe context L(D,f)(h) has several synonymous names such as the general-\nization error, the risk, or the true error of h, and we will use these names\ninterchangeably throughout the book We use the letter L for the error,\nsince we view this error as the loss of the learner We will later also discuss\nother possible formulations of such loss • A note about the information available to the learner The learner is\nblind to the underlying distribution D over the world and to the labeling\nfunction f In our papayas example, we have just arrived in a new island\nand we have no clue as to how papayas are distributed and how to predict\ntheir tastiness The only way the learner can interact with the environment\nis through observing the training set In the next section we describe a simple learning paradigm for the preceding\nsetup and analyze its performance 2.2\nEmpirical Risk Minimization\nAs mentioned earlier, a learning algorithm receives as input a training set S,\nsampled from an unknown distribution D and labeled by some target function\nf, and should output a predictor hS : X →Y (the subscript S emphasizes the\nfact that the output predictor depends on S) The goal of the algorithm is to\nﬁnd hS that minimizes the error with respect to the unknown D and f",
      "word_count": 243,
      "source_page": 35,
      "start_position": 8790,
      "end_position": 9032,
      "sentences_count": 11
    },
    {
      "chunk_id": 42,
      "text": "2.2\nEmpirical Risk Minimization\nAs mentioned earlier, a learning algorithm receives as input a training set S,\nsampled from an unknown distribution D and labeled by some target function\nf, and should output a predictor hS : X →Y (the subscript S emphasizes the\nfact that the output predictor depends on S) The goal of the algorithm is to\nﬁnd hS that minimizes the error with respect to the unknown D and f Since the learner does not know what D and f are, the true error is not directly\navailable to the learner A useful notion of error that can be calculated by the\nlearner is the training error – the error the classiﬁer incurs over the training\nsample:\nLS(h)\ndef\n=\n|{i ∈[m] : h(xi) ̸= yi}|\nm\n,\n(2.2)\nwhere [m] = {1, , m} The terms empirical error and empirical risk are often used interchangeably\nfor this error Since the training sample is the snapshot of the world that is available to the\nlearner, it makes sense to search for a solution that works well on that data This learning paradigm – coming up with a predictor h that minimizes LS(h) –\nis called Empirical Risk Minimization or ERM for short 2.2.1\nSomething May Go Wrong – Overﬁtting\nAlthough the ERM rule seems very natural, without being careful, this approach\nmay fail miserably To demonstrate such a failure, let us go back to the problem of learning to",
      "word_count": 242,
      "source_page": 35,
      "start_position": 8960,
      "end_position": 9201,
      "sentences_count": 10
    },
    {
      "chunk_id": 43,
      "text": "36\nA Gentle Start\npredict the taste of a papaya on the basis of its softness and color Consider a\nsample as depicted in the following:\nAssume that the probability distribution D is such that instances are distributed\nuniformly within the gray square and the labeling function, f, determines the\nlabel to be 1 if the instance is within the inner blue square, and 0 otherwise The\narea of the gray square in the picture is 2 and the area of the blue square is 1 Consider the following predictor:\nhS(x) =\n(\nyi\nif ∃i ∈[m] s.t xi = x\n0\notherwise (2.3)\nWhile this predictor might seem rather artiﬁcial, in Exercise 1 we show a natural\nrepresentation of it using polynomials Clearly, no matter what the sample is,\nLS(hS) = 0, and therefore this predictor may be chosen by an ERM algorithm (it\nis one of the empirical-minimum-cost hypotheses; no classiﬁer can have smaller\nerror) On the other hand, the true error of any classiﬁer that predicts the label\n1 only on a ﬁnite number of instances is, in this case, 1/2 Thus, LD(hS) = 1/2 We have found a predictor whose performance on the training set is excellent,\nyet its performance on the true “world” is very poor This phenomenon is called\noverﬁtting",
      "word_count": 216,
      "source_page": 36,
      "start_position": 9202,
      "end_position": 9417,
      "sentences_count": 11
    },
    {
      "chunk_id": 44,
      "text": "We have found a predictor whose performance on the training set is excellent,\nyet its performance on the true “world” is very poor This phenomenon is called\noverﬁtting Intuitively, overﬁtting occurs when our hypothesis ﬁts the training\ndata “too well” (perhaps like the everyday experience that a person who provides\na perfect detailed explanation for each of his single actions may raise suspicion) 2.3\nEmpirical Risk Minimization with Inductive Bias\nWe have just demonstrated that the ERM rule might lead to overﬁtting Rather\nthan giving up on the ERM paradigm, we will look for ways to rectify it We will\nsearch for conditions under which there is a guarantee that ERM does not overﬁt,\nnamely, conditions under which when the ERM predictor has good performance\nwith respect to the training data, it is also highly likely to perform well over the\nunderlying data distribution A common solution is to apply the ERM learning rule over a restricted search\nspace Formally, the learner should choose in advance (before seeing the data) a\nset of predictors This set is called a hypothesis class and is denoted by H Each\nh ∈H is a function mapping from X to Y For a given class H, and a training\nsample, S, the ERMH learner uses the ERM rule to choose a predictor h ∈H,",
      "word_count": 220,
      "source_page": 36,
      "start_position": 9390,
      "end_position": 9609,
      "sentences_count": 11
    },
    {
      "chunk_id": 45,
      "text": "2.3 Empirical Risk Minimization with Inductive Bias\n37\nwith the lowest possible error over S Formally,\nERMH(S) ∈argmin\nh∈H\nLS(h),\nwhere argmin stands for the set of hypotheses in H that achieve the minimum\nvalue of LS(h) over H By restricting the learner to choosing a predictor from\nH, we bias it toward a particular set of predictors Such restrictions are often\ncalled an inductive bias Since the choice of such a restriction is determined\nbefore the learner sees the training data, it should ideally be based on some\nprior knowledge about the problem to be learned For example, for the papaya\ntaste prediction problem we may choose the class H to be the set of predictors\nthat are determined by axis aligned rectangles (in the space determined by the\ncolor and softness coordinates) We will later show that ERMH over this class is\nguaranteed not to overﬁt On the other hand, the example of overﬁtting that we\nhave seen previously, demonstrates that choosing H to be a class of predictors\nthat includes all functions that assign the value 1 to a ﬁnite set of domain points\ndoes not suﬃce to guarantee that ERMH will not overﬁt A fundamental question in learning theory is, over which hypothesis classes\nERMH learning will not result in overﬁtting We will study this question later\nin the book Intuitively, choosing a more restricted hypothesis class better protects us\nagainst overﬁtting but at the same time might cause us a stronger inductive\nbias",
      "word_count": 248,
      "source_page": 37,
      "start_position": 9610,
      "end_position": 9857,
      "sentences_count": 11
    },
    {
      "chunk_id": 46,
      "text": "We will study this question later\nin the book Intuitively, choosing a more restricted hypothesis class better protects us\nagainst overﬁtting but at the same time might cause us a stronger inductive\nbias We will get back to this fundamental tradeoﬀlater 2.3.1\nFinite Hypothesis Classes\nThe simplest type of restriction on a class is imposing an upper bound on its size\n(that is, the number of predictors h in H) In this section, we show that if H is\na ﬁnite class then ERMH will not overﬁt, provided it is based on a suﬃciently\nlarge training sample (this size requirement will depend on the size of H) Limiting the learner to prediction rules within some ﬁnite hypothesis class may\nbe considered as a reasonably mild restriction For example, H can be the set of\nall predictors that can be implemented by a C++ program written in at most\n109 bits of code In our papayas example, we mentioned previously the class of\naxis aligned rectangles While this is an inﬁnite class, if we discretize the repre-\nsentation of real numbers, say, by using a 64 bits ﬂoating-point representation,\nthe hypothesis class becomes a ﬁnite class Let us now analyze the performance of the ERMH learning rule assuming that\nH is a ﬁnite class For a training sample, S, labeled according to some f : X →Y,\nlet hS denote a result of applying ERMH to S, namely,\nhS ∈argmin\nh∈H\nLS(h)",
      "word_count": 241,
      "source_page": 37,
      "start_position": 9825,
      "end_position": 10065,
      "sentences_count": 11
    },
    {
      "chunk_id": 47,
      "text": "38\nA Gentle Start\ndefinition 2.1 (The Realizability Assumption)\nThere exists h⋆∈H s.t L(D,f)(h⋆) = 0 Note that this assumption implies that with probability 1 over\nrandom samples, S, where the instances of S are sampled according to D and\nare labeled by f, we have LS(h⋆) = 0 The realizability assumption implies that for every ERM hypothesis we have\nthat3 LS(hS) = 0 However, we are interested in the true risk of hS, L(D,f)(hS),\nrather than its empirical risk Clearly, any guarantee on the error with respect to the underlying distribution,\nD, for an algorithm that has access only to a sample S should depend on the\nrelationship between D and S The common assumption in statistical machine\nlearning is that the training sample S is generated by sampling points from the\ndistribution D independently of each other Formally,\n• The i.i.d assumption: The examples in the training set are independently\nand identically distributed (i.i.d.) according to the distribution D That is,\nevery xi in S is freshly sampled according to D and then labeled according\nto the labeling function, f We denote this assumption by S ∼Dm where\nm is the size of S, and Dm denotes the probability over m-tuples induced\nby applying D to pick each element of the tuple independently of the other\nmembers of the tuple Intuitively, the training set S is a window through which the learner\ngets partial information about the distribution D over the world and the\nlabeling function, f",
      "word_count": 249,
      "source_page": 38,
      "start_position": 10084,
      "end_position": 10332,
      "sentences_count": 12
    },
    {
      "chunk_id": 48,
      "text": "We denote this assumption by S ∼Dm where\nm is the size of S, and Dm denotes the probability over m-tuples induced\nby applying D to pick each element of the tuple independently of the other\nmembers of the tuple Intuitively, the training set S is a window through which the learner\ngets partial information about the distribution D over the world and the\nlabeling function, f The larger the sample gets, the more likely it is to\nreﬂect more accurately the distribution and labeling used to generate it Since L(D,f)(hS) depends on the training set, S, and that training set is picked\nby a random process, there is randomness in the choice of the predictor hS\nand, consequently, in the risk L(D,f)(hS) Formally, we say that it is a random\nvariable It is not realistic to expect that with full certainty S will suﬃce to\ndirect the learner toward a good classiﬁer (from the point of view of D), as\nthere is always some probability that the sampled training data happens to\nbe very nonrepresentative of the underlying D If we go back to the papaya\ntasting example, there is always some (small) chance that all the papayas we\nhave happened to taste were not tasty, in spite of the fact that, say, 70% of the\npapayas in our island are tasty",
      "word_count": 223,
      "source_page": 38,
      "start_position": 10266,
      "end_position": 10488,
      "sentences_count": 7
    },
    {
      "chunk_id": 49,
      "text": "It is not realistic to expect that with full certainty S will suﬃce to\ndirect the learner toward a good classiﬁer (from the point of view of D), as\nthere is always some probability that the sampled training data happens to\nbe very nonrepresentative of the underlying D If we go back to the papaya\ntasting example, there is always some (small) chance that all the papayas we\nhave happened to taste were not tasty, in spite of the fact that, say, 70% of the\npapayas in our island are tasty In such a case, ERMH(S) may be the constant\nfunction that labels every papaya as “not tasty” (and has 70% error on the true\ndistribution of papapyas in the island) We will therefore address the probability\nto sample a training set for which L(D,f)(hS) is not too large Usually, we denote\nthe probability of getting a nonrepresentative sample by δ, and call (1 −δ) the\nconﬁdence parameter of our prediction On top of that, since we cannot guarantee perfect label prediction, we intro-\nduce another parameter for the quality of prediction, the accuracy parameter,\n3 Mathematically speaking, this holds with probability 1 To simplify the presentation, we\nsometimes omit the “with probability 1” speciﬁer.",
      "word_count": 205,
      "source_page": 38,
      "start_position": 10398,
      "end_position": 10602,
      "sentences_count": 7
    },
    {
      "chunk_id": 50,
      "text": "2.3 Empirical Risk Minimization with Inductive Bias\n39\ncommonly denoted by ϵ We interpret the event L(D,f)(hS) > ϵ as a failure of the\nlearner, while if L(D,f)(hS) ≤ϵ we view the output of the algorithm as an approx-\nimately correct predictor Therefore (ﬁxing some labeling function f : X →Y),\nwe are interested in upper bounding the probability to sample m-tuple of in-\nstances that will lead to failure of the learner Formally, let S|x = (x1, , xm)\nbe the instances of the training set We would like to upper bound\nDm({S|x : L(D,f)(hS) > ϵ}) Let HB be the set of “bad” hypotheses, that is,\nHB = {h ∈H : L(D,f)(h) > ϵ} In addition, let\nM = {S|x : ∃h ∈HB, LS(h) = 0}\nbe the set of misleading samples: Namely, for every S|x ∈M, there is a “bad”\nhypothesis, h ∈HB, that looks like a “good” hypothesis on S|x Now, recall that\nwe would like to bound the probability of the event L(D,f)(hS) > ϵ But, since\nthe realizability assumption implies that LS(hS) = 0, it follows that the event\nL(D,f)(hS) > ϵ can only happen if for some h ∈HB we have LS(h) = 0 In\nother words, this event will only happen if our sample is in the set of misleading\nsamples, M Formally, we have shown that\n{S|x : L(D,f)(hS) > ϵ} ⊆M Note that we can rewrite M as\nM =\n[\nh∈HB\n{S|x : LS(h) = 0}",
      "word_count": 247,
      "source_page": 39,
      "start_position": 10603,
      "end_position": 10849,
      "sentences_count": 13
    },
    {
      "chunk_id": 51,
      "text": "40\nA Gentle Start\nto the event ∀i, h(xi) = f(xi) Since the examples in the training set are sampled\ni.i.d we get that\nDm({S|x : LS(h) = 0}) = Dm({S|x : ∀i, h(xi) = f(xi)})\n=\nm\nY\ni=1\nD({xi : h(xi) = f(xi)}) (2.8)\nFor each individual sampling of an element of the training set we have\nD({xi : h(xi) = yi}) = 1 −L(D,f)(h) ≤1 −ϵ,\nwhere the last inequality follows from the fact that h ∈HB Combining the\nprevious equation with Equation (2.8) and using the inequality 1 −ϵ ≤e−ϵ we\nobtain that for every h ∈HB,\nDm({S|x : LS(h) = 0}) ≤(1 −ϵ)m ≤e−ϵm (2.9)\nCombining this equation with Equation (2.7) we conclude that\nDm({S|x : L(D,f)(hS) > ϵ}) ≤|HB| e−ϵm ≤|H| e−ϵ m A graphical illustration which explains how we used the union bound is given in\nFigure 2.1 Figure 2.1 Each point in the large circle represents a possible m-tuple of instances Each colored oval represents the set of “misleading” m-tuple of instances for some\n“bad” predictor h ∈HB The ERM can potentially overﬁt whenever it gets a\nmisleading training set S That is, for some h ∈HB we have LS(h) = 0 Equation (2.9) guarantees that for each individual bad hypothesis, h ∈HB, at most\n(1 −ϵ)m-fraction of the training sets would be misleading In particular, the larger m\nis, the smaller each of these colored ovals becomes",
      "word_count": 236,
      "source_page": 40,
      "start_position": 10961,
      "end_position": 11196,
      "sentences_count": 13
    },
    {
      "chunk_id": 52,
      "text": "2.4 Exercises\n41\nand let m be an integer that satisﬁes\nm ≥log(|H|/δ)\nϵ Then, for any labeling function, f, and for any distribution, D, for which the\nrealizability assumption holds (that is, for some h ∈H, L(D,f)(h) = 0), with\nprobability of at least 1 −δ over the choice of an i.i.d sample S of size m, we\nhave that for every ERM hypothesis, hS, it holds that\nL(D,f)(hS) ≤ϵ The preceeding corollary tells us that for a suﬃciently large m, the ERMH rule\nover a ﬁnite hypothesis class will be probably (with conﬁdence 1−δ) approximately\n(up to an error of ϵ) correct In the next chapter we formally deﬁne the model\nof Probably Approximately Correct (PAC) learning 2.4\nExercises\n1 Overﬁtting of polynomial matching: We have shown that the predictor\ndeﬁned in Equation (2.3) leads to overﬁtting While this predictor seems to\nbe very unnatural, the goal of this exercise is to show that it can be described\nas a thresholded polynomial That is, show that given a training set S =\n{(xi, f(xi))}m\ni=1 ⊆(Rd × {0, 1})m, there exists a polynomial pS such that\nhS(x) = 1 if and only if pS(x) ≥0, where hS is as deﬁned in Equation (2.3) It follows that learning the class of all thresholded polynomials using the ERM\nrule may lead to overﬁtting 2 Let H be a class of binary classiﬁers over a domain X",
      "word_count": 236,
      "source_page": 41,
      "start_position": 11283,
      "end_position": 11518,
      "sentences_count": 12
    },
    {
      "chunk_id": 53,
      "text": "2 Let H be a class of binary classiﬁers over a domain X Let D be an unknown\ndistribution over X, and let f be the target hypothesis in H Fix some h ∈H Show that the expected value of LS(h) over the choice of S|x equals L(D,f)(h),\nnamely,\nE\nS|x∼Dm[LS(h)] = L(D,f)(h) 3 Axis aligned rectangles: An axis aligned rectangle classiﬁer in the plane\nis a classiﬁer that assigns the value 1 to a point if and only if it is inside a\ncertain rectangle Formally, given real numbers a1 ≤b1, a2 ≤b2, deﬁne the\nclassiﬁer h(a1,b1,a2,b2) by\nh(a1,b1,a2,b2)(x1, x2) =\n(\n1\nif a1 ≤x1 ≤b1 and a2 ≤x2 ≤b2\n0\notherwise (2.10)\nThe class of all axis aligned rectangles in the plane is deﬁned as\nH2\nrec = {h(a1,b1,a2,b2) : a1 ≤b1, and a2 ≤b2} Note that this is an inﬁnite size hypothesis class Throughout this exercise we\nrely on the realizability assumption.",
      "word_count": 156,
      "source_page": 41,
      "start_position": 11506,
      "end_position": 11661,
      "sentences_count": 11
    },
    {
      "chunk_id": 54,
      "text": "42\nA Gentle Start\n1 Let A be the algorithm that returns the smallest rectangle enclosing all\npositive examples in the training set Show that A is an ERM 2 Show that if A receives a training set of size ≥4 log(4/δ)\nϵ\nthen, with proba-\nbility of at least 1 −δ it returns a hypothesis with error of at most ϵ Hint: Fix some distribution D over X, let R∗= R(a∗\n1, b∗\n1, a∗\n2, b∗\n2) be the rect-\nangle that generates the labels, and let f be the corresponding hypothesis Let a1 ≥a∗\n1 be a number such that the probability mass (with respect\nto D) of the rectangle R1 = R(a∗\n1, a1, a∗\n2, b∗\n2) is exactly ϵ/4 Similarly, let\nb1, a2, b2 be numbers such that the probability masses of the rectangles\nR2 = R(b1, b∗\n1, a∗\n2, b∗\n2), R3 = R(a∗\n1, b∗\n1, a∗\n2, a2), R4 = R(a∗\n1, b∗\n1, b2, b∗\n2) are all\nexactly ϵ/4 Let R(S) be the rectangle returned by A See illustration in\nFigure 2.2 +\n+\n+\n+\n+\n-\n-\nR∗\nR(S)\nR1\nFigure 2.2 Axis aligned rectangles • Show that R(S) ⊆R∗ • Show that if S contains (positive) examples in all of the rectangles\nR1, R2, R3, R4, then the hypothesis returned by A has error of at\nmost ϵ • For each i ∈{1,",
      "word_count": 238,
      "source_page": 42,
      "start_position": 11662,
      "end_position": 11899,
      "sentences_count": 14
    },
    {
      "chunk_id": 55,
      "text": "3\nA Formal Learning Model\nIn this chapter we deﬁne our main formal learning model – the PAC learning\nmodel and its extensions We will consider other notions of learnability in Chap-\nter 7 3.1\nPAC Learning\nIn the previous chapter we have shown that for a ﬁnite hypothesis class, if the\nERM rule with respect to that class is applied on a suﬃciently large training\nsample (whose size is independent of the underlying distribution or labeling\nfunction) then the output hypothesis will be probably approximately correct More generally, we now deﬁne Probably Approximately Correct (PAC) learning definition 3.1 (PAC Learnability)\nA hypothesis class H is PAC learnable\nif there exist a function mH : (0, 1)2 →N and a learning algorithm with the\nfollowing property: For every ϵ, δ ∈(0, 1), for every distribution D over X, and\nfor every labeling function f : X →{0, 1}, if the realizable assumption holds\nwith respect to H, D, f, then when running the learning algorithm on m ≥\nmH(ϵ, δ) i.i.d examples generated by D and labeled by f, the algorithm returns\na hypothesis h such that, with probability of at least 1 −δ (over the choice of\nthe examples), L(D,f)(h) ≤ϵ The deﬁnition of Probably Approximately Correct learnability contains two\napproximation parameters",
      "word_count": 213,
      "source_page": 43,
      "start_position": 11958,
      "end_position": 12170,
      "sentences_count": 7
    },
    {
      "chunk_id": 56,
      "text": "examples generated by D and labeled by f, the algorithm returns\na hypothesis h such that, with probability of at least 1 −δ (over the choice of\nthe examples), L(D,f)(h) ≤ϵ The deﬁnition of Probably Approximately Correct learnability contains two\napproximation parameters The accuracy parameter ϵ determines how far the\noutput classiﬁer can be from the optimal one (this corresponds to the “approx-\nimately correct”), and a conﬁdence parameter δ indicating how likely the clas-\nsiﬁer is to meet that accuracy requirement (corresponds to the “probably” part\nof “PAC”) Under the data access model that we are investigating, these ap-\nproximations are inevitable Since the training set is randomly generated, there\nmay always be a small chance that it will happen to be noninformative (for ex-\nample, there is always some chance that the training set will contain only one\ndomain point, sampled over and over again) Furthermore, even when we are\nlucky enough to get a training sample that does faithfully represent D, because\nit is just a ﬁnite sample, there may always be some ﬁne details of D that it fails\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 212,
      "source_page": 43,
      "start_position": 12129,
      "end_position": 12340,
      "sentences_count": 10
    },
    {
      "chunk_id": 57,
      "text": "44\nA Formal Learning Model\nto reﬂect Our accuracy parameter, ϵ, allows “forgiving” the learner’s classiﬁer\nfor making minor errors Sample Complexity\nThe function mH : (0, 1)2 →N determines the sample complexity of learning H:\nthat is, how many examples are required to guarantee a probably approximately\ncorrect solution The sample complexity is a function of the accuracy (ϵ) and\nconﬁdence (δ) parameters It also depends on properties of the hypothesis class\nH – for example, for a ﬁnite class we showed that the sample complexity depends\non log the size of H Note that if H is PAC learnable, there are many functions mH that satisfy the\nrequirements given in the deﬁnition of PAC learnability Therefore, to be precise,\nwe will deﬁne the sample complexity of learning H to be the “minimal function,”\nin the sense that for any ϵ, δ, mH(ϵ, δ) is the minimal integer that satisﬁes the\nrequirements of PAC learning with accuracy ϵ and conﬁdence δ Let us now recall the conclusion of the analysis of ﬁnite hypothesis classes\nfrom the previous chapter It can be rephrased as stating:\ncorollary 3.2\nEvery ﬁnite hypothesis class is PAC learnable with sample\ncomplexity\nmH(ϵ, δ) ≤\n\u0018log(|H|/δ)\nϵ\n\u0019 There are inﬁnite classes that are learnable as well (see, for example, Exer-\ncise 3) Later on we will show that what determines the PAC learnability of\na class is not its ﬁniteness but rather a combinatorial measure called the VC\ndimension",
      "word_count": 245,
      "source_page": 44,
      "start_position": 12341,
      "end_position": 12585,
      "sentences_count": 11
    },
    {
      "chunk_id": 58,
      "text": "There are inﬁnite classes that are learnable as well (see, for example, Exer-\ncise 3) Later on we will show that what determines the PAC learnability of\na class is not its ﬁniteness but rather a combinatorial measure called the VC\ndimension 3.2\nA More General Learning Model\nThe model we have just described can be readily generalized, so that it can be\nmade relevant to a wider scope of learning tasks We consider generalizations in\ntwo aspects:\nRemoving the Realizability Assumption\nWe have required that the learning algorithm succeeds on a pair of data distri-\nbution D and labeling function f provided that the realizability assumption is\nmet For practical learning tasks, this assumption may be too strong (can we\nreally guarantee that there is a rectangle in the color-hardness space that fully\ndetermines which papayas are tasty?) In the next subsection, we will describe\nthe agnostic PAC model in which this realizability assumption is waived.",
      "word_count": 157,
      "source_page": 44,
      "start_position": 12544,
      "end_position": 12700,
      "sentences_count": 6
    },
    {
      "chunk_id": 59,
      "text": "3.2 A More General Learning Model\n45\nLearning Problems beyond Binary Classiﬁcation\nThe learning task that we have been discussing so far has to do with predicting a\nbinary label to a given example (like being tasty or not) However, many learning\ntasks take a diﬀerent form For example, one may wish to predict a real valued\nnumber (say, the temperature at 9:00 p.m tomorrow) or a label picked from\na ﬁnite set of labels (like the topic of the main story in tomorrow’s paper) It\nturns out that our analysis of learning can be readily extended to such and many\nother scenarios by allowing a variety of loss functions We shall discuss that in\nSection 3.2.2 later 3.2.1\nReleasing the Realizability Assumption – Agnostic PAC Learning\nA More Realistic Model for the Data-Generating Distribution\nRecall that the realizability assumption requires that there exists h⋆∈H such\nthat Px∼D[h⋆(x) = f(x)] = 1 In many practical problems this assumption does\nnot hold Furthermore, it is maybe more realistic not to assume that the labels\nare fully determined by the features we measure on input elements (in the case of\nthe papayas, it is plausible that two papayas of the same color and softness will\nhave diﬀerent taste) In the following, we relax the realizability assumption by\nreplacing the “target labeling function” with a more ﬂexible notion, a data-labels\ngenerating distribution",
      "word_count": 229,
      "source_page": 45,
      "start_position": 12701,
      "end_position": 12929,
      "sentences_count": 10
    },
    {
      "chunk_id": 60,
      "text": "Furthermore, it is maybe more realistic not to assume that the labels\nare fully determined by the features we measure on input elements (in the case of\nthe papayas, it is plausible that two papayas of the same color and softness will\nhave diﬀerent taste) In the following, we relax the realizability assumption by\nreplacing the “target labeling function” with a more ﬂexible notion, a data-labels\ngenerating distribution Formally, from now on, let D be a probability distribution over X × Y, where,\nas before, X is our domain set and Y is a set of labels (usually we will consider\nY = {0, 1}) That is, D is a joint distribution over domain points and labels One\ncan view such a distribution as being composed of two parts: a distribution Dx\nover unlabeled domain points (sometimes called the marginal distribution) and\na conditional probability over labels for each domain point, D((x, y)|x) In the\npapaya example, Dx determines the probability of encountering a papaya whose\ncolor and hardness fall in some color-hardness values domain, and the conditional\nprobability is the probability that a papaya with color and hardness represented\nby x is tasty Indeed, such modeling allows for two papayas that share the same\ncolor and hardness to belong to diﬀerent taste categories The empirical and the True Error Revised\nFor a probability distribution, D, over X × Y, one can measure how likely h is\nto make an error when labeled points are randomly drawn according to D",
      "word_count": 250,
      "source_page": 45,
      "start_position": 12862,
      "end_position": 13111,
      "sentences_count": 8
    },
    {
      "chunk_id": 61,
      "text": "46\nA Formal Learning Model\nremains the same as before, namely,\nLS(h)\ndef\n=\n|{i ∈[m] : h(xi) ̸= yi}|\nm Given S, a learner can compute LS(h) for any function h : X →{0, 1} Note\nthat LS(h) = LD(uniform over S)(h) The Goal\nWe wish to ﬁnd some hypothesis, h : X →Y, that (probably approximately)\nminimizes the true risk, LD(h) The Bayes Optimal Predictor Given any probability distribution D over X × {0, 1}, the best label predicting\nfunction from X to {0, 1} will be\nfD(x) =\n(\n1\nif P[y = 1|x] ≥1/2\n0\notherwise\nIt is easy to verify (see Exercise 7) that for every probability distribution D,\nthe Bayes optimal predictor fD is optimal, in the sense that no other classiﬁer,\ng : X →{0, 1} has a lower error That is, for every classiﬁer g, LD(fD) ≤LD(g) Unfortunately, since we do not know D, we cannot utilize this optimal predictor\nfD What the learner does have access to is the training sample We can now\npresent the formal deﬁnition of agnostic PAC learnability, which is a natural\nextension of the deﬁnition of PAC learnability to the more realistic, nonrealizable,\nlearning setup we have just discussed Clearly, we cannot hope that the learning algorithm will ﬁnd a hypothesis\nwhose error is smaller than the minimal possible error, that of the Bayes predic-\ntor",
      "word_count": 230,
      "source_page": 46,
      "start_position": 13185,
      "end_position": 13414,
      "sentences_count": 11
    },
    {
      "chunk_id": 62,
      "text": "We can now\npresent the formal deﬁnition of agnostic PAC learnability, which is a natural\nextension of the deﬁnition of PAC learnability to the more realistic, nonrealizable,\nlearning setup we have just discussed Clearly, we cannot hope that the learning algorithm will ﬁnd a hypothesis\nwhose error is smaller than the minimal possible error, that of the Bayes predic-\ntor Furthermore, as we shall prove later, once we make no prior assumptions\nabout the data-generating distribution, no algorithm can be guaranteed to ﬁnd\na predictor that is as good as the Bayes optimal one Instead, we require that\nthe learning algorithm will ﬁnd a predictor whose error is not much larger than\nthe best possible error of a predictor in some given benchmark hypothesis class Of course, the strength of such a requirement depends on the choice of that\nhypothesis class definition 3.3 (Agnostic PAC Learnability)\nA hypothesis class H is agnostic\nPAC learnable if there exist a function mH : (0, 1)2 →N and a learning algorithm\nwith the following property: For every ϵ, δ ∈(0, 1) and for every distribution D\nover X ×Y, when running the learning algorithm on m ≥mH(ϵ, δ) i.i.d examples\ngenerated by D, the algorithm returns a hypothesis h such that, with probability\nof at least 1 −δ (over the choice of the m training examples),\nLD(h) ≤min\nh′∈H LD(h′) + ϵ.",
      "word_count": 229,
      "source_page": 46,
      "start_position": 13355,
      "end_position": 13583,
      "sentences_count": 7
    },
    {
      "chunk_id": 63,
      "text": "3.2 A More General Learning Model\n47\nClearly, if the realizability assumption holds, agnostic PAC learning provides\nthe same guarantee as PAC learning In that sense, agnostic PAC learning gener-\nalizes the deﬁnition of PAC learning When the realizability assumption does not\nhold, no learner can guarantee an arbitrarily small error Nevertheless, under the\ndeﬁnition of agnostic PAC learning, a learner can still declare success if its error\nis not much larger than the best error achievable by a predictor from the class H This is in contrast to PAC learning, in which the learner is required to achieve\na small error in absolute terms and not relative to the best error achievable by\nthe hypothesis class 3.2.2\nThe Scope of Learning Problems Modeled\nWe next extend our model so that it can be applied to a wide variety of learning\ntasks Let us consider some examples of diﬀerent learning tasks •\nMulticlass Classiﬁcation Our classiﬁcation does not have to be binary Take, for example, the task of document classiﬁcation: We wish to design a\nprogram that will be able to classify given documents according to topics\n(e.g., news, sports, biology, medicine) A learning algorithm for such a task\nwill have access to examples of correctly classiﬁed documents and, on the\nbasis of these examples, should output a program that can take as input a\nnew document and output a topic classiﬁcation for that document Here,\nthe domain set is the set of all potential documents",
      "word_count": 246,
      "source_page": 47,
      "start_position": 13584,
      "end_position": 13829,
      "sentences_count": 11
    },
    {
      "chunk_id": 64,
      "text": "A learning algorithm for such a task\nwill have access to examples of correctly classiﬁed documents and, on the\nbasis of these examples, should output a program that can take as input a\nnew document and output a topic classiﬁcation for that document Here,\nthe domain set is the set of all potential documents Once again, we would\nusually represent documents by a set of features that could include counts\nof diﬀerent key words in the document, as well as other possibly relevant\nfeatures like the size of the document or its origin The label set in this task\nwill be the set of possible document topics (so Y will be some large ﬁnite\nset) Once we determine our domain and label sets, the other components\nof our framework look exactly the same as in the papaya tasting example;\nOur training sample will be a ﬁnite sequence of (feature vector, label) pairs,\nthe learner’s output will be a function from the domain set to the label set,\nand, ﬁnally, for our measure of success, we can use the probability, over\n(document, topic) pairs, of the event that our predictor suggests a wrong\nlabel • Regression In this task, one wishes to ﬁnd some simple pattern in the data –\na functional relationship between the X and Y components of the data",
      "word_count": 221,
      "source_page": 47,
      "start_position": 13776,
      "end_position": 13996,
      "sentences_count": 6
    },
    {
      "chunk_id": 65,
      "text": "Once we determine our domain and label sets, the other components\nof our framework look exactly the same as in the papaya tasting example;\nOur training sample will be a ﬁnite sequence of (feature vector, label) pairs,\nthe learner’s output will be a function from the domain set to the label set,\nand, ﬁnally, for our measure of success, we can use the probability, over\n(document, topic) pairs, of the event that our predictor suggests a wrong\nlabel • Regression In this task, one wishes to ﬁnd some simple pattern in the data –\na functional relationship between the X and Y components of the data For\nexample, one wishes to ﬁnd a linear function that best predicts a baby’s\nbirth weight on the basis of ultrasound measures of his head circumference,\nabdominal circumference, and femur length Here, our domain set X is some\nsubset of R3 (the three ultrasound measurements), and the set of “labels,”\nY, is the the set of real numbers (the weight in grams) In this context,\nit is more adequate to call Y the target set Our training data as well as\nthe learner’s output are as before (a ﬁnite sequence of (x, y) pairs, and\na function from X to Y respectively) However, our measure of success is",
      "word_count": 214,
      "source_page": 47,
      "start_position": 13891,
      "end_position": 14104,
      "sentences_count": 7
    },
    {
      "chunk_id": 66,
      "text": "48\nA Formal Learning Model\ndiﬀerent We may evaluate the quality of a hypothesis function, h : X →Y,\nby the expected square diﬀerence between the true labels and their predicted\nvalues, namely,\nLD(h)\ndef\n=\nE\n(x,y)∼D(h(x) −y)2 (3.2)\nTo accommodate a wide range of learning tasks we generalize our formalism\nof the measure of success as follows:\nGeneralized Loss Functions\nGiven any set H (that plays the role of our hypotheses, or models) and some\ndomain Z let ℓbe any function from H×Z to the set of nonnegative real numbers,\nℓ: H × Z →R+ We call such functions loss functions Note that for prediction problems, we have that Z = X × Y However, our\nnotion of the loss function is generalized beyond prediction tasks, and therefore\nit allows Z to be any domain of examples (for instance, in unsupervised learning\ntasks such as the one described in Chapter 22, Z is not a product of an instance\ndomain and a label domain) We now deﬁne the risk function to be the expected loss of a classiﬁer, h ∈H,\nwith respect to a probability distribution D over Z, namely,\nLD(h)\ndef\n=\nE\nz∼D[ℓ(h, z)] (3.3)\nThat is, we consider the expectation of the loss of h over objects z picked ran-\ndomly according to D Similarly, we deﬁne the empirical risk to be the expected\nloss over a given sample S = (z1,",
      "word_count": 237,
      "source_page": 48,
      "start_position": 14105,
      "end_position": 14341,
      "sentences_count": 9
    },
    {
      "chunk_id": 67,
      "text": "(3.3)\nThat is, we consider the expectation of the loss of h over objects z picked ran-\ndomly according to D Similarly, we deﬁne the empirical risk to be the expected\nloss over a given sample S = (z1, , zm) ∈Zm, namely,\nLS(h)\ndef\n=\n1\nm\nm\nX\ni=1\nℓ(h, zi) (3.4)\nThe loss functions used in the preceding examples of classiﬁcation and regres-\nsion tasks are as follows:\n• 0–1 loss: Here, our random variable z ranges over the set of pairs X × Y and\nthe loss function is\nℓ0−1(h, (x, y))\ndef\n=\n(\n0\nif h(x) = y\n1\nif h(x) ̸= y\nThis loss function is used in binary or multiclass classiﬁcation problems One should note that, for a random variable, α, taking the values {0, 1},\nEα∼D[α] = Pα∼D[α = 1] Consequently, for this loss function, the deﬁni-\ntions of LD(h) given in Equation (3.3) and Equation (3.1) coincide • Square Loss: Here, our random variable z ranges over the set of pairs X ×Y\nand the loss function is\nℓsq(h, (x, y))\ndef\n= (h(x) −y)2.",
      "word_count": 185,
      "source_page": 48,
      "start_position": 14303,
      "end_position": 14487,
      "sentences_count": 7
    },
    {
      "chunk_id": 68,
      "text": "3.3 Summary\n49\nThis loss function is used in regression problems We will later see more examples of useful instantiations of loss functions To summarize, we formally deﬁne agnostic PAC learnability for general loss\nfunctions definition 3.4 (Agnostic PAC Learnability for General Loss Functions)\nA\nhypothesis class H is agnostic PAC learnable with respect to a set Z and a\nloss function ℓ: H × Z →R+, if there exist a function mH : (0, 1)2 →N\nand a learning algorithm with the following property: For every ϵ, δ ∈(0, 1)\nand for every distribution D over Z, when running the learning algorithm on\nm ≥mH(ϵ, δ) i.i.d examples generated by D, the algorithm returns h ∈H\nsuch that, with probability of at least 1 −δ (over the choice of the m training\nexamples),\nLD(h) ≤min\nh′∈H LD(h′) + ϵ,\nwhere LD(h) = Ez∼D[ℓ(h, z)] Remark 3.1 (A Note About Measurability*)\nIn the aforementioned deﬁnition,\nfor every h ∈H, we view the function ℓ(h, ·) : Z →R+ as a random variable and\ndeﬁne LD(h) to be the expected value of this random variable For that, we need\nto require that the function ℓ(h, ·) is measurable Formally, we assume that there\nis a σ-algebra of subsets of Z, over which the probability D is deﬁned, and that\nthe preimage of every initial segment in R+ is in this σ-algebra",
      "word_count": 230,
      "source_page": 49,
      "start_position": 14488,
      "end_position": 14717,
      "sentences_count": 8
    },
    {
      "chunk_id": 69,
      "text": "For that, we need\nto require that the function ℓ(h, ·) is measurable Formally, we assume that there\nis a σ-algebra of subsets of Z, over which the probability D is deﬁned, and that\nthe preimage of every initial segment in R+ is in this σ-algebra In the speciﬁc\ncase of binary classiﬁcation with the 0−1 loss, the σ-algebra is over X × {0, 1}\nand our assumption on ℓis equivalent to the assumption that for every h, the\nset {(x, h(x)) : x ∈X} is in the σ-algebra Remark 3.2 (Proper versus Representation-Independent Learning*)\nIn the pre-\nceding deﬁnition, we required that the algorithm will return a hypothesis from\nH In some situations, H is a subset of a set H′, and the loss function can be\nnaturally extended to be a function from H′ × Z to the reals In this case, we\nmay allow the algorithm to return a hypothesis h′ ∈H′, as long as it satisﬁes\nthe requirement LD(h′) ≤minh∈H LD(h) + ϵ Allowing the algorithm to output\na hypothesis from H′ is called representation independent learning, while proper\nlearning occurs when the algorithm must output a hypothesis from H Represen-\ntation independent learning is sometimes called “improper learning,” although\nthere is nothing improper in representation independent learning 3.3\nSummary\nIn this chapter we deﬁned our main formal learning model – PAC learning The\nbasic model relies on the realizability assumption, while the agnostic variant does",
      "word_count": 240,
      "source_page": 49,
      "start_position": 14672,
      "end_position": 14911,
      "sentences_count": 10
    },
    {
      "chunk_id": 70,
      "text": "50\nA Formal Learning Model\nnot impose any restrictions on the underlying distribution over the examples We\nalso generalized the PAC model to arbitrary loss functions We will sometimes\nrefer to the most general model simply as PAC learning, omitting the “agnostic”\npreﬁx and letting the reader infer what the underlying loss function is from the\ncontext When we would like to emphasize that we are dealing with the original\nPAC setting we mention that the realizability assumption holds In Chapter 7\nwe will discuss other notions of learnability 3.4\nBibliographic Remarks\nOur most general deﬁnition of agnostic PAC learning with general loss func-\ntions follows the works of Vladimir Vapnik and Alexey Chervonenkis (Vapnik &\nChervonenkis 1971) In particular, we follow Vapnik’s general setting of learning\n(Vapnik 1982, Vapnik 1992, Vapnik 1995, Vapnik 1998) PAC learning was introduced by Valiant (1984) Valiant was named the winner\nof the 2010 Turing Award for the introduction of the PAC model Valiant’s\ndeﬁnition requires that the sample complexity will be polynomial in 1/ϵ and\nin 1/δ, as well as in the representation size of hypotheses in the class (see also\nKearns & Vazirani (1994)) As we will see in Chapter 6, if a problem is at all PAC\nlearnable then the sample complexity depends polynomially on 1/ϵ and log(1/δ) Valiant’s deﬁnition also requires that the runtime of the learning algorithm will\nbe polynomial in these quantities",
      "word_count": 234,
      "source_page": 50,
      "start_position": 14912,
      "end_position": 15145,
      "sentences_count": 12
    },
    {
      "chunk_id": 71,
      "text": "As we will see in Chapter 6, if a problem is at all PAC\nlearnable then the sample complexity depends polynomially on 1/ϵ and log(1/δ) Valiant’s deﬁnition also requires that the runtime of the learning algorithm will\nbe polynomial in these quantities In contrast, we chose to distinguish between\nthe statistical aspect of learning and the computational aspect of learning We\nwill elaborate on the computational aspect later on in Chapter 8, where we\nintroduce the full PAC learning model of Valiant For expository reasons, we\nuse the term PAC learning even when we ignore the runtime aspect of learning Finally, the formalization of agnostic PAC learning is due to Haussler (1992) 3.5\nExercises\n1 Monotonicity of Sample Complexity: Let H be a hypothesis class for a\nbinary classiﬁcation task Suppose that H is PAC learnable and its sample\ncomplexity is given by mH(·, ·) Show that mH is monotonically nonincreasing\nin each of its parameters That is, show that given δ ∈(0, 1), and given 0 <\nϵ1 ≤ϵ2 < 1, we have that mH(ϵ1, δ) ≥mH(ϵ2, δ) Similarly, show that given\nϵ ∈(0, 1), and given 0 < δ1 ≤δ2 < 1, we have that mH(ϵ, δ1) ≥mH(ϵ, δ2) 2 Let X be a discrete domain, and let HSingleton = {hz : z ∈X} ∪{h−}, where\nfor each z ∈X, hz is the function deﬁned by hz(x) = 1 if x = z and hz(x) = 0\nif x ̸= z",
      "word_count": 243,
      "source_page": 50,
      "start_position": 15104,
      "end_position": 15346,
      "sentences_count": 14
    },
    {
      "chunk_id": 72,
      "text": "3.5 Exercises\n51\n1 Describe an algorithm that implements the ERM rule for learning HSingleton\nin the realizable setup 2 Show that HSingleton is PAC learnable Provide an upper bound on the\nsample complexity 3 Let X = R2, Y = {0, 1}, and let H be the class of concentric circles in the\nplane, that is, H = {hr : r ∈R+}, where hr(x) = 1[∥x∥≤r] Prove that H is\nPAC learnable (assume realizability), and its sample complexity is bounded\nby\nmH(ϵ, δ) ≤\n\u0018log(1/δ)\nϵ\n\u0019 4 In this question, we study the hypothesis class of Boolean conjunctions deﬁned\nas follows The instance space is X = {0, 1}d and the label set is Y = {0, 1} A\nliteral over the variables x1, , xd is a simple Boolean function that takes the\nform f(x) = xi, for some i ∈[d], or f(x) = 1−xi for some i ∈[d] We use the\nnotation ¯xi as a shorthand for 1−xi A conjunction is any product of literals In Boolean logic, the product is denoted using the ∧sign For example, the\nfunction h(x) = x1 · (1 −x2) is written as x1 ∧¯x2 We consider the hypothesis class of all conjunctions of literals over the d\nvariables The empty conjunction is interpreted as the all-positive hypothesis\n(namely, the function that returns h(x) = 1 for all x)",
      "word_count": 228,
      "source_page": 51,
      "start_position": 15378,
      "end_position": 15605,
      "sentences_count": 19
    },
    {
      "chunk_id": 73,
      "text": "We consider the hypothesis class of all conjunctions of literals over the d\nvariables The empty conjunction is interpreted as the all-positive hypothesis\n(namely, the function that returns h(x) = 1 for all x) The conjunction x1∧¯x1\n(and similarly any conjunction involving a literal and its negation) is allowed\nand interpreted as the all-negative hypothesis (namely, the conjunction that\nreturns h(x) = 0 for all x) We assume realizability: Namely, we assume\nthat there exists a Boolean conjunction that generates the labels Thus, each\nexample (x, y) ∈X × Y consists of an assignment to the d Boolean variables\nx1, , xd, and its truth value (0 for false and 1 for true) For instance, let d = 3 and suppose that the true conjunction is x1 ∧¯x2 Then, the training set S might contain the following instances:\n((1, 1, 1), 0), ((1, 0, 1), 1), ((0, 1, 0), 0)((1, 0, 0), 1) Prove that the hypothesis class of all conjunctions over d variables is\nPAC learnable and bound its sample complexity Propose an algorithm that\nimplements the ERM rule, whose runtime is polynomial in d · m 5 Let X be a domain and let D1, D2, , Dm be a sequence of distributions\nover X Let H be a ﬁnite class of binary classiﬁers over X and let f ∈H",
      "word_count": 222,
      "source_page": 51,
      "start_position": 15572,
      "end_position": 15793,
      "sentences_count": 14
    },
    {
      "chunk_id": 74,
      "text": "52\nA Formal Learning Model\nHint: Use the geometric-arithmetic mean inequality 6 Let H be a hypothesis class of binary classiﬁers Show that if H is agnostic\nPAC learnable, then H is PAC learnable as well Furthermore, if A is a suc-\ncessful agnostic PAC learner for H, then A is also a successful PAC learner\nfor H 7 (*) The Bayes optimal predictor: Show that for every probability distri-\nbution D, the Bayes optimal predictor fD is optimal, in the sense that for\nevery classiﬁer g from X to {0, 1}, LD(fD) ≤LD(g) 8 (*) We say that a learning algorithm A is better than B with respect to some\nprobability distribution, D, if\nLD(A(S)) ≤LD(B(S))\nfor all samples S ∈(X ×{0, 1})m We say that a learning algorithm A is better\nthan B, if it is better than B with respect to all probability distributions D\nover X × {0, 1} 1 A probabilistic label predictor is a function that assigns to every domain\npoint x a probability value, h(x) ∈[0, 1], that determines the probability of\npredicting the label 1 That is, given such an h and an input, x, the label for\nx is predicted by tossing a coin with bias h(x) toward Heads and predicting\n1 iﬀthe coin comes up Heads Formally, we deﬁne a probabilistic label\npredictor as a function, h : X →[0, 1]",
      "word_count": 231,
      "source_page": 52,
      "start_position": 15871,
      "end_position": 16101,
      "sentences_count": 14
    },
    {
      "chunk_id": 75,
      "text": "That is, given such an h and an input, x, the label for\nx is predicted by tossing a coin with bias h(x) toward Heads and predicting\n1 iﬀthe coin comes up Heads Formally, we deﬁne a probabilistic label\npredictor as a function, h : X →[0, 1] The loss of such h on an example\n(x, y) is deﬁned to be |h(x) −y|, which is exactly the probability that the\nprediction of h will not be equal to y Note that if h is deterministic, that\nis, returns values in {0, 1}, then |h(x) −y| = 1[h(x)̸=y] Prove that for every data-generating distribution D over X × {0, 1}, the\nBayes optimal predictor has the smallest risk (w.r.t the loss function\nℓ(h, (x, y)) = |h(x)−y|, among all possible label predictors, including prob-\nabilistic ones) 2 Let X be a domain and {0, 1} be a set of labels Prove that for every\ndistribution D over X × {0, 1}, there exist a learning algorithm AD that is\nbetter than any other learning algorithm with respect to D 3 Prove that for every learning algorithm A there exist a probability distri-\nbution, D, and a learning algorithm B such that A is not better than B\nw.r.t D 9 Consider a variant of the PAC model in which there are two example ora-\ncles: one that generates positive examples and one that generates negative\nexamples, both according to the underlying distribution D on X",
      "word_count": 245,
      "source_page": 52,
      "start_position": 16054,
      "end_position": 16298,
      "sentences_count": 14
    },
    {
      "chunk_id": 76,
      "text": "4\nLearning via Uniform Convergence\nThe ﬁrst formal learning model that we have discussed was the PAC model In Chapter 2 we have shown that under the realizability assumption, any ﬁnite\nhypothesis class is PAC learnable In this chapter we will develop a general tool,\nuniform convergence, and apply it to show that any ﬁnite class is learnable in\nthe agnostic PAC model with general loss functions, as long as the range loss\nfunction is bounded 4.1\nUniform Convergence Is Suﬃcient for Learnability\nThe idea behind the learning condition discussed in this chapter is very simple Recall that, given a hypothesis class, H, the ERM learning paradigm works\nas follows: Upon receiving a training sample, S, the learner evaluates the risk\n(or error) of each h in H on the given sample and outputs a member of H that\nminimizes this empirical risk The hope is that an h that minimizes the empirical\nrisk with respect to S is a risk minimizer (or has risk close to the minimum) with\nrespect to the true data probability distribution as well For that, it suﬃces to\nensure that the empirical risks of all members of H are good approximations of\ntheir true risk Put another way, we need that uniformly over all hypotheses in\nthe hypothesis class, the empirical risk will be close to the true risk, as formalized\nin the following definition 4.1 (ϵ-representative sample)\nA training set S is called ϵ-representative\n(w.r.t",
      "word_count": 242,
      "source_page": 54,
      "start_position": 16491,
      "end_position": 16732,
      "sentences_count": 9
    },
    {
      "chunk_id": 77,
      "text": "Put another way, we need that uniformly over all hypotheses in\nthe hypothesis class, the empirical risk will be close to the true risk, as formalized\nin the following definition 4.1 (ϵ-representative sample)\nA training set S is called ϵ-representative\n(w.r.t domain Z, hypothesis class H, loss function ℓ, and distribution D) if\n∀h ∈H,\n|LS(h) −LD(h)| ≤ϵ The next simple lemma states that whenever the sample is (ϵ/2)-representative,\nthe ERM learning rule is guaranteed to return a good hypothesis lemma 4.2\nAssume that a training set S is\nϵ\n2-representative (w.r.t domain\nZ, hypothesis class H, loss function ℓ, and distribution D) Then, any output of\nERMH(S), namely, any hS ∈argminh∈H LS(h), satisﬁes\nLD(hS) ≤\nmin\nh∈H LD(h) + ϵ Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 150,
      "source_page": 54,
      "start_position": 16692,
      "end_position": 16841,
      "sentences_count": 12
    },
    {
      "chunk_id": 78,
      "text": "4.2 Finite Classes Are Agnostic PAC Learnable\n55\nProof\nFor every h ∈H,\nLD(hS) ≤LS(hS) + ϵ\n2 ≤LS(h) + ϵ\n2 ≤LD(h) + ϵ\n2 + ϵ\n2 = LD(h) + ϵ,\nwhere the ﬁrst and third inequalities are due to the assumption that S is\nϵ\n2-\nrepresentative (Deﬁnition 4.1) and the second inequality holds since hS is an\nERM predictor The preceding lemma implies that to ensure that the ERM rule is an agnostic\nPAC learner, it suﬃces to show that with probability of at least 1 −δ over the\nrandom choice of a training set, it will be an ϵ-representative training set The\nuniform convergence condition formalizes this requirement definition 4.3 (Uniform Convergence)\nWe say that a hypothesis class H has\nthe uniform convergence property (w.r.t a domain Z and a loss function ℓ) if\nthere exists a function mUC\nH : (0, 1)2 →N such that for every ϵ, δ ∈(0, 1) and\nfor every probability distribution D over Z, if S is a sample of m ≥mUC\nH (ϵ, δ)\nexamples drawn i.i.d according to D, then, with probability of at least 1 −δ, S\nis ϵ-representative Similar to the deﬁnition of sample complexity for PAC learning, the function\nmUC\nH measures the (minimal) sample complexity of obtaining the uniform con-\nvergence property, namely, how many examples we need to ensure that with\nprobability of at least 1 −δ the sample would be ϵ-representative",
      "word_count": 240,
      "source_page": 55,
      "start_position": 16842,
      "end_position": 17081,
      "sentences_count": 7
    },
    {
      "chunk_id": 79,
      "text": "according to D, then, with probability of at least 1 −δ, S\nis ϵ-representative Similar to the deﬁnition of sample complexity for PAC learning, the function\nmUC\nH measures the (minimal) sample complexity of obtaining the uniform con-\nvergence property, namely, how many examples we need to ensure that with\nprobability of at least 1 −δ the sample would be ϵ-representative The term uniform here refers to having a ﬁxed sample size that works for all\nmembers of H and over all possible probability distributions over the domain The following corollary follows directly from Lemma 4.2 and the deﬁnition of\nuniform convergence corollary 4.4\nIf a class H has the uniform convergence property with a\nfunction mUC\nH then the class is agnostically PAC learnable with the sample com-\nplexity mH(ϵ, δ) ≤mUC\nH (ϵ/2, δ) Furthermore, in that case, the ERMH paradigm\nis a successful agnostic PAC learner for H 4.2\nFinite Classes Are Agnostic PAC Learnable\nIn view of Corollary 4.4, the claim that every ﬁnite hypothesis class is agnostic\nPAC learnable will follow once we establish that uniform convergence holds for\na ﬁnite hypothesis class To show that uniform convergence holds we follow a two step argument, similar\nto the derivation in Chapter 2 The ﬁrst step applies the union bound while the\nsecond step employs a measure concentration inequality We now explain these\ntwo steps in detail Fix some ϵ, δ",
      "word_count": 235,
      "source_page": 55,
      "start_position": 17021,
      "end_position": 17255,
      "sentences_count": 11
    },
    {
      "chunk_id": 80,
      "text": "56\nLearning via Uniform Convergence\ni.i.d from D we have that for all h ∈H, |LS(h) −LD(h)| ≤ϵ That is,\nDm({S : ∀h ∈H, |LS(h) −LD(h)| ≤ϵ}) ≥1 −δ Equivalently, we need to show that\nDm({S : ∃h ∈H, |LS(h) −LD(h)| > ϵ}) < δ Writing\n{S : ∃h ∈H, |LS(h) −LD(h)| > ϵ} = ∪h∈H{S : |LS(h) −LD(h)| > ϵ},\nand applying the union bound (Lemma 2.2) we obtain\nDm({S : ∃h ∈H, |LS(h) −LD(h)| > ϵ}) ≤\nX\nh∈H\nDm({S : |LS(h) −LD(h)| > ϵ}) (4.1)\nOur second step will be to argue that each summand of the right-hand side\nof this inequality is small enough (for a suﬃciently large m) That is, we will\nshow that for any ﬁxed hypothesis, h, (which is chosen in advance prior to the\nsampling of the training set), the gap between the true and empirical risks,\n|LS(h) −LD(h)|, is likely to be small Recall that LD(h) = Ez∼D[ℓ(h, z)] and that LS(h) =\n1\nm\nPm\ni=1 ℓ(h, zi) Since\neach zi is sampled i.i.d from D, the expected value of the random variable\nℓ(h, zi) is LD(h) By the linearity of expectation, it follows that LD(h) is also\nthe expected value of LS(h) Hence, the quantity |LD(h)−LS(h)| is the deviation\nof the random variable LS(h) from its expectation We therefore need to show\nthat the measure of LS(h) is concentrated around its expected value",
      "word_count": 234,
      "source_page": 56,
      "start_position": 17287,
      "end_position": 17520,
      "sentences_count": 13
    },
    {
      "chunk_id": 81,
      "text": "Hence, the quantity |LD(h)−LS(h)| is the deviation\nof the random variable LS(h) from its expectation We therefore need to show\nthat the measure of LS(h) is concentrated around its expected value A basic statistical fact, the law of large numbers, states that when m goes to\ninﬁnity, empirical averages converge to their true expectation This is true for\nLS(h), since it is the empirical average of m i.i.d random variables However, since\nthe law of large numbers is only an asymptotic result, it provides no information\nabout the gap between the empirically estimated error and its true value for any\ngiven, ﬁnite, sample size Instead, we will use a measure concentration inequality due to Hoeﬀding, which\nquantiﬁes the gap between empirical averages and their expected value lemma 4.5 (Hoeﬀding’s Inequality)\nLet θ1, , θm be a sequence of i.i.d ran-\ndom variables and assume that for all i, E[θi] = µ and P[a ≤θi ≤b] = 1 Then,\nfor any ϵ > 0\nP\n\"\f\f\f\f\f\n1\nm\nm\nX\ni=1\nθi −µ\n\f\f\f\f\f > ϵ\n#\n≤2 exp\n\u0000−2 m ϵ2/(b −a)2\u0001 The proof can be found in Appendix B Getting back to our problem, let θi be the random variable ℓ(h, zi) Since h\nis ﬁxed and z1, , zm are sampled i.i.d., it follows that θ1, , θm are also i.i.d random variables Furthermore, LS(h) =\n1\nm\nPm\ni=1 θi and LD(h) = µ Let us",
      "word_count": 238,
      "source_page": 56,
      "start_position": 17490,
      "end_position": 17727,
      "sentences_count": 18
    },
    {
      "chunk_id": 82,
      "text": "4.2 Finite Classes Are Agnostic PAC Learnable\n57\nfurther assume that the range of ℓis [0, 1] and therefore θi ∈[0, 1] We therefore\nobtain that\nDm({S : |LS(h) −LD(h)| > ϵ}) = P\n\"\f\f\f\f\f\n1\nm\nm\nX\ni=1\nθi −µ\n\f\f\f\f\f > ϵ\n#\n≤2 exp\n\u0000−2 m ϵ2\u0001 (4.2)\nCombining this with Equation (4.1) yields\nDm({S : ∃h ∈H, |LS(h) −LD(h)| > ϵ}) ≤\nX\nh∈H\n2 exp\n\u0000−2 m ϵ2\u0001\n= 2 |H| exp\n\u0000−2 m ϵ2\u0001 Finally, if we choose\nm ≥log(2|H|/δ)\n2ϵ2\nthen\nDm({S : ∃h ∈H, |LS(h) −LD(h)| > ϵ}) ≤δ corollary 4.6\nLet H be a ﬁnite hypothesis class, let Z be a domain, and let\nℓ: H × Z →[0, 1] be a loss function Then, H enjoys the uniform convergence\nproperty with sample complexity\nm\nUC\nH (ϵ, δ) ≤\n\u0018log(2|H|/δ)\n2ϵ2\n\u0019 Furthermore, the class is agnostically PAC learnable using the ERM algorithm\nwith sample complexity\nmH(ϵ, δ) ≤m\nUC\nH (ϵ/2, δ) ≤\n\u00182 log(2|H|/δ)\nϵ2\n\u0019 Remark 4.1 (The “Discretization Trick”)\nWhile the preceding corollary only\napplies to ﬁnite hypothesis classes, there is a simple trick that allows us to get\na very good estimate of the practical sample complexity of inﬁnite hypothesis\nclasses Consider a hypothesis class that is parameterized by d parameters For\nexample, let X = R, Y = {±1}, and the hypothesis class, H, be all functions\nof the form hθ(x) = sign(x −θ)",
      "word_count": 240,
      "source_page": 57,
      "start_position": 17728,
      "end_position": 17967,
      "sentences_count": 10
    },
    {
      "chunk_id": 83,
      "text": "Consider a hypothesis class that is parameterized by d parameters For\nexample, let X = R, Y = {±1}, and the hypothesis class, H, be all functions\nof the form hθ(x) = sign(x −θ) That is, each hypothesis is parameterized by\none parameter, θ ∈R, and the hypothesis outputs 1 for all instances larger than\nθ and outputs −1 for instances smaller than θ This is a hypothesis class of an\ninﬁnite size However, if we are going to learn this hypothesis class in practice,\nusing a computer, we will probably maintain real numbers using ﬂoating point\nrepresentation, say, of 64 bits It follows that in practice, our hypothesis class\nis parameterized by the set of scalars that can be represented using a 64 bits\nﬂoating point number There are at most 264 such numbers; hence the actual\nsize of our hypothesis class is at most 264 More generally, if our hypothesis class\nis parameterized by d numbers, in practice we learn a hypothesis class of size at\nmost 264d Applying Corollary 4.6 we obtain that the sample complexity of such",
      "word_count": 181,
      "source_page": 57,
      "start_position": 17934,
      "end_position": 18114,
      "sentences_count": 9
    },
    {
      "chunk_id": 84,
      "text": "58\nLearning via Uniform Convergence\nclasses is bounded by 128d+2 log(2/δ)\nϵ2 This upper bound on the sample complex-\nity has the deﬁciency of being dependent on the speciﬁc representation of real\nnumbers used by our machine In Chapter 6 we will introduce a rigorous way\nto analyze the sample complexity of inﬁnite size hypothesis classes Neverthe-\nless, the discretization trick can be used to get a rough estimate of the sample\ncomplexity in many practical situations 4.3\nSummary\nIf the uniform convergence property holds for a hypothesis class H then in most\ncases the empirical risks of hypotheses in H will faithfully represent their true\nrisks Uniform convergence suﬃces for agnostic PAC learnability using the ERM\nrule We have shown that ﬁnite hypothesis classes enjoy the uniform convergence\nproperty and are hence agnostic PAC learnable 4.4\nBibliographic Remarks\nClasses of functions for which the uniform convergence property holds are also\ncalled Glivenko-Cantelli classes, named after Valery Ivanovich Glivenko and\nFrancesco Paolo Cantelli, who proved the ﬁrst uniform convergence result in\nthe 1930s See (Dudley, Gine & Zinn 1991) The relation between uniform con-\nvergence and learnability was thoroughly studied by Vapnik – see (Vapnik 1992,\nVapnik 1995, Vapnik 1998) In fact, as we will see later in Chapter 6, the funda-\nmental theorem of learning theory states that in binary classiﬁcation problems,\nuniform convergence is not only a suﬃcient condition for learnability but is also\na necessary condition",
      "word_count": 239,
      "source_page": 58,
      "start_position": 18115,
      "end_position": 18353,
      "sentences_count": 11
    },
    {
      "chunk_id": 85,
      "text": "The relation between uniform con-\nvergence and learnability was thoroughly studied by Vapnik – see (Vapnik 1992,\nVapnik 1995, Vapnik 1998) In fact, as we will see later in Chapter 6, the funda-\nmental theorem of learning theory states that in binary classiﬁcation problems,\nuniform convergence is not only a suﬃcient condition for learnability but is also\na necessary condition This is not the case for more general learning problems\n(see (Shalev-Shwartz, Shamir, Srebro & Sridharan 2010)) 4.5\nExercises\n1 In this exercise, we show that the (ϵ, δ) requirement on the convergence of\nerrors in our deﬁnitions of PAC learning, is, in fact, quite close to a sim-\npler looking requirement about averages (or expectations) Prove that the\nfollowing two statements are equivalent (for any learning algorithm A, any\nprobability distribution D, and any loss function whose range is [0, 1]):\n1 For every ϵ, δ > 0, there exists m(ϵ, δ) such that ∀m ≥m(ϵ, δ)\nP\nS∼Dm [LD(A(S)) > ϵ] < δ\n2 lim\nm→∞\nE\nS∼Dm [LD(A(S))] = 0",
      "word_count": 173,
      "source_page": 58,
      "start_position": 18294,
      "end_position": 18466,
      "sentences_count": 8
    },
    {
      "chunk_id": 86,
      "text": "5\nThe Bias-Complexity Tradeoﬀ\nIn Chapter 2 we saw that unless one is careful, the training data can mislead the\nlearner, and result in overﬁtting To overcome this problem, we restricted the\nsearch space to some hypothesis class H Such a hypothesis class can be viewed\nas reﬂecting some prior knowledge that the learner has about the task – a belief\nthat one of the members of the class H is a low-error model for the task For\nexample, in our papayas taste problem, on the basis of our previous experience\nwith other fruits, we may assume that some rectangle in the color-hardness plane\npredicts (at least approximately) the papaya’s tastiness Is such prior knowledge really necessary for the success of learning Maybe\nthere exists some kind of universal learner, that is, a learner who has no prior\nknowledge about a certain task and is ready to be challenged by any task Let\nus elaborate on this point A speciﬁc learning task is deﬁned by an unknown\ndistribution D over X × Y, where the goal of the learner is to ﬁnd a predictor\nh : X →Y, whose risk, LD(h), is small enough The question is therefore whether\nthere exist a learning algorithm A and a training set size m, such that for every\ndistribution D, if A receives m i.i.d examples from D, there is a high chance it\noutputs a predictor h that has a low risk The ﬁrst part of this chapter addresses this question formally",
      "word_count": 250,
      "source_page": 60,
      "start_position": 18531,
      "end_position": 18780,
      "sentences_count": 11
    },
    {
      "chunk_id": 87,
      "text": "examples from D, there is a high chance it\noutputs a predictor h that has a low risk The ﬁrst part of this chapter addresses this question formally The No-Free-\nLunch theorem states that no such universal learner exists To be more precise,\nthe theorem states that for binary classiﬁcation prediction tasks, for every learner\nthere exists a distribution on which it fails We say that the learner fails if, upon\nreceiving i.i.d examples from that distribution, its output hypothesis is likely\nto have a large risk, say, ≥0.3, whereas for the same distribution, there exists\nanother learner that will output a hypothesis with a small risk In other words,\nthe theorem states that no learner can succeed on all learnable tasks – every\nlearner has tasks on which it fails while other learners succeed Therefore, when approaching a particular learning problem, deﬁned by some\ndistribution D, we should have some prior knowledge on D One type of such prior\nknowledge is that D comes from some speciﬁc parametric family of distributions We will study learning under such assumptions later on in Chapter 24 Another\ntype of prior knowledge on D, which we assumed when deﬁning the PAC learning\nmodel, is that there exists h in some predeﬁned hypothesis class H, such that\nLD(h) = 0 A softer type of prior knowledge on D is assuming that minh∈H LD(h)\nis small",
      "word_count": 231,
      "source_page": 60,
      "start_position": 18753,
      "end_position": 18983,
      "sentences_count": 12
    },
    {
      "chunk_id": 88,
      "text": "5.1 The No-Free-Lunch Theorem\n61\nagnostic PAC model, in which we require that the risk of the output hypothesis\nwill not be much larger than minh∈H LD(h) In the second part of this chapter we study the beneﬁts and pitfalls of using\na hypothesis class as a means of formalizing prior knowledge We decompose\nthe error of an ERM algorithm over a class H into two components The ﬁrst\ncomponent reﬂects the quality of our prior knowledge, measured by the minimal\nrisk of a hypothesis in our hypothesis class, minh∈H LD(h) This component is\nalso called the approximation error, or the bias of the algorithm toward choosing\na hypothesis from H The second component is the error due to overﬁtting,\nwhich depends on the size or the complexity of the class H and is called the\nestimation error These two terms imply a tradeoﬀbetween choosing a more\ncomplex H (which can decrease the bias but increases the risk of overﬁtting)\nor a less complex H (which might increase the bias but decreases the potential\noverﬁtting) 5.1\nThe No-Free-Lunch Theorem\nIn this part we prove that there is no universal learner We do this by showing\nthat no learner can succeed on all learning tasks, as formalized in the following\ntheorem:\ntheorem 5.1 (No-Free-Lunch)\nLet A be any learning algorithm for the task\nof binary classiﬁcation with respect to the 0 −1 loss over a domain X Let m\nbe any number smaller than |X|/2, representing a training set size",
      "word_count": 249,
      "source_page": 61,
      "start_position": 19027,
      "end_position": 19275,
      "sentences_count": 10
    },
    {
      "chunk_id": 89,
      "text": "We do this by showing\nthat no learner can succeed on all learning tasks, as formalized in the following\ntheorem:\ntheorem 5.1 (No-Free-Lunch)\nLet A be any learning algorithm for the task\nof binary classiﬁcation with respect to the 0 −1 loss over a domain X Let m\nbe any number smaller than |X|/2, representing a training set size Then, there\nexists a distribution D over X × {0, 1} such that:\n1 There exists a function f : X →{0, 1} with LD(f) = 0 2 With probability of at least 1/7 over the choice of S ∼Dm we have that\nLD(A(S)) ≥1/8 This theorem states that for every learner, there exists a task on which it fails,\neven though that task can be successfully learned by another learner Indeed, a\ntrivial successful learner in this case would be an ERM learner with the hypoth-\nesis class H = {f}, or more generally, ERM with respect to any ﬁnite hypothesis\nclass that contains f and whose size satisﬁes the equation m ≥8 log(7|H|/6) (see\nCorollary 2.3) Proof\nLet C be a subset of X of size 2m The intuition of the proof is that\nany learning algorithm that observes only half of the instances in C has no\ninformation on what should be the labels of the rest of the instances in C Therefore, there exists a “reality,” that is, some target function f, that would\ncontradict the labels that A(S) predicts on the unobserved instances in C",
      "word_count": 249,
      "source_page": 61,
      "start_position": 19217,
      "end_position": 19465,
      "sentences_count": 11
    },
    {
      "chunk_id": 90,
      "text": "62\nThe Bias-Complexity Tradeoﬀ\nC × {0, 1} deﬁned by\nDi({(x, y)}) =\n(\n1/|C|\nif y = fi(x)\n0\notherwise That is, the probability to choose a pair (x, y) is 1/|C| if the label y is indeed\nthe true label according to fi, and the probability is 0 if y ̸= fi(x) Clearly,\nLDi(fi) = 0 We will show that for every algorithm, A, that receives a training set of m\nexamples from C ×{0, 1} and returns a function A(S) : C →{0, 1}, it holds that\nmax\ni∈[T ]\nE\nS∼Dm\ni\n[LDi(A(S))] ≥1/4 (5.1)\nClearly, this means that for every algorithm, A′, that receives a training set of m\nexamples from X ×{0, 1} there exist a function f : X →{0, 1} and a distribution\nD over X × {0, 1}, such that LD(f) = 0 and\nE\nS∼Dm[LD(A′(S))] ≥1/4 (5.2)\nIt is easy to verify that the preceding suﬃces for showing that P[LD(A′(S)) ≥\n1/8] ≥1/7, which is what we need to prove (see Exercise 1) We now turn to proving that Equation (5.1) holds There are k = (2m)m\npossible sequences of m examples from C Denote these sequences by S1, , Sk Also, if Sj = (x1, , xm) we denote by Si\nj the sequence containing the instances\nin Sj labeled by the function fi, namely, Si\nj = ((x1, fi(x1)), , (xm, fi(xm))) If\nthe distribution is Di then the possible training sets A can receive are Si\n1,",
      "word_count": 250,
      "source_page": 62,
      "start_position": 19497,
      "end_position": 19746,
      "sentences_count": 14
    },
    {
      "chunk_id": 91,
      "text": "5.1 The No-Free-Lunch Theorem\n63\nfunction h : C →{0, 1} and every i we have\nLDi(h) =\n1\n2m\nX\nx∈C\n1[h(x)̸=fi(x)]\n≥\n1\n2m\np\nX\nr=1\n1[h(vr)̸=fi(vr)]\n≥1\n2p\np\nX\nr=1\n1[h(vr)̸=fi(vr)] (5.5)\nHence,\n1\nT\nT\nX\ni=1\nLDi(A(Si\nj)) ≥\n1\nT\nT\nX\ni=1\n1\n2p\np\nX\nr=1\n1[A(Si\nj)(vr)̸=fi(vr)]\n= 1\n2p\np\nX\nr=1\n1\nT\nT\nX\ni=1\n1[A(Si\nj)(vr)̸=fi(vr)]\n≥1\n2 · min\nr∈[p]\n1\nT\nT\nX\ni=1\n1[A(Si\nj)(vr)̸=fi(vr)] (5.6)\nNext, ﬁx some r ∈[p] We can partition all the functions in f1, , fT into T/2\ndisjoint pairs, where for a pair (fi, fi′) we have that for every c ∈C, fi(c) ̸= fi′(c)\nif and only if c = vr Since for such a pair we must have Si\nj = Si′\nj , it follows that\n1[A(Si\nj)(vr)̸=fi(vr)] + 1[A(Si′\nj )(vr)̸=fi′(vr)] = 1,\nwhich yields\n1\nT\nT\nX\ni=1\n1[A(Si\nj)(vr)̸=fi(vr)] = 1\n2 Combining this with Equation (5.6), Equation (5.4), and Equation (5.3), we\nobtain that Equation (5.1) holds, which concludes our proof 5.1.1\nNo-Free-Lunch and Prior Knowledge\nHow does the No-Free-Lunch result relate to the need for prior knowledge Let us\nconsider an ERM predictor over the hypothesis class H of all the functions f from\nX to {0, 1} This class represents lack of prior knowledge: Every possible function\nfrom the domain to the label set is considered a good candidate",
      "word_count": 243,
      "source_page": 63,
      "start_position": 19874,
      "end_position": 20116,
      "sentences_count": 10
    },
    {
      "chunk_id": 92,
      "text": "64\nThe Bias-Complexity Tradeoﬀ\nProof\nAssume, by way of contradiction, that the class is learnable Choose\nsome ϵ < 1/8 and δ < 1/7 By the deﬁnition of PAC learnability, there must\nbe some learning algorithm A and an integer m = m(ϵ, δ), such that for any\ndata-generating distribution over X ×{0, 1}, if for some function f : X →{0, 1},\nLD(f) = 0, then with probability greater than 1 −δ when A is applied to\nsamples S of size m, generated i.i.d by D, LD(A(S)) ≤ϵ However, applying\nthe No-Free-Lunch theorem, since |X| > 2m, for every learning algorithm (and\nin particular for the algorithm A), there exists a distribution D such that with\nprobability greater than 1/7 > δ, LD(A(S)) > 1/8 > ϵ, which leads to the\ndesired contradiction How can we prevent such failures We can escape the hazards foreseen by the\nNo-Free-Lunch theorem by using our prior knowledge about a speciﬁc learning\ntask, to avoid the distributions that will cause us to fail when learning that task Such prior knowledge can be expressed by restricting our hypothesis class But how should we choose a good hypothesis class On the one hand, we want\nto believe that this class includes the hypothesis that has no error at all (in the\nPAC setting), or at least that the smallest error achievable by a hypothesis from\nthis class is indeed rather small (in the agnostic setting)",
      "word_count": 240,
      "source_page": 64,
      "start_position": 20186,
      "end_position": 20425,
      "sentences_count": 10
    },
    {
      "chunk_id": 93,
      "text": "But how should we choose a good hypothesis class On the one hand, we want\nto believe that this class includes the hypothesis that has no error at all (in the\nPAC setting), or at least that the smallest error achievable by a hypothesis from\nthis class is indeed rather small (in the agnostic setting) On the other hand,\nwe have just seen that we cannot simply choose the richest class – the class of\nall functions over the given domain This tradeoﬀis discussed in the following\nsection 5.2\nError Decomposition\nTo answer this question we decompose the error of an ERMH predictor into two\ncomponents as follows Let hS be an ERMH hypothesis Then, we can write\nLD(hS) = ϵapp +ϵest\nwhere : ϵapp = min\nh∈H LD(h),\nϵest = LD(hS)−ϵapp (5.7)\n• The Approximation Error – the minimum risk achievable by a predictor\nin the hypothesis class This term measures how much risk we have because\nwe restrict ourselves to a speciﬁc class, namely, how much inductive bias we\nhave The approximation error does not depend on the sample size and is\ndetermined by the hypothesis class chosen Enlarging the hypothesis class\ncan decrease the approximation error Under the realizability assumption, the approximation error is zero",
      "word_count": 208,
      "source_page": 64,
      "start_position": 20371,
      "end_position": 20578,
      "sentences_count": 12
    },
    {
      "chunk_id": 94,
      "text": "5.3 Summary\n65\n• The Estimation Error – the diﬀerence between the approximation error\nand the error achieved by the ERM predictor The estimation error results\nbecause the empirical risk (i.e., training error) is only an estimate of the\ntrue risk, and so the predictor minimizing the empirical risk is only an\nestimate of the predictor minimizing the true risk The quality of this estimation depends on the training set size and\non the size, or complexity, of the hypothesis class As we have shown, for\na ﬁnite hypothesis class, ϵest increases (logarithmically) with |H| and de-\ncreases with m We can think of the size of H as a measure of its complexity In future chapters we will deﬁne other complexity measures of hypothesis\nclasses Since our goal is to minimize the total risk, we face a tradeoﬀ, called the bias-\ncomplexity tradeoﬀ On one hand, choosing H to be a very rich class decreases the\napproximation error but at the same time might increase the estimation error,\nas a rich H might lead to overﬁtting On the other hand, choosing H to be a\nvery small set reduces the estimation error but might increase the approximation\nerror or, in other words, might lead to underﬁtting Of course, a great choice for\nH is the class that contains only one classiﬁer – the Bayes optimal classiﬁer",
      "word_count": 227,
      "source_page": 65,
      "start_position": 20652,
      "end_position": 20878,
      "sentences_count": 10
    },
    {
      "chunk_id": 95,
      "text": "On the other hand, choosing H to be a\nvery small set reduces the estimation error but might increase the approximation\nerror or, in other words, might lead to underﬁtting Of course, a great choice for\nH is the class that contains only one classiﬁer – the Bayes optimal classiﬁer But\nthe Bayes optimal classiﬁer depends on the underlying distribution D, which we\ndo not know (indeed, learning would have been unnecessary had we known D) Learning theory studies how rich we can make H while still maintaining rea-\nsonable estimation error In many cases, empirical research focuses on designing\ngood hypothesis classes for a certain domain Here, “good” means classes for\nwhich the approximation error would not be excessively high The idea is that\nalthough we are not experts and do not know how to construct the optimal clas-\nsiﬁer, we still have some prior knowledge of the speciﬁc problem at hand, which\nenables us to design hypothesis classes for which both the approximation error\nand the estimation error are not too large Getting back to our papayas example,\nwe do not know how exactly the color and hardness of a papaya predict its taste,\nbut we do know that papaya is a fruit and on the basis of previous experience\nwith other fruit we conjecture that a rectangle in the color-hardness space may\nbe a good predictor 5.3\nSummary\nThe No-Free-Lunch theorem states that there is no universal learner",
      "word_count": 241,
      "source_page": 65,
      "start_position": 20829,
      "end_position": 21069,
      "sentences_count": 9
    },
    {
      "chunk_id": 96,
      "text": "66\nThe Bias-Complexity Tradeoﬀ\nbe small In the next chapter we will study in more detail the behavior of the\nestimation error In Chapter 7 we will discuss alternative ways to express prior\nknowledge 5.4\nBibliographic Remarks\n(Wolpert & Macready 1997) proved several no-free-lunch theorems for optimiza-\ntion, but these are rather diﬀerent from the theorem we prove here The theorem\nwe prove here is closely related to lower bounds in VC theory, as we will study\nin the next chapter 5.5\nExercises\n1 Prove that Equation (5.2) suﬃces for showing that P[LD(A(S)) ≥1/8] ≥1/7 Hint: Let θ be a random variable that receives values in [0, 1] and whose\nexpectation satisﬁes E[θ] ≥1/4 Use Lemma B.1 to show that P[θ ≥1/8] ≥\n1/7 2 Assume you are asked to design a learning algorithm to predict whether pa-\ntients are going to suﬀer a heart attack Relevant patient features the al-\ngorithm may have access to include blood pressure (BP), body-mass index\n(BMI), age (A), level of physical activity (P), and income (I) You have to choose between two algorithms; the ﬁrst picks an axis aligned\nrectangle in the two dimensional space spanned by the features BP and BMI\nand the other picks an axis aligned rectangle in the ﬁve dimensional space\nspanned by all the preceding features 1 Explain the pros and cons of each choice 2 Explain how the number of available labeled training samples will aﬀect\nyour choice 3",
      "word_count": 242,
      "source_page": 66,
      "start_position": 21152,
      "end_position": 21393,
      "sentences_count": 18
    },
    {
      "chunk_id": 97,
      "text": "6\nThe VC-Dimension\nIn the previous chapter, we decomposed the error of the ERMH rule into ap-\nproximation error and estimation error The approximation error depends on\nthe ﬁt of our prior knowledge (as reﬂected by the choice of the hypothesis class\nH) to the underlying unknown distribution In contrast, the deﬁnition of PAC\nlearnability requires that the estimation error would be bounded uniformly over\nall distributions Our current goal is to ﬁgure out which classes H are PAC learnable, and to\ncharacterize exactly the sample complexity of learning a given hypothesis class So far we have seen that ﬁnite classes are learnable, but that the class of all\nfunctions (over an inﬁnite size domain) is not What makes one class learnable\nand the other unlearnable Can inﬁnite-size classes be learnable, and, if so, what\ndetermines their sample complexity We begin the chapter by showing that inﬁnite classes can indeed be learn-\nable, and thus, ﬁniteness of the hypothesis class is not a necessary condition for\nlearnability We then present a remarkably crisp characterization of the family\nof learnable classes in the setup of binary valued classiﬁcation with the zero-one\nloss This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey\nChervonenkis in 1970 and relies on a combinatorial notion called the Vapnik-\nChervonenkis dimension (VC-dimension) We formally deﬁne the VC-dimension,\nprovide several examples, and then state the fundamental theorem of statistical\nlearning theory, which integrates the concepts of learnability, VC-dimension, the\nERM rule, and uniform convergence",
      "word_count": 247,
      "source_page": 67,
      "start_position": 21487,
      "end_position": 21733,
      "sentences_count": 11
    },
    {
      "chunk_id": 98,
      "text": "This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey\nChervonenkis in 1970 and relies on a combinatorial notion called the Vapnik-\nChervonenkis dimension (VC-dimension) We formally deﬁne the VC-dimension,\nprovide several examples, and then state the fundamental theorem of statistical\nlearning theory, which integrates the concepts of learnability, VC-dimension, the\nERM rule, and uniform convergence 6.1\nInﬁnite-Size Classes Can Be Learnable\nIn Chapter 4 we saw that ﬁnite classes are learnable, and in fact the sample\ncomplexity of a hypothesis class is upper bounded by the log of its size To show\nthat the size of the hypothesis class is not the right characterization of its sample\ncomplexity, we ﬁrst present a simple example of an inﬁnite-size hypothesis class\nthat is learnable Example 6.1\nLet H be the set of threshold functions over the real line, namely,\nH = {ha : a ∈R}, where ha : R →{0, 1} is a function such that ha(x) = 1[x<a] To remind the reader, 1[x<a] is 1 if x < a and 0 otherwise Clearly, H is of inﬁnite\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 206,
      "source_page": 67,
      "start_position": 21678,
      "end_position": 21883,
      "sentences_count": 11
    },
    {
      "chunk_id": 99,
      "text": "68\nThe VC-Dimension\nsize Nevertheless, the following lemma shows that H is learnable in the PAC\nmodel using the ERM algorithm Lemma 6.1\nLet H be the class of thresholds as deﬁned earlier Then, H is\nPAC learnable, using the ERM rule, with sample complexity of mH(ϵ, δ) ≤\n⌈log(2/δ)/ϵ⌉ Proof\nLet a⋆be a threshold such that the hypothesis h⋆(x) = 1[x<a⋆] achieves\nLD(h⋆) = 0 Let Dx be the marginal distribution over the domain X and let\na0 < a⋆< a1 be such that\nP\nx∼Dx[x ∈(a0, a⋆)] =\nP\nx∼Dx[x ∈(a⋆, a1)] = ϵ a⋆\na0\na1\nϵ mass\nϵ mass\n(If Dx(−∞, a⋆) ≤ϵ we set a0 = −∞and similarly for a1) Given a training set\nS, let b0 = max{x : (x, 1) ∈S} and b1 = min{x : (x, 0) ∈S} (if no example in S\nis positive we set b0 = −∞and if no example in S is negative we set b1 = ∞) Let bS be a threshold corresponding to an ERM hypothesis, hS, which implies\nthat bS ∈(b0, b1) Therefore, a suﬃcient condition for LD(hS) ≤ϵ is that both\nb0 ≥a0 and b1 ≤a1 In other words,\nP\nS∼Dm[LD(hS) > ϵ] ≤\nP\nS∼Dm[b0 < a0 ∨b1 > a1],\nand using the union bound we can bound the preceding by\nP\nS∼Dm[LD(hS) > ϵ] ≤\nP\nS∼Dm[b0 < a0] +\nP\nS∼Dm[b1 > a1]",
      "word_count": 232,
      "source_page": 68,
      "start_position": 21884,
      "end_position": 22115,
      "sentences_count": 11
    },
    {
      "chunk_id": 100,
      "text": "Therefore, a suﬃcient condition for LD(hS) ≤ϵ is that both\nb0 ≥a0 and b1 ≤a1 In other words,\nP\nS∼Dm[LD(hS) > ϵ] ≤\nP\nS∼Dm[b0 < a0 ∨b1 > a1],\nand using the union bound we can bound the preceding by\nP\nS∼Dm[LD(hS) > ϵ] ≤\nP\nS∼Dm[b0 < a0] +\nP\nS∼Dm[b1 > a1] (6.1)\nThe event b0 < a0 happens if and only if all examples in S are not in the interval\n(a0, a∗), whose probability mass is deﬁned to be ϵ, namely,\nP\nS∼Dm[b0 < a0] =\nP\nS∼Dm[∀(x, y) ∈S, x ̸∈(a0, a⋆)] = (1 −ϵ)m ≤e−ϵ m Since we assume m > log(2/δ)/ϵ it follows that the equation is at most δ/2 In the same way it is easy to see that PS∼Dm[b1 > a1] ≤δ/2 Combining with\nEquation (6.1) we conclude our proof 6.2\nThe VC-Dimension\nWe see, therefore, that while ﬁniteness of H is a suﬃcient condition for learn-\nability, it is not a necessary condition As we will show, a property called the\nVC-dimension of a hypothesis class gives the correct characterization of its learn-\nability To motivate the deﬁnition of the VC-dimension, let us recall the No-Free-\nLunch theorem (Theorem 5.1) and its proof There, we have shown that without",
      "word_count": 210,
      "source_page": 68,
      "start_position": 22061,
      "end_position": 22270,
      "sentences_count": 10
    },
    {
      "chunk_id": 101,
      "text": "6.2 The VC-Dimension\n69\nrestricting the hypothesis class, for any learning algorithm, an adversary can\nconstruct a distribution for which the learning algorithm will perform poorly,\nwhile there is another learning algorithm that will succeed on the same distri-\nbution To do so, the adversary used a ﬁnite set C ⊂X and considered a family\nof distributions that are concentrated on elements of C Each distribution was\nderived from a “true” target function from C to {0, 1} To make any algorithm\nfail, the adversary used the power of choosing a target function from the set of\nall possible functions from C to {0, 1} When considering PAC learnability of a hypothesis class H, the adversary\nis restricted to constructing distributions for which some hypothesis h ∈H\nachieves a zero risk Since we are considering distributions that are concentrated\non elements of C, we should study how H behaves on C, which leads to the\nfollowing deﬁnition definition 6.2 (Restriction of H to C)\nLet H be a class of functions from X\nto {0, 1} and let C = {c1, , cm} ⊂X The restriction of H to C is the set of\nfunctions from C to {0, 1} that can be derived from H That is,\nHC = {(h(c1), , h(cm)) : h ∈H},\nwhere we represent each function from C to {0, 1} as a vector in {0, 1}|C|",
      "word_count": 232,
      "source_page": 69,
      "start_position": 22271,
      "end_position": 22502,
      "sentences_count": 11
    },
    {
      "chunk_id": 102,
      "text": "That is,\nHC = {(h(c1), , h(cm)) : h ∈H},\nwhere we represent each function from C to {0, 1} as a vector in {0, 1}|C| If the restriction of H to C is the set of all functions from C to {0, 1}, then\nwe say that H shatters the set C Formally:\ndefinition 6.3 (Shattering)\nA hypothesis class H shatters a ﬁnite set C ⊂X\nif the restriction of H to C is the set of all functions from C to {0, 1} That is,\n|HC| = 2|C| Example 6.2\nLet H be the class of threshold functions over R Take a set\nC = {c1} Now, if we take a = c1 + 1, then we have ha(c1) = 1, and if we take\na = c1 −1, then we have ha(c1) = 0 Therefore, HC is the set of all functions\nfrom C to {0, 1}, and H shatters C Now take a set C = {c1, c2}, where c1 ≤c2 No h ∈H can account for the labeling (0, 1), because any threshold that assigns\nthe label 0 to c1 must assign the label 0 to c2 as well Therefore not all functions\nfrom C to {0, 1} are included in HC; hence C is not shattered by H",
      "word_count": 214,
      "source_page": 69,
      "start_position": 22477,
      "end_position": 22690,
      "sentences_count": 12
    },
    {
      "chunk_id": 103,
      "text": "No h ∈H can account for the labeling (0, 1), because any threshold that assigns\nthe label 0 to c1 must assign the label 0 to c2 as well Therefore not all functions\nfrom C to {0, 1} are included in HC; hence C is not shattered by H Getting back to the construction of an adversarial distribution as in the proof\nof the No-Free-Lunch theorem (Theorem 5.1), we see that whenever some set C\nis shattered by H, the adversary is not restricted by H, as they can construct\na distribution over C based on any target function from C to {0, 1}, while still\nmaintaining the realizability assumption This immediately yields:\ncorollary 6.4\nLet H be a hypothesis class of functions from X to {0, 1} Let\nm be a training set size Assume that there exists a set C ⊂X of size 2m that is\nshattered by H Then, for any learning algorithm, A, there exist a distribution D\nover X × {0, 1} and a predictor h ∈H such that LD(h) = 0 but with probability\nof at least 1/7 over the choice of S ∼Dm we have that LD(A(S)) ≥1/8.",
      "word_count": 195,
      "source_page": 69,
      "start_position": 22642,
      "end_position": 22836,
      "sentences_count": 7
    },
    {
      "chunk_id": 104,
      "text": "70\nThe VC-Dimension\nCorollary 6.4 tells us that if H shatters some set C of size 2m then we cannot\nlearn H using m examples Intuitively, if a set C is shattered by H, and we\nreceive a sample containing half the instances of C, the labels of these instances\ngive us no information about the labels of the rest of the instances in C – every\npossible labeling of the rest of the instances can be explained by some hypothesis\nin H Philosophically,\nIf someone can explain every phenomenon, his explanations are worthless This leads us directly to the deﬁnition of the VC dimension definition 6.5 (VC-dimension)\nThe VC-dimension of a hypothesis class H,\ndenoted VCdim(H), is the maximal size of a set C ⊂X that can be shattered\nby H If H can shatter sets of arbitrarily large size we say that H has inﬁnite\nVC-dimension A direct consequence of Corollary 6.4 is therefore:\ntheorem 6.6\nLet H be a class of inﬁnite VC-dimension Then, H is not PAC\nlearnable Proof\nSince H has an inﬁnite VC-dimension, for any training set size m, there\nexists a shattered set of size 2m, and the claim follows by Corollary 6.4 We shall see later in this chapter that the converse is also true: A ﬁnite VC-\ndimension guarantees learnability Hence, the VC-dimension characterizes PAC\nlearnability But before delving into more theory, we ﬁrst show several examples 6.3\nExamples\nIn this section we calculate the VC-dimension of several hypothesis classes",
      "word_count": 249,
      "source_page": 70,
      "start_position": 22837,
      "end_position": 23085,
      "sentences_count": 13
    },
    {
      "chunk_id": 105,
      "text": "6.3 Examples\n71\n6.3.2\nIntervals\nLet H be the class of intervals over R, namely, H = {ha,b : a, b ∈R, a < b},\nwhere ha,b : R →{0, 1} is a function such that ha,b(x) = 1[x∈(a,b)] Take the set\nC = {1, 2} Then, H shatters C (make sure you understand why) and therefore\nVCdim(H) ≥2 Now take an arbitrary set C = {c1, c2, c3} and assume without\nloss of generality that c1 ≤c2 ≤c3 Then, the labeling (1, 0, 1) cannot be obtained\nby an interval and therefore H does not shatter C We therefore conclude that\nVCdim(H) = 2 6.3.3\nAxis Aligned Rectangles\nLet H be the class of axis aligned rectangles, formally:\nH = {h(a1,a2,b1,b2) : a1 ≤a2 and b1 ≤b2}\nwhere\nh(a1,a2,b1,b2)(x1, x2) =\n(\n1\nif a1 ≤x1 ≤a2 and b1 ≤x2 ≤b2\n0\notherwise\n(6.2)\nWe shall show in the following that VCdim(H) = 4 To prove this we need\nto ﬁnd a set of 4 points that are shattered by H, and show that no set of 5\npoints can be shattered by H Finding a set of 4 points that are shattered is\neasy (see Figure 6.1) Now, consider any set C ⊂R2 of 5 points In C, take a\nleftmost point (whose ﬁrst coordinate is the smallest in C), a rightmost point\n(ﬁrst coordinate is the largest), a lowest point (second coordinate is the smallest),\nand a highest point (second coordinate is the largest)",
      "word_count": 247,
      "source_page": 71,
      "start_position": 23187,
      "end_position": 23433,
      "sentences_count": 11
    },
    {
      "chunk_id": 106,
      "text": "Now, consider any set C ⊂R2 of 5 points In C, take a\nleftmost point (whose ﬁrst coordinate is the smallest in C), a rightmost point\n(ﬁrst coordinate is the largest), a lowest point (second coordinate is the smallest),\nand a highest point (second coordinate is the largest) Without loss of generality,\ndenote C = {c1, , c5} and let c5 be the point that was not selected Now,\ndeﬁne the labeling (1, 1, 1, 1, 0) It is impossible to obtain this labeling by an\naxis aligned rectangle Indeed, such a rectangle must contain c1, , c4; but in\nthis case the rectangle contains c5 as well, because its coordinates are within\nthe intervals deﬁned by the selected points So, C is not shattered by H, and\ntherefore VCdim(H) = 4 c1\nc2\nc3\nc4\nc5\nFigure 6.1 Left: 4 points that are shattered by axis aligned rectangles Right: Any axis\naligned rectangle cannot label c5 by 0 and the rest of the points by 1.",
      "word_count": 167,
      "source_page": 71,
      "start_position": 23386,
      "end_position": 23552,
      "sentences_count": 11
    },
    {
      "chunk_id": 107,
      "text": "72\nThe VC-Dimension\n6.3.4\nFinite Classes\nLet H be a ﬁnite class Then, clearly, for any set C we have |HC| ≤|H| and thus C\ncannot be shattered if |H| < 2|C| This implies that VCdim(H) ≤log2(|H|) This\nshows that the PAC learnability of ﬁnite classes follows from the more general\nstatement of PAC learnability of classes with ﬁnite VC-dimension, which we shall\nsee in the next section Note, however, that the VC-dimension of a ﬁnite class\nH can be signiﬁcantly smaller than log2(|H|) For example, let X = {1, , k},\nfor some integer k, and consider the class of threshold functions (as deﬁned in\nExample 6.2) Then, |H| = k but VCdim(H) = 1 Since k can be arbitrarily\nlarge, the gap between log2(|H|) and VCdim(H) can be arbitrarily large 6.3.5\nVC-Dimension and the Number of Parameters\nIn the previous examples, the VC-dimension happened to equal the number of\nparameters deﬁning the hypothesis class While this is often the case, it is not\nalways true Consider, for example, the domain X = R, and the hypothesis class\nH = {hθ : θ ∈R} where hθ : X →{0, 1} is deﬁned by hθ(x) = ⌈0.5 sin(θx)⌉ It\nis possible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d\npoints that are shattered by H (see Exercise 8) 6.4\nThe Fundamental Theorem of PAC learning\nWe have already shown that a class of inﬁnite VC-dimension is not learnable",
      "word_count": 244,
      "source_page": 72,
      "start_position": 23553,
      "end_position": 23796,
      "sentences_count": 14
    },
    {
      "chunk_id": 108,
      "text": "It\nis possible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d\npoints that are shattered by H (see Exercise 8) 6.4\nThe Fundamental Theorem of PAC learning\nWe have already shown that a class of inﬁnite VC-dimension is not learnable The\nconverse statement is also true, leading to the fundamental theorem of statistical\nlearning theory:\ntheorem 6.7 (The Fundamental Theorem of Statistical Learning)\nLet H be a\nhypothesis class of functions from a domain X to {0, 1} and let the loss function\nbe the 0 −1 loss Then, the following are equivalent:\n1 H has the uniform convergence property 2 Any ERM rule is a successful agnostic PAC learner for H 3 H is agnostic PAC learnable 4 H is PAC learnable 5 Any ERM rule is a successful PAC learner for H 6 H has a ﬁnite VC-dimension The proof of the theorem is given in the next section Not only does the VC-dimension characterize PAC learnability; it even deter-\nmines the sample complexity theorem 6.8 (The Fundamental Theorem of Statistical Learning – Quantita-\ntive Version)\nLet H be a hypothesis class of functions from a domain X to {0, 1}\nand let the loss function be the 0 −1 loss Assume that VCdim(H) = d < ∞ Then, there are absolute constants C1, C2 such that:",
      "word_count": 225,
      "source_page": 72,
      "start_position": 23751,
      "end_position": 23975,
      "sentences_count": 20
    },
    {
      "chunk_id": 109,
      "text": "6.5 Proof of Theorem 6.7\n73\n1 H has the uniform convergence property with sample complexity\nC1\nd + log(1/δ)\nϵ2\n≤m\nUC\nH (ϵ, δ) ≤C2\nd + log(1/δ)\nϵ2\n2 H is agnostic PAC learnable with sample complexity\nC1\nd + log(1/δ)\nϵ2\n≤mH(ϵ, δ) ≤C2\nd + log(1/δ)\nϵ2\n3 H is PAC learnable with sample complexity\nC1\nd + log(1/δ)\nϵ\n≤mH(ϵ, δ) ≤C2\nd log(1/ϵ) + log(1/δ)\nϵ\nThe proof of this theorem is given in Chapter 28 Remark 6.3\nWe stated the fundamental theorem for binary classiﬁcation tasks A similar result holds for some other learning problems such as regression with\nthe absolute loss or the squared loss However, the theorem does not hold for\nall learning tasks In particular, learnability is sometimes possible even though\nthe uniform convergence property does not hold (we will see an example in\nChapter 13, Exercise 2) Furthermore, in some situations, the ERM rule fails\nbut learnability is possible with other learning rules 6.5\nProof of Theorem 6.7\nWe have already seen that 1 →2 in Chapter 4 The implications 2 →3 and\n3 →4 are trivial and so is 2 →5 The implications 4 →6 and 5 →6 follow from\nthe No-Free-Lunch theorem The diﬃcult part is to show that 6 →1",
      "word_count": 215,
      "source_page": 73,
      "start_position": 23976,
      "end_position": 24190,
      "sentences_count": 13
    },
    {
      "chunk_id": 110,
      "text": "The implications 4 →6 and 5 →6 follow from\nthe No-Free-Lunch theorem The diﬃcult part is to show that 6 →1 The proof\nis based on two main claims:\n• If VCdim(H) = d, then even though H might be inﬁnite, when restricting it\nto a ﬁnite set C ⊂X, its “eﬀective” size, |HC|, is only O(|C|d) That is,\nthe size of HC grows polynomially rather than exponentially with |C| This\nclaim is often referred to as Sauer’s lemma, but it has also been stated and\nproved independently by Shelah and by Perles The formal statement is\ngiven in Section 6.5.1 later • In Section 4 we have shown that ﬁnite hypothesis classes enjoy the uniform\nconvergence property In Section 6.5.2 later we generalize this result and\nshow that uniform convergence holds whenever the hypothesis class has a\n“small eﬀective size.” By “small eﬀective size” we mean classes for which\n|HC| grows polynomially with |C| 6.5.1\nSauer’s Lemma and the Growth Function\nWe deﬁned the notion of shattering, by considering the restriction of H to a ﬁnite\nset of instances The growth function measures the maximal “eﬀective” size of\nH on a set of m examples Formally:",
      "word_count": 197,
      "source_page": 73,
      "start_position": 24170,
      "end_position": 24366,
      "sentences_count": 11
    },
    {
      "chunk_id": 111,
      "text": "74\nThe VC-Dimension\ndefinition 6.9 (Growth Function)\nLet H be a hypothesis class Then the\ngrowth function of H, denoted τH : N →N, is deﬁned as\nτH(m) =\nmax\nC⊂X:|C|=m\n\f\fHC In words, τH(m) is the number of diﬀerent functions from a set C of size m to\n{0, 1} that can be obtained by restricting H to C Obviously, if VCdim(H) = d then for any m ≤d we have τH(m) = 2m In\nsuch cases, H induces all possible functions from C to {0, 1} The following beau-\ntiful lemma, proposed independently by Sauer, Shelah, and Perles, shows that\nwhen m becomes larger than the VC-dimension, the growth function increases\npolynomially rather than exponentially with m lemma 6.10 (Sauer-Shelah-Perles)\nLet H be a hypothesis class with VCdim(H) ≤\nd < ∞ Then, for all m, τH(m) ≤Pd\ni=0\n\u0000m\ni\n\u0001 In particular, if m > d + 1 then\nτH(m) ≤(em/d)d Proof of Sauer’s Lemma *\nTo prove the lemma it suﬃces to prove the following stronger claim: For any\nC = {c1, , cm} we have\n∀H,\n|HC| ≤|{B ⊆C : H shatters B}| (6.3)\nThe reason why Equation (6.3) is suﬃcient to prove the lemma is that if VCdim(H) ≤\nd then no set whose size is larger than d is shattered by H and therefore\n|{B ⊆C : H shatters B}| ≤\nd\nX\ni=0\n\u0012m\ni\n\u0013",
      "word_count": 235,
      "source_page": 74,
      "start_position": 24367,
      "end_position": 24601,
      "sentences_count": 12
    },
    {
      "chunk_id": 112,
      "text": ", cm} we have\n∀H,\n|HC| ≤|{B ⊆C : H shatters B}| (6.3)\nThe reason why Equation (6.3) is suﬃcient to prove the lemma is that if VCdim(H) ≤\nd then no set whose size is larger than d is shattered by H and therefore\n|{B ⊆C : H shatters B}| ≤\nd\nX\ni=0\n\u0012m\ni\n\u0013 When m > d + 1 the right-hand side of the preceding is at most (em/d)d (see\nLemma A.5 in Appendix A) We are left with proving Equation (6.3) and we do it using an inductive argu-\nment For m = 1, no matter what H is, either both sides of Equation (6.3) equal\n1 or both sides equal 2 (the empty set is always considered to be shattered by\nH) Assume Equation (6.3) holds for sets of size k < m and let us prove it for\nsets of size m Fix H and C = {c1, , cm} Denote C′ = {c2, , cm} and in\naddition, deﬁne the following two sets:\nY0 = {(y2, , ym) : (0, y2, , ym) ∈HC ∨(1, y2, , ym) ∈HC},\nand\nY1 = {(y2, , ym) : (0, y2, , ym) ∈HC ∧(1, y2, , ym) ∈HC} It is easy to verify that |HC| = |Y0| + |Y1|",
      "word_count": 216,
      "source_page": 74,
      "start_position": 24544,
      "end_position": 24759,
      "sentences_count": 17
    },
    {
      "chunk_id": 113,
      "text": "6.5 Proof of Theorem 6.7\n75\nNext, deﬁne H′ ⊆H to be\nH′ = {h ∈H : ∃h′ ∈H s.t (1 −h′(c1), h′(c2), , h′(cm))\n= (h(c1), h(c2), , h(cm)},\nnamely, H′ contains pairs of hypotheses that agree on C′ and diﬀer on c1 Using\nthis deﬁnition, it is clear that if H′ shatters a set B ⊆C′ then it also shatters\nthe set B ∪{c1} and vice versa Combining this with the fact that Y1 = H′\nC′ and\nusing the inductive assumption (now applied on H′ and C′) we obtain that\n|Y1| = |H′\nC′| ≤|{B ⊆C′ : H′ shatters B}| = |{B ⊆C′ : H′ shatters B ∪{c1}}|\n= |{B ⊆C : c1 ∈B ∧H′ shatters B}| ≤|{B ⊆C : c1 ∈B ∧H shatters B}| Overall, we have shown that\n|HC| = |Y0| + |Y1|\n≤|{B ⊆C : c1 ̸∈B ∧H shatters B}| + |{B ⊆C : c1 ∈B ∧H shatters B}|\n= |{B ⊆C : H shatters B}|,\nwhich concludes our proof 6.5.2\nUniform Convergence for Classes of Small Eﬀective Size\nIn this section we prove that if H has small eﬀective size then it enjoys the\nuniform convergence property Formally,\ntheorem 6.11\nLet H be a class and let τH be its growth function Then, for\nevery D and every δ ∈(0, 1), with probability of at least 1 −δ over the choice of\nS ∼Dm we have\n|LD(h) −LS(h)| ≤4 +\np\nlog(τH(2m))\nδ\n√\n2m",
      "word_count": 242,
      "source_page": 75,
      "start_position": 24795,
      "end_position": 25036,
      "sentences_count": 10
    },
    {
      "chunk_id": 114,
      "text": "Formally,\ntheorem 6.11\nLet H be a class and let τH be its growth function Then, for\nevery D and every δ ∈(0, 1), with probability of at least 1 −δ over the choice of\nS ∼Dm we have\n|LD(h) −LS(h)| ≤4 +\np\nlog(τH(2m))\nδ\n√\n2m Before proving the theorem, let us ﬁrst conclude the proof of Theorem 6.7 Proof of Theorem 6.7\nIt suﬃces to prove that if the VC-dimension is ﬁnite then\nthe uniform convergence property holds We will prove that\nm\nUC\nH (ϵ, δ) ≤4 16d\n(δϵ)2 log\n\u0012 16d\n(δϵ)2\n\u0013\n+ 16 d log(2e/d)\n(δϵ)2 From Sauer’s lemma we have that for m > d, τH(2m) ≤(2em/d)d Combining\nthis with Theorem 6.11 we obtain that with probability of at least 1 −δ,\n|LS(h) −LD(h)| ≤4 +\np\nd log(2em/d)\nδ\n√\n2m For simplicity assume that\np\nd log(2em/d) ≥4; hence,\n|LS(h) −LD(h)| ≤1\nδ\nr\n2d log(2em/d)\nm\n.",
      "word_count": 158,
      "source_page": 75,
      "start_position": 24989,
      "end_position": 25146,
      "sentences_count": 8
    },
    {
      "chunk_id": 115,
      "text": "76\nThe VC-Dimension\nTo ensure that the preceding is at most ϵ we need that\nm ≥2d log(m)\n(δϵ)2\n+ 2 d log(2e/d)\n(δϵ)2 Standard algebraic manipulations (see Lemma A.2 in Appendix A) show that a\nsuﬃcient condition for the preceding to hold is that\nm ≥4\n2d\n(δϵ)2 log\n\u0012 2d\n(δϵ)2\n\u0013\n+ 4 d log(2e/d)\n(δϵ)2 Remark 6.4\nThe upper bound on mUC\nH we derived in the proof Theorem 6.7\nis not the tightest possible A tighter analysis that yields the bounds given in\nTheorem 6.8 can be found in Chapter 28 Proof of Theorem 6.11 *\nWe will start by showing that\nE\nS∼Dm\n\u0014\nsup\nh∈H\n|LD(h) −LS(h)|\n\u0015\n≤4 +\np\nlog(τH(2m))\n√\n2m (6.4)\nSince the random variable suph∈H |LD(h) −LS(h)| is nonnegative, the proof of\nthe theorem follows directly from the preceding using Markov’s inequality (see\nSection B.1) To bound the left-hand side of Equation (6.4) we ﬁrst note that for every\nh ∈H, we can rewrite LD(h) = ES′∼Dm[LS′(h)], where S′ = z′\n1, , z′\nm is an\nadditional i.i.d sample Therefore,\nE\nS∼Dm\n\u0014\nsup\nh∈H\n|LD(h) −LS(h)|\n\u0015\n=\nE\nS∼Dm\n\u0014\nsup\nh∈H\n\f\f\f\nE\nS′∼Dm LS′(h) −LS(h)\n\f\f\f\n\u0015 A generalization of the triangle inequality yields\n\f\f\f\nE\nS′∼Dm[LS′(h) −LS(h)]\n\f\f\f ≤\nE\nS′∼Dm |LS′(h) −LS(h)|,\nand the fact that supermum of expectation is smaller than expectation of supre-\nmum yields\nsup\nh∈H\nE\nS′∼Dm |LS′(h) −LS(h)| ≤\nE\nS′∼Dm sup\nh∈H\n|LS′(h) −LS(h)|",
      "word_count": 245,
      "source_page": 76,
      "start_position": 25147,
      "end_position": 25391,
      "sentences_count": 11
    },
    {
      "chunk_id": 116,
      "text": "6.5 Proof of Theorem 6.7\n77\nThe expectation on the right-hand side is over a choice of two i.i.d samples\nS = z1, , zm and S′ = z′\n1, , z′\nm Since all of these 2m vectors are chosen\ni.i.d., nothing will change if we replace the name of the random vector zi with the\nname of the random vector z′\ni If we do it, instead of the term (ℓ(h, z′\ni)−ℓ(h, zi))\nin Equation (6.5) we will have the term −(ℓ(h, z′\ni) −ℓ(h, zi)) It follows that for\nevery σ ∈{±1}m we have that Equation (6.5) equals\nE\nS,S′∼Dm\n\"\nsup\nh∈H\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n#\nSince this holds for every σ ∈{±1}m, it also holds if we sample each component\nof σ uniformly at random from the uniform distribution over {±1}, denoted U± Hence, Equation (6.5) also equals\nE\nσ∼U m\n±\nE\nS,S′∼Dm\n\"\nsup\nh∈H\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n#\n,\nand by the linearity of expectation it also equals\nE\nS,S′∼Dm\nE\nσ∼U m\n±\n\"\nsup\nh∈H\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n# Next, ﬁx S and S′, and let C be the instances appearing in S and S′ Then, we\ncan take the supremum only over h ∈HC",
      "word_count": 227,
      "source_page": 77,
      "start_position": 25441,
      "end_position": 25667,
      "sentences_count": 10
    },
    {
      "chunk_id": 117,
      "text": "Next, ﬁx S and S′, and let C be the instances appearing in S and S′ Then, we\ncan take the supremum only over h ∈HC Therefore,\nE\nσ∼U m\n±\n\"\nsup\nh∈H\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n#\n=\nE\nσ∼U m\n±\n\"\nmax\nh∈HC\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n# Fix some h ∈HC and denote θh = 1\nm\nPm\ni=1 σi(ℓ(h, z′\ni)−ℓ(h, zi)) Since E[θh] = 0\nand θh is an average of independent variables, each of which takes values in\n[−1, 1], we have by Hoeﬀding’s inequality that for every ρ > 0,\nP[|θh| > ρ] ≤2 exp\n\u0000−2 m ρ2\u0001 Applying the union bound over h ∈HC, we obtain that for any ρ > 0,\nP\n\u0014\nmax\nh∈HC |θh| > ρ\n\u0015\n≤2 |HC| exp\n\u0000−2 m ρ2\u0001 Finally, Lemma A.4 in Appendix A tells us that the preceding implies\nE\n\u0014\nmax\nh∈HC |θh|\n\u0015\n≤4 +\np\nlog(|HC|)\n√\n2m Combining all with the deﬁnition of τH, we have shown that\nE\nS∼Dm\n\u0014\nsup\nh∈H\n|LD(h) −LS(h)|\n\u0015\n≤4 +\np\nlog(τH(2m))\n√\n2m\n.",
      "word_count": 198,
      "source_page": 77,
      "start_position": 25642,
      "end_position": 25839,
      "sentences_count": 8
    },
    {
      "chunk_id": 118,
      "text": "78\nThe VC-Dimension\n6.6\nSummary\nThe fundamental theorem of learning theory characterizes PAC learnability of\nclasses of binary classiﬁers using VC-dimension The VC-dimension of a class\nis a combinatorial property that denotes the maximal sample size that can be\nshattered by the class The fundamental theorem states that a class is PAC learn-\nable if and only if its VC-dimension is ﬁnite and speciﬁes the sample complexity\nrequired for PAC learning The theorem also shows that if a problem is at all\nlearnable, then uniform convergence holds and therefore the problem is learnable\nusing the ERM rule 6.7\nBibliographic remarks\nThe deﬁnition of VC-dimension and its relation to learnability and to uniform\nconvergence is due to the seminal work of Vapnik & Chervonenkis (1971) The\nrelation to the deﬁnition of PAC learnability is due to Blumer, Ehrenfeucht,\nHaussler & Warmuth (1989) Several generalizations of the VC-dimension have been proposed For exam-\nple, the fat-shattering dimension characterizes learnability of some regression\nproblems (Kearns, Schapire & Sellie 1994, Alon, Ben-David, Cesa-Bianchi &\nHaussler 1997, Bartlett, Long & Williamson 1994, Anthony & Bartlet 1999), and\nthe Natarajan dimension characterizes learnability of some multiclass learning\nproblems (Natarajan 1989) However, in general, there is no equivalence between\nlearnability and uniform convergence See (Shalev-Shwartz, Shamir, Srebro &\nSridharan 2010, Daniely, Sabato, Ben-David & Shalev-Shwartz 2011) Sauer’s lemma has been proved by Sauer in response to a problem of Erdos\n(Sauer 1972)",
      "word_count": 235,
      "source_page": 78,
      "start_position": 25840,
      "end_position": 26074,
      "sentences_count": 11
    },
    {
      "chunk_id": 119,
      "text": "See (Shalev-Shwartz, Shamir, Srebro &\nSridharan 2010, Daniely, Sabato, Ben-David & Shalev-Shwartz 2011) Sauer’s lemma has been proved by Sauer in response to a problem of Erdos\n(Sauer 1972) Shelah (with Perles) proved it as a useful lemma for Shelah’s theory\nof stable models (Shelah 1972) Gil Kalai tells1 us that at some later time, Benjy\nWeiss asked Perles about such a result in the context of ergodic theory, and\nPerles, who forgot that he had proved it once, proved it again Vapnik and\nChervonenkis proved the lemma in the context of statistical learning theory 6.8\nExercises\n1 Show the following monotonicity property of VC-dimension: For every two\nhypothesis classes if H′ ⊆H then VCdim(H′) ≤VCdim(H) 2 Given some ﬁnite domain set, X, and a number k ≤|X|, ﬁgure out the VC-\ndimension of each of the following classes (and prove your claims):\n1 HX\n=k = {h ∈{0, 1}X : |{x : h(x) = 1}| = k} That is, the set of all functions\nthat assign the value 1 to exactly k elements of X 1 http://gilkalai.wordpress.com/2008/09/28/\nextremal-combinatorics-iii-some-basic-theorems",
      "word_count": 179,
      "source_page": 78,
      "start_position": 26046,
      "end_position": 26224,
      "sentences_count": 12
    },
    {
      "chunk_id": 120,
      "text": "6.8 Exercises\n79\n2 Hat−most−k = {h ∈{0, 1}X : |{x : h(x) = 1}| ≤k or |{x : h(x) = 0}| ≤k} 3 Let X be the Boolean hypercube {0, 1}n For a set I ⊆{1, 2, , n} we deﬁne\na parity function hI as follows On a binary vector x = (x1, x2, , xn) ∈\n{0, 1}n,\nhI(x) =\n X\ni∈I\nxi mod 2 (That is, hI computes parity of bits in I.) What is the VC-dimension of the\nclass of all such parity functions, Hn-parity = {hI : I ⊆{1, 2, , n}} 4 We proved Sauer’s lemma by proving that for every class H of ﬁnite VC-\ndimension d, and every subset A of the domain,\n|HA| ≤|{B ⊆A : H shatters B}| ≤\nd\nX\ni=0\n\u0012|A|\ni\n\u0013 Show that there are cases in which the previous two inequalities are strict\n(namely, the ≤can be replaced by <) and cases in which they can be replaced\nby equalities Demonstrate all four combinations of = and < 5 VC-dimension of axis aligned rectangles in Rd: Let Hd\nrec be the class of\naxis aligned rectangles in Rd We have already seen that VCdim(H2\nrec) = 4 Prove that in general, VCdim(Hd\nrec) = 2d 6 VC-dimension of Boolean conjunctions: Let Hd\ncon be the class of Boolean\nconjunctions over the variables x1, , xd (d ≥2) We already know that this\nclass is ﬁnite and thus (agnostic) PAC learnable",
      "word_count": 246,
      "source_page": 79,
      "start_position": 26225,
      "end_position": 26470,
      "sentences_count": 23
    },
    {
      "chunk_id": 121,
      "text": ", xd (d ≥2) We already know that this\nclass is ﬁnite and thus (agnostic) PAC learnable In this question we calculate\nVCdim(Hd\ncon) 1 Show that |Hd\ncon| ≤3d + 1 2 Conclude that VCdim(H) ≤d log 3 3 Show that Hd\ncon shatters the set of unit vectors {ei : i ≤d} 4 (**) Show that VCdim(Hd\ncon) ≤d Hint: Assume by contradiction that there exists a set C = {c1, , cd+1}\nthat is shattered by Hd\ncon Let h1, , hd+1 be hypotheses in Hd\ncon that\nsatisfy\n∀i, j ∈[d + 1], hi(cj) =\n(\n0\ni = j\n1\notherwise\nFor each i ∈[d + 1], hi (or more accurately, the conjunction that corre-\nsponds to hi) contains some literal ℓi which is false on ci and true on cj\nfor each j ̸= i Use the Pigeonhole principle to show that there must be a\npair i < j ≤d + 1 such that ℓi and ℓj use the same xk and use that fact\nto derive a contradiction to the requirements from the conjunctions hi, hj 5 Consider the class Hd\nmcon of monotone Boolean conjunctions over {0, 1}d Monotonicity here means that the conjunctions do not contain negations.",
      "word_count": 207,
      "source_page": 79,
      "start_position": 26454,
      "end_position": 26660,
      "sentences_count": 19
    },
    {
      "chunk_id": 122,
      "text": "80\nThe VC-Dimension\nAs in Hd\ncon, the empty conjunction is interpreted as the all-positive hy-\npothesis We augment Hd\nmcon with the all-negative hypothesis h− Show\nthat VCdim(Hd\nmcon) = d 7 We have shown that for a ﬁnite hypothesis class H, VCdim(H) ≤⌊log(|H|)⌋ However, this is just an upper bound The VC-dimension of a class can be\nmuch lower than that:\n1 Find an example of a class H of functions over the real interval X = [0, 1]\nsuch that H is inﬁnite while VCdim(H) = 1 2 Give an example of a ﬁnite hypothesis class H over the domain X = [0, 1],\nwhere VCdim(H) = ⌊log2(|H|)⌋ 8 (*) It is often the case that the VC-dimension of a hypothesis class equals (or\ncan be bounded above by) the number of parameters one needs to set in order\nto deﬁne each hypothesis in the class For instance, if H is the class of axis\naligned rectangles in Rd, then VCdim(H) = 2d, which is equal to the number\nof parameters used to deﬁne a rectangle in Rd Here is an example that shows\nthat this is not always the case We will see that a hypothesis class might\nbe very complex and even not learnable, although it has a small number of\nparameters Consider the domain X = R, and the hypothesis class\nH = {x 7→⌈sin(θx)⌉: θ ∈R}\n(here, we take ⌈−1⌉= 0) Prove that VCdim(H) = ∞",
      "word_count": 243,
      "source_page": 80,
      "start_position": 26661,
      "end_position": 26903,
      "sentences_count": 17
    },
    {
      "chunk_id": 123,
      "text": "Consider the domain X = R, and the hypothesis class\nH = {x 7→⌈sin(θx)⌉: θ ∈R}\n(here, we take ⌈−1⌉= 0) Prove that VCdim(H) = ∞ Hint: There is more than one way to prove the required result One option\nis by applying the following lemma: If 0.x1x2x3 ., is the binary expansion of\nx ∈(0, 1), then for any natural number m, ⌈sin(2mπx)⌉= (1 −xm), provided\nthat ∃k ≥m s.t xk = 1 9 Let H be the class of signed intervals, that is,\nH = {ha,b,s : a ≤b, s ∈{−1, 1}} where\nha,b,s(x) =\n(\ns\nif x ∈[a, b]\n−s\nif x /∈[a, b]\nCalculate VCdim(H) 10 Let H be a class of functions from X to {0, 1} 1 Prove that if VCdim(H) ≥d, for any d, then for some probability distri-\nbution D over X × {0, 1}, for every sample size, m,\nE\nS∼Dm[LD(A(S))] ≥min\nh∈H LD(h) + d −m\n2d\nHint: Use Exercise 3 in Chapter 5 2 Prove that for every H that is PAC learnable, VCdim(H) < ∞ (Note that\nthis is the implication 3 →6 in Theorem 6.7.)\n11 VC of union: Let H1, , Hr be hypothesis classes over some ﬁxed domain\nset X Let d = maxi VCdim(Hi) and assume for simplicity that d ≥3.",
      "word_count": 218,
      "source_page": 80,
      "start_position": 26878,
      "end_position": 27095,
      "sentences_count": 18
    },
    {
      "chunk_id": 124,
      "text": "6.8 Exercises\n81\n1 Prove that\nVCdim (∪r\ni=1Hi) ≤4d log(2d) + 2 log(r) Hint: Take a set of k examples and assume that they are shattered by\nthe union class Therefore, the union class can produce all 2k possible\nlabelings on these examples Use Sauer’s lemma to show that the union\nclass cannot produce more than rkd labelings Therefore, 2k < rkd Now\nuse Lemma A.2 2 (*) Prove that for r = 2 it holds that\nVCdim (H1 ∪H2) ≤2d + 1 12 Dudley classes: In this question we discuss an algebraic framework for\ndeﬁning concept classes over Rn and show a connection between the VC\ndimension of such classes and their algebraic properties Given a function\nf : Rn →R we deﬁne the corresponding function, POS(f)(x) = 1[f(x)>0] For\na class F of real valued functions we deﬁne a corresponding class of functions\nPOS(F) = {POS(f) : f ∈F} We say that a family, F, of real valued func-\ntions is linearly closed\nif for all f, g ∈F and r ∈R, (f + rg) ∈F (where\naddition and scalar multiplication of functions are deﬁned point wise, namely,\nfor all x ∈Rn, (f + rg)(x) = f(x) + rg(x)) Note that if a family of functions\nis linearly closed then we can view it as a vector space over the reals For a\nfunction g : Rn →R and a family of functions F, let F +g\ndef\n= {f +g : f ∈F}",
      "word_count": 247,
      "source_page": 81,
      "start_position": 27096,
      "end_position": 27342,
      "sentences_count": 16
    },
    {
      "chunk_id": 125,
      "text": "Note that if a family of functions\nis linearly closed then we can view it as a vector space over the reals For a\nfunction g : Rn →R and a family of functions F, let F +g\ndef\n= {f +g : f ∈F} Hypothesis classes that have a representation as POS(F + g) for some vector\nspace of functions F and some function g are called Dudley classes 1 Show that for every g : Rn →R and every vector space of functions F as\ndeﬁned earlier, VCdim(POS(F + g)) = VCdim(POS(F)) 2 (**) For every linearly closed family of real valued functions F, the VC-\ndimension of the corresponding class POS(F) equals the linear dimension\nof F (as a vector space) Hint: Let f1, , fd be a basis for the vector space\nF Consider the mapping x 7→(f1(x), , fd(x)) (from Rn to Rd) Note\nthat this mapping induces a matching between functions over Rn of the\nform POS(f) and homogeneous linear halfspaces in Rd (the VC-dimension\nof the class of homogeneous linear halfspaces is analyzed in Chapter 9) 3 Show that each of the following classes can be represented as a Dudley\nclass:\n1 The class HSn of halfspaces over Rn (see Chapter 9) 2 The class HHSn of all homogeneous halfspaces over Rn (see Chapter 9) 3 The class Bd of all functions deﬁned by (open) balls in Rd Use the\nDudley representation to ﬁgure out the VC-dimension of this class 4",
      "word_count": 248,
      "source_page": 81,
      "start_position": 27298,
      "end_position": 27545,
      "sentences_count": 21
    },
    {
      "chunk_id": 126,
      "text": "7\nNonuniform Learnability\nThe notions of PAC learnability discussed so far in the book allow the sample\nsizes to depend on the accuracy and conﬁdence parameters, but they are uniform\nwith respect to the labeling rule and the underlying data distribution Conse-\nquently, classes that are learnable in that respect are limited (they must have\na ﬁnite VC-dimension, as stated by Theorem 6.7) In this chapter we consider\nmore relaxed, weaker notions of learnability We discuss the usefulness of such\nnotions and provide characterization of the concept classes that are learnable\nusing these deﬁnitions We begin this discussion by deﬁning a notion of “nonuniform learnability” that\nallows the sample size to depend on the hypothesis to which the learner is com-\npared We then provide a characterization of nonuniform learnability and show\nthat nonuniform learnability is a strict relaxation of agnostic PAC learnability We also show that a suﬃcient condition for nonuniform learnability is that H is\na countable union of hypothesis classes, each of which enjoys the uniform con-\nvergence property These results will be proved in Section 7.2 by introducing a\nnew learning paradigm, which is called Structural Risk Minimization (SRM) In\nSection 7.3 we specify the SRM paradigm for countable hypothesis classes, which\nyields the Minimum Description Length (MDL) paradigm The MDL paradigm\ngives a formal justiﬁcation to a philosophical principle of induction called Oc-\ncam’s razor Next, in Section 7.4 we introduce consistency as an even weaker\nnotion of learnability",
      "word_count": 244,
      "source_page": 83,
      "start_position": 27691,
      "end_position": 27934,
      "sentences_count": 11
    },
    {
      "chunk_id": 127,
      "text": "84\nNonuniform Learnability\nwith a low risk compared to the minimal risk achieved by hypotheses in our class\n(in the agnostic case) Therefore, the sample size depends only on the accuracy\nand conﬁdence parameters In nonuniform learnability, however, we allow the\nsample size to be of the form mH(ϵ, δ, h); namely, it depends also on the h with\nwhich we are competing Formally,\ndefinition 7.1\nA hypothesis class H is nonuniformly learnable if there exist a\nlearning algorithm, A, and a function mNUL\nH\n: (0, 1)2×H →N such that, for every\nϵ, δ ∈(0, 1) and for every h ∈H, if m ≥mNUL\nH (ϵ, δ, h) then for every distribution\nD, with probability of at least 1 −δ over the choice of S ∼Dm, it holds that\nLD(A(S)) ≤LD(h) + ϵ At this point it might be useful to recall the deﬁnition of agnostic PAC learn-\nability (Deﬁnition 3.3):\nA hypothesis class H is agnostically PAC learnable if there exist a learning algo-\nrithm, A, and a function mH : (0, 1)2 →N such that, for every ϵ, δ ∈(0, 1) and\nfor every distribution D, if m ≥mH(ϵ, δ), then with probability of at least 1 −δ\nover the choice of S ∼Dm it holds that\nLD(A(S)) ≤min\nh′∈H LD(h′) + ϵ Note that this implies that for every h ∈H\nLD(A(S)) ≤LD(h) + ϵ",
      "word_count": 229,
      "source_page": 84,
      "start_position": 28053,
      "end_position": 28281,
      "sentences_count": 6
    },
    {
      "chunk_id": 128,
      "text": "At this point it might be useful to recall the deﬁnition of agnostic PAC learn-\nability (Deﬁnition 3.3):\nA hypothesis class H is agnostically PAC learnable if there exist a learning algo-\nrithm, A, and a function mH : (0, 1)2 →N such that, for every ϵ, δ ∈(0, 1) and\nfor every distribution D, if m ≥mH(ϵ, δ), then with probability of at least 1 −δ\nover the choice of S ∼Dm it holds that\nLD(A(S)) ≤min\nh′∈H LD(h′) + ϵ Note that this implies that for every h ∈H\nLD(A(S)) ≤LD(h) + ϵ In both types of learnability, we require that the output hypothesis will be\n(ϵ, δ)-competitive with every other hypothesis in the class But the diﬀerence\nbetween these two notions of learnability is the question of whether the sample\nsize m may depend on the hypothesis h to which the error of A(S) is compared Note that that nonuniform learnability is a relaxation of agnostic PAC learn-\nability That is, if a class is agnostic PAC learnable then it is also nonuniformly\nlearnable 7.1.1\nCharacterizing Nonuniform Learnability\nOur goal now is to characterize nonuniform learnability In the previous chapter\nwe have found a crisp characterization of PAC learnable classes, by showing\nthat a class of binary classiﬁers is agnostic PAC learnable if and only if its VC-\ndimension is ﬁnite In the following theorem we ﬁnd a diﬀerent characterization\nfor nonuniform learnable classes for the task of binary classiﬁcation",
      "word_count": 243,
      "source_page": 84,
      "start_position": 28187,
      "end_position": 28429,
      "sentences_count": 9
    },
    {
      "chunk_id": 129,
      "text": "7.2 Structural Risk Minimization\n85\ntheorem 7.3\nLet H be a hypothesis class that can be written as a countable\nunion of hypothesis classes, H = S\nn∈N Hn, where each Hn enjoys the uniform\nconvergence property Then, H is nonuniformly learnable Recall that in Chapter 4 we have shown that uniform convergence is suﬃcient\nfor agnostic PAC learnability Theorem 7.3 generalizes this result to nonuni-\nform learnability The proof of this theorem will be given in the next section by\nintroducing a new learning paradigm We now turn to proving Theorem 7.2 Proof of Theorem 7.2\nFirst assume that H = S\nn∈N Hn where each Hn is ag-\nnostic PAC learnable Using the fundamental theorem of statistical learning, it\nfollows that each Hn has the uniform convergence property Therefore, using\nTheorem 7.3 we obtain that H is nonuniform learnable For the other direction, assume that H is nonuniform learnable using some\nalgorithm A For every n ∈N, let Hn = {h ∈H : mNUL\nH (1/8, 1/7, h) ≤n} Clearly, H = ∪n∈NHn In addition, using the deﬁnition of mNUL\nH\nwe know that\nfor any distribution D that satisﬁes the realizability assumption with respect to\nHn, with probability of at least 6/7 over S ∼Dn we have that LD(A(S)) ≤1/8 Using the fundamental theorem of statistical learning, this implies that the VC-\ndimension of Hn must be ﬁnite, and therefore Hn is agnostic PAC learnable",
      "word_count": 238,
      "source_page": 85,
      "start_position": 28471,
      "end_position": 28708,
      "sentences_count": 14
    },
    {
      "chunk_id": 130,
      "text": "In addition, using the deﬁnition of mNUL\nH\nwe know that\nfor any distribution D that satisﬁes the realizability assumption with respect to\nHn, with probability of at least 6/7 over S ∼Dn we have that LD(A(S)) ≤1/8 Using the fundamental theorem of statistical learning, this implies that the VC-\ndimension of Hn must be ﬁnite, and therefore Hn is agnostic PAC learnable The following example shows that nonuniform learnability is a strict relax-\nation of agnostic PAC learnability; namely, there are hypothesis classes that are\nnonuniform learnable but are not agnostic PAC learnable Example 7.1\nConsider a binary classiﬁcation problem with the instance domain\nbeing X = R For every n ∈N let Hn be the class of polynomial classiﬁers of\ndegree n; namely, Hn is the set of all classiﬁers of the form h(x) = sign(p(x))\nwhere p : R →R is a polynomial of degree n Let H = S\nn∈N Hn Therefore, H is\nthe class of all polynomial classiﬁers over R It is easy to verify that VCdim(H) =\n∞while VCdim(Hn) = n + 1 (see Exercise 12) Hence, H is not PAC learnable,\nwhile on the basis of Theorem 7.3, H is nonuniformly learnable 7.2\nStructural Risk Minimization\nSo far, we have encoded our prior knowledge by specifying a hypothesis class\nH, which we believe includes a good predictor for the learning task at hand Yet another way to express our prior knowledge is by specifying preferences over\nhypotheses within H",
      "word_count": 247,
      "source_page": 85,
      "start_position": 28646,
      "end_position": 28892,
      "sentences_count": 11
    },
    {
      "chunk_id": 131,
      "text": "86\nNonuniform Learnability\nConcretely, let H be a hypothesis class that can be written as H = S\nn∈N Hn For example, H may be the class of all polynomial classiﬁers where each Hn is\nthe class of polynomial classiﬁers of degree n (see Example 7.1) Assume that for\neach n, the class Hn enjoys the uniform convergence property (see Deﬁnition 4.3\nin Chapter 4) with a sample complexity function mUC\nHn(ϵ, δ) Let us also deﬁne\nthe function ϵn : N × (0, 1) →(0, 1) by\nϵn(m, δ) = min{ϵ ∈(0, 1) : m\nUC\nHn(ϵ, δ) ≤m} (7.1)\nIn words, we have a ﬁxed sample size m, and we are interested in the lowest\npossible upper bound on the gap between empirical and true risks achievable by\nusing a sample of m examples From the deﬁnitions of uniform convergence and ϵn, it follows that for every\nm and δ, with probability of at least 1 −δ over the choice of S ∼Dm we have\nthat\n∀h ∈Hn,\n|LD(h) −LS(h)| ≤ϵn(m, δ) (7.2)\nLet w : N →[0, 1] be a function such that P∞\nn=1 w(n) ≤1 We refer to w as\na weight function over the hypothesis classes H1, H2, Such a weight function\ncan reﬂect the importance that the learner attributes to each hypothesis class,\nor some measure of the complexity of diﬀerent hypothesis classes",
      "word_count": 230,
      "source_page": 86,
      "start_position": 28978,
      "end_position": 29207,
      "sentences_count": 9
    },
    {
      "chunk_id": 132,
      "text": "We refer to w as\na weight function over the hypothesis classes H1, H2, Such a weight function\ncan reﬂect the importance that the learner attributes to each hypothesis class,\nor some measure of the complexity of diﬀerent hypothesis classes If H is a ﬁnite\nunion of N hypothesis classes, one can simply assign the same weight of 1/N to\nall hypothesis classes This equal weighting corresponds to no a priori preference\nto any hypothesis class Of course, if one believes (as prior knowledge) that a\ncertain hypothesis class is more likely to contain the correct target function,\nthen it should be assigned a larger weight, reﬂecting this prior knowledge When\nH is a (countable) inﬁnite union of hypothesis classes, a uniform weighting is\nnot possible but many other weighting schemes may work For example, one can\nchoose w(n) =\n6\nπ2n2 or w(n) = 2−n Later in this chapter we will provide another\nconvenient way to deﬁne weighting functions using description languages The SRM rule follows a “bound minimization” approach This means that\nthe goal of the paradigm is to ﬁnd a hypothesis that minimizes a certain upper\nbound on the true risk The bound that the SRM rule wishes to minimize is\ngiven in the following theorem theorem 7.4\nLet w : N →[0, 1] be a function such that P∞\nn=1 w(n) ≤1",
      "word_count": 226,
      "source_page": 86,
      "start_position": 29168,
      "end_position": 29393,
      "sentences_count": 12
    },
    {
      "chunk_id": 133,
      "text": "The SRM paradigm searches for h that minimizes this bound, as formalized\nin the following pseudocode:\nStructural Risk Minimization (SRM)\nprior knowledge:\nH = S\nn Hn where Hn has uniform convergence with mUC\nHn\nw : N →[0, 1] where P\nn w(n) ≤1\ndeﬁne: ϵn as in Equation (7.1) ; n(h) as in Equation (7.4)\ninput: training set S ∼Dm, conﬁdence δ\noutput: h ∈argminh∈H\n\u0002\nLS(h) + ϵn(h)(m, w(n(h)) · δ)\n\u0003\nUnlike the ERM paradigm discussed in previous chapters, we no longer just care\nabout the empirical risk, LS(h), but we are willing to trade some of our bias\ntoward low empirical risk with a bias toward classes for which ϵn(h)(m, w(n(h))·δ)\nis smaller, for the sake of a smaller estimation error Next we show that the SRM paradigm can be used for nonuniform learning\nof every class, which is a countable union of uniformly converging hypothesis\nclasses theorem 7.5\nLet H be a hypothesis class such that H = S\nn∈N Hn, where\neach Hn has the uniform convergence property with sample complexity mUC\nHn Let\nw : N →[0, 1] be such that w(n) =\n6\nn2π2 Then, H is nonuniformly learnable\nusing the SRM rule with rate\nm\nNUL\nH (ϵ, δ, h) ≤m\nUC\nHn(h)\n\u0010\nϵ/2 ,\n6δ\n(πn(h))2\n\u0011\n.",
      "word_count": 220,
      "source_page": 87,
      "start_position": 29626,
      "end_position": 29845,
      "sentences_count": 5
    },
    {
      "chunk_id": 134,
      "text": "88\nNonuniform Learnability\nProof\nLet A be the SRM algorithm with respect to the weighting function w For every h ∈H, ϵ, and δ, let m ≥mUC\nHn(h)(ϵ, w(n(h))δ) Using the fact that\nP\nn w(n) = 1, we can apply Theorem 7.4 to get that, with probability of at least\n1 −δ over the choice of S ∼Dm, we have that for every h′ ∈H,\nLD(h′) ≤LS(h′) + ϵn(h′)(m, w(n(h′))δ) The preceding holds in particular for the hypothesis A(S) returned by the SRM\nrule By the deﬁnition of SRM we obtain that\nLD(A(S)) ≤min\nh′\n\u0002\nLS(h′) + ϵn(h′)(m, w(n(h′))δ)\n\u0003\n≤LS(h) + ϵn(h)(m, w(n(h))δ) Finally, if m ≥mUC\nHn(h)(ϵ/2, w(n(h))δ) then clearly ϵn(h)(m, w(n(h))δ) ≤ϵ/2 In\naddition, from the uniform convergence property of each Hn we have that with\nprobability of more than 1 −δ,\nLS(h) ≤LD(h) + ϵ/2 Combining all the preceding we obtain that LD(A(S)) ≤LD(h) + ϵ, which con-\ncludes our proof Note that the previous theorem also proves Theorem 7.3 Remark 7.2 (No-Free-Lunch for Nonuniform Learnability)\nWe have shown that\nany countable union of classes of ﬁnite VC-dimension is nonuniformly learnable It turns out that, for any inﬁnite domain set, X, the class of all binary valued\nfunctions over X is not a countable union of classes of ﬁnite VC-dimension We\nleave the proof of this claim as a (nontrivial) exercise (see Exercise 5)",
      "word_count": 230,
      "source_page": 88,
      "start_position": 29846,
      "end_position": 30075,
      "sentences_count": 12
    },
    {
      "chunk_id": 135,
      "text": "It turns out that, for any inﬁnite domain set, X, the class of all binary valued\nfunctions over X is not a countable union of classes of ﬁnite VC-dimension We\nleave the proof of this claim as a (nontrivial) exercise (see Exercise 5) It follows\nthat, in some sense, the no free lunch theorem holds for nonuniform learning\nas well: namely, whenever the domain is not ﬁnite, there exists no nonuniform\nlearner with respect to the class of all deterministic binary classiﬁers (although\nfor each such classiﬁer there exists a trivial algorithm that learns it – ERM with\nrespect to the hypothesis class that contains only this classiﬁer) It is interesting to compare the nonuniform learnability result given in The-\norem 7.5 to the task of agnostic PAC learning any speciﬁc Hn separately The\nprior knowledge, or bias, of a nonuniform learner for H is weaker – it is searching\nfor a model throughout the entire class H, rather than being focused on one spe-\nciﬁc Hn The cost of this weakening of prior knowledge is the increase in sample\ncomplexity needed to compete with any speciﬁc h ∈Hn For a concrete evalua-\ntion of this gap, consider the task of binary classiﬁcation with the zero-one loss Assume that for all n, VCdim(Hn) = n",
      "word_count": 215,
      "source_page": 88,
      "start_position": 30033,
      "end_position": 30247,
      "sentences_count": 8
    },
    {
      "chunk_id": 136,
      "text": "7.3 Minimum Description Length and Occam’s Razor\n89\nthe index of the ﬁrst class in which h resides That cost increases with the index\nof the class, which can be interpreted as reﬂecting the value of knowing a good\npriority order on the hypotheses in H 7.3\nMinimum Description Length and Occam’s Razor\nLet H be a countable hypothesis class Then, we can write H as a countable\nunion of singleton classes, namely, H = S\nn∈N{hn} By Hoeﬀding’s inequality\n(Lemma 4.5), each singleton class has the uniform convergence property with\nrate mUC(ϵ, δ) =\nlog(2/δ)\n2ϵ2 Therefore, the function ϵn given in Equation (7.1)\nbecomes ϵn(m, δ) =\nq\nlog(2/δ)\n2m\nand the SRM rule becomes\nargmin\nhn∈H\n\"\nLS(h) +\nr\n−log(w(n)) + log(2/δ)\n2m\n# Equivalently, we can think of w as a function from H to [0, 1], and then the SRM\nrule becomes\nargmin\nh∈H\n\"\nLS(h) +\nr\n−log(w(h)) + log(2/δ)\n2m\n# It follows that in this case, the prior knowledge is solely determined by the weight\nwe assign to each hypothesis We assign higher weights to hypotheses that we\nbelieve are more likely to be the correct one, and in the learning algorithm we\nprefer hypotheses that have higher weights In this section we discuss a particular convenient way to deﬁne a weight func-\ntion over H, which is derived from the length of descriptions given to hypotheses",
      "word_count": 236,
      "source_page": 89,
      "start_position": 30314,
      "end_position": 30549,
      "sentences_count": 10
    },
    {
      "chunk_id": 137,
      "text": "We assign higher weights to hypotheses that we\nbelieve are more likely to be the correct one, and in the learning algorithm we\nprefer hypotheses that have higher weights In this section we discuss a particular convenient way to deﬁne a weight func-\ntion over H, which is derived from the length of descriptions given to hypotheses Having a hypothesis class, one can wonder about how we describe, or represent,\neach hypothesis in the class We naturally ﬁx some description language This\ncan be English, or a programming language, or some set of mathematical formu-\nlas In any of these languages, a description consists of ﬁnite strings of symbols\n(or characters) drawn from some ﬁxed alphabet We shall now formalize these\nnotions Let H be the hypothesis class we wish to describe Fix some ﬁnite set Σ\nof symbols (or “characters”), which we call the alphabet For concreteness, we\nlet Σ = {0, 1} A string is a ﬁnite sequence of symbols from Σ; for example,\nσ = (0, 1, 1, 1, 0) is a string of length 5 We denote by |σ| the length of a string The set of all ﬁnite length strings is denoted Σ∗ A description language for H\nis a function d : H →Σ∗, mapping each member h of H to a string d(h) d(h) is\ncalled “the description of h,” and its length is denoted by |h|",
      "word_count": 234,
      "source_page": 89,
      "start_position": 30493,
      "end_position": 30726,
      "sentences_count": 15
    },
    {
      "chunk_id": 138,
      "text": "90\nNonuniform Learnability\nlemma 7.6 (Kraft Inequality)\nIf S ⊆{0, 1}∗is a preﬁx-free set of strings, then\nX\nσ∈S\n1\n2|σ| ≤1 Proof\nDeﬁne a probability distribution over the members of S as follows: Re-\npeatedly toss an unbiased coin, with faces labeled 0 and 1, until the sequence\nof outcomes is a member of S; at that point, stop For each σ ∈S, let P(σ)\nbe the probability that this process generates the string σ Note that since S is\npreﬁx-free, for every σ ∈S, if the coin toss outcomes follow the bits of σ then\nwe will stop only once the sequence of outcomes equals σ We therefore get that,\nfor every σ ∈S, P(σ) =\n1\n2|σ| Since probabilities add up to at most 1, our proof\nis concluded In light of Kraft’s inequality, any preﬁx-free description language of a hypoth-\nesis class, H, gives rise to a weighting function w over that hypothesis class – we\nwill simply set w(h) =\n1\n2|h| This observation immediately yields the following:\ntheorem 7.7\nLet H be a hypothesis class and let d : H →{0, 1}∗be a preﬁx-\nfree description language for H Then, for every sample size, m, every conﬁdence\nparameter, δ > 0, and every probability distribution, D, with probability greater\nthan 1 −δ over the choice of S ∼Dm we have that,\n∀h ∈H,\nLD(h) ≤LS(h) +\nr\n|h| + ln(2/δ)\n2m\n,\nwhere |h| is the length of d(h)",
      "word_count": 245,
      "source_page": 90,
      "start_position": 30778,
      "end_position": 31022,
      "sentences_count": 9
    },
    {
      "chunk_id": 139,
      "text": "This observation immediately yields the following:\ntheorem 7.7\nLet H be a hypothesis class and let d : H →{0, 1}∗be a preﬁx-\nfree description language for H Then, for every sample size, m, every conﬁdence\nparameter, δ > 0, and every probability distribution, D, with probability greater\nthan 1 −δ over the choice of S ∼Dm we have that,\n∀h ∈H,\nLD(h) ≤LS(h) +\nr\n|h| + ln(2/δ)\n2m\n,\nwhere |h| is the length of d(h) Proof\nChoose w(h) = 1/2|h|, apply Theorem 7.4 with ϵn(m, δ) =\nq\nln(2/δ)\n2m\n, and\nnote that ln(2|h|) = |h| ln(2) < |h| As was the case with Theorem 7.4, this result suggests a learning paradigm\nfor H – given a training set, S, search for a hypothesis h ∈H that minimizes\nthe bound, LS(h) +\nq\n|h|+ln(2/δ)\n2m In particular, it suggests trading oﬀempirical\nrisk for saving description length This yields the Minimum Description Length\nlearning paradigm Minimum Description Length (MDL)\nprior knowledge:\nH is a countable hypothesis class\nH is described by a preﬁx-free language over {0, 1}\nFor every h ∈H, |h| is the length of the representation of h\ninput: A training set S ∼Dm, conﬁdence δ\noutput: h ∈argminh∈H\n\u0014\nLS(h) +\nq\n|h|+ln(2/δ)\n2m\n\u0015\nExample 7.3\nLet H be the class of all predictors that can be implemented using\nsome programming language, say, C++ Let us represent each program using the",
      "word_count": 238,
      "source_page": 90,
      "start_position": 30945,
      "end_position": 31182,
      "sentences_count": 8
    },
    {
      "chunk_id": 140,
      "text": "7.3 Minimum Description Length and Occam’s Razor\n91\nbinary string obtained by running the gzip command on the program (this yields\na preﬁx-free description language over the alphabet {0, 1}) Then, |h| is simply\nthe length (in bits) of the output of gzip when running on the C++ program\ncorresponding to h 7.3.1\nOccam’s Razor\nTheorem 7.7 suggests that, having two hypotheses sharing the same empirical\nrisk, the true risk of the one that has shorter description can be bounded by a\nlower value Thus, this result can be viewed as conveying a philosophical message:\nA short explanation (that is, a hypothesis that has a short length) tends to be more valid\nthan a long explanation This is a well known principle, called Occam’s razor, after William of Ockham,\na 14th-century English logician, who is believed to have been the ﬁrst to phrase\nit explicitly Here, we provide one possible justiﬁcation to this principle The\ninequality of Theorem 7.7 shows that the more complex a hypothesis h is (in the\nsense of having a longer description), the larger the sample size it has to ﬁt to\nguarantee that it has a small true risk, LD(h) At a second glance, our Occam razor claim might seem somewhat problematic In the context in which the Occam razor principle is usually invoked in science,\nthe language according to which complexity is measured is a natural language,\nwhereas here we may consider any arbitrary abstract description language",
      "word_count": 243,
      "source_page": 91,
      "start_position": 31183,
      "end_position": 31425,
      "sentences_count": 9
    },
    {
      "chunk_id": 141,
      "text": "At a second glance, our Occam razor claim might seem somewhat problematic In the context in which the Occam razor principle is usually invoked in science,\nthe language according to which complexity is measured is a natural language,\nwhereas here we may consider any arbitrary abstract description language As-\nsume that we have two hypotheses such that |h′| is much smaller than |h| By\nthe preceding result, if both have the same error on a given training set, S, then\nthe true error of h may be much higher than the true error of h′, so one should\nprefer h′ over h However, we could have chosen a diﬀerent description language,\nsay, one that assigns a string of length 3 to h and a string of length 100000 to h′ Suddenly it looks as if one should prefer h over h′ But these are the same h and\nh′ for which we argued two sentences ago that h′ should be preferable Where is\nthe catch here Indeed, there is no inherent generalizability diﬀerence between hypotheses The crucial aspect here is the dependency order between the initial choice of\nlanguage (or, preference over hypotheses) and the training set As we know from\nthe basic Hoeﬀding’s bound (Equation (4.2)), if we commit to any hypothesis be-\nfore seeing the data, then we are guaranteed a rather small estimation error term\nLD(h) ≤LS(h) +\nq\nln(2/δ)\n2m",
      "word_count": 234,
      "source_page": 91,
      "start_position": 31378,
      "end_position": 31611,
      "sentences_count": 11
    },
    {
      "chunk_id": 142,
      "text": "92\nNonuniform Learnability\n7.4\nOther Notions of Learnability – Consistency\nThe notion of learnability can be further relaxed by allowing the needed sample\nsizes to depend not only on ϵ, δ, and h but also on the underlying data-generating\nprobability distribution D (that is used to generate the training sample and to\ndetermine the risk) This type of performance guarantee is captured by the notion\nof consistency1 of a learning rule definition 7.8 (Consistency)\nLet Z be a domain set, let P be a set of\nprobability distributions over Z, and let H be a hypothesis class A learn-\ning rule A is consistent with respect to H and P if there exists a function\nmCON\nH\n: (0, 1)2 × H × P →N such that, for every ϵ, δ ∈(0, 1), every h ∈H, and\nevery D ∈P, if m ≥mNUL\nH (ϵ, δ, h, D) then with probability of at least 1 −δ over\nthe choice of S ∼Dm it holds that\nLD(A(S)) ≤LD(h) + ϵ If P is the set of all distributions,2 we say that A is universally consistent with\nrespect to H The notion of consistency is, of course, a relaxation of our previous notion\nof nonuniform learnability Clearly if an algorithm nonuniformly learns a class\nH it is also universally consistent for that class The relaxation is strict in the\nsense that there are consistent learning rules that are not successful nonuniform\nlearners",
      "word_count": 240,
      "source_page": 92,
      "start_position": 31685,
      "end_position": 31924,
      "sentences_count": 8
    },
    {
      "chunk_id": 143,
      "text": "Clearly if an algorithm nonuniformly learns a class\nH it is also universally consistent for that class The relaxation is strict in the\nsense that there are consistent learning rules that are not successful nonuniform\nlearners For example, the algorithm Memorize deﬁned in Example 7.4 later is\nuniversally consistent for the class of all binary classiﬁers over N However, as\nwe have argued before, this class is not nonuniformly learnable Example 7.4\nConsider the classiﬁcation prediction algorithm Memorize deﬁned\nas follows The algorithm memorizes the training examples, and, given a test\npoint x, it predicts the majority label among all labeled instances of x that exist\nin the training sample (and some ﬁxed default label if no instance of x appears\nin the training set) It is possible to show (see Exercise 6) that the Memorize\nalgorithm is universally consistent for every countable domain X and a ﬁnite\nlabel set Y (w.r.t the zero-one loss) Intuitively, it is not obvious that the Memorize algorithm should be viewed as a\nlearner, since it lacks the aspect of generalization, namely, of using observed data\nto predict the labels of unseen examples The fact that Memorize is a consistent\nalgorithm for the class of all functions over any countable domain set therefore\nraises doubt about the usefulness of consistency guarantees",
      "word_count": 217,
      "source_page": 92,
      "start_position": 31889,
      "end_position": 32105,
      "sentences_count": 10
    },
    {
      "chunk_id": 144,
      "text": "7.5 Discussing the Diﬀerent Notions of Learnability\n93\nwhich led to overﬁtting, is in fact the Memorize algorithm In the next section\nwe discuss the signiﬁcance of the diﬀerent notions of learnability and revisit the\nNo-Free-Lunch theorem in light of the diﬀerent deﬁnitions of learnability 7.5\nDiscussing the Diﬀerent Notions of Learnability\nWe have given three deﬁnitions of learnability and we now discuss their useful-\nness As is usually the case, the usefulness of a mathematical deﬁnition depends\non what we need it for We therefore list several possible goals that we aim to\nachieve by deﬁning learnability and discuss the usefulness of the diﬀerent deﬁni-\ntions in light of these goals What Is the Risk of the Learned Hypothesis The ﬁrst possible goal of deriving performance guarantees on a learning algo-\nrithm is bounding the risk of the output predictor Here, both PAC learning\nand nonuniform learning give us an upper bound on the true risk of the learned\nhypothesis based on its empirical risk Consistency guarantees do not provide\nsuch a bound However, it is always possible to estimate the risk of the output\npredictor using a validation set (as will be described in Chapter 11) How Many Examples Are Required to Be as Good as the Best Hypothesis\nin H When approaching a learning problem, a natural question is how many exam-\nples we need to collect in order to learn it Here, PAC learning gives a crisp\nanswer",
      "word_count": 242,
      "source_page": 93,
      "start_position": 32183,
      "end_position": 32424,
      "sentences_count": 13
    },
    {
      "chunk_id": 145,
      "text": "When approaching a learning problem, a natural question is how many exam-\nples we need to collect in order to learn it Here, PAC learning gives a crisp\nanswer However, for both nonuniform learning and consistency, we do not know\nin advance how many examples are required to learn H In nonuniform learning\nthis number depends on the best hypothesis in H, and in consistency it also\ndepends on the underlying distribution In this sense, PAC learning is the only\nuseful deﬁnition of learnability On the ﬂip side, one should keep in mind that\neven if the estimation error of the predictor we learn is small, its risk may still\nbe large if H has a large approximation error So, for the question “How many\nexamples are required to be as good as the Bayes optimal predictor?” even PAC\nguarantees do not provide us with a crisp answer This reﬂects the fact that the\nusefulness of PAC learning relies on the quality of our prior knowledge PAC guarantees also help us to understand what we should do next if our\nlearning algorithm returns a hypothesis with a large risk, since we can bound\nthe part of the error that stems from estimation error and therefore know how\nmuch of the error is attributed to approximation error If the approximation error\nis large, we know that we should use a diﬀerent hypothesis class Similarly, if a\nnonuniform algorithm fails, we can consider a diﬀerent weighting function over\n(subsets of) hypotheses",
      "word_count": 249,
      "source_page": 93,
      "start_position": 32396,
      "end_position": 32644,
      "sentences_count": 11
    },
    {
      "chunk_id": 146,
      "text": "94\nNonuniform Learnability\nerror term, we do not know how many more examples are needed to make the\nestimation error small How to Learn How to Express Prior Knowledge Maybe the most useful aspect of the theory of learning is in providing an answer\nto the question of “how to learn.” The deﬁnition of PAC learning yields the\nlimitation of learning (via the No-Free-Lunch theorem) and the necessity of prior\nknowledge It gives us a crisp way to encode prior knowledge by choosing a\nhypothesis class, and once this choice is made, we have a generic learning rule –\nERM The deﬁnition of nonuniform learnability also yields a crisp way to encode\nprior knowledge by specifying weights over (subsets of) hypotheses of H Once\nthis choice is made, we again have a generic learning rule – SRM The SRM rule\nis also advantageous in model selection tasks, where prior knowledge is partial We elaborate on model selection in Chapter 11 and here we give a brief example Consider the problem of ﬁtting a one dimensional polynomial to data; namely,\nour goal is to learn a function, h : R →R, and as prior knowledge we consider\nthe hypothesis class of polynomials",
      "word_count": 201,
      "source_page": 94,
      "start_position": 32680,
      "end_position": 32880,
      "sentences_count": 10
    },
    {
      "chunk_id": 147,
      "text": "We elaborate on model selection in Chapter 11 and here we give a brief example Consider the problem of ﬁtting a one dimensional polynomial to data; namely,\nour goal is to learn a function, h : R →R, and as prior knowledge we consider\nthe hypothesis class of polynomials However, we might be uncertain regarding\nwhich degree d would give the best results for our data set: A small degree might\nnot ﬁt the data well (i.e., it will have a large approximation error), whereas a\nhigh degree might lead to overﬁtting (i.e., it will have a large estimation error) In the following we depict the result of ﬁtting a polynomial of degrees 2, 3, and\n10 to the same training set degree 2\ndegree 3\ndegree 10\nIt is easy to see that the empirical risk decreases as we enlarge the degree Therefore, if we choose H to be the class of all polynomials up to degree 10 then\nthe ERM rule with respect to this class would output a 10 degree polynomial\nand would overﬁt On the other hand, if we choose too small a hypothesis class,\nsay, polynomials up to degree 2, then the ERM would suﬀer from underﬁtting\n(i.e., a large approximation error)",
      "word_count": 207,
      "source_page": 94,
      "start_position": 32832,
      "end_position": 33038,
      "sentences_count": 7
    },
    {
      "chunk_id": 148,
      "text": "Therefore, if we choose H to be the class of all polynomials up to degree 10 then\nthe ERM rule with respect to this class would output a 10 degree polynomial\nand would overﬁt On the other hand, if we choose too small a hypothesis class,\nsay, polynomials up to degree 2, then the ERM would suﬀer from underﬁtting\n(i.e., a large approximation error) In contrast, we can use the SRM rule on the\nset of all polynomials, while ordering subsets of H according to their degree, and\nthis will yield a 3rd degree polynomial since the combination of its empirical\nrisk and the bound on its estimation error is the smallest In other words, the\nSRM rule enables us to select the right model on the basis of the data itself The\nprice we pay for this ﬂexibility (besides a slight increase of the estimation error\nrelative to PAC learning w.r.t the optimal degree) is that we do not know in",
      "word_count": 162,
      "source_page": 94,
      "start_position": 32975,
      "end_position": 33136,
      "sentences_count": 6
    },
    {
      "chunk_id": 149,
      "text": "7.5 Discussing the Diﬀerent Notions of Learnability\n95\nadvance how many examples are needed to compete with the best hypothesis in\nH Unlike the notions of PAC learnability and nonuniform learnability, the deﬁni-\ntion of consistency does not yield a natural learning paradigm or a way to encode\nprior knowledge In fact, in many cases there is no need for prior knowledge at\nall For example, we saw that even the Memorize algorithm, which intuitively\nshould not be called a learning algorithm, is a consistent algorithm for any class\ndeﬁned over a countable domain and a ﬁnite label set This hints that consistency\nis a very weak requirement Which Learning Algorithm Should We Prefer One may argue that even though consistency is a weak requirement, it is desirable\nthat a learning algorithm will be consistent with respect to the set of all functions\nfrom X to Y, which gives us a guarantee that for enough training examples, we\nwill always be as good as the Bayes optimal predictor Therefore, if we have\ntwo algorithms, where one is consistent and the other one is not consistent, we\nshould prefer the consistent algorithm However, this argument is problematic for\ntwo reasons First, maybe it is the case that for most “natural” distributions we\nwill observe in practice that the sample complexity of the consistent algorithm\nwill be so large so that in every practical situation we will not obtain enough\nexamples to enjoy this guarantee",
      "word_count": 243,
      "source_page": 95,
      "start_position": 33137,
      "end_position": 33379,
      "sentences_count": 10
    },
    {
      "chunk_id": 150,
      "text": "However, this argument is problematic for\ntwo reasons First, maybe it is the case that for most “natural” distributions we\nwill observe in practice that the sample complexity of the consistent algorithm\nwill be so large so that in every practical situation we will not obtain enough\nexamples to enjoy this guarantee Second, it is not very hard to make any PAC\nor nonuniform learner consistent with respect to the class of all functions from\nX to Y Concretely, consider a countable domain, X, a ﬁnite label set Y, and\na hypothesis class, H, of functions from X to Y We can make any nonuniform\nlearner for H be consistent with respect to the class of all classiﬁers from X to Y\nusing the following simple trick: Upon receiving a training set, we will ﬁrst run\nthe nonuniform learner over the training set, and then we will obtain a bound\non the true risk of the learned predictor If this bound is small enough we are\ndone Otherwise, we revert to the Memorize algorithm This simple modiﬁcation\nmakes the algorithm consistent with respect to all functions from X to Y Since\nit is easy to make any algorithm consistent, it may not be wise to prefer one\nalgorithm over the other just because of consistency considerations 7.5.1\nThe No-Free-Lunch Theorem Revisited\nRecall that the No-Free-Lunch theorem (Theorem 5.1 from Chapter 5) implies\nthat no algorithm can learn the class of all classiﬁers over an inﬁnite domain",
      "word_count": 246,
      "source_page": 95,
      "start_position": 33328,
      "end_position": 33573,
      "sentences_count": 10
    },
    {
      "chunk_id": 151,
      "text": "96\nNonuniform Learnability\nwill get a sample of m i.i.d training examples, labeled by h⋆, then A is likely to\nreturn a classiﬁer with a larger error The consistency of Memorize implies the following: For every distribution over\nX and a labeling function h⋆: X →Y, there exists a training set size m (that\ndepends on the distribution and on h⋆) such that if Memorize receives at least\nm examples it is likely to return a classiﬁer with a small error We see that in the No-Free-Lunch theorem, we ﬁrst ﬁx the training set size,\nand then ﬁnd a distribution and a labeling function that are bad for this training\nset size In contrast, in consistency guarantees, we ﬁrst ﬁx the distribution and\nthe labeling function, and only then do we ﬁnd a training set size that suﬃces\nfor learning this particular distribution and labeling function 7.6\nSummary\nWe introduced nonuniform learnability as a relaxation of PAC learnability and\nconsistency as a relaxation of nonuniform learnability This means that even\nclasses of inﬁnite VC-dimension can be learnable, in some weaker sense of learn-\nability We discussed the usefulness of the diﬀerent deﬁnitions of learnability For hypothesis classes that are countable, we can apply the Minimum Descrip-\ntion Length scheme, where hypotheses with shorter descriptions are preferred,\nfollowing the principle of Occam’s razor",
      "word_count": 222,
      "source_page": 96,
      "start_position": 33666,
      "end_position": 33887,
      "sentences_count": 9
    },
    {
      "chunk_id": 152,
      "text": "We discussed the usefulness of the diﬀerent deﬁnitions of learnability For hypothesis classes that are countable, we can apply the Minimum Descrip-\ntion Length scheme, where hypotheses with shorter descriptions are preferred,\nfollowing the principle of Occam’s razor An interesting example is the hypothe-\nsis class of all predictors we can implement in C++ (or any other programming\nlanguage), which we can learn (nonuniformly) using the MDL scheme Arguably, the class of all predictors we can implement in C++ is a powerful\nclass of functions and probably contains all that we can hope to learn in prac-\ntice The ability to learn this class is impressive, and, seemingly, this chapter\nshould have been the last chapter of this book This is not the case, because of\nthe computational aspect of learning: that is, the runtime needed to apply the\nlearning rule For example, to implement the MDL paradigm with respect to\nall C++ programs, we need to perform an exhaustive search over all C++ pro-\ngrams, which will take forever Even the implementation of the ERM paradigm\nwith respect to all C++ programs of description length at most 1000 bits re-\nquires an exhaustive search over 21000 hypotheses While the sample complexity\nof learning this class is just 1000+log(2/δ)\nϵ2\n, the runtime is ≥21000 This is a huge\nnumber – much larger than the number of atoms in the visible universe In the\nnext chapter we formally deﬁne the computational complexity of learning",
      "word_count": 244,
      "source_page": 96,
      "start_position": 33850,
      "end_position": 34093,
      "sentences_count": 11
    },
    {
      "chunk_id": 153,
      "text": "7.7 Bibliographic Remarks\n97\n7.7\nBibliographic Remarks\nOur deﬁnition of nonuniform learnability is related to the deﬁnition of an Occam-\nalgorithm in Blumer, Ehrenfeucht, Haussler & Warmuth (1987) The concept of\nSRM is due to (Vapnik & Chervonenkis 1974, Vapnik 1995) The concept of MDL\nis due to (Rissanen 1978, Rissanen 1983) The relation between SRM and MDL\nis discussed in Vapnik (1995) These notions are also closely related to the notion\nof regularization (e.g Tikhonov (1943)) We will elaborate on regularization in\nthe second part of this book The notion of consistency of estimators dates back to Fisher (1922) Our pre-\nsentation of consistency follows Steinwart & Christmann (2008), who also derived\nseveral no-free-lunch theorems 7.8\nExercises\n1 Prove that for any ﬁnite class H, and any description language d : H →\n{0, 1}∗, the VC-dimension of H is at most 2 sup{|d(h)| : h ∈H} – the maxi-\nmum description length of a predictor in H Furthermore, if d is a preﬁx-free\ndescription then VCdim(H) ≤sup{|d(h)| : h ∈H} 2 Let H = {hn : n ∈N} be an inﬁnite countable hypothesis class for binary\nclassiﬁcation Show that it is impossible to assign weights to the hypotheses\nin H such that\n• H could be learnt nonuniformly using these weights That is, the weighting\nfunction w : H →[0, 1] should satisfy the condition P\nh∈H w(h) ≤1 • The weights would be monotonically nondecreasing That is, if i < j, then\nw(hi) ≤w(hj) 3",
      "word_count": 248,
      "source_page": 97,
      "start_position": 34117,
      "end_position": 34364,
      "sentences_count": 19
    },
    {
      "chunk_id": 154,
      "text": "That is, if i < j, then\nw(hi) ≤w(hj) 3 • Consider a hypothesis class H = S∞\nn=1 Hn, where for every n ∈N, Hn is\nﬁnite Find a weighting function w : H →[0, 1] such that P\nh∈H w(h) ≤\n1 and so that for all h ∈H, w(h) is determined by n(h) = min{n : h ∈\nHn} and by |Hn(h)| • (*) Deﬁne such a function w when for all n Hn is countable (possibly\ninﬁnite) 4 Let H be some hypothesis class For any h ∈H, let |h| denote the description\nlength of h, according to some ﬁxed description language Consider the MDL\nlearning paradigm in which the algorithm returns:\nhS ∈arg min\nh∈H\n\"\nLS(h) +\nr\n|h| + ln(2/δ)\n2m\n#\n,\nwhere S is a sample of size m For any B > 0, let HB = {h ∈H : |h| ≤B},\nand deﬁne\nh∗\nB = arg min\nh∈HB LD(h).",
      "word_count": 160,
      "source_page": 97,
      "start_position": 34355,
      "end_position": 34514,
      "sentences_count": 10
    },
    {
      "chunk_id": 155,
      "text": "98\nNonuniform Learnability\nProve a bound on LD(hS)−LD(h∗\nB) in terms of B, the conﬁdence parameter\nδ, and the size of the training set m • Note: Such bounds are known as oracle inequalities in the literature: We\nwish to estimate how good we are compared to a reference classiﬁer (or\n“oracle”) h∗\nB 5 In this question we wish to show a No-Free-Lunch result for nonuniform learn-\nability: namely, that, over any inﬁnite domain, the class of all functions is not\nlearnable even under the relaxed nonuniform variation of learning Recall that an algorithm, A, nonuniformly learns a hypothesis class H if\nthere exists a function mNUL\nH\n: (0, 1)2 ×H →N such that, for every ϵ, δ ∈(0, 1)\nand for every h ∈H, if m ≥mNUL\nH (ϵ, δ, h) then for every distribution D, with\nprobability of at least 1 −δ over the choice of S ∼Dm, it holds that\nLD(A(S)) ≤LD(h) + ϵ If such an algorithm exists then we say that H is nonuniformly learnable 1 Let A be a nonuniform learner for a class H For each n ∈N deﬁne HA\nn =\n{h ∈H : mNUL(0.1, 0.1, h) ≤n} Prove that each such class Hn has a ﬁnite\nVC-dimension 2 Prove that if a class H is nonuniformly learnable then there are classes Hn\nso that H = S\nn∈N Hn and, for every n ∈N, VCdim(Hn) is ﬁnite 3 Let H be a class that shatters an inﬁnite set",
      "word_count": 249,
      "source_page": 98,
      "start_position": 34515,
      "end_position": 34763,
      "sentences_count": 14
    },
    {
      "chunk_id": 156,
      "text": "3 Let H be a class that shatters an inﬁnite set Then, for every sequence\nof classes (Hn : n ∈N) such that H = S\nn∈N Hn, there exists some n for\nwhich VCdim(Hn) = ∞ Hint: Given a class H that shatters some inﬁnite set K, and a sequence of\nclasses (Hn : n ∈N), each having a ﬁnite VC-dimension, start by deﬁning\nsubsets Kn ⊆K such that, for all n, |Kn| > VCdim(Hn) and for any\nn ̸= m, Kn ∩Km = ∅ Now, pick for each such Kn a function fn : Kn →\n{0, 1} so that no h ∈Hn agrees with fn on the domain Kn Finally, deﬁne\nf : X →{0, 1} by combining these fn’s and prove that f ∈\n\u0000H \\ S\nn∈N Hn\n\u0001 4 Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable 5 Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable 6 In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain Let X be a countable domain and let D be a probability distribution over X 1 Let {xi : i ∈N} be an enumeration of the elements of X so that for all\ni ≤j, D({xi}) ≤D({xj})",
      "word_count": 242,
      "source_page": 98,
      "start_position": 34753,
      "end_position": 34994,
      "sentences_count": 15
    },
    {
      "chunk_id": 157,
      "text": "8\nThe Runtime of Learning\nSo far in the book we have studied the statistical perspective of learning, namely,\nhow many samples are needed for learning In other words, we focused on the\namount of information learning requires However, when considering automated\nlearning, computational resources also play a major role in determining the com-\nplexity of a task: that is, how much computation is involved in carrying out a\nlearning task Once a suﬃcient training sample is available to the learner, there\nis some computation to be done to extract a hypothesis or ﬁgure out the label of\na given test instance These computational resources are crucial in any practical\napplication of machine learning We refer to these two types of resources as the\nsample complexity and the computational complexity In this chapter, we turn\nour attention to the computational complexity of learning The computational complexity of learning should be viewed in the wider con-\ntext of the computational complexity of general algorithmic tasks This area has\nbeen extensively investigated; see, for example, (Sipser 2006) The introductory\ncomments that follow summarize the basic ideas of that general theory that are\nmost relevant to our discussion The actual runtime (in seconds) of an algorithm depends on the speciﬁc ma-\nchine the algorithm is being implemented on (e.g., what the clock rate of the\nmachine’s CPU is) To avoid dependence on the speciﬁc machine, it is common\nto analyze the runtime of algorithms in an asymptotic sense",
      "word_count": 245,
      "source_page": 100,
      "start_position": 35137,
      "end_position": 35381,
      "sentences_count": 12
    },
    {
      "chunk_id": 158,
      "text": "The actual runtime (in seconds) of an algorithm depends on the speciﬁc ma-\nchine the algorithm is being implemented on (e.g., what the clock rate of the\nmachine’s CPU is) To avoid dependence on the speciﬁc machine, it is common\nto analyze the runtime of algorithms in an asymptotic sense For example, we\nsay that the computational complexity of the merge-sort algorithm, which sorts\na list of n items, is O(n log(n)) This implies that we can implement the algo-\nrithm on any machine that satisﬁes the requirements of some accepted abstract\nmodel of computation, and the actual runtime in seconds will satisfy the follow-\ning: there exist constants c and n0, which can depend on the actual machine,\nsuch that, for any value of n > n0, the runtime in seconds of sorting any n items\nwill be at most c n log(n) It is common to use the term feasible or eﬃciently\ncomputable for tasks that can be performed by an algorithm whose running time\nis O(p(n)) for some polynomial function p One should note that this type of\nanalysis depends on deﬁning what is the input size n of any instance to which\nthe algorithm is expected to be applied",
      "word_count": 203,
      "source_page": 100,
      "start_position": 35332,
      "end_position": 35534,
      "sentences_count": 6
    },
    {
      "chunk_id": 159,
      "text": "8.1 Computational Complexity of Learning\n101\nnumber of bits in its representation) For machine learning tasks, the notion of\nan input size is not so clear An algorithm aims to detect some pattern in a data\nset and can only access random samples of that data We start the chapter by discussing this issue and deﬁne the computational\ncomplexity of learning For advanced students, we also provide a detailed formal\ndeﬁnition We then move on to consider the computational complexity of im-\nplementing the ERM rule We ﬁrst give several examples of hypothesis classes\nwhere the ERM rule can be eﬃciently implemented, and then consider some\ncases where, although the class is indeed eﬃciently learnable, ERM implemen-\ntation is computationally hard It follows that hardness of implementing ERM\ndoes not imply hardness of learning Finally, we brieﬂy discuss how one can show\nhardness of a given learning task, namely, that no learning algorithm can solve\nit eﬃciently 8.1\nComputational Complexity of Learning\nRecall that a learning algorithm has access to a domain of examples, Z, a hy-\npothesis class, H, a loss function, ℓ, and a training set of examples from Z that\nare sampled i.i.d according to an unknown distribution D Given parameters\nϵ, δ, the algorithm should output a hypothesis h such that with probability of\nat least 1 −δ,\nLD(h) ≤min\nh′∈H LD(h′) + ϵ As mentioned before, the actual runtime of an algorithm in seconds depends on\nthe speciﬁc machine",
      "word_count": 244,
      "source_page": 101,
      "start_position": 35610,
      "end_position": 35853,
      "sentences_count": 13
    },
    {
      "chunk_id": 160,
      "text": "Given parameters\nϵ, δ, the algorithm should output a hypothesis h such that with probability of\nat least 1 −δ,\nLD(h) ≤min\nh′∈H LD(h′) + ϵ As mentioned before, the actual runtime of an algorithm in seconds depends on\nthe speciﬁc machine To allow machine independent analysis, we use the standard\napproach in computational complexity theory First, we rely on a notion of an\nabstract machine, such as a Turing machine (or a Turing machine over the reals\n(Blum, Shub & Smale 1989)) Second, we analyze the runtime in an asymptotic\nsense, while ignoring constant factors, thus the speciﬁc machine is not important\nas long as it implements the abstract machine Usually, the asymptote is with\nrespect to the size of the input to the algorithm For example, for the merge-sort\nalgorithm mentioned before, we analyze the runtime as a function of the number\nof items that need to be sorted In the context of learning algorithms, there is no clear notion of “input size.”\nOne might deﬁne the input size to be the size of the training set the algorithm\nreceives, but that would be rather pointless If we give the algorithm a very\nlarge number of examples, much larger than the sample complexity of the learn-\ning problem, the algorithm can simply ignore the extra examples",
      "word_count": 218,
      "source_page": 101,
      "start_position": 35812,
      "end_position": 36029,
      "sentences_count": 9
    },
    {
      "chunk_id": 161,
      "text": "102\nThe Runtime of Learning\ndomain set, or some measures of the complexity of the hypothesis class with\nwhich the algorithm’s output is compared To illustrate this, consider a learning algorithm for the task of learning axis\naligned rectangles A speciﬁc problem of learning axis aligned rectangles is de-\nrived by specifying ϵ, δ, and the dimension of the instance space We can deﬁne a\nsequence of problems of the type “rectangles learning” by ﬁxing ϵ, δ and varying\nthe dimension to be d = 2, 3, 4, We can also deﬁne another sequence of “rect-\nangles learning” problems by ﬁxing d, δ and varying the target accuracy to be\nϵ = 1\n2, 1\n3, One can of course choose other sequences of such problems Once\na sequence of the problems is ﬁxed, one can analyze the asymptotic runtime as\na function of variables of that sequence Before we introduce the formal deﬁnition, there is one more subtlety we need\nto tackle On the basis of the preceding, a learning algorithm can “cheat,” by\ntransferring the computational burden to the output hypothesis For example,\nthe algorithm can simply deﬁne the output hypothesis to be the function that\nstores the training set in its memory, and whenever it gets a test example x\nit calculates the ERM hypothesis on the training set and applies it on x",
      "word_count": 227,
      "source_page": 102,
      "start_position": 36098,
      "end_position": 36324,
      "sentences_count": 10
    },
    {
      "chunk_id": 162,
      "text": "On the basis of the preceding, a learning algorithm can “cheat,” by\ntransferring the computational burden to the output hypothesis For example,\nthe algorithm can simply deﬁne the output hypothesis to be the function that\nstores the training set in its memory, and whenever it gets a test example x\nit calculates the ERM hypothesis on the training set and applies it on x Note\nthat in this case, our algorithm has a ﬁxed output (namely, the function that\nwe have just described) and can run in constant time However, learning is still\nhard – the hardness is now in implementing the output classiﬁer to obtain a\nlabel prediction To prevent this “cheating,” we shall require that the output of\na learning algorithm must be applied to predict the label of a new example in\ntime that does not exceed the runtime of training (that is, computing the output\nclassiﬁer from the input training sample) In the next subsection the advanced\nreader may ﬁnd a formal deﬁnition of the computational complexity of learning 8.1.1\nFormal Deﬁnition*\nThe deﬁnition that follows relies on a notion of an underlying abstract machine,\nwhich is usually either a Turing machine or a Turing machine over the reals",
      "word_count": 203,
      "source_page": 102,
      "start_position": 36261,
      "end_position": 36463,
      "sentences_count": 7
    },
    {
      "chunk_id": 163,
      "text": "In the next subsection the advanced\nreader may ﬁnd a formal deﬁnition of the computational complexity of learning 8.1.1\nFormal Deﬁnition*\nThe deﬁnition that follows relies on a notion of an underlying abstract machine,\nwhich is usually either a Turing machine or a Turing machine over the reals We\nwill measure the computational complexity of an algorithm using the number of\n“operations” it needs to perform, where we assume that for any machine that\nimplements the underlying abstract machine there exists a constant c such that\nany such “operation” can be performed on the machine using c seconds definition 8.1 (The Computational Complexity of a Learning Algorithm)\nWe deﬁne the complexity of learning in two steps First we consider the compu-\ntational complexity of a ﬁxed learning problem (determined by a triplet (Z, H, ℓ)\n– a domain set, a benchmark hypothesis class, and a loss function) Then, in the\nsecond step we consider the rate of change of that complexity along a sequence\nof such tasks 1 Given a function f : (0, 1)2 →N, a learning task (Z, H, ℓ), and a learning\nalgorithm, A, we say that A solves the learning task in time O(f) if there\nexists some constant number c, such that for every probability distribution D",
      "word_count": 212,
      "source_page": 102,
      "start_position": 36416,
      "end_position": 36627,
      "sentences_count": 8
    },
    {
      "chunk_id": 164,
      "text": "8.2 Implementing the ERM Rule\n103\nover Z, and input ϵ, δ ∈(0, 1), when A has access to samples generated i.i.d by D,\n• A terminates after performing at most cf(ϵ, δ) operations\n• The output of A, denoted hA, can be applied to predict the label of a new\nexample while performing at most cf(ϵ, δ) operations\n• The output of A is probably approximately correct; namely, with proba-\nbility of at least 1 −δ (over the random samples A receives), LD(hA) ≤\nminh′∈H LD(h′) + ϵ\n2 Consider a sequence of learning problems, (Zn, Hn, ℓn)∞\nn=1, where problem n\nis deﬁned by a domain Zn, a hypothesis class Hn, and a loss function ℓn Let A be a learning algorithm designed for solving learning problems of\nthis form Given a function g : N × (0, 1)2 →N, we say that the runtime of\nA with respect to the preceding sequence is O(g), if for all n, A solves the\nproblem (Zn, Hn, ℓn) in time O(fn), where fn : (0, 1)2 →N is deﬁned by\nfn(ϵ, δ) = g(n, ϵ, δ) We say that A is an eﬃcient algorithm with respect to a sequence (Zn, Hn, ℓn)\nif its runtime is O(p(n, 1/ϵ, 1/δ)) for some polynomial p From this deﬁnition we see that the question whether a general learning prob-\nlem can be solved eﬃciently depends on how it can be broken into a sequence\nof speciﬁc learning problems",
      "word_count": 245,
      "source_page": 103,
      "start_position": 36628,
      "end_position": 36872,
      "sentences_count": 7
    },
    {
      "chunk_id": 165,
      "text": "We say that A is an eﬃcient algorithm with respect to a sequence (Zn, Hn, ℓn)\nif its runtime is O(p(n, 1/ϵ, 1/δ)) for some polynomial p From this deﬁnition we see that the question whether a general learning prob-\nlem can be solved eﬃciently depends on how it can be broken into a sequence\nof speciﬁc learning problems For example, consider the problem of learning a\nﬁnite hypothesis class As we showed in previous chapters, the ERM rule over\nH is guaranteed to (ϵ, δ)-learn H if the number of training examples is order of\nmH(ϵ, δ) = log(|H|/δ)/ϵ2 Assuming that the evaluation of a hypothesis on an\nexample takes a constant time, it is possible to implement the ERM rule in time\nO(|H| mH(ϵ, δ)) by performing an exhaustive search over H with a training set\nof size mH(ϵ, δ) For any ﬁxed ﬁnite H, the exhaustive search algorithm runs\nin polynomial time Furthermore, if we deﬁne a sequence of problems in which\n|Hn| = n, then the exhaustive search is still considered to be eﬃcient However, if\nwe deﬁne a sequence of problems for which |Hn| = 2n, then the sample complex-\nity is still polynomial in n but the computational complexity of the exhaustive\nsearch algorithm grows exponentially with n (thus, rendered ineﬃcient) 8.2\nImplementing the ERM Rule\nGiven a hypothesis class H, the ERMH rule is maybe the most natural learning\nparadigm",
      "word_count": 237,
      "source_page": 103,
      "start_position": 36814,
      "end_position": 37050,
      "sentences_count": 9
    },
    {
      "chunk_id": 166,
      "text": "104\nThe Runtime of Learning\nOn a ﬁnite input sample S ∈Zm output some h ∈H that minimizes the empirical loss,\nLS(h) =\n1\n|S|\nP\nz∈S ℓ(h, z) This section studies the runtime of implementing the ERM rule for several\nexamples of learning tasks 8.2.1\nFinite Classes\nLimiting the hypothesis class to be a ﬁnite class may be considered as a reason-\nably mild restriction For example, H can be the set of all predictors that can be\nimplemented by a C++ program written in at most 10000 bits of code Other ex-\namples of useful ﬁnite classes are any hypothesis class that can be parameterized\nby a ﬁnite number of parameters, where we are satisﬁed with a representation\nof each of the parameters using a ﬁnite number of bits, for example, the class of\naxis aligned rectangles in the Euclidean space, Rd, when the parameters deﬁning\nany given rectangle are speciﬁed up to some limited precision As we have shown in previous chapters, the sample complexity of learning a\nﬁnite class is upper bounded by mH(ϵ, δ) = c log(c|H|/δ)/ϵc, where c = 1 in\nthe realizable case and c = 2 in the nonrealizable case Therefore, the sample\ncomplexity has a mild dependence on the size of H In the example of C++\nprograms mentioned before, the number of hypotheses is 210,000 but the sample\ncomplexity is only c(10, 000 + log(c/δ))/ϵc",
      "word_count": 235,
      "source_page": 104,
      "start_position": 37113,
      "end_position": 37347,
      "sentences_count": 8
    },
    {
      "chunk_id": 167,
      "text": "Therefore, the sample\ncomplexity has a mild dependence on the size of H In the example of C++\nprograms mentioned before, the number of hypotheses is 210,000 but the sample\ncomplexity is only c(10, 000 + log(c/δ))/ϵc A straightforward approach for implementing the ERM rule over a ﬁnite hy-\npothesis class is to perform an exhaustive search That is, for each h ∈H we\ncalculate the empirical risk, LS(h), and return a hypothesis that minimizes\nthe empirical risk Assuming that the evaluation of ℓ(h, z) on a single exam-\nple takes a constant amount of time, k, the runtime of this exhaustive search\nbecomes k|H|m, where m is the size of the training set If we let m to be the\nupper bound on the sample complexity mentioned, then the runtime becomes\nk|H|c log(c|H|/δ)/ϵc The linear dependence of the runtime on the size of H makes this approach\nineﬃcient (and unrealistic) for large classes Formally, if we deﬁne a sequence of\nproblems (Zn, Hn, ℓn)∞\nn=1 such that log(|Hn|) = n, then the exhaustive search\napproach yields an exponential runtime In the example of C++ programs, if Hn\nis the set of functions that can be implemented by a C++ program written in\nat most n bits of code, then the runtime grows exponentially with n, implying\nthat the exhaustive search approach is unrealistic for practical use",
      "word_count": 227,
      "source_page": 104,
      "start_position": 37311,
      "end_position": 37537,
      "sentences_count": 9
    },
    {
      "chunk_id": 168,
      "text": "Formally, if we deﬁne a sequence of\nproblems (Zn, Hn, ℓn)∞\nn=1 such that log(|Hn|) = n, then the exhaustive search\napproach yields an exponential runtime In the example of C++ programs, if Hn\nis the set of functions that can be implemented by a C++ program written in\nat most n bits of code, then the runtime grows exponentially with n, implying\nthat the exhaustive search approach is unrealistic for practical use In fact, this\nproblem is one of the reasons we are dealing with other hypothesis classes, like\nclasses of linear predictors, which we will encounter in the next chapter, and not\njust focusing on ﬁnite classes It is important to realize that the ineﬃciency of one algorithmic approach\n(such as the exhaustive search) does not yet imply that no eﬃcient ERM imple-\nmentation exists Indeed, we will show examples in which the ERM rule can be\nimplemented eﬃciently.",
      "word_count": 151,
      "source_page": 104,
      "start_position": 37465,
      "end_position": 37615,
      "sentences_count": 5
    },
    {
      "chunk_id": 169,
      "text": "8.2 Implementing the ERM Rule\n105\n8.2.2\nAxis Aligned Rectangles\nLet Hn be the class of axis aligned rectangles in Rn, namely,\nHn = {h(a1,...,an,b1,...,bn) : ∀i, ai ≤bi}\nwhere\nh(a1,...,an,b1,...,bn)(x, y) =\n(\n1\nif ∀i, xi ∈[ai, bi]\n0\notherwise\n(8.1)\nEﬃciently Learnable in the Realizable Case\nConsider implementing the ERM rule in the realizable case That is, we are given\na training set S = (x1, y1), , (xm, ym) of examples, such that there exists an\naxis aligned rectangle, h ∈Hn, for which h(xi) = yi for all i Our goal is to ﬁnd\nsuch an axis aligned rectangle with a zero training error, namely, a rectangle\nthat is consistent with all the labels in S We show later that this can be done in time O(nm) Indeed, for each i ∈[n],\nset ai = min{xi : (x, 1) ∈S} and bi = max{xi : (x, 1) ∈S} In words, we take\nai to be the minimal value of the i’th coordinate of a positive example in S and\nbi to be the maximal value of the i’th coordinate of a positive example in S It is easy to verify that the resulting rectangle has zero training error and that\nthe runtime of ﬁnding each ai and bi is O(m) Hence, the total runtime of this\nprocedure is O(nm)",
      "word_count": 223,
      "source_page": 105,
      "start_position": 37616,
      "end_position": 37838,
      "sentences_count": 9
    },
    {
      "chunk_id": 170,
      "text": "It is easy to verify that the resulting rectangle has zero training error and that\nthe runtime of ﬁnding each ai and bi is O(m) Hence, the total runtime of this\nprocedure is O(nm) Not Eﬃciently Learnable in the Agnostic Case\nIn the agnostic case, we do not assume that some hypothesis h perfectly predicts\nthe labels of all the examples in the training set Our goal is therefore to ﬁnd\nh that minimizes the number of examples for which yi ̸= h(xi) It turns out\nthat for many common hypothesis classes, including the classes of axis aligned\nrectangles we consider here, solving the ERM problem in the agnostic setting is\nNP-hard (and, in most cases, it is even NP-hard to ﬁnd some h ∈H whose error\nis no more than some constant c > 1 times that of the empirical risk minimizer\nin H) That is, unless P = NP, there is no algorithm whose running time is\npolynomial in m and n that is guaranteed to ﬁnd an ERM hypothesis for these\nproblems (Ben-David, Eiron & Long 2003) On the other hand, it is worthwhile noticing that, if we ﬁx one speciﬁc hypoth-\nesis class, say, axis aligned rectangles in some ﬁxed dimension, n, then there exist\neﬃcient learning algorithms for this class In other words, there are successful\nagnostic PAC learners that run in time polynomial in 1/ϵ and 1/δ (but their\ndependence on the dimension n is not polynomial)",
      "word_count": 243,
      "source_page": 105,
      "start_position": 37805,
      "end_position": 38047,
      "sentences_count": 8
    },
    {
      "chunk_id": 171,
      "text": "106\nThe Runtime of Learning\nthe rectangle with the minimal training error This procedure is guaranteed to\nﬁnd an ERM hypothesis, and the runtime of the procedure is mO(n) It follows\nthat if n is ﬁxed, the runtime is polynomial in the sample size This does not\ncontradict the aforementioned hardness result, since there we argued that unless\nP=NP one cannot have an algorithm whose dependence on the dimension n is\npolynomial as well 8.2.3\nBoolean Conjunctions\nA Boolean conjunction is a mapping from X = {0, 1}n to Y = {0, 1} that can be\nexpressed as a proposition formula of the form xi1 ∧ ∧xik ∧¬xj1 ∧ ∧¬xjr,\nfor some indices i1, , ik, j1, , jr ∈[n] The function that such a proposition\nformula deﬁnes is\nh(x) =\n(\n1\nif xi1 = · · · = xik = 1 and xj1 = · · · = xjr = 0\n0\notherwise\nLet Hn\nC be the class of all Boolean conjunctions over {0, 1}n The size of Hn\nC is\nat most 3n +1 (since in a conjunction formula, each element of x either appears,\nor appears with a negation sign, or does not appear at all, and we also have the\nall negative formula) Hence, the sample complexity of learning Hn\nC using the\nERM rule is at most n log(3/δ)/ϵ",
      "word_count": 226,
      "source_page": 106,
      "start_position": 38120,
      "end_position": 38345,
      "sentences_count": 12
    },
    {
      "chunk_id": 172,
      "text": "The size of Hn\nC is\nat most 3n +1 (since in a conjunction formula, each element of x either appears,\nor appears with a negation sign, or does not appear at all, and we also have the\nall negative formula) Hence, the sample complexity of learning Hn\nC using the\nERM rule is at most n log(3/δ)/ϵ Eﬃciently Learnable in the Realizable Case\nNext, we show that it is possible to solve the ERM problem for Hn\nC in time\npolynomial in n and m The idea is to deﬁne an ERM conjunction by including\nin the hypothesis conjunction all the literals that do not contradict any positively\nlabeled example Let v1, , vm+ be all the positively labeled instances in the\ninput sample S We deﬁne, by induction on i ≤m+, a sequence of hypotheses\n(or conjunctions) Let h0 be the conjunction of all possible literals That is,\nh0 = x1 ∧¬x1 ∧x2 ∧ ∧xn ∧¬xn Note that h0 assigns the label 0 to all the\nelements of X We obtain hi+1 by deleting from the conjunction hi all the literals\nthat are not satisﬁed by vi+1 The algorithm outputs the hypothesis hm+ Note\nthat hm+ labels positively all the positively labeled examples in S Furthermore,\nfor every i ≤m+, hi is the most restrictive conjunction that labels v1, , vi\npositively Now, since we consider learning in the realizable setup, there exists\na conjunction hypothesis, f ∈Hn\nC, that is consistent with all the examples in\nS",
      "word_count": 250,
      "source_page": 106,
      "start_position": 38288,
      "end_position": 38537,
      "sentences_count": 17
    },
    {
      "chunk_id": 173,
      "text": "8.3 Eﬃciently Learnable, but Not by a Proper ERM\n107\nNot Eﬃciently Learnable in the Agnostic Case\nAs in the case of axis aligned rectangles, unless P = NP, there is no algorithm\nwhose running time is polynomial in m and n that guaranteed to ﬁnd an ERM\nhypothesis for the class of Boolean conjunctions in the unrealizable case 8.2.4\nLearning 3-Term DNF\nWe next show that a slight generalization of the class of Boolean conjunctions\nleads to intractability of solving the ERM problem even in the realizable case Consider the class of 3-term disjunctive normal form formulae (3-term DNF) The instance space is X = {0, 1}n and each hypothesis is represented by the\nBoolean formula of the form h(x) = A1(x) ∨A2(x) ∨A3(x), where each Ai(x) is\na Boolean conjunction (as deﬁned in the previous section) The output of h(x) is\n1 if either A1(x) or A2(x) or A3(x) outputs the label 1 If all three conjunctions\noutput the label 0 then h(x) = 0 Let Hn\n3DNF be the hypothesis class of all such 3-term DNF formulae The size\nof Hn\n3DNF is at most 33n Hence, the sample complexity of learning Hn\n3DNF using\nthe ERM rule is at most 3n log(3/δ)/ϵ However, from the computational perspective, this learning problem is hard It has been shown (see (Pitt & Valiant 1988, Kearns et al",
      "word_count": 228,
      "source_page": 107,
      "start_position": 38594,
      "end_position": 38821,
      "sentences_count": 11
    },
    {
      "chunk_id": 174,
      "text": "However, from the computational perspective, this learning problem is hard It has been shown (see (Pitt & Valiant 1988, Kearns et al 1994)) that unless\nRP = NP, there is no polynomial time algorithm that properly learns a sequence\nof 3-term DNF learning problems in which the dimension of the n’th problem is\nn By “properly” we mean that the algorithm should output a hypothesis that is\na 3-term DNF formula In particular, since ERMHn\n3DNF outputs a 3-term DNF\nformula it is a proper learner and therefore it is hard to implement it The proof\nuses a reduction of the graph 3-coloring problem to the problem of PAC learning\n3-term DNF The detailed technique is given in Exercise 3 See also (Kearns &\nVazirani 1994, Section 1.4) 8.3\nEﬃciently Learnable, but Not by a Proper ERM\nIn the previous section we saw that it is impossible to implement the ERM rule\neﬃciently for the class Hn\n3DNF of 3-DNF formulae In this section we show that it\nis possible to learn this class eﬃciently, but using ERM with respect to a larger\nclass Representation Independent Learning Is Not Hard\nNext we show that it is possible to learn 3-term DNF formulae eﬃciently There\nis no contradiction to the hardness result mentioned in the previous section as we\nnow allow “representation independent” learning That is, we allow the learning\nalgorithm to output a hypothesis that is not a 3-term DNF formula",
      "word_count": 241,
      "source_page": 107,
      "start_position": 38800,
      "end_position": 39040,
      "sentences_count": 13
    },
    {
      "chunk_id": 175,
      "text": "108\nThe Runtime of Learning\nalgorithm might return a hypothesis that does not belong to the original hypoth-\nesis class; hence the name “representation independent” learning We emphasize\nthat in most situations, returning a hypothesis with good predictive ability is\nwhat we are really interested in doing We start by noting that because ∨distributes over ∧, each 3-term DNF formula\ncan be rewritten as\nA1 ∨A2 ∨A3 =\n^\nu∈A1,v∈A2,w∈A3\n(u ∨v ∨w)\nNext, let us deﬁne: ψ : {0, 1}n →{0, 1}(2n)3 such that for each triplet of literals\nu, v, w there is a variable in the range of ψ indicating if u ∨v ∨w is true or false So, for each 3-DNF formula over {0, 1}n there is a conjunction over {0, 1}(2n)3,\nwith the same truth table Since we assume that the data is realizable, we can\nsolve the ERM problem with respect to the class of conjunctions over {0, 1}(2n)3 Furthermore, the sample complexity of learning the class of conjunctions in the\nhigher dimensional space is at most n3 log(1/δ)/ϵ Thus, the overall runtime of\nthis approach is polynomial in n Intuitively, the idea is as follows We started with a hypothesis class for which\nlearning is hard We switched to another representation where the hypothesis\nclass is larger than the original class but has more structure, which allows for a\nmore eﬃcient ERM search In the new representation, solving the ERM problem\nis easy",
      "word_count": 240,
      "source_page": 108,
      "start_position": 39071,
      "end_position": 39310,
      "sentences_count": 11
    },
    {
      "chunk_id": 176,
      "text": "8.4 Hardness of Learning*\n109\nto some partial information about it On that high level intuitive sense, results\nabout the cryptographic security of some system translate into results about\nthe unlearnability of some corresponding task Regrettably, currently one has no\nway of proving that a cryptographic protocol is not breakable Even the common\nassumption of P ̸= NP does not suﬃce for that (although it can be shown to\nbe necessary for most common cryptographic scenarios) The common approach\nfor proving that cryptographic protocols are secure is to start with some cryp-\ntographic assumptions The more these are used as a basis for cryptography, the\nstronger is our belief that they really hold (or, at least, that algorithms that will\nrefute them are hard to come by) We now brieﬂy describe the basic idea of how to deduce hardness of learnabil-\nity from cryptographic assumptions Many cryptographic systems rely on the\nassumption that there exists a one way function Roughly speaking, a one way\nfunction is a function f : {0, 1}n →{0, 1}n (more formally, it is a sequence of\nfunctions, one for each dimension n) that is easy to compute but is hard to in-\nvert More formally, f can be computed in time poly(n) but for any randomized\npolynomial time algorithm A, and for every polynomial p(·),\nP[f(A(f(x))) = f(x)] <\n1\np(n),\nwhere the probability is taken over a random choice of x according to the uniform\ndistribution over {0, 1}n and the randomness of A",
      "word_count": 250,
      "source_page": 109,
      "start_position": 39411,
      "end_position": 39660,
      "sentences_count": 10
    },
    {
      "chunk_id": 177,
      "text": "Roughly speaking, a one way\nfunction is a function f : {0, 1}n →{0, 1}n (more formally, it is a sequence of\nfunctions, one for each dimension n) that is easy to compute but is hard to in-\nvert More formally, f can be computed in time poly(n) but for any randomized\npolynomial time algorithm A, and for every polynomial p(·),\nP[f(A(f(x))) = f(x)] <\n1\np(n),\nwhere the probability is taken over a random choice of x according to the uniform\ndistribution over {0, 1}n and the randomness of A A one way function, f, is called trapdoor one way function if, for some poly-\nnomial function p, for every n there exists a bit-string sn (called a secret key) of\nlength ≤p(n), such that there is a polynomial time algorithm that, for every n\nand every x ∈{0, 1}n, on input (f(x), sn) outputs x In other words, although\nf is hard to invert, once one has access to its secret key, inverting f becomes\nfeasible Such functions are parameterized by their secret key Now, let Fn be a family of trapdoor functions over {0, 1}n that can be calcu-\nlated by some polynomial time algorithm That is, we ﬁx an algorithm that given\na secret key (representing one function in Fn) and an input vector, it calculates\nthe value of the function corresponding to the secret key on the input vector in\npolynomial time",
      "word_count": 237,
      "source_page": 109,
      "start_position": 39570,
      "end_position": 39806,
      "sentences_count": 7
    },
    {
      "chunk_id": 178,
      "text": "Now, let Fn be a family of trapdoor functions over {0, 1}n that can be calcu-\nlated by some polynomial time algorithm That is, we ﬁx an algorithm that given\na secret key (representing one function in Fn) and an input vector, it calculates\nthe value of the function corresponding to the secret key on the input vector in\npolynomial time Consider the task of learning the class of the corresponding\ninverses, Hn\nF = {f −1 : f ∈Fn} Since each function in this class can be inverted\nby some secret key sn of size polynomial in n, the class Hn\nF can be parameter-\nized by these keys and its size is at most 2p(n) Its sample complexity is therefore\npolynomial in n We claim that there can be no eﬃcient learner for this class If\nthere were such a learner, L, then by sampling uniformly at random a polynomial\nnumber of strings in {0, 1}n, and computing f over them, we could generate a\nlabeled training sample of pairs (f(x), x), which should suﬃce for our learner to\nﬁgure out an (ϵ, δ) approximation of f −1 (w.r.t the uniform distribution over\nthe range of f), which would violate the one way property of f A more detailed treatment, as well as a concrete example, can be found in\n(Kearns & Vazirani 1994, Chapter 6) Using reductions, they also show that",
      "word_count": 234,
      "source_page": 109,
      "start_position": 39746,
      "end_position": 39979,
      "sentences_count": 10
    },
    {
      "chunk_id": 179,
      "text": "110\nThe Runtime of Learning\nthe class of functions that can be calculated by small Boolean circuits is not\neﬃciently learnable, even in the realizable case 8.5\nSummary\nThe runtime of learning algorithms is asymptotically analyzed as a function of\ndiﬀerent parameters of the learning problem, such as the size of the hypothe-\nsis class, our measure of accuracy, our measure of conﬁdence, or the size of the\ndomain set We have demonstrated cases in which the ERM rule can be imple-\nmented eﬃciently For example, we derived eﬃcient algorithms for solving the\nERM problem for the class of Boolean conjunctions and the class of axis aligned\nrectangles, under the realizability assumption However, implementing ERM for\nthese classes in the agnostic case is NP-hard Recall that from the statistical\nperspective, there is no diﬀerence between the realizable and agnostic cases (i.e.,\na class is learnable in both cases if and only if it has a ﬁnite VC-dimension) In contrast, as we saw, from the computational perspective the diﬀerence is im-\nmense We have also shown another example, the class of 3-term DNF, where\nimplementing ERM is hard even in the realizable case, yet the class is eﬃciently\nlearnable by another algorithm Hardness of implementing the ERM rule for several natural hypothesis classes\nhas motivated the development of alternative learning methods, which we will\ndiscuss in the next part of this book",
      "word_count": 231,
      "source_page": 110,
      "start_position": 39980,
      "end_position": 40210,
      "sentences_count": 9
    },
    {
      "chunk_id": 180,
      "text": "We have also shown another example, the class of 3-term DNF, where\nimplementing ERM is hard even in the realizable case, yet the class is eﬃciently\nlearnable by another algorithm Hardness of implementing the ERM rule for several natural hypothesis classes\nhas motivated the development of alternative learning methods, which we will\ndiscuss in the next part of this book 8.6\nBibliographic Remarks\nValiant (1984) introduced the eﬃcient PAC learning model in which the runtime\nof the algorithm is required to be polynomial in 1/ϵ, 1/δ, and the representation\nsize of hypotheses in the class A detailed discussion and thorough bibliographic\nnotes are given in Kearns & Vazirani (1994) 8.7\nExercises\n1 Let H be the class of intervals on the line (formally equivalent to axis aligned\nrectangles in dimension n = 1) Propose an implementation of the ERMH\nlearning rule (in the agnostic case) that given a training set of size m, runs\nin time O(m2) Hint: Use dynamic programming 2 Let H1, H2, be a sequence of hypothesis classes for binary classiﬁcation Assume that there is a learning algorithm that implements the ERM rule in\nthe realizable case such that the output hypothesis of the algorithm for each\nclass Hn only depends on O(n) examples out of the training set Furthermore,",
      "word_count": 213,
      "source_page": 110,
      "start_position": 40151,
      "end_position": 40363,
      "sentences_count": 13
    },
    {
      "chunk_id": 181,
      "text": "8.7 Exercises\n111\nassume that such a hypothesis can be calculated given these O(n) examples\nin time O(n), and that the empirical risk of each such hypothesis can be\nevaluated in time O(mn) For example, if Hn is the class of axis aligned\nrectangles in Rn, we saw that it is possible to ﬁnd an ERM hypothesis in the\nrealizable case that is deﬁned by at most 2n examples Prove that in such\ncases, it is possible to ﬁnd an ERM hypothesis for Hn in the unrealizable case\nin time O(mn mO(n)) 3 In this exercise, we present several classes for which ﬁnding an ERM classi-\nﬁer is computationally hard First, we introduce the class of n-dimensional\nhalfspaces, HSn, for a domain X = Rn This is the class of all functions of\nthe form hw,b(x) = sign(⟨w, x⟩+ b) where w, x ∈Rn, ⟨w, x⟩is their inner\nproduct, and b ∈R See a detailed description in Chapter 9 1 Show that ERMH over the class H = HSn of linear predictors is compu-\ntationally hard More precisely, we consider the sequence of problems in\nwhich the dimension n grows linearly and the number of examples m is set\nto be some constant times n",
      "word_count": 205,
      "source_page": 111,
      "start_position": 40364,
      "end_position": 40568,
      "sentences_count": 11
    },
    {
      "chunk_id": 182,
      "text": "Show that ERMH over the class H = HSn of linear predictors is compu-\ntationally hard More precisely, we consider the sequence of problems in\nwhich the dimension n grows linearly and the number of examples m is set\nto be some constant times n Hint: You can prove the hardness by a reduction from the following prob-\nlem:\nMax FS: Given a system of linear inequalities, Ax > b with A ∈Rm×n and b ∈\nRm (that is, a system of m linear inequalities in n variables, x = (x1, , xn)),\nﬁnd a subsystem containing as many inequalities as possible that has a solution\n(such a subsystem is called feasible) It has been shown (Sankaran 1993) that the problem Max FS is NP-hard Show that any algorithm that ﬁnds an ERMHSn hypothesis for any training\nsample S ∈(Rn ×{+1, −1})m can be used to solve the Max FS problem of\nsize m, n Hint: Deﬁne a mapping that transforms linear inequalities in n\nvariables into labeled points in Rn, and a mapping that transforms vectors\nin Rn to halfspaces, such that a vector w satisﬁes an inequality q if and\nonly if the labeled point that corresponds to q is classiﬁed correctly by the\nhalfspace corresponding to w",
      "word_count": 210,
      "source_page": 111,
      "start_position": 40524,
      "end_position": 40733,
      "sentences_count": 7
    },
    {
      "chunk_id": 183,
      "text": "Show that any algorithm that ﬁnds an ERMHSn hypothesis for any training\nsample S ∈(Rn ×{+1, −1})m can be used to solve the Max FS problem of\nsize m, n Hint: Deﬁne a mapping that transforms linear inequalities in n\nvariables into labeled points in Rn, and a mapping that transforms vectors\nin Rn to halfspaces, such that a vector w satisﬁes an inequality q if and\nonly if the labeled point that corresponds to q is classiﬁed correctly by the\nhalfspace corresponding to w Conclude that the problem of empirical risk\nminimization for halfspaces in also NP-hard (that is, if it can be solved in\ntime polynomial in the sample size, m, and the Euclidean dimension, n,\nthen every problem in the class NP can be solved in polynomial time) 2 Let X = Rn and let Hn\nk be the class of all intersections of k-many linear\nhalfspaces in Rn In this exercise, we wish to show that ERMHn\nk is com-\nputationally hard for every k ≥3 Precisely, we consider a sequence of\nproblems where k ≥3 is a constant and n grows linearly The training set\nsize, m, also grows linearly with n Towards this goal, consider the k-coloring problem for graphs, deﬁned as\nfollows:\nGiven a graph G = (V, E), and a number k, determine whether there exists a\nfunction f : V →{1 k} so that for every (u, v) ∈E, f(u) ̸= f(v)",
      "word_count": 241,
      "source_page": 111,
      "start_position": 40649,
      "end_position": 40889,
      "sentences_count": 10
    },
    {
      "chunk_id": 184,
      "text": "112\nThe Runtime of Learning\nWe wish to reduce the k-coloring problem to ERMHn\nk : that is, to prove\nthat if there is an algorithm that solves the ERMHn\nk problem in time\npolynomial in k, n, and the sample size m, then there is a polynomial time\nalgorithm for the graph k-coloring problem Given a graph G = (V, E), let {v1 vn} be the vertices in V Construct\na sample S(G) ∈(Rn × {±1})m, where m = |V | + |E|, as follows:\n• For every vi ∈V , construct an instance ei with a negative label • For every edge (vi, vj) ∈E, construct an instance (ei + ej)/2 with a\npositive label 1 Prove that if there exists some h ∈Hn\nk that has zero error over S(G)\nthen G is k-colorable Hint: Let h = Tk\nj=1 hj be an ERM classiﬁer in Hn\nk over S Deﬁne a\ncoloring of V by setting f(vi) to be the minimal j such that hj(ei) = −1 Use the fact that halfspaces are convex sets to show that it cannot be\ntrue that two vertices that are connected by an edge have the same\ncolor 2 Prove that if G is k-colorable then there exists some h ∈Hn\nk that has\nzero error over S(G) Hint: Given a coloring f of the vertices of G, we should come up with k\nhyperplanes, h1 hk whose intersection is a perfect classiﬁer for S(G)",
      "word_count": 246,
      "source_page": 112,
      "start_position": 40904,
      "end_position": 41149,
      "sentences_count": 14
    },
    {
      "chunk_id": 185,
      "text": "Hint: Given a coloring f of the vertices of G, we should come up with k\nhyperplanes, h1 hk whose intersection is a perfect classiﬁer for S(G) Let b = 0.6 for all of these hyperplanes and, for t ≤k let the i’th weight\nof the t’th hyperplane, wt,i, be −1 if f(vi) = t and 0 otherwise 3 Based on the above, prove that for any k ≥3, the ERMHn\nk problem is\nNP-hard 4 In this exercise we show that hardness of solving the ERM problem is equiv-\nalent to hardness of proper PAC learning Recall that by “properness” of the\nalgorithm we mean that it must output a hypothesis from the hypothesis\nclass To formalize this statement, we ﬁrst need the following deﬁnition definition 8.2\nThe complexity class Randomized Polynomial (RP) time\nis the class of all decision problems (that is, problems in which on any instance\none has to ﬁnd out whether the answer is YES or NO) for which there exists a\nprobabilistic algorithm (namely, the algorithm is allowed to ﬂip random coins\nwhile it is running) with these properties:\n• On any input instance the algorithm runs in polynomial time in the input\nsize • If the correct answer is NO, the algorithm must return NO • If the correct answer is YES, the algorithm returns YES with probability\na ≥1/2 and returns NO with probability 1 −a.1\nClearly the class RP contains the class P",
      "word_count": 242,
      "source_page": 112,
      "start_position": 41123,
      "end_position": 41364,
      "sentences_count": 12
    },
    {
      "chunk_id": 186,
      "text": "8.7 Exercises\n113\nlarger than RP In particular, it is believed that NP-hard problems cannot be\nsolved by a randomized polynomial time algorithm • Show that if a class H is properly PAC learnable by a polynomial time\nalgorithm, then the ERMH problem is in the class RP In particular, this\nimplies that whenever the ERMH problem is NP-hard (for example, the\nclass of intersections of halfspaces discussed in the previous exercise),\nthen, unless NP = RP, there exists no polynomial time proper PAC\nlearning algorithm for H Hint: Assume you have an algorithm A that properly PAC learns a\nclass H in time polynomial in some class parameter n as well as in 1/ϵ\nand 1/δ Your goal is to use that algorithm as a subroutine to contract\nan algorithm B for solving the ERMH problem in random polynomial\ntime Given a training set, S ∈(X × {±1}m), and some h ∈H whose\nerror on S is zero, apply the PAC learning algorithm to the uniform\ndistribution over S and run it so that with probability ≥0.3 it ﬁnds a\nfunction h ∈H that has error less than ϵ = 1/|S| (with respect to that\nuniform distribution) Show that the algorithm just described satisﬁes\nthe requirements for being a RP solver for ERMH.",
      "word_count": 214,
      "source_page": 113,
      "start_position": 41415,
      "end_position": 41628,
      "sentences_count": 8
    },
    {
      "chunk_id": 187,
      "text": "9\nLinear Predictors\nIn this chapter we will study the family of linear predictors, one of the most\nuseful families of hypothesis classes Many learning algorithms that are being\nwidely used in practice rely on linear predictors, ﬁrst and foremost because of\nthe ability to learn them eﬃciently in many cases In addition, linear predictors\nare intuitive, are easy to interpret, and ﬁt the data reasonably well in many\nnatural learning problems We will introduce several hypothesis classes belonging to this family – halfspaces,\nlinear regression predictors, and logistic regression predictors – and present rele-\nvant learning algorithms: linear programming and the Perceptron algorithm for\nthe class of halfspaces and the Least Squares algorithm for linear regression This chapter is focused on learning linear predictors using the ERM approach;\nhowever, in later chapters we will see alternative paradigms for learning these\nhypothesis classes First, we deﬁne the class of aﬃne functions as\nLd = {hw,b : w ∈Rd, b ∈R},\nwhere\nhw,b(x) = ⟨w, x⟩+ b =\n d\nX\ni=1\nwixi + b It will be convenient also to use the notation\nLd = {x 7→⟨w, x⟩+ b : w ∈Rd, b ∈R},\nwhich reads as follows: Ld is a set of functions, where each function is parame-\nterized by w ∈Rd and b ∈R, and each such function takes as input a vector x\nand returns as output the scalar ⟨w, x⟩+ b",
      "word_count": 234,
      "source_page": 117,
      "start_position": 41629,
      "end_position": 41862,
      "sentences_count": 8
    },
    {
      "chunk_id": 188,
      "text": "+ b It will be convenient also to use the notation\nLd = {x 7→⟨w, x⟩+ b : w ∈Rd, b ∈R},\nwhich reads as follows: Ld is a set of functions, where each function is parame-\nterized by w ∈Rd and b ∈R, and each such function takes as input a vector x\nand returns as output the scalar ⟨w, x⟩+ b The diﬀerent hypothesis classes of linear predictors are compositions of a func-\ntion φ : R →Y on Ld For example, in binary classiﬁcation, we can choose φ to\nbe the sign function, and for regression problems, where Y = R, φ is simply the\nidentity function It may be more convenient to incorporate b, called the bias, into w as an\nextra coordinate and add an extra coordinate with a value of 1 to all x ∈X;\nnamely, let w′ = (b, w1, w2, wd) ∈Rd+1 and let x′ = (1, x1, x2, , xd) ∈\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 189,
      "source_page": 117,
      "start_position": 41800,
      "end_position": 41988,
      "sentences_count": 11
    },
    {
      "chunk_id": 189,
      "text": "118\nLinear Predictors\nRd+1 Therefore,\nhw,b(x) = ⟨w, x⟩+ b = ⟨w′, x′⟩ It follows that each aﬃne function in Rd can be rewritten as a homogenous linear\nfunction in Rd+1 applied over the transformation that appends the constant 1\nto each input vector Therefore, whenever it simpliﬁes the presentation, we will\nomit the bias term and refer to Ld as the class of homogenous linear functions\nof the form hw(x) = ⟨w, x⟩ Throughout the book we often use the general term “linear functions” for both\naﬃne functions and (homogenous) linear functions 9.1\nHalfspaces\nThe ﬁrst hypothesis class we consider is the class of halfspaces, designed for\nbinary classiﬁcation problems, namely, X = Rd and Y = {−1, +1} The class of\nhalfspaces is deﬁned as follows:\nHSd = sign ◦Ld = {x 7→sign(hw,b(x)) : hw,b ∈Ld} In other words, each halfspace hypothesis in HSd is parameterized by w ∈\nRd and b ∈R and upon receiving a vector x the hypothesis returns the label\nsign(⟨w, x⟩+ b) To illustrate this hypothesis class geometrically, it is instructive to consider\nthe case d = 2 Each hypothesis forms a hyperplane that is perpendicular to the\nvector w and intersects the vertical axis at the point (0, −b/w2) The instances\nthat are “above” the hyperplane, that is, share an acute angle with w, are labeled\npositively Instances that are “below” the hyperplane, that is, share an obtuse\nangle with w, are labeled negatively",
      "word_count": 242,
      "source_page": 118,
      "start_position": 41989,
      "end_position": 42230,
      "sentences_count": 12
    },
    {
      "chunk_id": 190,
      "text": "9.1 Halfspaces\n119\nin the nonseparable case (i.e., the agnostic case) is known to be computationally\nhard (Ben-David & Simon 2001) There are several approaches to learning non-\nseparable data The most popular one is to use surrogate loss functions, namely,\nto learn a halfspace that does not necessarily minimize the empirical risk with\nthe 0 −1 loss, but rather with respect to a diﬀferent loss function For example,\nin Section 9.3 we will describe the logistic regression approach, which can be\nimplemented eﬃciently even in the nonseparable case We will study surrogate\nloss functions in more detail later on in Chapter 12 9.1.1\nLinear Programming for the Class of Halfspaces\nLinear programs (LP) are problems that can be expressed as maximizing a linear\nfunction subject to linear inequalities That is,\nmax\nw∈Rd\n⟨u, w⟩\nsubject to\nAw ≥v\nwhere w ∈Rd is the vector of variables we wish to determine, A is an m ×\nd matrix, and v ∈Rm, u ∈Rd are vectors Linear programs can be solved\neﬃciently,1 and furthermore, there are publicly available implementations of LP\nsolvers We will show that the ERM problem for halfspaces in the realizable case can\nbe expressed as a linear program For simplicity, we assume the homogenous\ncase Let S = {(xi, yi)}m\ni=1 be a training set of size m Since we assume the\nrealizable case, an ERM predictor should have zero errors on the training set",
      "word_count": 238,
      "source_page": 119,
      "start_position": 42336,
      "end_position": 42573,
      "sentences_count": 12
    },
    {
      "chunk_id": 191,
      "text": "Let S = {(xi, yi)}m\ni=1 be a training set of size m Since we assume the\nrealizable case, an ERM predictor should have zero errors on the training set That is, we are looking for some vector w ∈Rd for which\nsign(⟨w, xi⟩) = yi,\n∀i = 1, , m Equivalently, we are looking for some vector w for which\nyi⟨w, xi⟩> 0,\n∀i = 1, , m Let w∗be a vector that satisﬁes this condition (it must exist since we assume\nrealizability) Deﬁne γ = mini(yi⟨w∗, xi⟩) and let ¯w = w∗\nγ Therefore, for all i\nwe have\nyi⟨¯w, xi⟩= 1\nγ yi⟨w∗, xi⟩≥1 We have thus shown that there exists a vector that satisﬁes\nyi⟨w, xi⟩≥1,\n∀i = 1, , m (9.1)\nAnd clearly, such a vector is an ERM predictor To ﬁnd a vector that satisﬁes Equation (9.1) we can rely on an LP solver as\nfollows Set A to be the m × d matrix whose rows are the instances multiplied\n1 Namely, in time polynomial in m, d, and in the representation size of real numbers.",
      "word_count": 183,
      "source_page": 119,
      "start_position": 42544,
      "end_position": 42726,
      "sentences_count": 14
    },
    {
      "chunk_id": 192,
      "text": "120\nLinear Predictors\nby yi That is, Ai,j = yi xi,j, where xi,j is the j’th element of the vector xi Let\nv be the vector (1, , 1) ∈Rm Then, Equation (9.1) can be rewritten as\nAw ≥v The LP form requires a maximization objective, yet all the w that satisfy the\nconstraints are equal candidates as output hypotheses Thus, we set a “dummy”\nobjective, u = (0, , 0) ∈Rd 9.1.2\nPerceptron for Halfspaces\nA diﬀerent implementation of the ERM rule is the Perceptron algorithm of\nRosenblatt (Rosenblatt 1958) The Perceptron is an iterative algorithm that\nconstructs a sequence of vectors w(1), w(2), Initially, w(1) is set to be the\nall-zeros vector At iteration t, the Perceptron ﬁnds an example i that is mis-\nlabeled by w(t), namely, an example for which sign(⟨w(t), xi⟩) ̸= yi Then, the\nPerceptron updates w(t) by adding to it the instance xi scaled by the label yi That is, w(t+1) = w(t) + yixi Recall that our goal is to have yi⟨w, xi⟩> 0 for\nall i and note that\nyi⟨w(t+1), xi⟩= yi⟨w(t) + yixi, xi⟩= yi⟨w(t), xi⟩+ ∥xi∥2 Hence, the update of the Perceptron guides the solution to be “more correct” on\nthe i’th example Batch Perceptron\ninput: A training set (x1, y1), , (xm, ym)\ninitialize: w(1) = (0, , 0)\nfor t = 1, 2, if (∃i s.t",
      "word_count": 229,
      "source_page": 120,
      "start_position": 42727,
      "end_position": 42955,
      "sentences_count": 20
    },
    {
      "chunk_id": 193,
      "text": "9.1 Halfspaces\n121\nyi⟨w⋆, xi⟩≥1 for all i, and among all vectors that satisfy these constraints, w⋆\nis of minimal norm The idea of the proof is to show that after performing T iterations, the cosine\nof the angle between w⋆and w(T +1) is at least\n√\nT\nRB That is, we will show that\n⟨w⋆, w(T +1)⟩\n∥w⋆∥∥w(T +1)∥≥\n√\nT\nRB (9.2)\nBy the Cauchy-Schwartz inequality, the left-hand side of Equation (9.2) is at\nmost 1 Therefore, Equation (9.2) would imply that\n1 ≥\n√\nT\nRB\n⇒\nT ≤(RB)2,\nwhich will conclude our proof To show that Equation (9.2) holds, we ﬁrst show that ⟨w⋆, w(T +1)⟩≥T Indeed, at the ﬁrst iteration, w(1) = (0, , 0) and therefore ⟨w⋆, w(1)⟩= 0,\nwhile on iteration t, if we update using example (xi, yi) we have that\n⟨w⋆, w(t+1)⟩−⟨w⋆, w(t)⟩= ⟨w⋆, w(t+1) −w(t)⟩\n= ⟨w⋆, yixi⟩= yi⟨w⋆, xi⟩\n≥1 Therefore, after performing T iterations, we get:\n⟨w⋆, w(T +1)⟩=\nT\nX\nt=1\n\u0010\n⟨w⋆, w(t+1)⟩−⟨w⋆, w(t)⟩\n\u0011\n≥T,\n(9.3)\nas required Next, we upper bound ∥w(T +1)∥ For each iteration t we have that\n∥w(t+1)∥2 = ∥w(t) + yixi∥2\n= ∥w(t)∥2 + 2yi⟨w(t), xi⟩+ y2\ni ∥xi∥2\n≤∥w(t)∥2 + R2\n(9.4)\nwhere the last inequality is due to the fact that example i is necessarily such\nthat yi⟨w(t), xi⟩≤0, and the norm of xi is at most R",
      "word_count": 230,
      "source_page": 121,
      "start_position": 43095,
      "end_position": 43324,
      "sentences_count": 11
    },
    {
      "chunk_id": 194,
      "text": "122\nLinear Predictors\nRemark 9.1\nThe Perceptron is simple to implement and is guaranteed to con-\nverge However, the convergence rate depends on the parameter B, which in\nsome situations might be exponentially large in d In such cases, it would be\nbetter to implement the ERM problem by solving a linear program, as described\nin the previous section Nevertheless, for many natural data sets, the size of B\nis not too large, and the Perceptron converges quite fast 9.1.3\nThe VC Dimension of Halfspaces\nTo compute the VC dimension of halfspaces, we start with the homogenous case theorem 9.2\nThe VC dimension of the class of homogenous halfspaces in Rd\nis d Proof\nFirst, consider the set of vectors e1, , ed, where for every i the vector\nei is the all zeros vector except 1 in the i’th coordinate This set is shattered\nby the class of homogenous halfspaces Indeed, for every labeling y1, , yd, set\nw = (y1, , yd), and then ⟨w, ei⟩= yi for all i Next, let x1, , xd+1 be a set of d + 1 vectors in Rd Then, there must exist\nreal numbers a1, , ad+1, not all of them are zero, such that Pd+1\ni=1 aixi = 0 Let I = {i : ai > 0} and J = {j : aj < 0} Either I or J is nonempty Let us\nﬁrst assume that both of them are nonempty Then,\nX\ni∈I\naixi =\nX\nj∈J\n|aj|xj",
      "word_count": 249,
      "source_page": 122,
      "start_position": 43395,
      "end_position": 43643,
      "sentences_count": 20
    },
    {
      "chunk_id": 195,
      "text": "Let us\nﬁrst assume that both of them are nonempty Then,\nX\ni∈I\naixi =\nX\nj∈J\n|aj|xj Now, suppose that x1, , xd+1 are shattered by the class of homogenous classes Then, there must exist a vector w such that ⟨w, xi⟩> 0 for all i ∈I while\n⟨w, xj⟩< 0 for every j ∈J It follows that\n0 <\nX\ni∈I\nai⟨xi, w⟩=\n*X\ni∈I\naixi, w\n+\n=\n*X\nj∈J\n|aj|xj, w\n+\n=\nX\nj∈J\n|aj|⟨xj, w⟩< 0,\nwhich leads to a contradiction Finally, if J (respectively, I) is empty then the\nright-most (respectively, left-most) inequality should be replaced by an equality,\nwhich still leads to a contradiction theorem 9.3\nThe VC dimension of the class of nonhomogenous halfspaces in\nRd is d + 1 Proof\nFirst, as in the proof of Theorem 9.2, it is easy to verify that the set\nof vectors 0, e1, , ed is shattered by the class of nonhomogenous halfspaces Second, suppose that the vectors x1, , xd+2 are shattered by the class of non-\nhomogenous halfspaces But, using the reduction we have shown in the beginning\nof this chapter, it follows that there are d + 2 vectors in Rd+1 that are shattered\nby the class of homogenous halfspaces But this contradicts Theorem 9.2.",
      "word_count": 215,
      "source_page": 122,
      "start_position": 43626,
      "end_position": 43840,
      "sentences_count": 14
    },
    {
      "chunk_id": 196,
      "text": "9.2 Linear Regression\n123\nr\nr\nr\nr\nr r\nr\nr\nr r\nr\nFigure 9.1 Linear regression for d = 1 For instance, the x-axis may denote the age of\nthe baby, and the y-axis her weight 9.2\nLinear Regression\nLinear regression is a common statistical tool for modeling the relationship be-\ntween some “explanatory” variables and some real valued outcome Cast as a\nlearning problem, the domain set X is a subset of Rd, for some d, and the la-\nbel set Y is the set of real numbers We would like to learn a linear function\nh : Rd →R that best approximates the relationship between our variables (say,\nfor example, predicting the weight of a baby as a function of her age and weight\nat birth) Figure 9.1 shows an example of a linear regression predictor for d = 1 The hypothesis class of linear regression predictors is simply the set of linear\nfunctions,\nHreg = Ld = {x 7→⟨w, x⟩+ b : w ∈Rd, b ∈R} Next we need to deﬁne a loss function for regression While in classiﬁcation the\ndeﬁnition of the loss is straightforward, as ℓ(h, (x, y)) simply indicates whether\nh(x) correctly predicts y or not, in regression, if the baby’s weight is 3 kg, both\nthe predictions 3.00001 kg and 4 kg are “wrong,” but we would clearly prefer\nthe former over the latter",
      "word_count": 234,
      "source_page": 123,
      "start_position": 43841,
      "end_position": 44074,
      "sentences_count": 9
    },
    {
      "chunk_id": 197,
      "text": "124\nLinear Predictors\nIn the next subsection, we will see how to implement the ERM rule for linear\nregression with respect to the squared loss Of course, there are a variety of other\nloss functions that one can use, for example, the absolute value loss function,\nℓ(h, (x, y)) = |h(x) −y| The ERM rule for the absolute value loss function can\nbe implemented using linear programming (see Exercise 1.)\nNote that since linear regression is not a binary prediction task, we cannot an-\nalyze its sample complexity using the VC-dimension One possible analysis of the\nsample complexity of linear regression is by relying on the “discretization trick”\n(see Remark 4.1 in Chapter 4); namely, if we are happy with a representation of\neach element of the vector w and the bias b using a ﬁnite number of bits (say\na 64 bits ﬂoating point representation), then the hypothesis class becomes ﬁnite\nand its size is at most 264(d+1) We can now rely on sample complexity bounds\nfor ﬁnite hypothesis classes as described in Chapter 4 Note, however, that to\napply the sample complexity bounds from Chapter 4 we also need that the loss\nfunction will be bounded Later in the book we will describe more rigorous means\nto analyze the sample complexity of regression problems 9.2.1\nLeast Squares\nLeast squares is the algorithm that solves the ERM problem for the hypoth-\nesis class of linear regression predictors with respect to the squared loss",
      "word_count": 244,
      "source_page": 124,
      "start_position": 44133,
      "end_position": 44376,
      "sentences_count": 8
    },
    {
      "chunk_id": 198,
      "text": "9.2 Linear Regression\n125\nOr, in matrix form:\nA =\n\n\n\n x1 xm \n\n\n\n\n\n\n x1 xm \n\n\n\n⊤\n,\n(9.7)\nb =\n\n\n\n x1 xm \n\n\n\n\n\n\ny1 ym\n\n\n (9.8)\nIf A is invertible then the solution to the ERM problem is\nw = A−1 b The case in which A is not invertible requires a few standard tools from linear\nalgebra, which are available in Appendix C It can be easily shown that if the\ntraining instances do not span the entire space of Rd then A is not invertible Nevertheless, we can always ﬁnd a solution to the system Aw = b because b\nis in the range of A Indeed, since A is symmetric we can write it using its\neigenvalue decomposition as A = V DV ⊤, where D is a diagonal matrix and V\nis an orthonormal matrix (that is, V ⊤V is the identity d × d matrix) Deﬁne\nD+ to be the diagonal matrix such that D+\ni,i = 0 if Di,i = 0 and otherwise\nD+\ni,i = 1/Di,i Now, deﬁne\nA+ = V D+V ⊤\nand\nˆw = A+b Let vi denote the i’th column of V Then, we have\nA ˆw = AA+b = V DV ⊤V D+V ⊤b = V DD+V ⊤b =\nX\ni:Di,i̸=0\nviv⊤\ni b",
      "word_count": 243,
      "source_page": 125,
      "start_position": 44473,
      "end_position": 44715,
      "sentences_count": 20
    },
    {
      "chunk_id": 199,
      "text": "Let vi denote the i’th column of V Then, we have\nA ˆw = AA+b = V DV ⊤V D+V ⊤b = V DD+V ⊤b =\nX\ni:Di,i̸=0\nviv⊤\ni b That is, A ˆw is the projection of b onto the span of those vectors vi for which\nDi,i ̸= 0 Since the linear span of x1, , xm is the same as the linear span of\nthose vi, and b is in the linear span of the xi, we obtain that A ˆw = b, which\nconcludes our argument 9.2.2\nLinear Regression for Polynomial Regression Tasks\nSome learning tasks call for nonlinear predictors, such as polynomial predictors Take, for instance, a one dimensional polynomial function of degree n, that is,\np(x) = a0 + a1x + a2x2 + · · · + anxn\nwhere (a0, , an) is a vector of coeﬃcients of size n + 1 In the following we\ndepict a training set that is better ﬁtted using a 3rd degree polynomial predictor\nthan using a linear predictor.",
      "word_count": 172,
      "source_page": 125,
      "start_position": 44685,
      "end_position": 44856,
      "sentences_count": 9
    },
    {
      "chunk_id": 200,
      "text": "126\nLinear Predictors\nWe will focus here on the class of one dimensional, n-degree, polynomial re-\ngression predictors, namely,\nHn\npoly = {x 7→p(x)},\nwhere p is a one dimensional polynomial of degree n, parameterized by a vector\nof coeﬃcients (a0, , an) Note that X = R, since this is a one dimensional\npolynomial, and Y = R, as this is a regression problem One way to learn this class is by reduction to the problem of linear regression,\nwhich we have already shown how to solve To translate a polynomial regression\nproblem to a linear regression problem, we deﬁne the mapping ψ : R →Rn+1\nsuch that ψ(x) = (1, x, x2, , xn) Then we have that\np(ψ(x)) = a0 + a1x + a2x2 + · · · + anxn = ⟨a, ψ(x)⟩\nand we can ﬁnd the optimal vector of coeﬃcients a by using the Least Squares\nalgorithm as shown earlier 9.3\nLogistic Regression\nIn logistic regression we learn a family of functions h from Rd to the interval [0, 1] However, logistic regression is used for classiﬁcation tasks: We can interpret h(x)\nas the probability that the label of x is 1 The hypothesis class associated with\nlogistic regression is the composition of a sigmoid function φsig : R →[0, 1] over\nthe class of linear functions Ld In particular, the sigmoid function used in logistic\nregression is the logistic function, deﬁned as\nφsig(z) =\n1\n1 + exp(−z)",
      "word_count": 244,
      "source_page": 126,
      "start_position": 44857,
      "end_position": 45100,
      "sentences_count": 11
    },
    {
      "chunk_id": 201,
      "text": "9.3 Logistic Regression\n127\nThe hypothesis class is therefore (where for simplicity we are using homogenous\nlinear functions):\nHsig = φsig ◦Ld = {x 7→φsig(⟨w, x⟩) : w ∈Rd} Note that when ⟨w, x⟩is very large then φsig(⟨w, x⟩) is close to 1, whereas if\n⟨w, x⟩is very small then φsig(⟨w, x⟩) is close to 0 Recall that the prediction of the\nhalfspace corresponding to a vector w is sign(⟨w, x⟩) Therefore, the predictions\nof the halfspace hypothesis and the logistic hypothesis are very similar whenever\n|⟨w, x⟩| is large However, when |⟨w, x⟩| is close to 0 we have that φsig(⟨w, x⟩) ≈\n1\n2 Intuitively, the logistic hypothesis is not sure about the value of the label so it\nguesses that the label is sign(⟨w, x⟩) with probability slightly larger than 50% In contrast, the halfspace hypothesis always outputs a deterministic prediction\nof either 1 or −1, even if |⟨w, x⟩| is very close to 0 Next, we need to specify a loss function That is, we should deﬁne how bad it\nis to predict some hw(x) ∈[0, 1] given that the true label is y ∈{±1} Clearly,\nwe would like that hw(x) would be large if y = 1 and that 1 −hw(x) (i.e., the\nprobability of predicting −1) would be large if y = −1 Note that\n1 −hw(x) = 1 −\n1\n1 + exp(−⟨w, x⟩) =\nexp(−⟨w, x⟩)\n1 + exp(−⟨w, x⟩) =\n1\n1 + exp(⟨w, x⟩)",
      "word_count": 244,
      "source_page": 127,
      "start_position": 45118,
      "end_position": 45361,
      "sentences_count": 11
    },
    {
      "chunk_id": 202,
      "text": "Clearly,\nwe would like that hw(x) would be large if y = 1 and that 1 −hw(x) (i.e., the\nprobability of predicting −1) would be large if y = −1 Note that\n1 −hw(x) = 1 −\n1\n1 + exp(−⟨w, x⟩) =\nexp(−⟨w, x⟩)\n1 + exp(−⟨w, x⟩) =\n1\n1 + exp(⟨w, x⟩) Therefore, any reasonable loss function would increase monotonically with\n1\n1+exp(y⟨w,x⟩),\nor equivalently, would increase monotonically with 1 + exp(−y⟨w, x⟩) The lo-\ngistic loss function used in logistic regression penalizes hw based on the log of\n1 + exp(−y⟨w, x⟩) (recall that log is a monotonic function) That is,\nℓ(hw, (x, y)) = log (1 + exp(−y⟨w, x⟩)) Therefore, given a training set S = (x1, y1), , (xm, ym), the ERM problem\nassociated with logistic regression is\nargmin\nw∈Rd\n1\nm\nm\nX\ni=1\nlog (1 + exp(−yi⟨w, xi⟩)) (9.10)\nThe advantage of the logistic loss function is that it is a convex function with\nrespect to w; hence the ERM problem can be solved eﬃciently using standard\nmethods We will study how to learn with convex functions, and in particular\nspecify a simple algorithm for minimizing convex functions, in later chapters The ERM problem associated with logistic regression (Equation (9.10)) is iden-\ntical to the problem of ﬁnding a Maximum Likelihood Estimator, a well-known\nstatistical approach for ﬁnding the parameters that maximize the joint probabil-\nity of a given data set assuming a speciﬁc parametric probability function",
      "word_count": 245,
      "source_page": 127,
      "start_position": 45307,
      "end_position": 45551,
      "sentences_count": 10
    },
    {
      "chunk_id": 203,
      "text": "128\nLinear Predictors\n9.4\nSummary\nThe family of linear predictors is one of the most useful families of hypothesis\nclasses, and many learning algorithms that are being widely used in practice\nrely on linear predictors We have shown eﬃcient algorithms for learning linear\npredictors with respect to the zero-one loss in the separable case and with respect\nto the squared and logistic losses in the unrealizable case In later chapters we\nwill present the properties of the loss function that enable eﬃcient learning Naturally, linear predictors are eﬀective whenever we assume, as prior knowl-\nedge, that some linear predictor attains low risk with respect to the underlying\ndistribution In the next chapter we show how to construct nonlinear predictors\nby composing linear predictors on top of simple classes This will enable us to\nemploy linear predictors for a variety of prior knowledge assumptions 9.5\nBibliographic Remarks\nThe Perceptron algorithm dates back to Rosenblatt (1958) The proof of its\nconvergence rate is due to (Agmon 1954, Novikoﬀ1962) Least Squares regression\ngoes back to Gauss (1795), Legendre (1805), and Adrain (1808) 9.6\nExercises\n1 Show how to cast the ERM problem of linear regression with respect to the\nabsolute value loss function, ℓ(h, (x, y)) = |h(x) −y|, as a linear program;\nnamely, show how to write the problem\nmin\nw\nm\nX\ni=1\n|⟨w, xi⟩−yi|\nas a linear program Hint: Start with proving that for any c ∈R,\n|c| = min\na≥0 a s.t c ≤a and c ≥−a 2",
      "word_count": 249,
      "source_page": 128,
      "start_position": 45562,
      "end_position": 45810,
      "sentences_count": 14
    },
    {
      "chunk_id": 204,
      "text": "9.6 Exercises\n129\nThus, (BR)2 ≤m • When running the Perceptron on this sequence of examples it makes m\nupdates before converging Hint: Choose d = m and for every i choose xi = ei 4 (*) Given any number m, ﬁnd an example of a sequence of labeled examples\n((x1, y1), , (xm, ym)) ∈(R3 × {−1, +1})m on which the upper bound of\nTheorem 9.1 equals m and the perceptron algorithm is bound to make m\nmistakes Hint: Set each xi to be a third dimensional vector of the form (a, b, yi), where\na2 + b2 = R2 −1 Let w∗be the vector (0, 0, 1) Now, go over the proof of\nthe Perceptron’s upper bound (Theorem 9.1), see where we used inequalities\n(≤) rather than equalities (=), and ﬁgure out scenarios where the inequality\nactually holds with equality 5 Suppose we modify the Perceptron algorithm as follows: In the update step,\ninstead of performing w(t+1) = w(t) + yixi whenever we make a mistake, we\nperform w(t+1) = w(t) + ηyixi for some η > 0 Prove that the modiﬁed Per-\nceptron will perform the same number of iterations as the vanilla Perceptron\nand will converge to a vector that points to the same direction as the output\nof the vanilla Perceptron 6",
      "word_count": 217,
      "source_page": 129,
      "start_position": 45906,
      "end_position": 46122,
      "sentences_count": 13
    },
    {
      "chunk_id": 205,
      "text": "Prove that the modiﬁed Per-\nceptron will perform the same number of iterations as the vanilla Perceptron\nand will converge to a vector that points to the same direction as the output\nof the vanilla Perceptron 6 In this problem, we will get bounds on the VC-dimension of the class of\n(closed) balls in Rd, that is,\nBd = {Bv,r : v ∈Rd, r > 0},\nwhere\nBv,r(x) =\n\u001a 1\nif ∥x −v∥≤r\n0\notherwise 1 Consider the mapping φ : Rd →Rd+1 deﬁned by φ(x) = (x, ∥x∥2) Show\nthat if x1, , xm are shattered by Bd then φ(x1), , φ(xm) are shattered\nby the class of halfspaces in Rd+1 (in this question we assume that sign(0) =\n1) What does this tell us about VCdim(Bd) 2 (*) Find a set of d + 1 points in Rd that is shattered by Bd Conclude that\nd + 1 ≤VCdim(Bd) ≤d + 2.",
      "word_count": 155,
      "source_page": 129,
      "start_position": 46086,
      "end_position": 46240,
      "sentences_count": 12
    },
    {
      "chunk_id": 206,
      "text": "10\nBoosting\nBoosting is an algorithmic paradigm that grew out of a theoretical question and\nbecame a very practical machine learning tool The boosting approach uses a\ngeneralization of linear predictors to address two major issues that have been\nraised earlier in the book The ﬁrst is the bias-complexity tradeoﬀ We have\nseen (in Chapter 5) that the error of an ERM learner can be decomposed into\na sum of approximation error and estimation error The more expressive the\nhypothesis class the learner is searching over, the smaller the approximation\nerror is, but the larger the estimation error becomes A learner is thus faced with\nthe problem of picking a good tradeoﬀbetween these two considerations The\nboosting paradigm allows the learner to have smooth control over this tradeoﬀ The learning starts with a basic class (that might have a large approximation\nerror), and as it progresses the class that the predictor may belong to grows\nricher The second issue that boosting addresses is the computational complexity of\nlearning As seen in Chapter 8, for many interesting concept classes the task\nof ﬁnding an ERM hypothesis may be computationally infeasible A boosting\nalgorithm ampliﬁes the accuracy of weak learners Intuitively, one can think of\na weak learner as an algorithm that uses a simple “rule of thumb” to output a\nhypothesis that comes from an easy-to-learn hypothesis class and performs just\nslightly better than a random guess",
      "word_count": 236,
      "source_page": 130,
      "start_position": 46241,
      "end_position": 46476,
      "sentences_count": 12
    },
    {
      "chunk_id": 207,
      "text": "A boosting\nalgorithm ampliﬁes the accuracy of weak learners Intuitively, one can think of\na weak learner as an algorithm that uses a simple “rule of thumb” to output a\nhypothesis that comes from an easy-to-learn hypothesis class and performs just\nslightly better than a random guess When a weak learner can be implemented\neﬃciently, boosting provides a tool for aggregating such weak hypotheses to\napproximate gradually good predictors for larger, and harder to learn, classes In this chapter we will describe and analyze a practically useful boosting algo-\nrithm, AdaBoost (a shorthand for Adaptive Boosting) The AdaBoost algorithm\noutputs a hypothesis that is a linear combination of simple hypotheses In other\nwords, AdaBoost relies on the family of hypothesis classes obtained by composing\na linear predictor on top of simple classes We will show that AdaBoost enables\nus to control the tradeoﬀbetween the approximation and estimation errors by\nvarying a single parameter AdaBoost demonstrates a general theme, that will recur later in the book, of\nexpanding the expressiveness of linear predictors by composing them on top of\nother functions This will be elaborated in Section 10.3 AdaBoost stemmed from the theoretical question of whether an eﬃcient weak\nlearner can be “boosted” into an eﬃcient strong learner This question was raised\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 240,
      "source_page": 130,
      "start_position": 46430,
      "end_position": 46669,
      "sentences_count": 15
    },
    {
      "chunk_id": 208,
      "text": "10.1 Weak Learnability\n131\nby Kearns and Valiant in 1988 and solved in 1990 by Robert Schapire, then\na graduate student at MIT However, the proposed mechanism was not very\npractical In 1995, Robert Schapire and Yoav Freund proposed the AdaBoost\nalgorithm, which was the ﬁrst truly practical implementation of boosting This\nsimple and elegant algorithm became hugely popular, and Freund and Schapire’s\nwork has been recognized by numerous awards Furthermore, boosting is a great example for the practical impact of learning\ntheory While boosting originated as a purely theoretical problem, it has led to\npopular and widely used algorithms Indeed, as we shall demonstrate later in\nthis chapter, AdaBoost has been successfully used for learning to detect faces in\nimages 10.1\nWeak Learnability\nRecall the deﬁnition of PAC learning given in Chapter 3: A hypothesis class,\nH, is PAC learnable if there exist mH : (0, 1)2 →N and a learning algorithm\nwith the following property: For every ϵ, δ ∈(0, 1), for every distribution D over\nX, and for every labeling function f : X →{±1}, if the realizable assumption\nholds with respect to H, D, f, then when running the learning algorithm on\nm ≥mH(ϵ, δ) i.i.d examples generated by D and labeled by f, the algorithm\nreturns a hypothesis h such that, with probability of at least 1−δ, L(D,f)(h) ≤ϵ",
      "word_count": 224,
      "source_page": 131,
      "start_position": 46670,
      "end_position": 46893,
      "sentences_count": 9
    },
    {
      "chunk_id": 209,
      "text": "10.1\nWeak Learnability\nRecall the deﬁnition of PAC learning given in Chapter 3: A hypothesis class,\nH, is PAC learnable if there exist mH : (0, 1)2 →N and a learning algorithm\nwith the following property: For every ϵ, δ ∈(0, 1), for every distribution D over\nX, and for every labeling function f : X →{±1}, if the realizable assumption\nholds with respect to H, D, f, then when running the learning algorithm on\nm ≥mH(ϵ, δ) i.i.d examples generated by D and labeled by f, the algorithm\nreturns a hypothesis h such that, with probability of at least 1−δ, L(D,f)(h) ≤ϵ Furthermore, the fundamental theorem of learning theory (Theorem 6.8 in\nChapter 6) characterizes the family of learnable classes and states that every PAC\nlearnable class can be learned using any ERM algorithm However, the deﬁnition\nof PAC learning and the fundamental theorem of learning theory ignores the\ncomputational aspect of learning Indeed, as we have shown in Chapter 8, there\nare cases in which implementing the ERM rule is computationally hard (even in\nthe realizable case) However, perhaps we can trade computational hardness with the requirement\nfor accuracy Given a distribution D and a target labeling function f, maybe there\nexists an eﬃciently computable learning algorithm whose error is just slightly\nbetter than a random guess This motivates the following deﬁnition",
      "word_count": 224,
      "source_page": 131,
      "start_position": 46791,
      "end_position": 47014,
      "sentences_count": 8
    },
    {
      "chunk_id": 210,
      "text": "132\nBoosting\nThis deﬁnition is almost identical to the deﬁnition of PAC learning, which\nhere we will call strong learning, with one crucial diﬀerence: Strong learnability\nimplies the ability to ﬁnd an arbitrarily good classiﬁer (with error rate at most\nϵ for an arbitrarily small ϵ > 0) In weak learnability, however, we only need to\noutput a hypothesis whose error rate is at most 1/2 −γ, namely, whose error\nrate is slightly better than what a random labeling would give us The hope is\nthat it may be easier to come up with eﬃcient weak learners than with eﬃcient\n(full) PAC learners The fundamental theorem of learning (Theorem 6.8) states that if a hypothesis\nclass H has a VC dimension d, then the sample complexity of PAC learning H\nsatisﬁes mH(ϵ, δ) ≥C1\nd+log(1/δ)\nϵ\n, where C1 is a constant Applying this with\nϵ = 1/2−γ we immediately obtain that if d = ∞then H is not γ-weak-learnable This implies that from the statistical perspective (i.e., if we ignore computational\ncomplexity), weak learnability is also characterized by the VC dimension of H\nand therefore is just as hard as PAC (strong) learning However, when we do\nconsider computational complexity, the potential advantage of weak learning is\nthat maybe there is an algorithm that satisﬁes the requirements of weak learning\nand can be implemented eﬃciently",
      "word_count": 226,
      "source_page": 132,
      "start_position": 47125,
      "end_position": 47350,
      "sentences_count": 7
    },
    {
      "chunk_id": 211,
      "text": "This implies that from the statistical perspective (i.e., if we ignore computational\ncomplexity), weak learnability is also characterized by the VC dimension of H\nand therefore is just as hard as PAC (strong) learning However, when we do\nconsider computational complexity, the potential advantage of weak learning is\nthat maybe there is an algorithm that satisﬁes the requirements of weak learning\nand can be implemented eﬃciently One possible approach is to take a “simple” hypothesis class, denoted B, and\nto apply ERM with respect to B as the weak learning algorithm For this to\nwork, we need that B will satisfy two requirements:\n• ERMB is eﬃciently implementable • For every sample that is labeled by some hypothesis from H, any ERMB\nhypothesis will have an error of at most 1/2 −γ Then, the immediate question is whether we can boost an eﬃcient weak learner\ninto an eﬃcient strong learner In the next section we will show that this is\nindeed possible, but before that, let us show an example in which eﬃcient weak\nlearnability of a class H is possible using a base hypothesis class B",
      "word_count": 187,
      "source_page": 132,
      "start_position": 47285,
      "end_position": 47471,
      "sentences_count": 7
    },
    {
      "chunk_id": 212,
      "text": "Then, the immediate question is whether we can boost an eﬃcient weak learner\ninto an eﬃcient strong learner In the next section we will show that this is\nindeed possible, but before that, let us show an example in which eﬃcient weak\nlearnability of a class H is possible using a base hypothesis class B Example 10.1 (Weak Learning of 3-Piece Classiﬁers Using Decision Stumps)\nLet X = R and let H be the class of 3-piece classiﬁers, namely, H = {hθ1,θ2,b :\nθ1, θ2 ∈R, θ1 < θ2, b ∈{±1}}, where for every x,\nhθ1,θ2,b(x) =\n(\n+b\nif x < θ1 or x > θ2\n−b\nif θ1 ≤x ≤θ2\nAn example hypothesis (for b = 1) is illustrated as follows:\nθ1\nθ2\n+\n+\n−\nLet B be the class of Decision Stumps, that is, B = {x 7→sign(x −θ) · b :\nθ ∈\nR, b ∈{±1}} In the following we show that ERMB is a γ-weak learner for H,\nfor γ = 1/12.",
      "word_count": 168,
      "source_page": 132,
      "start_position": 47417,
      "end_position": 47584,
      "sentences_count": 4
    },
    {
      "chunk_id": 213,
      "text": "10.1 Weak Learnability\n133\nTo see that, we ﬁrst show that for every distribution that is consistent with\nH, there exists a decision stump with LD(h) ≤1/3 Indeed, just note that\nevery classiﬁer in H consists of three regions (two unbounded rays and a center\ninterval) with alternate labels For any pair of such regions, there exists a decision\nstump that agrees with the labeling of these two components Note that for every\ndistribution D over R and every partitioning of the line into three such regions,\none of these regions must have D-weight of at most 1/3 Let h ∈H be a zero\nerror hypothesis A decision stump that disagrees with h only on such a region\nhas an error of at most 1/3 Finally, since the VC-dimension of decision stumps is 2, if the sample size is\ngreater than Ω(log(1/δ)/ϵ2), then with probability of at least 1 −δ, the ERMB\nrule returns a hypothesis with an error of at most 1/3 + ϵ Setting ϵ = 1/12 we\nobtain that the error of ERMB is at most 1/3 + 1/12 = 1/2 −1/12 We see that ERMB is a γ-weak learner for H We next show how to implement\nthe ERM rule eﬃciently for decision stumps 10.1.1\nEﬃcient Implementation of ERM for Decision Stumps\nLet X = Rd and consider the base hypothesis class of decision stumps over Rd,\nnamely,\nHDS = {x 7→sign(θ −xi) · b : θ ∈R, i ∈[d], b ∈{±1}}",
      "word_count": 246,
      "source_page": 133,
      "start_position": 47585,
      "end_position": 47830,
      "sentences_count": 11
    },
    {
      "chunk_id": 214,
      "text": "We next show how to implement\nthe ERM rule eﬃciently for decision stumps 10.1.1\nEﬃcient Implementation of ERM for Decision Stumps\nLet X = Rd and consider the base hypothesis class of decision stumps over Rd,\nnamely,\nHDS = {x 7→sign(θ −xi) · b : θ ∈R, i ∈[d], b ∈{±1}} For simplicity, assume that b = 1; that is, we focus on all the hypotheses in\nHDS of the form sign(θ −xi) Let S = ((x1, y1), , (xm, ym)) be a training set We will show how to implement an ERM rule, namely, how to ﬁnd a decision\nstump that minimizes LS(h) Furthermore, since in the next section we will\nshow that AdaBoost requires ﬁnding a hypothesis with a small risk relative to\nsome distribution over S, we will show here how to minimize such risk functions Concretely, let D be a probability vector in Rm (that is, all elements of D are\nnonnegative and P\ni Di = 1) The weak learner we describe later receives D and\nS and outputs a decision stump h : X →Y that minimizes the risk w.r.t D,\nLD(h) =\nm\nX\ni=1\nDi1[h(xi)̸=yi] Note that if D = (1/m, , 1/m) then LD(h) = LS(h) Recall that each decision stump is parameterized by an index j ∈[d] and a\nthreshold θ Therefore, minimizing LD(h) amounts to solving the problem\nmin\nj∈[d] min\nθ∈R\n\nX\ni:yi=1\nDi1[xi,j>θ] +\nX\ni:yi=−1\nDi1[xi,j≤θ]\n\n",
      "word_count": 243,
      "source_page": 133,
      "start_position": 47780,
      "end_position": 48022,
      "sentences_count": 14
    },
    {
      "chunk_id": 215,
      "text": "134\nBoosting\nthreshold θ Therefore, instead of minimizing over θ ∈R we can minimize over\nθ ∈Θj This already gives us an eﬃcient procedure: Choose j ∈[d] and θ ∈Θj that\nminimize the objective value of Equation (10.1) For every j and θ ∈Θj we\nhave to calculate a sum over m examples; therefore the runtime of this approach\nwould be O(dm2) We next show a simple trick that enables us to minimize the\nobjective in time O(dm) The observation is as follows Suppose we have calculated the objective for\nθ ∈(xi−1,j, xi,j) Let F(θ) be the value of the objective Then, when we consider\nθ′ ∈(xi,j, xi+1,j) we have that\nF(θ′) = F(θ) −Di1[yi=1] + Di1[yi=−1] = F(θ) −yiDi Therefore, we can calculate the objective at θ′ in a constant time, given the\nobjective at the previous threshold, θ It follows that after a preprocessing step\nin which we sort the examples with respect to each coordinate, the minimization\nproblem can be performed in time O(dm) This yields the following pseudocode ERM for Decision Stumps\ninput:\ntraining set S = (x1, y1), , (xm, ym)\ndistribution vector D\ngoal: Find j⋆, θ⋆that solve Equation (10.1)\ninitialize: F ⋆= ∞\nfor j = 1, , d\nsort S using the j’th coordinate, and denote\nx1,j ≤x2,j ≤· · · ≤xm,j ≤xm+1,j\ndef\n= xm,j + 1\nF = P\ni:yi=1 Di\nif F < F ⋆\nF ⋆= F, θ⋆= x1,j −1, j⋆= j\nfor i = 1,",
      "word_count": 248,
      "source_page": 134,
      "start_position": 48072,
      "end_position": 48319,
      "sentences_count": 15
    },
    {
      "chunk_id": 216,
      "text": ", (xm, ym)\ndistribution vector D\ngoal: Find j⋆, θ⋆that solve Equation (10.1)\ninitialize: F ⋆= ∞\nfor j = 1, , d\nsort S using the j’th coordinate, and denote\nx1,j ≤x2,j ≤· · · ≤xm,j ≤xm+1,j\ndef\n= xm,j + 1\nF = P\ni:yi=1 Di\nif F < F ⋆\nF ⋆= F, θ⋆= x1,j −1, j⋆= j\nfor i = 1, , m\nF = F −yiDi\nif F < F ⋆and xi,j ̸= xi+1,j\nF ⋆= F, θ⋆= 1\n2(xi,j + xi+1,j), j⋆= j\noutput j⋆, θ⋆\n10.2\nAdaBoost\nAdaBoost (short for Adaptive Boosting) is an algorithm that has access to a\nweak learner and ﬁnds a hypothesis with a low empirical risk The AdaBoost\nalgorithm receives as input a training set of examples S = (x1, y1), , (xm, ym),\nwhere for each i, yi = f(xi) for some labeling function f The boosting process\nproceeds in a sequence of consecutive rounds At round t, the booster ﬁrst deﬁnes",
      "word_count": 165,
      "source_page": 134,
      "start_position": 48255,
      "end_position": 48419,
      "sentences_count": 7
    },
    {
      "chunk_id": 217,
      "text": "10.2 AdaBoost\n135\na distribution over the examples in S, denoted D(t) That is, D(t) ∈Rm\n+ and\nPm\ni=1 D(t)\ni\n= 1 Then, the booster passes the distribution D(t) and the sample S\nto the weak learner (That way, the weak learner can construct i.i.d examples\naccording to D(t) and f.) The weak learner is assumed to return a “weak”\nhypothesis, ht, whose error,\nϵt\ndef\n= LD(t)(ht)\ndef\n=\nm\nX\ni=1\nD(t)\ni 1[ht(xi)̸=yi],\nis at most 1\n2−γ (of course, there is a probability of at most δ that the weak learner\nfails) Then, AdaBoost assigns a weight for ht as follows: wt = 1\n2 log\n\u0010\n1\nϵt −1\n\u0011 That is, the weight of ht is inversely proportional to the error of ht At the end\nof the round, AdaBoost updates the distribution so that examples on which ht\nerrs will get a higher probability mass while examples on which ht is correct will\nget a lower probability mass Intuitively, this will force the weak learner to focus\non the problematic examples in the next round The output of the AdaBoost\nalgorithm is a “strong” classiﬁer that is based on a weighted sum of all the weak\nhypotheses The pseudocode of AdaBoost is presented in the following AdaBoost\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nweak learner WL\nnumber of rounds T\ninitialize D(1) = ( 1\nm, , 1\nm) for t = 1,",
      "word_count": 246,
      "source_page": 135,
      "start_position": 48420,
      "end_position": 48665,
      "sentences_count": 15
    },
    {
      "chunk_id": 218,
      "text": ", 1\nm) for t = 1, , T:\ninvoke weak learner ht = WL(D(t), S)\ncompute ϵt = Pm\ni=1 D(t)\ni\n1[yi̸=ht(xi)]\nlet wt = 1\n2 log\n\u0010\n1\nϵt −1\n\u0011\nupdate D(t+1)\ni\n=\nD(t)\ni\nexp(−wtyiht(xi))\nPm\nj=1 D(t)\nj\nexp(−wtyjht(xj)) for all i = 1, , m\noutput the hypothesis hs(x) = sign\n\u0010PT\nt=1 wtht(x)\n\u0011 The following theorem shows that the training error of the output hypothesis\ndecreases exponentially fast with the number of boosting rounds theorem 10.2\nLet S be a training set and assume that at each iteration of\nAdaBoost, the weak learner returns a hypothesis for which ϵt ≤1/2 −γ Then,\nthe training error of the output hypothesis of AdaBoost is at most\nLS(hs) = 1\nm\nm\nX\ni=1\n1[hs(xi)̸=yi] ≤\nexp(−2 γ2 T) Proof\nFor each t, denote ft = P\np≤t wphp Therefore, the output of AdaBoost",
      "word_count": 152,
      "source_page": 135,
      "start_position": 48659,
      "end_position": 48810,
      "sentences_count": 9
    },
    {
      "chunk_id": 219,
      "text": "136\nBoosting\nis fT In addition, denote\nZt = 1\nm\nm\nX\ni=1\ne−yift(xi) Note that for any hypothesis we have that 1[h(x)̸=y] ≤e−yh(x) Therefore, LS(fT ) ≤\nZT , so it suﬃces to show that ZT ≤e−2γ2T To upper bound ZT we rewrite it\nas\nZT = ZT\nZ0\n=\nZT\nZT −1\n· ZT −1\nZT −2\n· · · Z2\nZ1\n· Z1\nZ0\n,\n(10.2)\nwhere we used the fact that Z0 = 1 because f0 ≡0 Therefore, it suﬃces to show\nthat for every round t,\nZt+1\nZt\n≤e−2γ2 (10.3)\nTo do so, we ﬁrst note that using a simple inductive argument, for all t and i,\nD(t+1)\ni\n=\ne−yift(xi)\nPm\nj=1 e−yjft(xj) Hence,\nZt+1\nZt\n=\nPm\ni=1 e−yift+1(xi)\nm\nP\nj=1\ne−yjft(xj)\n=\nPm\ni=1 e−yift(xi)e−yiwt+1ht+1(xi)\nm\nP\nj=1\ne−yjft(xj)\n=\nm\nX\ni=1\nD(t+1)\ni\ne−yiwt+1ht+1(xi)\n= e−wt+1\nX\ni:yiht+1(xi)=1\nD(t+1)\ni\n+ ewt+1\nX\ni:yiht+1(xi)=−1\nD(t+1)\ni\n= e−wt+1(1 −ϵt+1) + ewt+1ϵt+1\n=\n1\np\n1/ϵt+1 −1\n(1 −ϵt+1) +\np\n1/ϵt+1 −1 ϵt+1\n=\nr\nϵt+1\n1 −ϵt+1\n(1 −ϵt+1) +\ns\n1 −ϵt+1\nϵt+1\nϵt+1\n= 2\np\nϵt+1(1 −ϵt+1) By our assumption, ϵt+1 ≤1\n2 −γ Since the function g(a) = a(1 −a) is mono-\ntonically increasing in [0, 1/2], we obtain that\n2\np\nϵt+1(1 −ϵt+1) ≤2\ns\u00121\n2 −γ\n\u0013 \u00121\n2 + γ\n\u0013\n=\np\n1 −4γ2.",
      "word_count": 235,
      "source_page": 136,
      "start_position": 48811,
      "end_position": 49045,
      "sentences_count": 10
    },
    {
      "chunk_id": 220,
      "text": "10.3 Linear Combinations of Base Hypotheses\n137\nFinally, using the inequality 1 −a ≤e−a we have that\np\n1 −4γ2 ≤e−4γ2/2 =\ne−2γ2 This shows that Equation (10.3) holds and thus concludes our proof Each iteration of AdaBoost involves O(m) operations as well as a single call to\nthe weak learner Therefore, if the weak learner can be implemented eﬃciently\n(as happens in the case of ERM with respect to decision stumps) then the total\ntraining process will be eﬃcient Remark 10.2\nTheorem 10.2 assumes that at each iteration of AdaBoost, the\nweak learner returns a hypothesis with weighted sample error of at most 1/2−γ According to the deﬁnition of a weak learner, it can fail with probability δ Using\nthe union bound, the probability that the weak learner will not fail at all of the\niterations is at least 1 −δT As we show in Exercise 1, the dependence of the\nsample complexity on δ can always be logarithmic in 1/δ, and therefore invoking\nthe weak learner with a very small δ is not problematic We can therefore assume\nthat δT is also small Furthermore, since the weak learner is only applied with\ndistributions over the training set, in many cases we can implement the weak\nlearner so that it will have a zero probability of failure (i.e., δ = 0) This is the\ncase, for example, in the weak learner that ﬁnds the minimum value of LD(h)\nfor decision stumps, as described in the previous section",
      "word_count": 248,
      "source_page": 137,
      "start_position": 49046,
      "end_position": 49293,
      "sentences_count": 11
    },
    {
      "chunk_id": 221,
      "text": "Furthermore, since the weak learner is only applied with\ndistributions over the training set, in many cases we can implement the weak\nlearner so that it will have a zero probability of failure (i.e., δ = 0) This is the\ncase, for example, in the weak learner that ﬁnds the minimum value of LD(h)\nfor decision stumps, as described in the previous section Theorem 10.2 tells us that the empirical risk of the hypothesis constructed by\nAdaBoost goes to zero as T grows However, what we really care about is the\ntrue risk of the output hypothesis To argue about the true risk, we note that the\noutput of AdaBoost is in fact a composition of a halfspace over the predictions\nof the T weak hypotheses constructed by the weak learner In the next section\nwe show that if the weak hypotheses come from a base hypothesis class of low\nVC-dimension, then the estimation error of AdaBoost will be small; namely, the\ntrue risk of the output of AdaBoost would not be very far from its empirical risk 10.3\nLinear Combinations of Base Hypotheses\nAs mentioned previously, a popular approach for constructing a weak learner\nis to apply the ERM rule with respect to a base hypothesis class (e.g., ERM\nover decision stumps) We have also seen that boosting outputs a composition\nof a halfspace over the predictions of the weak hypotheses",
      "word_count": 232,
      "source_page": 137,
      "start_position": 49231,
      "end_position": 49462,
      "sentences_count": 8
    },
    {
      "chunk_id": 222,
      "text": "138\nBoosting\n(h1(x), , hT (x)) ∈RT , and then applying the (homogenous) halfspace deﬁned\nby w on ψ(x) In this section we analyze the estimation error of L(B, T) by bounding the\nVC-dimension of L(B, T) in terms of the VC-dimension of B and T We will\nshow that, up to logarithmic factors, the VC-dimension of L(B, T) is bounded\nby T times the VC-dimension of B It follows that the estimation error of Ad-\naBoost grows linearly with T On the other hand, the empirical risk of AdaBoost\ndecreases with T In fact, as we demonstrate later, T can be used to decrease\nthe approximation error of L(B, T) Therefore, the parameter T of AdaBoost\nenables us to control the bias-complexity tradeoﬀ To demonstrate how the expressive power of L(B, T) increases with T, consider\nthe simple example, in which X = R and the base class is Decision Stumps,\nHDS1 = {x 7→sign(x −θ) · b :\nθ ∈R, b ∈{±1}} Note that in this one dimensional case, HDS1 is in fact equivalent to (nonho-\nmogenous) halfspaces on R Now, let H be the rather complex class (compared to halfspaces on the line)\nof piece-wise constant functions Let gr be a piece-wise constant function with at\nmost r pieces; that is, there exist thresholds −∞= θ0 < θ1 < θ2 < · · · < θr = ∞\nsuch that\ngr(x) =\nr\nX\ni=1\nαi1[x∈(θi−1,θi]]\n∀i,\nαi ∈{±1}",
      "word_count": 242,
      "source_page": 138,
      "start_position": 49550,
      "end_position": 49791,
      "sentences_count": 12
    },
    {
      "chunk_id": 223,
      "text": "Now, let H be the rather complex class (compared to halfspaces on the line)\nof piece-wise constant functions Let gr be a piece-wise constant function with at\nmost r pieces; that is, there exist thresholds −∞= θ0 < θ1 < θ2 < · · · < θr = ∞\nsuch that\ngr(x) =\nr\nX\ni=1\nαi1[x∈(θi−1,θi]]\n∀i,\nαi ∈{±1} Denote by Gr the class of all such piece-wise constant classiﬁers with at most r\npieces In the following we show that GT ⊆L(HDS1, T); namely, the class of halfspaces\nover T decision stumps yields all the piece-wise constant classiﬁers with at most\nT pieces Indeed, without loss of generality consider any g ∈GT with αt = (−1)t This\nimplies that if x is in the interval (θt−1, θt], then g(x) = (−1)t For example:\nNow, the function\nh(x) = sign\n T\nX\nt=1\nwt sign(x −θt−1) ,\n(10.5)\nwhere w1 = 0.5 and for t > 1, wt = (−1)t, is in L(HDS1, T) and is equal to g\n(see Exercise 2).",
      "word_count": 173,
      "source_page": 138,
      "start_position": 49732,
      "end_position": 49904,
      "sentences_count": 8
    },
    {
      "chunk_id": 224,
      "text": "10.3 Linear Combinations of Base Hypotheses\n139\nFrom this example we obtain that L(HDS1, T) can shatter any set of T + 1\ninstances in R; hence the VC-dimension of L(HDS1, T) is at least T +1 Therefore,\nT is a parameter that can control the bias-complexity tradeoﬀ: Enlarging T\nyields a more expressive hypothesis class but on the other hand might increase\nthe estimation error In the next subsection we formally upper bound the VC-\ndimension of L(B, T) for any base class B 10.3.1\nThe VC-Dimension of L(B, T)\nThe following lemma tells us that the VC-dimension of L(B, T) is upper bounded\nby ˜O(VCdim(B) T) (the ˜O notation ignores constants and logarithmic factors) lemma 10.3\nLet B be a base class and let L(B, T) be as deﬁned in Equa-\ntion (10.4) Assume that both T and VCdim(B) are at least 3 Then,\nVCdim(L(B, T)) ≤T (VCdim(B) + 1) (3 log(T (VCdim(B) + 1)) + 2) Proof\nDenote d = VCdim(B) Let C = {x1, , xm} be a set that is shat-\ntered by L(B, T) Each labeling of C by h ∈L(B, T) is obtained by ﬁrst choos-\ning h1, , hT ∈B and then applying a halfspace hypothesis over the vector\n(h1(x), , hT (x)) By Sauer’s lemma, there are at most (em/d)d diﬀerent di-\nchotomies (i.e., labelings) induced by B over C Therefore, we need to choose\nT hypotheses, out of at most (em/d)d diﬀerent hypotheses",
      "word_count": 243,
      "source_page": 139,
      "start_position": 49905,
      "end_position": 50147,
      "sentences_count": 15
    },
    {
      "chunk_id": 225,
      "text": "By Sauer’s lemma, there are at most (em/d)d diﬀerent di-\nchotomies (i.e., labelings) induced by B over C Therefore, we need to choose\nT hypotheses, out of at most (em/d)d diﬀerent hypotheses There are at most\n(em/d)dT ways to do it Next, for each such choice, we apply a linear predictor,\nwhich yields at most (em/T)T dichotomies Therefore, the overall number of\ndichotomies we can construct is upper bounded by\n(em/d)dT (em/T)T ≤m(d+1)T ,\nwhere we used the assumption that both d and T are at least 3 Since we assume\nthat C is shattered, we must have that the preceding is at least 2m, which yields\n2m ≤m(d+1)T Therefore,\nm ≤log(m)(d + 1)T\nlog(2) Lemma A.1 in Chapter A tells us that a necessary condition for the above to\nhold is that\nm ≤2(d + 1)T\nlog(2)\nlog (d + 1)T\nlog(2)\n≤(d + 1)T(3 log((d + 1)T) + 2),\nwhich concludes our proof In Exercise 4 we show that for some base classes, B, it also holds that VCdim(L(B, T)) ≥\nΩ(VCdim(B) T).",
      "word_count": 175,
      "source_page": 139,
      "start_position": 50116,
      "end_position": 50290,
      "sentences_count": 9
    },
    {
      "chunk_id": 226,
      "text": "140\nBoosting\nA\nB\nC\nD\nFigure 10.1 The four types of functions, g, used by the base hypotheses for face\nrecognition The value of g for type A or B is the diﬀerence between the sum of the\npixels within two rectangular regions These regions have the same size and shape and\nare horizontally or vertically adjacent For type C, the value of g is the sum within\ntwo outside rectangles subtracted from the sum in a center rectangle For type D, we\ncompute the diﬀerence between diagonal pairs of rectangles 10.4\nAdaBoost for Face Recognition\nWe now turn to a base hypothesis that has been proposed by Viola and Jones for\nthe task of face recognition In this task, the instance space is images, represented\nas matrices of gray level values of pixels To be concrete, let us take images of\nsize 24 × 24 pixels, and therefore our instance space is the set of real valued\nmatrices of size 24 × 24 The goal is to learn a classiﬁer, h : X →{±1}, that\ngiven an image as input, should output whether the image is of a human face or\nnot Each hypothesis in the base class is of the form h(x) = f(g(x)), where f is a\ndecision stump hypothesis and g : R24,24 →R is a function that maps an image\nto a scalar Each function g is parameterized by\n• An axis aligned rectangle R",
      "word_count": 241,
      "source_page": 140,
      "start_position": 50291,
      "end_position": 50531,
      "sentences_count": 11
    },
    {
      "chunk_id": 227,
      "text": "Each hypothesis in the base class is of the form h(x) = f(g(x)), where f is a\ndecision stump hypothesis and g : R24,24 →R is a function that maps an image\nto a scalar Each function g is parameterized by\n• An axis aligned rectangle R Since each image is of size 24 × 24, there are at\nmost 244 axis aligned rectangles • A type, t ∈{A, B, C, D} Each type corresponds to a mask, as depicted in\nFigure 10.1 To calculate g we stretch the mask t to ﬁt the rectangle R and then calculate\nthe sum of the pixels (that is, sum of their gray level values) that lie within the\nred rectangles and subtract it from the sum of pixels in the blue rectangles Since the number of such functions g is at most 244 · 4, we can implement a\nweak learner for the base hypothesis class by ﬁrst calculating all the possible\noutputs of g on each image, and then apply the weak learner of decision stumps\ndescribed in the previous subsection It is possible to perform the ﬁrst step very",
      "word_count": 189,
      "source_page": 140,
      "start_position": 50485,
      "end_position": 50673,
      "sentences_count": 8
    },
    {
      "chunk_id": 228,
      "text": "10.5 Summary\n141\nFigure 10.2 The ﬁrst and second features selected by AdaBoost, as implemented by\nViola and Jones The two features are shown in the top row and then overlaid on a\ntypical training face in the bottom row The ﬁrst feature measures the diﬀerence in\nintensity between the region of the eyes and a region across the upper cheeks The\nfeature capitalizes on the observation that the eye region is often darker than the\ncheeks The second feature compares the intensities in the eye regions to the intensity\nacross the bridge of the nose eﬃciently by a preprocessing step in which we calculate the integral image of\neach image in the training set See Exercise 5 for details In Figure 10.2 we depict the ﬁrst two features selected by AdaBoost when\nrunning it with the base features proposed by Viola and Jones 10.5\nSummary\nBoosting is a method for amplifying the accuracy of weak learners In this chapter\nwe described the AdaBoost algorithm We have shown that after T iterations of\nAdaBoost, it returns a hypothesis from the class L(B, T), obtained by composing\na linear classiﬁer on T hypotheses from a base class B We have demonstrated\nhow the parameter T controls the tradeoﬀbetween approximation and estimation\nerrors In the next chapter we will study how to tune parameters such as T, based\non the data",
      "word_count": 229,
      "source_page": 141,
      "start_position": 50674,
      "end_position": 50902,
      "sentences_count": 13
    },
    {
      "chunk_id": 229,
      "text": "142\nBoosting\nH is weakly learnable using B For example, Klivans & Sherstov (2006) have\nshown that PAC learning of the class of intersection of halfspaces is hard (even\nin the realizable case) This hardness result can be used to show that agnostic\nPAC learning of a single halfspace is also computationally hard (Shalev-Shwartz,\nShamir & Sridharan 2010) The idea is to show that an agnostic PAC learner\nfor a single halfspace can yield a weak learner for the class of intersection of\nhalfspaces, and since such a weak learner can be boosted, we will obtain a strong\nlearner for the class of intersection of halfspaces AdaBoost also shows an equivalence between the existence of a weak learner\nand separability of the data using a linear classiﬁer over the predictions of base\nhypotheses This result is closely related to von Neumann’s minimax theorem\n(von Neumann 1928), a fundamental result in game theory AdaBoost is also related to the concept of margin, which we will study later on\nin Chapter 15 It can also be viewed as a forward greedy selection algorithm, a\ntopic that will be presented in Chapter 25 A recent book by Schapire & Freund\n(2012) covers boosting from all points of view, and gives easy access to the wealth\nof research that this ﬁeld has produced 10.7\nExercises\n1",
      "word_count": 222,
      "source_page": 142,
      "start_position": 51017,
      "end_position": 51238,
      "sentences_count": 10
    },
    {
      "chunk_id": 230,
      "text": "A recent book by Schapire & Freund\n(2012) covers boosting from all points of view, and gives easy access to the wealth\nof research that this ﬁeld has produced 10.7\nExercises\n1 Boosting the Conﬁdence: Let A be an algorithm that guarantees the fol-\nlowing: There exist some constant δ0 ∈(0, 1) and a function mH : (0, 1) →N\nsuch that for every ϵ ∈(0, 1), if m ≥mH(ϵ) then for every distribution D it\nholds that with probability of at least 1 −δ0, LD(A(S)) ≤minh∈H LD(h) + ϵ Suggest a procedure that relies on A and learns H in the usual agnostic\nPAC learning model and has a sample complexity of\nmH(ϵ, δ) ≤k mH(ϵ) +\n\u00182 log(4k/δ)\nϵ2\n\u0019\n,\nwhere\nk = ⌈log(δ)/ log(δ0)⌉ Hint: Divide the data into k + 1 chunks, where each of the ﬁrst k chunks\nis of size mH(ϵ) examples Train the ﬁrst k chunks using A Argue that the\nprobability that for all of these chunks we have LD(A(S)) > minh∈H LD(h)+ϵ\nis at most δk\n0 ≤δ/2 Finally, use the last chunk to choose from the k hypotheses\nthat A generated from the k chunks (by relying on Corollary 4.6) 2 Prove that the function h given in Equation (10.5) equals the piece-wise con-\nstant function deﬁned according to the same thresholds as h 3",
      "word_count": 226,
      "source_page": 142,
      "start_position": 51207,
      "end_position": 51432,
      "sentences_count": 11
    },
    {
      "chunk_id": 231,
      "text": "10.7 Exercises\n143\nShow that the error of ht w.r.t the distribution D(t+1) is exactly 1/2 That\nis, show that for every t ∈[T]\nm\nX\ni=1\nD(t+1)\ni\n1[yi̸=ht(xi)] = 1/2 4 In this exercise we discuss the VC-dimension of classes of the form L(B, T) We proved an upper bound of O(dT log(dT)), where d = VCdim(B) Here we\nwish to prove an almost matching lower bound However, that will not be the\ncase for all classes B 1 Note that for every class B and every number T ≥1, VCdim(B) ≤\nVCdim(L(B, T)) Find a class B for which VCdim(B) = VCdim(L(B, T))\nfor every T ≥1 Hint: Take X to be a ﬁnite set 2 Let Bd be the class of decision stumps over Rd Prove that log(d) ≤\nVCdim(Bd) ≤5 + 2 log(d) Hints:\n• For the upper bound, rely on Exercise 11 • For the lower bound, assume d = 2k Let A be a k × d matrix whose\ncolumns are all the d binary vectors in {±1}k The rows of A form\na set of k vectors in Rd Show that this set is shattered by decision\nstumps over Rd 3 Let T ≥1 be any integer Prove that VCdim(L(Bd, T)) ≥0.5 T log(d) Hint: Construct a set of T\n2 k instances by taking the rows of the matrix A\nfrom the previous question, and the rows of the matrices 2A, 3A, 4A, , T\n2 A",
      "word_count": 246,
      "source_page": 143,
      "start_position": 51472,
      "end_position": 51717,
      "sentences_count": 25
    },
    {
      "chunk_id": 232,
      "text": "11\nModel Selection and Validation\nIn the previous chapter we have described the AdaBoost algorithm and have\nshown how the parameter T of AdaBoost controls the bias-complexity trade-\noﬀ But, how do we set T in practice More generally, when approaching some\npractical problem, we usually can think of several algorithms that may yield a\ngood solution, each of which might have several parameters How can we choose\nthe best algorithm for the particular problem at hand And how do we set the\nalgorithm’s parameters This task is often called model selection To illustrate the model selection task, consider the problem of learning a one\ndimensional regression function, h : R →R Suppose that we obtain a training\nset as depicted in the ﬁgure We can consider ﬁtting a polynomial to the data, as described in Chapter 9 However, we might be uncertain regarding which degree d would give the best\nresults for our data set: A small degree may not ﬁt the data well (i.e., it will\nhave a large approximation error), whereas a high degree may lead to overﬁtting\n(i.e., it will have a large estimation error) In the following we depict the result\nof ﬁtting a polynomial of degrees 2, 3, and 10 It is easy to see that the empirical\nrisk decreases as we enlarge the degree However, looking at the graphs, our\nintuition tells us that setting the degree to 3 may be better than setting it to 10",
      "word_count": 244,
      "source_page": 144,
      "start_position": 51823,
      "end_position": 52066,
      "sentences_count": 13
    },
    {
      "chunk_id": 233,
      "text": "11.1 Model Selection Using SRM\n145\nIn this chapter we will present two approaches for model selection The ﬁrst\napproach is based on the Structural Risk Minimization (SRM) paradigm we\nhave described and analyzed in Chapter 7.2 SRM is particularly useful when\na learning algorithm depends on a parameter that controls the bias-complexity\ntradeoﬀ(such as the degree of the ﬁtted polynomial in the preceding example\nor the parameter T in AdaBoost) The second approach relies on the concept\nof validation The basic idea is to partition the training set into two sets One\nis used for training each of the candidate models, and the second is used for\ndeciding which of them yields the best results In model selection tasks, we try to ﬁnd the right balance between approxi-\nmation and estimation errors More generally, if our learning algorithm fails to\nﬁnd a predictor with a small risk, it is important to understand whether we\nsuﬀer from overﬁtting or underﬁtting In Section 11.3 we discuss how this can\nbe achieved 11.1\nModel Selection Using SRM\nThe SRM paradigm has been described and analyzed in Section 7.2 Here we\nshow how SRM can be used for tuning the tradeoﬀbetween bias and complexity\nwithout deciding on a speciﬁc hypothesis class in advance Consider a countable\nsequence of hypothesis classes H1, H2, H3, For example, in the problem of\npolynomial regression mentioned, we can take Hd to be the set of polynomials\nof degree at most d",
      "word_count": 244,
      "source_page": 145,
      "start_position": 52115,
      "end_position": 52358,
      "sentences_count": 13
    },
    {
      "chunk_id": 234,
      "text": "Consider a countable\nsequence of hypothesis classes H1, H2, H3, For example, in the problem of\npolynomial regression mentioned, we can take Hd to be the set of polynomials\nof degree at most d Another example is taking Hd to be the class L(B, d) used\nby AdaBoost, as described in the previous chapter We assume that for every d, the class Hd enjoys the uniform convergence\nproperty (see Deﬁnition 4.3 in Chapter 4) with a sample complexity function of\nthe form\nm\nUC\nHd(ϵ, δ) ≤g(d) log(1/δ)\nϵ2\n,\n(11.1)\nwhere g : N →R is some monotonically increasing function For example, in the\ncase of binary classiﬁcation problems, we can take g(d) to be the VC-dimension\nof the class Hd multiplied by a universal constant (the one appearing in the\nfundamental theorem of learning; see Theorem 6.8) For the classes L(B, d) used\nby AdaBoost, the function g will simply grow with d Recall that the SRM rule follows a “bound minimization” approach, where in\nour case the bound is as follows: With probability of at least 1 −δ, for every\nd ∈N and h ∈Hd,\nLD(h) ≤LS(h) +\nr\ng(d)(log(1/δ) + 2 log(d) + log(π2/6))\nm (11.2)\nThis bound, which follows directly from Theorem 7.4, shows that for every d and\nevery h ∈Hd, the true risk is bounded by two terms – the empirical risk, LS(h),",
      "word_count": 230,
      "source_page": 145,
      "start_position": 52325,
      "end_position": 52554,
      "sentences_count": 8
    },
    {
      "chunk_id": 235,
      "text": "146\nModel Selection and Validation\nand a complexity term that depends on d The SRM rule will search for d and\nh ∈Hd that minimize the right-hand side of Equation (11.2) Getting back to the example of polynomial regression described earlier, even\nthough the empirical risk of the 10th degree polynomial is smaller than that of\nthe 3rd degree polynomial, we would still prefer the 3rd degree polynomial since\nits complexity (as reﬂected by the value of the function g(d)) is much smaller While the SRM approach can be useful in some situations, in many practical\ncases the upper bound given in Equation (11.2) is pessimistic In the next section\nwe present a more practical approach 11.2\nValidation\nWe would often like to get a better estimation of the true risk of the output pre-\ndictor of a learning algorithm So far we have derived bounds on the estimation\nerror of a hypothesis class, which tell us that for all hypotheses in the class, the\ntrue risk is not very far from the empirical risk However, these bounds might be\nloose and pessimistic, as they hold for all hypotheses and all possible data dis-\ntributions A more accurate estimation of the true risk can be obtained by using\nsome of the training data as a validation set, over which one can evalutate the\nsuccess of the algorithm’s output predictor This procedure is called validation",
      "word_count": 234,
      "source_page": 146,
      "start_position": 52555,
      "end_position": 52788,
      "sentences_count": 10
    },
    {
      "chunk_id": 236,
      "text": "A more accurate estimation of the true risk can be obtained by using\nsome of the training data as a validation set, over which one can evalutate the\nsuccess of the algorithm’s output predictor This procedure is called validation Naturally, a better estimation of the true risk is useful for model selection, as\nwe will describe in Section 11.2.2 11.2.1\nHold Out Set\nThe simplest way to estimate the true error of a predictor h is by sampling an ad-\nditional set of examples, independent of the training set, and using the empirical\nerror on this validation set as our estimator Formally, let V = (x1, y1), , (xmv, ymv)\nbe a set of fresh mv examples that are sampled according to D (independently of\nthe m examples of the training set S) Using Hoeﬀding’s inequality ( Lemma 4.5)\nwe have the following:\ntheorem 11.1\nLet h be some predictor and assume that the loss function is in\n[0, 1] Then, for every δ ∈(0, 1), with probability of at least 1 −δ over the choice\nof a validation set V of size mv we have\n|LV (h) −LD(h)|\n≤\ns\nlog(2/δ)\n2 mv The bound in Theorem 11.1 does not depend on the algorithm or the training\nset used to construct h and is tighter than the usual bounds that we have seen so\nfar",
      "word_count": 226,
      "source_page": 146,
      "start_position": 52750,
      "end_position": 52975,
      "sentences_count": 9
    },
    {
      "chunk_id": 237,
      "text": "11.2 Validation\n147\nwith respect to a hypothesis class of VC-dimension d, over a training set of m\nexamples Then, from the fundamental theorem of learning (Theorem 6.8) we\nobtain the bound\nLD(h) ≤LS(h) +\nr\nC d + log(1/δ)\nm\n,\nwhere C is the constant appearing in Theorem 6.8 In contrast, from Theo-\nrem 11.1 we obtain the bound\nLD(h) ≤LV (h) +\ns\nlog(2/δ)\n2mv Therefore, taking mv to be order of m, we obtain an estimate that is more\naccurate by a factor that depends on the VC-dimension On the other hand, the\nprice we pay for using such an estimate is that it requires an additional sample\non top of the sample used for training the learner Sampling a training set and then sampling an independent validation set is\nequivalent to randomly partitioning our random set of examples into two parts,\nusing one part for training and the other one for validation For this reason, the\nvalidation set is often referred to as a hold out set 11.2.2\nValidation for Model Selection\nValidation can be naturally used for model selection as follows We ﬁrst train\ndiﬀerent algorithms (or the same algorithm with diﬀerent parameters) on the\ngiven training set Let H = {h1, , hr} be the set of all output predictors of the\ndiﬀerent algorithms For example, in the case of training polynomial regressors,\nwe would have each hr be the output of polynomial regression of degree r",
      "word_count": 244,
      "source_page": 147,
      "start_position": 53021,
      "end_position": 53264,
      "sentences_count": 12
    },
    {
      "chunk_id": 238,
      "text": ", hr} be the set of all output predictors of the\ndiﬀerent algorithms For example, in the case of training polynomial regressors,\nwe would have each hr be the output of polynomial regression of degree r Now,\nto choose a single predictor from H we sample a fresh validation set and choose\nthe predictor that minimizes the error over the validation set In other words,\nwe apply ERMH over the validation set This process is very similar to learning a ﬁnite hypothesis class The only\ndiﬀerence is that H is not ﬁxed ahead of time but rather depends on the train-\ning set However, since the validation set is independent of the training set we\nget that it is also independent of H and therefore the same technique we used\nto derive bounds for ﬁnite hypothesis classes holds here as well In particular,\ncombining Theorem 11.1 with the union bound we obtain:\ntheorem 11.2\nLet H = {h1, , hr} be an arbitrary set of predictors and\nassume that the loss function is in [0, 1] Assume that a validation set V of size\nmv is sampled independent of H Then, with probability of at least 1−δ over the\nchoice of V we have\n∀h ∈H, |LD(h) −LV (h)|\n≤\ns\nlog(2|H|/δ)\n2 mv\n.",
      "word_count": 215,
      "source_page": 147,
      "start_position": 53229,
      "end_position": 53443,
      "sentences_count": 11
    },
    {
      "chunk_id": 239,
      "text": "148\nModel Selection and Validation\nThis theorem tells us that the error on the validation set approximates the\ntrue error as long as H is not too large However, if we try too many methods\n(resulting in |H| that is large relative to the size of the validation set) then we’re\nin danger of overﬁtting To illustrate how validation is useful for model selection, consider again the\nexample of ﬁtting a one dimensional polynomial as described in the beginning\nof this chapter In the following we depict the same training set, with ERM\npolynomials of degree 2, 3, and 10, but this time we also depict an additional\nvalidation set (marked as red, unﬁlled circles) The polynomial of degree 10 has\nminimal training error, yet the polynomial of degree 3 has the minimal validation\nerror, and hence it will be chosen as the best model 11.2.3\nThe Model-Selection Curve\nThe model selection curve shows the training error and validation error as a func-\ntion of the complexity of the model considered For example, for the polynomial\nﬁtting problem mentioned previously, the curve will look like:",
      "word_count": 185,
      "source_page": 148,
      "start_position": 53444,
      "end_position": 53628,
      "sentences_count": 7
    },
    {
      "chunk_id": 240,
      "text": "11.2 Validation\n149\n2\n4\n6\n8\n10\n0\n0.1\n0.2\n0.3\n0.4\nd\nerror\ntrain\nvalidation\nAs can be shown, the training error is monotonically decreasing as we increase\nthe polynomial degree (which is the complexity of the model in our case) On\nthe other hand, the validation error ﬁrst decreases but then starts to increase,\nwhich indicates that we are starting to suﬀer from overﬁtting Plotting such curves can help us understand whether we are searching the\ncorrect regime of our parameter space Often, there may be more than a single\nparameter to tune, and the possible number of values each parameter can take\nmight be quite large For example, in Chapter 13 we describe the concept of\nregularization, in which the parameter of the learning algorithm is a real number In such cases, we start with a rough grid of values for the parameter(s) and plot\nthe corresponding model-selection curve On the basis of the curve we will zoom\nin to the correct regime and employ a ﬁner grid to search over It is important to\nverify that we are in the relevant regime For example, in the polynomial ﬁtting\nproblem described, if we start searching degrees from the set of values {1, 10, 20}\nand do not employ a ﬁner grid based on the resulting curve, we will end up with\na rather poor model",
      "word_count": 229,
      "source_page": 149,
      "start_position": 53629,
      "end_position": 53857,
      "sentences_count": 9
    },
    {
      "chunk_id": 241,
      "text": "It is important to\nverify that we are in the relevant regime For example, in the polynomial ﬁtting\nproblem described, if we start searching degrees from the set of values {1, 10, 20}\nand do not employ a ﬁner grid based on the resulting curve, we will end up with\na rather poor model 11.2.4\nk-Fold Cross Validation\nThe validation procedure described so far assumes that data is plentiful and that\nwe have the ability to sample a fresh validation set But in some applications,\ndata is scarce and we do not want to “waste” data on validation The k-fold\ncross validation technique is designed to give an accurate estimate of the true\nerror without wasting too much data In k-fold cross validation the original training set is partitioned into k subsets\n(folds) of size m/k (for simplicity, assume that m/k is an integer) For each fold,\nthe algorithm is trained on the union of the other folds and then the error of its\noutput is estimated using the fold Finally, the average of all these errors is the",
      "word_count": 179,
      "source_page": 149,
      "start_position": 53804,
      "end_position": 53982,
      "sentences_count": 8
    },
    {
      "chunk_id": 242,
      "text": "150\nModel Selection and Validation\nestimate of the true error The special case k = m, where m is the number of\nexamples, is called leave-one-out (LOO) k-Fold cross validation is often used for model selection (or parameter tuning),\nand once the best parameter is chosen, the algorithm is retrained using this\nparameter on the entire training set A pseudocode of k-fold cross validation\nfor model selection is given in the following The procedure receives as input a\ntraining set, S, a set of possible parameter values, Θ, an integer, k, representing\nthe number of folds, and a learning algorithm, A, which receives as input a\ntraining set as well as a parameter θ ∈Θ It outputs the best parameter as well\nas the hypothesis trained by this parameter on the entire training set k-Fold Cross Validation for Model Selection\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nset of parameter values Θ\nlearning algorithm A\ninteger k\npartition S into S1, S2, , Sk\nforeach θ ∈Θ\nfor i = 1 k\nhi,θ = A(S \\ Si; θ)\nerror(θ) = 1\nk\nPk\ni=1 LSi(hi,θ)\noutput\nθ⋆= argminθ [error(θ)]\nhθ⋆= A(S; θ⋆)\nThe cross validation method often works very well in practice However, it\nmight sometime fail, as the artiﬁcial example given in Exercise 1 shows Rig-\norously understanding the exact behavior of cross validation is still an open\nproblem",
      "word_count": 233,
      "source_page": 150,
      "start_position": 53983,
      "end_position": 54215,
      "sentences_count": 12
    },
    {
      "chunk_id": 243,
      "text": "However, it\nmight sometime fail, as the artiﬁcial example given in Exercise 1 shows Rig-\norously understanding the exact behavior of cross validation is still an open\nproblem Rogers and Wagner (Rogers & Wagner 1978) have shown that for k\nlocal rules (e.g., k Nearest Neighbor; see Chapter 19) the cross validation proce-\ndure gives a very good estimate of the true error Other papers show that cross\nvalidation works for stable algorithms (we will study stability and its relation to\nlearnability in Chapter 13) 11.2.5\nTrain-Validation-Test Split\nIn most practical applications, we split the available examples into three sets The ﬁrst set is used for training our algorithm and the second is used as a\nvalidation set for model selection After we select the best model, we test the\nperformance of the output predictor on the third set, which is often called the\n“test set.” The number obtained is used as an estimator of the true error of the\nlearned predictor.",
      "word_count": 162,
      "source_page": 150,
      "start_position": 54188,
      "end_position": 54349,
      "sentences_count": 7
    },
    {
      "chunk_id": 244,
      "text": "11.3 What to Do If Learning Fails\n151\n11.3\nWhat to Do If Learning Fails\nConsider the following scenario: You were given a learning task and have ap-\nproached it with a choice of a hypothesis class, a learning algorithm, and param-\neters You used a validation set to tune the parameters and tested the learned\npredictor on a test set The test results, unfortunately, turn out to be unsatis-\nfactory What went wrong then, and what should you do next There are many elements that can be “ﬁxed.” The main approaches are listed\nin the following:\n• Get a larger sample\n• Change the hypothesis class by:\n– Enlarging it\n– Reducing it\n– Completely changing it\n– Changing the parameters you consider\n• Change the feature representation of the data\n• Change the optimization algorithm used to apply your learning rule\nIn order to ﬁnd the best remedy, it is essential ﬁrst to understand the cause\nof the bad performance Recall that in Chapter 5 we decomposed the true er-\nror of the learned predictor into approximation error and estimation error The\napproximation error is deﬁned to be LD(h⋆) for some h⋆∈argminh∈H LD(h),\nwhile the estimation error is deﬁned to be LD(hS) −LD(h⋆), where hS is the\nlearned predictor (which is based on the training set S) The approximation error of the class does not depend on the sample size or\non the algorithm being used",
      "word_count": 238,
      "source_page": 151,
      "start_position": 54350,
      "end_position": 54587,
      "sentences_count": 8
    },
    {
      "chunk_id": 245,
      "text": "The\napproximation error is deﬁned to be LD(h⋆) for some h⋆∈argminh∈H LD(h),\nwhile the estimation error is deﬁned to be LD(hS) −LD(h⋆), where hS is the\nlearned predictor (which is based on the training set S) The approximation error of the class does not depend on the sample size or\non the algorithm being used It only depends on the distribution D and on the\nhypothesis class H Therefore, if the approximation error is large, it will not help\nus to enlarge the training set size, and it also does not make sense to reduce the\nhypothesis class What can be beneﬁcial in this case is to enlarge the hypothesis\nclass or completely change it (if we have some alternative prior knowledge in\nthe form of a diﬀerent hypothesis class) We can also consider applying the\nsame hypothesis class but on a diﬀerent feature representation of the data (see\nChapter 25) The estimation error of the class does depend on the sample size Therefore, if\nwe have a large estimation error we can make an eﬀort to obtain more training\nexamples We can also consider reducing the hypothesis class However, it doesn’t\nmake sense to enlarge the hypothesis class in that case Error Decomposition Using Validation\nWe see that understanding whether our problem is due to approximation error\nor estimation error is very useful for ﬁnding the best remedy In the previous\nsection we saw how to estimate LD(hS) using the empirical risk on a validation\nset",
      "word_count": 247,
      "source_page": 151,
      "start_position": 54533,
      "end_position": 54779,
      "sentences_count": 12
    },
    {
      "chunk_id": 246,
      "text": "152\nModel Selection and Validation\nInstead, we give a diﬀerent error decomposition, one that can be estimated from\nthe train and validation sets LD(hS) = (LD(hS) −LV (hS)) + (LV (hS) −LS(hS)) + LS(hS) The ﬁrst term, (LD(hS) −LV (hS)), can be bounded quite tightly using Theo-\nrem 11.1 Intuitively, when the second term, (LV (hS) −LS(hS)), is large we say\nthat our algorithm suﬀers from “overﬁtting” while when the empirical risk term,\nLS(hS), is large we say that our algorithm suﬀers from “underﬁtting.” Note that\nthese two terms are not necessarily good estimates of the estimation and ap-\nproximation errors To illustrate this, consider the case in which H is a class of\nVC-dimension d, and D is a distribution such that the approximation error of H\nwith respect to D is 1/4 As long as the size of our training set is smaller than\nd we will have LS(hS) = 0 for every ERM hypothesis Therefore, the training\nrisk, LS(hS), and the approximation error, LD(h⋆), can be signiﬁcantly diﬀerent Nevertheless, as we show later, the values of LS(hS) and (LV (hS)−LS(hS)) still\nprovide us useful information Consider ﬁrst the case in which LS(hS) is large We can write\nLS(hS) = (LS(hS) −LS(h⋆)) + (LS(h⋆) −LD(h⋆)) + LD(h⋆) When hS is an ERMH hypothesis we have that LS(hS)−LS(h⋆) ≤0 In addition,\nsince h⋆does not depend on S, the term (LS(h⋆)−LD(h⋆)) can be bounded quite\ntightly (as in Theorem 11.1) The last term is the approximation error",
      "word_count": 247,
      "source_page": 152,
      "start_position": 54793,
      "end_position": 55039,
      "sentences_count": 13
    },
    {
      "chunk_id": 247,
      "text": "In addition,\nsince h⋆does not depend on S, the term (LS(h⋆)−LD(h⋆)) can be bounded quite\ntightly (as in Theorem 11.1) The last term is the approximation error It follows\nthat if LS(hS) is large then so is the approximation error, and the remedy to the\nfailure of our algorithm should be tailored accordingly (as discussed previously) Remark 11.1\nIt is possible that the approximation error of our class is small,\nyet the value of LS(hS) is large For example, maybe we had a bug in our ERM\nimplementation, and the algorithm returns a hypothesis hS that is not an ERM It may also be the case that ﬁnding an ERM hypothesis is computationally hard,\nand our algorithm applies some heuristic trying to ﬁnd an approximate ERM In\nsome cases, it is hard to know how good hS is relative to an ERM hypothesis But,\nsometimes it is possible at least to know whether there are better hypotheses For example, in the next chapter we will study convex learning problems in\nwhich there are optimality conditions that can be checked to verify whether\nour optimization algorithm converged to an ERM solution In other cases, the\nsolution may depend on randomness in initializing the algorithm, so we can try\ndiﬀerent randomly selected initial points to see whether better solutions pop out Next consider the case in which LS(hS) is small As we argued before, this\ndoes not necessarily imply that the approximation error is small",
      "word_count": 243,
      "source_page": 152,
      "start_position": 55013,
      "end_position": 55255,
      "sentences_count": 12
    },
    {
      "chunk_id": 248,
      "text": "11.3 What to Do If Learning Fails\n153\nm\nerror\ntrain error\nvalidation error\nm\nerror\ntrain error\nvalidation error\nFigure 11.1 Examples of learning curves Left: This learning curve corresponds to the\nscenario in which the number of examples is always smaller than the VC dimension of\nthe class Right: This learning curve corresponds to the scenario in which the\napproximation error is zero and the number of examples is larger than the VC\ndimension of the class approximation error of the class is zero In both cases LS(hS) = 0 How can we\ndistinguish between the two cases Learning Curves\nOne possible way to distinguish between the two cases is by plotting learning\ncurves To produce a learning curve we train the algorithm on preﬁxes of the\ndata of increasing sizes For example, we can ﬁrst train the algorithm on the\nﬁrst 10% of the examples, then on 20% of them, and so on For each preﬁx we\ncalculate the training error (on the preﬁx the algorithm is being trained on)\nand the validation error (on a predeﬁned validation set) Such learning curves\ncan help us distinguish between the two aforementioned scenarios In the ﬁrst\nscenario we expect the validation error to be approximately 1/2 for all preﬁxes,\nas we didn’t really learn anything In the second scenario the validation error\nwill start as a constant but then should start decreasing (it must start decreasing\nonce the training set size is larger than the VC-dimension)",
      "word_count": 247,
      "source_page": 153,
      "start_position": 55319,
      "end_position": 55565,
      "sentences_count": 13
    },
    {
      "chunk_id": 249,
      "text": "In the ﬁrst\nscenario we expect the validation error to be approximately 1/2 for all preﬁxes,\nas we didn’t really learn anything In the second scenario the validation error\nwill start as a constant but then should start decreasing (it must start decreasing\nonce the training set size is larger than the VC-dimension) An illustration of\nthe two cases is given in Figure 11.1 In general, as long as the approximation error is greater than zero we expect\nthe training error to grow with the sample size, as a larger amount of data points\nmakes it harder to provide an explanation for all of them On the other hand,\nthe validation error tends to decrease with the increase in sample size If the\nVC-dimension is ﬁnite, when the sample size goes to inﬁnity, the validation and\ntrain errors converge to the approximation error Therefore, by extrapolating\nthe training and validation curves we can try to guess the value of the approx-\nimation error, or at least to get a rough estimate on an interval in which the\napproximation error resides Getting back to the problem of ﬁnding the best remedy for the failure of\nour algorithm, if we observe that LS(hS) is small while the validation error is\nlarge, then in any case we know that the size of our training set is not suﬃcient\nfor learning the class H We can then plot a learning curve If we see that the",
      "word_count": 242,
      "source_page": 153,
      "start_position": 55513,
      "end_position": 55754,
      "sentences_count": 10
    },
    {
      "chunk_id": 250,
      "text": "154\nModel Selection and Validation\nvalidation error is starting to decrease then the best solution is to increase the\nnumber of examples (if we can aﬀord to enlarge the data) Another reasonable\nsolution is to decrease the complexity of the hypothesis class On the other hand,\nif we see that the validation error is kept around 1/2 then we have no evidence\nthat the approximation error of H is good It may be the case that increasing\nthe training set size will not help us at all Obtaining more data can still help\nus, as at some point we can see whether the validation error starts to decrease\nor whether the training error starts to increase But, if more data is expensive,\nit may be better ﬁrst to try to reduce the complexity of the hypothesis class To summarize the discussion, the following steps should be applied:\n1 If learning involves parameter tuning, plot the model-selection curve to make\nsure that you tuned the parameters appropriately (see Section 11.2.3) 2 If the training error is excessively large consider enlarging the hypothesis class,\ncompletely change it, or change the feature representation of the data 3 If the training error is small, plot learning curves and try to deduce from them\nwhether the problem is estimation error or approximation error 4 If the approximation error seems to be small enough, try to obtain more data If this is not possible, consider reducing the complexity of the hypothesis class 5",
      "word_count": 247,
      "source_page": 154,
      "start_position": 55755,
      "end_position": 56001,
      "sentences_count": 16
    },
    {
      "chunk_id": 251,
      "text": "If this is not possible, consider reducing the complexity of the hypothesis class 5 If the approximation error seems to be large as well, try to change the hy-\npothesis class or the feature representation of the data completely 11.4\nSummary\nModel selection is the task of selecting an appropriate model for the learning\ntask based on the data itself We have shown how this can be done using the\nSRM learning paradigm or using the more practical approach of validation If\nour learning algorithm fails, a decomposition of the algorithm’s error should be\nperformed using learning curves, so as to ﬁnd the best remedy 11.5\nExercises\n1 Failure of k-fold cross validation Consider a case in that the label is\nchosen at random according to P[y = 1] = P[y = 0] = 1/2 Consider a\nlearning algorithm that outputs the constant predictor h(x) = 1 if the parity\nof the labels on the training set is 1 and otherwise the algorithm outputs the\nconstant predictor h(x) = 0 Prove that the diﬀerence between the leave-one-\nout estimate and the true error in such a case is always 1/2 2 Let H1, , Hk be k hypothesis classes Suppose you are given m i.i.d training\nexamples and you would like to learn the class H = ∪k\ni=1Hi Consider two\nalternative approaches:\n• Learn H on the m examples using the ERM rule",
      "word_count": 234,
      "source_page": 154,
      "start_position": 55988,
      "end_position": 56221,
      "sentences_count": 16
    },
    {
      "chunk_id": 252,
      "text": "12\nConvex Learning Problems\nIn this chapter we introduce convex learning problems Convex learning comprises\nan important family of learning problems, mainly because most of what we can\nlearn eﬃciently falls into it We have already encountered linear regression with\nthe squared loss and logistic regression, which are convex problems, and indeed\nthey can be learned eﬃciently We have also seen nonconvex problems, such as\nhalfspaces with the 0-1 loss, which is known to be computationally hard to learn\nin the unrealizable case In general, a convex learning problem is a problem whose hypothesis class is a\nconvex set, and whose loss function is a convex function for each example We be-\ngin the chapter with some required deﬁnitions of convexity Besides convexity, we\nwill deﬁne Lipschitzness and smoothness, which are additional properties of the\nloss function that facilitate successful learning We next turn to deﬁning convex\nlearning problems and demonstrate the necessity for further constraints such as\nBoundedness and Lipschitzness or Smoothness We deﬁne these more restricted\nfamilies of learning problems and claim that Convex-Smooth/Lipschitz-Bounded\nproblems are learnable These claims will be proven in the next two chapters, in\nwhich we will present two learning paradigms that successfully learn all problems\nthat are either convex-Lipschitz-bounded or convex-smooth-bounded Finally, in Section 12.3, we show how one can handle some nonconvex problems\nby minimizing “surrogate” loss functions that are convex (instead of the original\nnonconvex loss function)",
      "word_count": 236,
      "source_page": 156,
      "start_position": 56323,
      "end_position": 56558,
      "sentences_count": 11
    },
    {
      "chunk_id": 253,
      "text": "These claims will be proven in the next two chapters, in\nwhich we will present two learning paradigms that successfully learn all problems\nthat are either convex-Lipschitz-bounded or convex-smooth-bounded Finally, in Section 12.3, we show how one can handle some nonconvex problems\nby minimizing “surrogate” loss functions that are convex (instead of the original\nnonconvex loss function) Surrogate convex loss functions give rise to eﬃcient\nsolutions but might increase the risk of the learned predictor 12.1\nConvexity, Lipschitzness, and Smoothness\n12.1.1\nConvexity\ndefinition 12.1 (Convex Set)\nA set C in a vector space is convex if for any\ntwo vectors u, v in C, the line segment between u and v is contained in C That\nis, for any α ∈[0, 1] we have that αu + (1 −α)v ∈C Examples of convex and nonconvex sets in R2 are given in the following For\nthe nonconvex sets, we depict two points in the set such that the line between\nthe two points is not contained in the set Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 197,
      "source_page": 156,
      "start_position": 56502,
      "end_position": 56698,
      "sentences_count": 12
    },
    {
      "chunk_id": 254,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n157\nnon-convex\nconvex\nGiven α ∈[0, 1], the combination, αu + (1 −α)v of the points u, v is called a\nconvex combination definition 12.2 (Convex Function)\nLet C be a convex set A function f :\nC →R is convex if for every u, v ∈C and α ∈[0, 1],\nf(αu + (1 −α)v) ≤αf(u) + (1 −α)f(v) In words, f is convex if for any u, v, the graph of f between u and v lies below\nthe line segment joining f(u) and f(v) An illustration of a convex function,\nf : R →R, is depicted in the following f(u)\nf(v)\nu\nαu + (1 −α)v\nv\nαf(u) + (1 −α)f(v)\nf(αu + (1 −α)v)\nThe epigraph of a function f is the set\nepigraph(f) = {(x, β) : f(x) ≤β} (12.1)\nIt is easy to verify that a function f is convex if and only if its epigraph is a\nconvex set An illustration of a nonconvex function f : R →R, along with its\nepigraph, is given in the following.",
      "word_count": 179,
      "source_page": 157,
      "start_position": 56699,
      "end_position": 56877,
      "sentences_count": 8
    },
    {
      "chunk_id": 255,
      "text": "158\nConvex Learning Problems\nx\nf(x)\nAn important property of convex functions is that every local minimum of the\nfunction is also a global minimum Formally, let B(u, r) = {v : ∥v −u∥≤r} be\na ball of radius r centered around u We say that f(u) is a local minimum of f\nat u if there exists some r > 0 such that for all v ∈B(u, r) we have f(v) ≥f(u) It follows that for any v (not necessarily in B), there is a small enough α > 0\nsuch that u + α(v −u) ∈B(u, r) and therefore\nf(u) ≤f(u + α(v −u)) (12.2)\nIf f is convex, we also have that\nf(u + α(v −u)) = f(αv + (1 −α)u) ≤(1 −α)f(u) + αf(v) (12.3)\nCombining these two equations and rearranging terms, we conclude that f(u) ≤\nf(v) Since this holds for every v, it follows that f(u) is also a global minimum\nof f Another important property of convex functions is that for every w we can\nconstruct a tangent to f at w that lies below f everywhere If f is diﬀerentiable,\nthis tangent is the linear function l(u) = f(w) + ⟨∇f(w), u −w⟩, where ∇f(w)\nis the gradient of f at w, namely, the vector of partial derivatives of f, ∇f(w) =\n\u0010\n∂f(w)\n∂w1 , , ∂f(w)\n∂wd\n\u0011 That is, for convex diﬀerentiable functions,\n∀u,\nf(u) ≥f(w) + ⟨∇f(w), u −w⟩",
      "word_count": 241,
      "source_page": 158,
      "start_position": 56878,
      "end_position": 57118,
      "sentences_count": 11
    },
    {
      "chunk_id": 256,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n159\nf(w)\nf(u)\nw\nu\nf(w) + ⟨u −w, ∇f(w)⟩\nIf f is a scalar diﬀerentiable function, there is an easy way to check if it is\nconvex lemma 12.3\nLet f : R →R be a scalar twice diﬀerential function, and let\nf ′, f ′′ be its ﬁrst and second derivatives, respectively Then, the following are\nequivalent:\n1 f is convex\n2 f ′ is monotonically nondecreasing\n3 f ′′ is nonnegative\nExample 12.1\n• The scalar function f(x) = x2 is convex To see this, note that f ′(x) = 2x\nand f ′′(x) = 2 > 0 • The scalar function f(x) = log(1+exp(x)) is convex To see this, observe that\nf ′(x) =\nexp(x)\n1+exp(x) =\n1\nexp(−x)+1 This is a monotonically increasing function\nsince the exponent function is a monotonically increasing function The following claim shows that the composition of a convex scalar function\nwith a linear function yields a convex vector-valued function claim 12.4\nAssume that f : Rd →R can be written as f(w) = g(⟨w, x⟩+ y),\nfor some x ∈Rd, y ∈R, and g : R →R Then, convexity of g implies the\nconvexity of f Proof\nLet w1, w2 ∈Rd and α ∈[0, 1]",
      "word_count": 210,
      "source_page": 159,
      "start_position": 57141,
      "end_position": 57350,
      "sentences_count": 14
    },
    {
      "chunk_id": 257,
      "text": "160\nConvex Learning Problems\n• Given some x ∈Rd and y ∈R, let f : Rd →R be deﬁned as f(w) =\n(⟨w, x⟩−y)2 Then, f is a composition of the function g(a) = a2 onto a\nlinear function, and hence f is a convex function • Given some x ∈Rd and y ∈{±1}, let f : Rd →R be deﬁned as f(w) =\nlog(1 + exp(−y⟨w, x⟩)) Then, f is a composition of the function g(a) =\nlog(1 + exp(a)) onto a linear function, and hence f is a convex function Finally, the following lemma shows that the maximum of convex functions is\nconvex and that a weighted sum of convex functions, with nonnegative weights,\nis also convex claim 12.5\nFor i = 1, , r, let fi : Rd →R be a convex function The\nfollowing functions from Rd to R are also convex • g(x) = maxi∈[r] fi(x)\n• g(x) = Pr\ni=1 wifi(x), where for all i, wi ≥0 Proof\nThe ﬁrst claim follows by\ng(αu + (1 −α)v) = max\ni\nfi(αu + (1 −α)v)\n≤max\ni\n[αfi(u) + (1 −α)fi(v)]\n≤α max\ni\nfi(u) + (1 −α) max\ni\nfi(v)\n= αg(u) + (1 −α)g(v) For the second claim\ng(αu + (1 −α)v) =\nX\ni\nwifi(αu + (1 −α)v)\n≤\nX\ni\nwi [αfi(u) + (1 −α)fi(v)]\n= α\nX\ni\nwifi(u) + (1 −α)\nX\ni\nwifi(v)\n= αg(u) + (1 −α)g(v) Example 12.3\nThe function g(x) = |x| is convex",
      "word_count": 249,
      "source_page": 160,
      "start_position": 57400,
      "end_position": 57648,
      "sentences_count": 12
    },
    {
      "chunk_id": 258,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n161\nIntuitively, a Lipschitz function cannot change too fast Note that if f : R →R\nis diﬀerentiable, then by the mean value theorem we have\nf(w1) −f(w2) = f ′(u)(w1 −w2) ,\nwhere u is some point between w1 and w2 It follows that if the derivative of f\nis everywhere bounded (in absolute value) by ρ, then the function is ρ-Lipschitz Example 12.4\n• The function f(x) = |x| is 1-Lipschitz over R This follows from the triangle\ninequality: For every x1, x2,\n|x1| −|x2| = |x1 −x2 + x2| −|x2| ≤|x1 −x2| + |x2| −|x2| = |x1 −x2| Since this holds for both x1, x2 and x2, x1, we obtain that ||x1| −|x2|| ≤\n|x1 −x2| • The function f(x) = log(1+exp(x)) is 1-Lipschitz over R To see this, observe\nthat\n|f ′(x)| =\n\f\f\f\f\nexp(x)\n1 + exp(x)\n\f\f\f\f =\n\f\f\f\f\n1\nexp(−x) + 1\n\f\f\f\f ≤1 • The function f(x) = x2 is not ρ-Lipschitz over R for any ρ To see this, take\nx1 = 0 and x2 = 1 + ρ, then\nf(x2) −f(x1) = (1 + ρ)2 > ρ(1 + ρ) = ρ|x2 −x1| However, this function is ρ-Lipschitz over the set C = {x : |x| ≤ρ/2} Indeed, for any x1, x2 ∈C we have\n|x2\n1 −x2\n2| = |x1 + x2| |x1 −x2| ≤2(ρ/2) |x1 −x2| = ρ|x1 −x2|",
      "word_count": 231,
      "source_page": 161,
      "start_position": 57730,
      "end_position": 57960,
      "sentences_count": 12
    },
    {
      "chunk_id": 259,
      "text": "162\nConvex Learning Problems\n12.1.3\nSmoothness\nThe deﬁnition of a smooth function relies on the notion of gradient Recall that\nthe gradient of a diﬀerentiable function f : Rd →R at w, denoted ∇f(w), is the\nvector of partial derivatives of f, namely, ∇f(w) =\n\u0010\n∂f(w)\n∂w1 , , ∂f(w)\n∂wd\n\u0011 definition 12.8 (Smoothness)\nA diﬀerentiable function f : Rd →R is β-\nsmooth if its gradient is β-Lipschitz; namely, for all v, w we have ∥∇f(v) −\n∇f(w)∥≤β∥v −w∥ It is possible to show that smoothness implies that for all v, w we have\nf(v) ≤f(w) + ⟨∇f(w), v −w⟩+ β\n2 ∥v −w∥2 (12.5)\nRecall that convexity of f implies that f(v) ≥f(w)+⟨∇f(w), v−w⟩ Therefore,\nwhen a function is both convex and smooth, we have both upper and lower\nbounds on the diﬀerence between the function and its ﬁrst order approximation Setting v = w −1\nβ ∇f(w) in the right-hand side of Equation (12.5) and rear-\nranging terms, we obtain\n1\n2β ∥∇f(w)∥2 ≤f(w) −f(v) If we further assume that f(v) ≥0 for all v we conclude that smoothness implies\nthe following:\n∥∇f(w)∥2 ≤2βf(w) (12.6)\nA function that satisﬁes this property is also called a self-bounded function Example 12.5\n• The function f(x) = x2 is 2-smooth This follows directly from the fact that\nf ′(x) = 2x Note that for this particular function Equation (12.5) and\nEquation (12.6) hold with equality • The function f(x) = log(1 + exp(x)) is (1/4)-smooth",
      "word_count": 247,
      "source_page": 162,
      "start_position": 58059,
      "end_position": 58305,
      "sentences_count": 14
    },
    {
      "chunk_id": 260,
      "text": "12.2 Convex Learning Problems\n163\nProof\nBy the chain rule we have that ∇f(w) = g′(⟨w, x⟩+ b)x, where g′ is the\nderivative of g Using the smoothness of g and the Cauchy-Schwartz inequality\nwe therefore obtain\nf(v) = g(⟨v, x⟩+ b)\n≤g(⟨w, x⟩+ b) + g′(⟨w, x⟩+ b)⟨v −w, x⟩+ β\n2 (⟨v −w, x⟩)2\n≤g(⟨w, x⟩+ b) + g′(⟨w, x⟩+ b)⟨v −w, x⟩+ β\n2 (∥v −w∥∥x∥)2\n= f(w) + ⟨∇f(w), v −w⟩+ β∥x∥2\n2\n∥v −w∥2 Example 12.6\n• For any x ∈Rd and y ∈R, let f(w) = (⟨w, x⟩−y)2 Then, f is (2∥x∥2)-\nsmooth • For any x ∈Rd and y ∈{±1}, let f(w) = log(1 + exp(−y⟨w, x⟩)) Then, f is\n(∥x∥2/4)-smooth 12.2\nConvex Learning Problems\nRecall that in our general deﬁnition of learning (Deﬁnition 3.4 in Chapter 3), we\nhave a hypothesis class H, a set of examples Z, and a loss function ℓ: H × Z →\nR+ So far in the book we have mainly thought of Z as being the product of an\ninstance space and a target space, Z = X ×Y, and H being a set of functions from\nX to Y However, H can be an arbitrary set Indeed, throughout this chapter,\nwe consider hypothesis classes H that are subsets of the Euclidean space Rd That is, every hypothesis is some real-valued vector We shall, therefore, denote\na hypothesis in H by w",
      "word_count": 236,
      "source_page": 163,
      "start_position": 58391,
      "end_position": 58626,
      "sentences_count": 12
    },
    {
      "chunk_id": 261,
      "text": "That is, every hypothesis is some real-valued vector We shall, therefore, denote\na hypothesis in H by w Now we can ﬁnally deﬁne convex learning problems:\ndefinition 12.10 (Convex Learning Problem)\nA learning problem, (H, Z, ℓ),\nis called convex if the hypothesis class H is a convex set and for all z ∈Z, the\nloss function, ℓ(·, z), is a convex function (where, for any z, ℓ(·, z) denotes the\nfunction f : H →R deﬁned by f(w) = ℓ(w, z)) Example 12.7 (Linear Regression with the Squared Loss)\nRecall that linear\nregression is a tool for modeling the relationship between some “explanatory”\nvariables and some real valued outcome (see Chapter 9) The domain set X\nis a subset of Rd, for some d, and the label set Y is the set of real numbers We would like to learn a linear function h : Rd →R that best approximates\nthe relationship between our variables In Chapter 9 we deﬁned the hypothesis\nclass as the set of homogenous linear functions, H = {x 7→⟨w, x⟩: w ∈Rd},\nand used the squared loss function, ℓ(h, (x, y)) = (h(x) −y)2 However, we can\nequivalently model the learning problem as a convex learning problem as follows.",
      "word_count": 205,
      "source_page": 163,
      "start_position": 58609,
      "end_position": 58813,
      "sentences_count": 8
    },
    {
      "chunk_id": 262,
      "text": "164\nConvex Learning Problems\nEach linear function is parameterized by a vector w ∈Rd Hence, we can deﬁne\nH to be the set of all such parameters, namely, H = Rd The set of examples is\nZ = X ×Y = Rd×R = Rd+1, and the loss function is ℓ(w, (x, y)) = (⟨w, x⟩−y)2 Clearly, the set H is a convex set The loss function is also convex with respect\nto its ﬁrst argument (see Example 12.2) lemma 12.11\nIf ℓis a convex loss function and the class H is convex, then the\nERMH problem, of minimizing the empirical loss over H, is a convex optimiza-\ntion problem (that is, a problem of minimizing a convex function over a convex\nset) Proof\nRecall that the ERMH problem is deﬁned by\nERMH(S) = argmin\nw∈H\nLS(w) Since, for a sample S = z1, , zm, for every w, LS(w) =\n1\nm\nPm\ni=1 ℓ(w, zi),\nClaim 12.5 implies that LS(w) is a convex function Therefore, the ERM rule\nis a problem of minimizing a convex function subject to the constraint that the\nsolution should be in a convex set Under mild conditions, such problems can be solved eﬃciently using generic\noptimization algorithms In particular, in Chapter 14 we will present a very\nsimple algorithm for minimizing convex functions 12.2.1\nLearnability of Convex Learning Problems\nWe have argued that for many cases, implementing the ERM rule for convex\nlearning problems can be done eﬃciently",
      "word_count": 244,
      "source_page": 164,
      "start_position": 58814,
      "end_position": 59057,
      "sentences_count": 13
    },
    {
      "chunk_id": 263,
      "text": "In particular, in Chapter 14 we will present a very\nsimple algorithm for minimizing convex functions 12.2.1\nLearnability of Convex Learning Problems\nWe have argued that for many cases, implementing the ERM rule for convex\nlearning problems can be done eﬃciently But is convexity a suﬃcient condition\nfor the learnability of a problem To make the quesion more speciﬁc: In VC theory, we saw that halfspaces in\nd-dimension are learnable (perhaps ineﬃciently) We also argued in Chapter 9\nusing the “discretization trick” that if the problem is of d parameters, it is\nlearnable with a sample complexity being a function of d That is, for a constant\nd, the problem should be learnable So, maybe all convex learning problems over\nRd, are learnable Example 12.8 later shows that the answer is negative, even when d is low Not\nall convex learning problems over Rd are learnable There is no contradiction\nto VC theory since VC theory only deals with binary classiﬁcation while here\nwe consider a wide family of problems There is also no contradiction to the\n“discretization trick” as there we assumed that the loss function is bounded and\nalso assumed that a representation of each parameter using a ﬁnite number of\nbits suﬃces As we will show later, under some additional restricting conditions\nthat hold in many practical scenarios, convex problems are learnable",
      "word_count": 225,
      "source_page": 164,
      "start_position": 59017,
      "end_position": 59241,
      "sentences_count": 12
    },
    {
      "chunk_id": 264,
      "text": "12.2 Convex Learning Problems\n165\nhomogenous case) Let A be any deterministic algorithm.1 Assume, by way of\ncontradiction, that A is a successful PAC learner for this problem That is, there\nexists a function m(·, ·), such that for every distribution D and for every ϵ, δ if\nA receives a training set of size m ≥m(ϵ, δ), it should output, with probability\nof at least 1 −δ, a hypothesis ˆw = A(S), such that LD( ˆw) −minw LD(w) ≤ϵ Choose ϵ = 1/100, δ = 1/2, let m ≥m(ϵ, δ), and set µ = log(100/99)\n2m We will\ndeﬁne two distributions, and will show that A is likely to fail on at least one\nof them The ﬁrst distribution, D1, is supported on two examples, z1 = (1, 0)\nand z2 = (µ, −1), where the probability mass of the ﬁrst example is µ while the\nprobability mass of the second example is 1 −µ The second distribution, D2, is\nsupported entirely on z2 Observe that for both distributions, the probability that all examples of the\ntraining set will be of the second type is at least 99% This is trivially true for\nD2, whereas for D1, the probability of this event is\n(1 −µ)m ≥e−2µm = 0.99 Since we assume that A is a deterministic algorithm, upon receiving a training\nset of m examples, each of which is (µ, −1), the algorithm will output some ˆw",
      "word_count": 238,
      "source_page": 165,
      "start_position": 59274,
      "end_position": 59511,
      "sentences_count": 10
    },
    {
      "chunk_id": 265,
      "text": "This is trivially true for\nD2, whereas for D1, the probability of this event is\n(1 −µ)m ≥e−2µm = 0.99 Since we assume that A is a deterministic algorithm, upon receiving a training\nset of m examples, each of which is (µ, −1), the algorithm will output some ˆw Now, if ˆw < −1/(2µ), we will set the distribution to be D1 Hence,\nLD1( ˆw) ≥µ( ˆw)2 ≥1/(4µ) However,\nmin\nw LD1(w) ≤LD1(0) = (1 −µ) It follows that\nLD1( ˆw) −min\nw LD1(w) ≥1\n4µ −(1 −µ) > ϵ Therefore, such algorithm A fails on D1 On the other hand, if ˆw ≥−1/(2µ)\nthen we’ll set the distribution to be D2 Then we have that LD2( ˆw) ≥1/4 while\nminw LD2(w) = 0, so A fails on D2 In summary, we have shown that for every\nA there exists a distribution on which A fails, which implies that the problem is\nnot PAC learnable A possible solution to this problem is to add another constraint on the hypoth-\nesis class In addition to the convexity requirement, we require that H will be\nbounded; namely, we assume that for some predeﬁned scalar B, every hypothesis\nw ∈H satisﬁes ∥w∥≤B Boundedness and convexity alone are still not suﬃcient for ensuring that the\nproblem is learnable, as the following example demonstrates Example 12.9\nAs in Example 12.8, consider a regression problem with the\nsquared loss",
      "word_count": 233,
      "source_page": 165,
      "start_position": 59463,
      "end_position": 59695,
      "sentences_count": 14
    },
    {
      "chunk_id": 266,
      "text": "166\nConvex Learning Problems\nhypothesis class It is easy to verify that H is convex The argument will be\nthe same as in Example 12.8, except that now the two distributions, D1, D2 will\nbe supported on z1 = (1/µ, 0) and z2 = (1, −1) If the algorithm A returns\nˆw < −1/2 upon receiving m examples of the second type, then we will set the\ndistribution to be D1 and have that\nLD1( ˆw) −min\nw LD1(w) ≥µ( ˆw/µ)2 −LD1(0) ≥1/(4µ) −(1 −µ) > ϵ Similarly, if ˆw ≥−1/2 we will set the distribution to be D2 and have that\nLD2( ˆw) −min\nw LD2(w) ≥(−1/2 + 1)2 −0 > ϵ This example shows that we need additional assumptions on the learning\nproblem, and this time the solution is in Lipschitzness or smoothness of the\nloss function This motivates a deﬁnition of two families of learning problems,\nconvex-Lipschitz-bounded and convex-smooth-bounded, which are deﬁned later 12.2.2\nConvex-Lipschitz/Smooth-Bounded Learning Problems\ndefinition 12.12 (Convex-Lipschitz-Bounded Learning Problem)\nA learning\nproblem, (H, Z, ℓ), is called Convex-Lipschitz-Bounded, with parameters ρ, B if\nthe following holds:\n• The hypothesis class H is a convex set and for all w ∈H we have ∥w∥≤B • For all z ∈Z, the loss function, ℓ(·, z), is a convex and ρ-Lipschitz function Example 12.10\nLet X = {x ∈Rd : ∥x∥≤ρ} and Y = R Let H = {w ∈Rd :\n∥w∥≤B} and let the loss function be ℓ(w, (x, y)) = |⟨w, x⟩−y|",
      "word_count": 247,
      "source_page": 166,
      "start_position": 59745,
      "end_position": 59991,
      "sentences_count": 11
    },
    {
      "chunk_id": 267,
      "text": "Example 12.10\nLet X = {x ∈Rd : ∥x∥≤ρ} and Y = R Let H = {w ∈Rd :\n∥w∥≤B} and let the loss function be ℓ(w, (x, y)) = |⟨w, x⟩−y| This corre-\nsponds to a regression problem with the absolute-value loss, where we assume\nthat the instances are in a ball of radius ρ and we restrict the hypotheses to be\nhomogenous linear functions deﬁned by a vector w whose norm is bounded by\nB Then, the resulting problem is Convex-Lipschitz-Bounded with parameters\nρ, B definition 12.13 (Convex-Smooth-Bounded Learning Problem)\nA learning\nproblem, (H, Z, ℓ), is called Convex-Smooth-Bounded, with parameters β, B if\nthe following holds:\n• The hypothesis class H is a convex set and for all w ∈H we have ∥w∥≤B • For all z ∈Z, the loss function, ℓ(·, z), is a convex, nonnegative, and β-smooth\nfunction Note that we also required that the loss function is nonnegative This is needed\nto ensure that the loss function is self-bounded, as described in the previous\nsection.",
      "word_count": 171,
      "source_page": 166,
      "start_position": 59960,
      "end_position": 60130,
      "sentences_count": 8
    },
    {
      "chunk_id": 268,
      "text": "12.3 Surrogate Loss Functions\n167\nExample 12.11\nLet X = {x ∈Rd : ∥x∥≤β/2} and Y = R Let H = {w ∈\nRd : ∥w∥≤B} and let the loss function be ℓ(w, (x, y)) = (⟨w, x⟩−y)2 This\ncorresponds to a regression problem with the squared loss, where we assume that\nthe instances are in a ball of radius β/2 and we restrict the hypotheses to be\nhomogenous linear functions deﬁned by a vector w whose norm is bounded by B Then, the resulting problem is Convex-Smooth-Bounded with parameters β, B We claim that these two families of learning problems are learnable That is,\nthe properties of convexity, boundedness, and Lipschitzness or smoothness of the\nloss function are suﬃcient for learnability We will prove this claim in the next\nchapters by introducing algorithms that learn these problems successfully 12.3\nSurrogate Loss Functions\nAs mentioned, and as we will see in the next chapters, convex problems can\nbe learned eﬀﬁciently However, in many cases, the natural loss function is not\nconvex and, in particular, implementing the ERM rule is hard As an example, consider the problem of learning the hypothesis class of half-\nspaces with respect to the 0 −1 loss That is,\nℓ0−1(w, (x, y)) = 1[y̸=sign(⟨w,x⟩)] = 1[y⟨w,x⟩≤0] This loss function is not convex with respect to w and indeed, when trying to\nminimize the empirical risk with respect to this loss function we might encounter\nlocal minima (see Exercise 1)",
      "word_count": 243,
      "source_page": 167,
      "start_position": 60131,
      "end_position": 60373,
      "sentences_count": 12
    },
    {
      "chunk_id": 269,
      "text": "That is,\nℓ0−1(w, (x, y)) = 1[y̸=sign(⟨w,x⟩)] = 1[y⟨w,x⟩≤0] This loss function is not convex with respect to w and indeed, when trying to\nminimize the empirical risk with respect to this loss function we might encounter\nlocal minima (see Exercise 1) Furthermore, as discussed in Chapter 8, solving\nthe ERM problem with respect to the 0−1 loss in the unrealizable case is known\nto be NP-hard To circumvent the hardness result, one popular approach is to upper bound\nthe nonconvex loss function by a convex surrogate loss function As its name\nindicates, the requirements from a convex surrogate loss are as follows:\n1 It should be convex 2 It should upper bound the original loss For example, in the context of learning halfspaces, we can deﬁne the so-called\nhinge loss as a convex surrogate for the 0 −1 loss, as follows:\nℓhinge(w, (x, y))\ndef\n= max{0, 1 −y⟨w, x⟩} Clearly, for all w and all (x, y), ℓ0−1(w, (x, y)) ≤ℓhinge(w, (x, y)) In addition,\nthe convexity of the hinge loss follows directly from Claim 12.5 Hence, the hinge\nloss satisﬁes the requirements of a convex surrogate loss function for the zero-one\nloss An illustration of the functions ℓ0−1 and ℓhinge is given in the following.",
      "word_count": 208,
      "source_page": 167,
      "start_position": 60332,
      "end_position": 60539,
      "sentences_count": 13
    },
    {
      "chunk_id": 270,
      "text": "168\nConvex Learning Problems\ny⟨w, x⟩\nℓhinge\nℓ0−1\n1\n1\nOnce we have deﬁned the surrogate convex loss, we can learn the problem with\nrespect to it The generalization requirement from a hinge loss learner will have\nthe form\nLhinge\nD\n(A(S)) ≤\nmin\nw∈H Lhinge\nD\n(w) + ϵ,\nwhere Lhinge\nD\n(w) = E(x,y)∼D[ℓhinge(w, (x, y))] Using the surrogate property, we\ncan lower bound the left-hand side by L0−1\nD\n(A(S)), which yields\nL0−1\nD\n(A(S)) ≤\nmin\nw∈H Lhinge\nD\n(w) + ϵ We can further rewrite the upper bound as follows:\nL0−1\nD\n(A(S)) ≤\nmin\nw∈H L0−1\nD\n(w) +\n\u0012\nmin\nw∈H Lhinge\nD\n(w) −min\nw∈H L0−1\nD\n(w)\n\u0013\n+ ϵ That is, the 0−1 error of the learned predictor is upper bounded by three terms:\n• Approximation error: This is the term minw∈H L0−1\nD\n(w), which measures how\nwell the hypothesis class performs on the distribution We already elabo-\nrated on this error term in Chapter 5 • Estimation error: This is the error that results from the fact that we only\nreceive a training set and do not observe the distribution D We already\nelaborated on this error term in Chapter 5 • Optimization error: This is the term\n\u0010\nminw∈H Lhinge\nD\n(w) −minw∈H L0−1\nD\n(w)\n\u0011\nthat measures the diﬀerence between the approximation error with respect\nto the surrogate loss and the approximation error with respect to the orig-\ninal loss",
      "word_count": 245,
      "source_page": 168,
      "start_position": 60540,
      "end_position": 60784,
      "sentences_count": 9
    },
    {
      "chunk_id": 271,
      "text": "12.5 Bibliographic Remarks\n169\nlearning algorithms for these families We also introduced the notion of convex\nsurrogate loss function, which enables us also to utilize the convex machinery for\nnonconvex problems 12.5\nBibliographic Remarks\nThere are several excellent books on convex analysis and optimization (Boyd &\nVandenberghe 2004, Borwein & Lewis 2006, Bertsekas 1999, Hiriart-Urruty &\nLemar´echal 1996) Regarding learning problems, the family of convex-Lipschitz-\nbounded problems was ﬁrst studied by Zinkevich (2003) in the context of online\nlearning and by Shalev-Shwartz, Shamir, Sridharan & Srebro (2009) in the con-\ntext of PAC learning 12.6\nExercises\n1 Construct an example showing that the 0−1 loss function may suﬀer from\nlocal minima; namely, construct a training sample S ∈(X × {±1})m (say, for\nX = R2), for which there exist a vector w and some ϵ > 0 such that\n1 For any w′ such that ∥w −w′∥≤ϵ we have LS(w) ≤LS(w′) (where the\nloss here is the 0−1 loss) This means that w is a local minimum of LS 2 There exists some w∗such that LS(w∗) < LS(w) This means that w is\nnot a global minimum of LS 2 Consider the learning problem of logistic regression: Let H = X = {x ∈\nRd : ∥x∥≤B}, for some scalar B > 0, let Y = {±1}, and let the loss\nfunction ℓbe deﬁned as ℓ(w, (x, y)) = log(1 + exp(−y⟨w, x⟩)) Show that\nthe resulting learning problem is both convex-Lipschitz-bounded and convex-\nsmooth-bounded",
      "word_count": 245,
      "source_page": 169,
      "start_position": 60849,
      "end_position": 61093,
      "sentences_count": 14
    },
    {
      "chunk_id": 272,
      "text": "Consider the learning problem of logistic regression: Let H = X = {x ∈\nRd : ∥x∥≤B}, for some scalar B > 0, let Y = {±1}, and let the loss\nfunction ℓbe deﬁned as ℓ(w, (x, y)) = log(1 + exp(−y⟨w, x⟩)) Show that\nthe resulting learning problem is both convex-Lipschitz-bounded and convex-\nsmooth-bounded Specify the parameters of Lipschitzness and smoothness 3 Consider the problem of learning halfspaces with the hinge loss We limit our\ndomain to the Euclidean ball with radius R That is, X = {x : ∥x∥2 ≤R} The label set is Y = {±1} and the loss function ℓis deﬁned by ℓ(w, (x, y)) =\nmax{0, 1 −y⟨w, x⟩} We already know that the loss function is convex Show\nthat it is R-Lipschitz 4 (*) Convex-Lipschitz-Boundedness Is Not Suﬃcient for Computa-\ntional Eﬃciency:\nIn the next chapter we show that from the statistical\nperspective, all convex-Lipschitz-bounded problems are learnable (in the ag-\nnostic PAC model) However, our main motivation to learn such problems\nresulted from the computational perspective – convex optimization is often\neﬃciently solvable Yet the goal of this exercise is to show that convexity\nalone is not suﬃcient for eﬃciency We show that even for the case d = 1,\nthere is a convex-Lipschitz-bounded problem which cannot be learned by any\ncomputable learner Let the hypothesis class be H = [0, 1] and let the example domain, Z, be",
      "word_count": 236,
      "source_page": 169,
      "start_position": 61039,
      "end_position": 61274,
      "sentences_count": 16
    },
    {
      "chunk_id": 273,
      "text": "13\nRegularization and Stability\nIn the previous chapter we introduced the families of convex-Lipschitz-bounded\nand convex-smooth-bounded learning problems In this section we show that all\nlearning problems in these two families are learnable For some learning problems\nof this type it is possible to show that uniform convergence holds; hence they\nare learnable using the ERM rule However, this is not true for all learning\nproblems of this type Yet, we will introduce another learning rule and will show\nthat it learns all convex-Lipschitz-bounded and convex-smooth-bounded learning\nproblems The new learning paradigm we introduce in this chapter is called Regularized\nLoss Minimization, or RLM for short In RLM we minimize the sum of the em-\npirical risk and a regularization function Intuitively, the regularization function\nmeasures the complexity of hypotheses Indeed, one interpretation of the reg-\nularization function is the structural risk minimization paradigm we discussed\nin Chapter 7 Another view of regularization is as a stabilizer of the learning\nalgorithm An algorithm is considered stable if a slight change of its input does\nnot change its output much We will formally deﬁne the notion of stability (what\nwe mean by “slight change of input” and by “does not change much the out-\nput”) and prove its close relation to learnability Finally, we will show that using\nthe squared ℓ2 norm as a regularization function stabilizes all convex-Lipschitz or\nconvex-smooth learning problems Hence, RLM can be used as a general learning\nrule for these families of learning problems",
      "word_count": 248,
      "source_page": 171,
      "start_position": 61382,
      "end_position": 61629,
      "sentences_count": 14
    },
    {
      "chunk_id": 274,
      "text": "172\nRegularization and Stability\ntion, and the algorithm balances between low empirical risk and “simpler,” or\n“less complex,” hypotheses There are many possible regularization functions one can use, reﬂecting some\nprior belief about the problem (similarly to the description language in Minimum\nDescription Length) Throughout this section we will focus on one of the most\nsimple regularization functions: R(w) = λ∥w∥2, where λ > 0 is a scalar and the\nnorm is the ℓ2 norm, ∥w∥=\nqPd\ni=1 w2\ni This yields the learning rule:\nA(S) = argmin\nw\n\u0000LS(w) + λ∥w∥2\u0001 (13.2)\nThis type of regularization function is often called Tikhonov regularization As mentioned before, one interpretation of Equation (13.2) is using structural\nrisk minimization, where the norm of w is a measure of its “complexity.” Recall\nthat in the previous chapter we introduced the notion of bounded hypothesis\nclasses Therefore, we can deﬁne a sequence of hypothesis classes, H1 ⊂H2 ⊂\nH3 ., where Hi = {w : ∥w∥2 ≤i} If the sample complexity of each Hi depends\non i then the RLM rule is similar to the SRM rule for this sequence of nested\nclasses A diﬀerent interpretation of regularization is as a stabilizer In the next section\nwe deﬁne the notion of stability and prove that stable learning rules do not\noverﬁt But ﬁrst, let us demonstrate the RLM rule for linear regression with the\nsquared loss",
      "word_count": 231,
      "source_page": 172,
      "start_position": 61742,
      "end_position": 61972,
      "sentences_count": 12
    },
    {
      "chunk_id": 275,
      "text": "In the next section\nwe deﬁne the notion of stability and prove that stable learning rules do not\noverﬁt But ﬁrst, let us demonstrate the RLM rule for linear regression with the\nsquared loss 13.1.1\nRidge Regression\nApplying the RLM rule with Tikhonov regularization to linear regression with\nthe squared loss, we obtain the following learning rule:\nargmin\nw∈Rd\n \nλ∥w∥2\n2 + 1\nm\nm\nX\ni=1\n1\n2(⟨w, xi⟩−yi)2 (13.3)\nPerforming linear regression using Equation (13.3) is called ridge regression To solve Equation (13.3) we compare the gradient of the objective to zero and\nobtain the set of linear equations\n(2λmI + A)w = b,\nwhere I is the identity matrix and A, b are as deﬁned in Equation (9.6), namely,\nA =\n m\nX\ni=1\nxi x⊤\ni and\nb =\nm\nX\ni=1\nyixi (13.4)\nSince A is a positive semideﬁnite matrix, the matrix 2λmI + A has all its eigen-\nvalues bounded below by 2λm Hence, this matrix is invertible and the solution\nto ridge regression becomes\nw = (2λmI + A)−1 b (13.5)",
      "word_count": 178,
      "source_page": 172,
      "start_position": 61939,
      "end_position": 62116,
      "sentences_count": 9
    },
    {
      "chunk_id": 276,
      "text": "13.2 Stable Rules Do Not Overﬁt\n173\nIn the next section we formally show how regularization stabilizes the algo-\nrithm and prevents overﬁtting In particular, the analysis presented in the next\nsections (particularly, Corollary 13.11) will yield:\ntheorem 13.1\nLet D be a distribution over X × [−1, 1], where X = {x ∈\nRd : ∥x∥≤1} Let H = {w ∈Rd : ∥w∥≤B} For any ϵ ∈(0, 1), let m ≥\n150 B2/ϵ2 Then, applying the ridge regression algorithm with parameter λ =\nϵ/(3B2) satisﬁes\nE\nS∼Dm [LD(A(S))]\n≤\nmin\nw∈H LD(w) + ϵ Remark 13.1\nThe preceding theorem tells us how many examples are needed\nto guarantee that the expected value of the risk of the learned predictor will be\nbounded by the approximation error of the class plus ϵ In the usual deﬁnition\nof agnostic PAC learning we require that the risk of the learned predictor will\nbe bounded with probability of at least 1 −δ In Exercise 1 we show how an\nalgorithm with a bounded expected risk can be used to construct an agnostic\nPAC learner 13.2\nStable Rules Do Not Overﬁt\nIntuitively, a learning algorithm is stable if a small change of the input to the\nalgorithm does not change the output of the algorithm much Of course, there\nare many ways to deﬁne what we mean by “a small change of the input” and\nwhat we mean by “does not change the output much”",
      "word_count": 241,
      "source_page": 173,
      "start_position": 62117,
      "end_position": 62357,
      "sentences_count": 10
    },
    {
      "chunk_id": 277,
      "text": "13.2\nStable Rules Do Not Overﬁt\nIntuitively, a learning algorithm is stable if a small change of the input to the\nalgorithm does not change the output of the algorithm much Of course, there\nare many ways to deﬁne what we mean by “a small change of the input” and\nwhat we mean by “does not change the output much” In this section we deﬁne\na speciﬁc notion of stability and prove that under this deﬁnition, stable rules do\nnot overﬁt Let A be a learning algorithm, let S = (z1, , zm) be a training set of m\nexamples, and let A(S) denote the output of A The algorithm A suﬀers from\noverﬁtting if the diﬀerence between the true risk of its output, LD(A(S)), and the\nempirical risk of its output, LS(A(S)), is large As mentioned in Remark 13.1,\nthroughout this chapter we focus on the expectation (with respect to the choice\nof S) of this quantity, namely, ES[LD(A(S)) −LS(A(S))] We next deﬁne the notion of stability Given the training set S and an ad-\nditional example z′, let S(i) be the training set obtained by replacing the i’th\nexample of S with z′; namely, S(i) = (z1, , zi−1, z′, zi+1, , zm) In our deﬁ-\nnition of stability, “a small change of the input” means that we feed A with S(i)\ninstead of with S That is, we only replace one training example",
      "word_count": 236,
      "source_page": 173,
      "start_position": 62298,
      "end_position": 62533,
      "sentences_count": 13
    },
    {
      "chunk_id": 278,
      "text": "174\nRegularization and Stability\nlearning algorithm drastically changes its prediction on zi if it observes it in the\ntraining set This is formalized in the following theorem theorem 13.2\nLet D be a distribution Let S = (z1, , zm) be an i.i.d se-\nquence of examples and let z′ be another i.i.d example Let U(m) be the uniform\ndistribution over [m] Then, for any learning algorithm,\nE\nS∼Dm[LD(A(S)) −LS(A(S))] =\nE\n(S,z′)∼Dm+1,i∼U(m)[ℓ(A(S(i), zi)) −ℓ(A(S), zi)] (13.6)\nProof\nSince S and z′ are both drawn i.i.d from D, we have that for every i,\nE\nS[LD(A(S))] = E\nS,z′[ℓ(A(S), z′)] = E\nS,z′[ℓ(A(S(i)), zi)] On the other hand, we can write\nE\nS[LS(A(S))] = E\nS,i[ℓ(A(S), zi)] Combining the two equations we conclude our proof When the right-hand side of Equation (13.6) is small, we say that A is a stable\nalgorithm – changing a single example in the training set does not lead to a\nsigniﬁcant change Formally,\ndefinition 13.3 (On-Average-Replace-One-Stable)\nLet ϵ : N →R be a mono-\ntonically decreasing function We say that a learning algorithm A is on-average-\nreplace-one-stable with rate ϵ(m) if for every distribution D\nE\n(S,z′)∼Dm+1,i∼U(m)[ℓ(A(S(i), zi)) −ℓ(A(S), zi)] ≤ϵ(m) Theorem 13.2 tells us that a learning algorithm does not overﬁt if and only\nif it is on-average-replace-one-stable Of course, a learning algorithm that does\nnot overﬁt is not necessarily a good learning algorithm – take, for example, an\nalgorithm A that always outputs the same hypothesis",
      "word_count": 244,
      "source_page": 174,
      "start_position": 62623,
      "end_position": 62866,
      "sentences_count": 18
    },
    {
      "chunk_id": 279,
      "text": "Theorem 13.2 tells us that a learning algorithm does not overﬁt if and only\nif it is on-average-replace-one-stable Of course, a learning algorithm that does\nnot overﬁt is not necessarily a good learning algorithm – take, for example, an\nalgorithm A that always outputs the same hypothesis A useful algorithm should\nﬁnd a hypothesis that on one hand ﬁts the training set (i.e., has a low empirical\nrisk) and on the other hand does not overﬁt Or, in light of Theorem 13.2, the\nalgorithm should both ﬁt the training set and at the same time be stable As we\nshall see, the parameter λ of the RLM rule balances between ﬁtting the training\nset and being stable 13.3\nTikhonov Regularization as a Stabilizer\nIn the previous section we saw that stable rules do not overﬁt In this section we\nshow that applying the RLM rule with Tikhonov regularization, λ∥w∥2, leads to\na stable algorithm We will assume that the loss function is convex and that it\nis either Lipschitz or smooth The main property of the Tikhonov regularization that we rely on is that it\nmakes the objective of RLM strongly convex, as deﬁned in the following.",
      "word_count": 197,
      "source_page": 174,
      "start_position": 62820,
      "end_position": 63016,
      "sentences_count": 9
    },
    {
      "chunk_id": 280,
      "text": "13.3 Tikhonov Regularization as a Stabilizer\n175\ndefinition 13.4 (Strongly Convex Functions)\nA function f is λ-strongly con-\nvex if for all w, u and α ∈(0, 1) we have\nf(αw + (1 −α)u) ≤αf(w) + (1 −α)f(u) −λ\n2 α(1 −α)∥w −u∥2 Clearly, every convex function is 0-strongly convex An illustration of strong\nconvexity is given in the following ﬁgure f(w)\nf(u)\nw\nαw + (1 −α)u\nu\n≥λ\n2 α(1 −α)∥u −w∥2\nThe following lemma implies that the objective of RLM is (2λ)-strongly con-\nvex In addition, it underscores an important property of strong convexity lemma 13.5\n1 The function f(w) = λ∥w∥2 is 2λ-strongly convex 2 If f is λ-strongly convex and g is convex, then f + g is λ-strongly convex 3 If f is λ-strongly convex and u is a minimizer of f, then, for any w,\nf(w) −f(u) ≥λ\n2 ∥w −u∥2 Proof\nThe ﬁrst two points follow directly from the deﬁnition To prove the last\npoint, we divide the deﬁnition of strong convexity by α and rearrange terms to\nget that\nf(u + α(w −u)) −f(u)\nα\n≤f(w) −f(u) −λ\n2 (1 −α)∥w −u∥2 Taking the limit α →0 we obtain that the right-hand side converges to f(w) −\nf(u) −λ\n2 ∥w −u∥2 On the other hand, the left-hand side becomes the derivative\nof the function g(α) = f(u + α(w −u)) at α = 0",
      "word_count": 234,
      "source_page": 175,
      "start_position": 63017,
      "end_position": 63250,
      "sentences_count": 15
    },
    {
      "chunk_id": 281,
      "text": "176\nRegularization and Stability\nDenote fS(w) = LS(w) + λ∥w∥2, and based on Lemma 13.5 we know that fS is\n(2λ)-strongly convex Relying on part 3 of the lemma, it follows that for any v,\nfS(v) −fS(A(S)) ≥λ∥v −A(S)∥2 (13.7)\nOn the other hand, for any v and u, and for all i, we have\nfS(v) −fS(u) = LS(v) + λ∥v∥2 −(LS(u) + λ∥u∥2)\n(13.8)\n= LS(i)(v) + λ∥v∥2 −(LS(i)(u) + λ∥u∥2)\n+ ℓ(v, zi) −ℓ(u, zi)\nm\n+ ℓ(u, z′) −ℓ(v, z′)\nm In particular, choosing v = A(S(i)), u = A(S), and using the fact that v mini-\nmizes LS(i)(w) + λ∥w∥2, we obtain that\nfS(A(S(i)))−fS(A(S)) ≤ℓ(A(S(i)), zi) −ℓ(A(S), zi)\nm\n+ℓ(A(S), z′) −ℓ(A(S(i)), z′)\nm (13.9)\nCombining this with Equation (13.7) we obtain that\nλ∥A(S(i))−A(S)∥2 ≤ℓ(A(S(i)), zi) −ℓ(A(S), zi)\nm\n+ ℓ(A(S), z′) −ℓ(A(S(i)), z′)\nm (13.10)\nThe two subsections that follow continue the stability analysis for either Lip-\nschitz or smooth loss functions For both families of loss functions we show that\nRLM is stable and therefore it does not overﬁt 13.3.1\nLipschitz Loss\nIf the loss function, ℓ(·, zi), is ρ-Lipschitz, then by the deﬁnition of Lipschitzness,\nℓ(A(S(i)), zi) −ℓ(A(S), zi) ≤ρ ∥A(S(i)) −A(S)∥ (13.11)\nSimilarly,\nℓ(A(S), z′) −ℓ(A(S(i)), z′) ≤ρ ∥A(S(i)) −A(S)∥ Plugging these inequalities into Equation (13.10) we obtain\nλ∥A(S(i)) −A(S)∥2 ≤2 ρ ∥A(S(i)) −A(S)∥\nm\n,\nwhich yields\n∥A(S(i)) −A(S)∥≤2 ρ\nλ m Plugging the preceding back into Equation (13.11) we conclude that\nℓ(A(S(i)), zi) −ℓ(A(S), zi) ≤2 ρ2\nλ m",
      "word_count": 249,
      "source_page": 176,
      "start_position": 63340,
      "end_position": 63588,
      "sentences_count": 11
    },
    {
      "chunk_id": 282,
      "text": "13.3 Tikhonov Regularization as a Stabilizer\n177\ncorollary 13.6\nAssume that the loss function is convex and ρ-Lipschitz Then, the RLM rule with the regularizer λ∥w∥2 is on-average-replace-one-stable\nwith rate 2 ρ2\nλ m It follows (using Theorem 13.2) that\nE\nS∼Dm[LD(A(S)) −LS(A(S))] ≤2 ρ2\nλ m 13.3.2\nSmooth and Nonnegative Loss\nIf the loss is β-smooth and nonnegative then it is also self-bounded (see Sec-\ntion 12.1):\n∥∇f(w)∥2 ≤2βf(w) (13.12)\nWe further assume that λ ≥\n2β\nm , or, in other words, that β ≤λm/2 By the\nsmoothness assumption we have that\nℓ(A(S(i)), zi)−ℓ(A(S), zi) ≤⟨∇ℓ(A(S), zi), A(S(i))−A(S)⟩+β\n2 ∥A(S(i))−A(S)∥2 (13.13)\nUsing the Cauchy-Schwartz inequality and Equation (12.6) we further obtain\nthat\nℓ(A(S(i)), zi) −ℓ(A(S), zi)\n≤∥∇ℓ(A(S), zi)∥∥A(S(i)) −A(S)∥+ β\n2 ∥A(S(i)) −A(S)∥2\n≤\np\n2βℓ(A(S), zi) ∥A(S(i)) −A(S)∥+ β\n2 ∥A(S(i)) −A(S)∥2 (13.14)\nBy a symmetric argument it holds that,\nℓ(A(S), z′) −ℓ(A(S(i)), z′)\n≤\nq\n2βℓ(A(S(i)), z′) ∥A(S(i)) −A(S)∥+ β\n2 ∥A(S(i)) −A(S)∥2 Plugging these inequalities into Equation (13.10) and rearranging terms we ob-\ntain that\n∥A(S(i)) −A(S)∥≤\n√2β\n(λ m −β)\n\u0012p\nℓ(A(S), zi) +\nq\nℓ(A(S(i)), z′)\n\u0013 Combining the preceding with the assumption β ≤λm/2 yields\n∥A(S(i)) −A(S)∥≤\n√8β\nλ m\n\u0012p\nℓ(A(S), zi) +\nq\nℓ(A(S(i)), z′)\n\u0013\n.",
      "word_count": 207,
      "source_page": 177,
      "start_position": 63600,
      "end_position": 63806,
      "sentences_count": 10
    },
    {
      "chunk_id": 283,
      "text": "178\nRegularization and Stability\nCombining the preceding with Equation (13.14) and again using the assumption\nβ ≤λm/2 yield\nℓ(A(S(i)), zi) −ℓ(A(S), zi)\n≤\np\n2βℓ(A(S), zi) ∥A(S(i)) −A(S)∥+ β\n2 ∥A(S(i)) −A(S)∥2\n≤\n\u0012 4β\nλm +\n8β2\n(λm)2\n\u0013 \u0012p\nℓ(A(S), zi) +\nq\nℓ(A(S(i)), z′)\n\u00132\n≤8β\nλm\n\u0012p\nℓ(A(S), zi) +\nq\nℓ(A(S(i)), z′)\n\u00132\n≤24β\nλm\n\u0010\nℓ(A(S), zi) + ℓ(A(S(i)), z′)\n\u0011\n,\nwhere in the last step we used the inequality (a+b)2 ≤3(a2+b2) Taking expecta-\ntion with respect to S, z′, i and noting that E[ℓ(A(S), zi)] = E[ℓ(A(S(i)), z′)] =\nE[LS(A(S))], we conclude that:\ncorollary 13.7\nAssume that the loss function is β-smooth and nonnegative Then, the RLM rule with the regularizer λ∥w∥2, where λ ≥2β\nm , satisﬁes\nE\nh\nℓ(A(S(i)), zi) −ℓ(A(S), zi)\ni\n≤48β\nλm E[LS(A(S))] Note that if for all z we have ℓ(0, z) ≤C, for some scalar C > 0, then for\nevery S,\nLS(A(S)) ≤LS(A(S)) + λ∥A(S)∥2 ≤LS(0) + λ∥0∥2 = LS(0) ≤C Hence, Corollary 13.7 also implies that\nE\nh\nℓ(A(S(i)), zi) −ℓ(A(S), zi)\ni\n≤48 β C\nλm 13.4\nControlling the Fitting-Stability Tradeoﬀ\nWe can rewrite the expected risk of a learning algorithm as\nE\nS[LD(A(S))] = E\nS[LS(A(S))] + E\nS[LD(A(S)) −LS(A(S))] (13.15)\nThe ﬁrst term reﬂects how well A(S) ﬁts the training set while the second term\nreﬂects the diﬀerence between the true and empirical risks of A(S)",
      "word_count": 236,
      "source_page": 178,
      "start_position": 63807,
      "end_position": 64042,
      "sentences_count": 7
    },
    {
      "chunk_id": 284,
      "text": "13.4 Controlling the Fitting-Stability Tradeoﬀ\n179\ntradeoﬀbetween ﬁtting and overﬁtting This tradeoﬀis quite similar to the bias-\ncomplexity tradeoﬀwe discussed previously in the book We now derive bounds on the empirical risk term for the RLM rule Recall\nthat the RLM rule is deﬁned as A(S) = argminw\n\u0000LS(w) + λ∥w∥2\u0001 Fix some\narbitrary vector w∗ We have\nLS(A(S)) ≤LS(A(S)) + λ∥A(S)∥2 ≤LS(w∗) + λ∥w∗∥2 Taking expectation of both sides with respect to S and noting that ES[LS(w∗)] =\nLD(w∗), we obtain that\nE\nS[LS(A(S))] ≤LD(w∗) + λ∥w∗∥2 (13.16)\nPlugging this into Equation (13.15) we obtain\nE\nS[LD(A(S))] ≤LD(w∗) + λ∥w∗∥2 + E\nS[LD(A(S)) −LS(A(S))] Combining the preceding with Corollary 13.6 we conclude:\ncorollary 13.8\nAssume that the loss function is convex and ρ-Lipschitz Then, the RLM rule with the regularization function λ∥w∥2 satisﬁes\n∀w∗, E\nS[LD(A(S))] ≤LD(w∗) + λ∥w∗∥2 + 2ρ2\nλ m This bound is often called an oracle inequality – if we think of w∗as a hy-\npothesis with low risk, the bound tells us how many examples are needed so that\nA(S) will be almost as good as w∗, had we known the norm of w∗ In practice,\nhowever, we usually do not know the norm of w∗ We therefore usually tune λ\non the basis of a validation set, as described in Chapter 11 We can also easily derive a PAC-like guarantee1 from Corollary 13.8 for convex-\nLipschitz-bounded learning problems:\ncorollary 13.9\nLet (H, Z, ℓ) be a convex-Lipschitz-bounded learning problem\nwith parameters ρ, B",
      "word_count": 250,
      "source_page": 179,
      "start_position": 64120,
      "end_position": 64369,
      "sentences_count": 14
    },
    {
      "chunk_id": 285,
      "text": "We therefore usually tune λ\non the basis of a validation set, as described in Chapter 11 We can also easily derive a PAC-like guarantee1 from Corollary 13.8 for convex-\nLipschitz-bounded learning problems:\ncorollary 13.9\nLet (H, Z, ℓ) be a convex-Lipschitz-bounded learning problem\nwith parameters ρ, B For any training set size m, let λ =\nq\n2ρ2\nB2 m Then, the\nRLM rule with the regularization function λ∥w∥2 satisﬁes\nE\nS[LD(A(S))] ≤min\nw∈H LD(w) + ρ B\nr\n8\nm In particular, for every ϵ > 0, if m ≥\n8ρ2B2\nϵ2\nthen for every distribution D,\nES[LD(A(S))] ≤minw∈H LD(w) + ϵ The preceding corollary holds for Lipschitz loss functions If instead the loss\nfunction is smooth and nonnegative, then we can combine Equation (13.16) with\nCorollary 13.7 to get:\n1 Again, the bound below is on the expected risk, but using Exercise 1 it can be used to\nderive an agnostic PAC learning guarantee.",
      "word_count": 157,
      "source_page": 179,
      "start_position": 64322,
      "end_position": 64478,
      "sentences_count": 7
    },
    {
      "chunk_id": 286,
      "text": "180\nRegularization and Stability\ncorollary 13.10\nAssume that the loss function is convex, β-smooth, and\nnonnegative Then, the RLM rule with the regularization function λ∥w∥2, for\nλ ≥2β\nm , satisﬁes the following for all w∗:\nE\nS[LD(A(S))] ≤\n\u0012\n1 + 48β\nλm\n\u0013\nE\nS[LS(A(S))] ≤\n\u0012\n1 + 48β\nλm\n\u0013 \u0000LD(w∗) + λ∥w∗∥2\u0001 For example, if we choose λ =\n48β\nm\nwe obtain from the preceding that the\nexpected true risk of A(S) is at most twice the expected empirical risk of A(S) Furthermore, for this value of λ, the expected empirical risk of A(S) is at most\nLD(w∗) + 48β\nm ∥w∗∥2 We can also derive a learnability guarantee for convex-smooth-bounded learn-\ning problems based on Corollary 13.10 corollary 13.11\nLet (H, Z, ℓ) be a convex-smooth-bounded learning problem\nwith parameters β, B Assume in addition that ℓ(0, z) ≤1 for all z ∈Z For any\nϵ ∈(0, 1) let m ≥150βB2\nϵ2\nand set λ = ϵ/(3B2) Then, for every distribution D,\nE\nS[LD(A(S))] ≤min\nw∈H LD(w) + ϵ 13.5\nSummary\nWe introduced stability and showed that if an algorithm is stable then it does not\noverﬁt Furthermore, for convex-Lipschitz-bounded or convex-smooth-bounded\nproblems, the RLM rule with Tikhonov regularization leads to a stable learning\nalgorithm We discussed how the regularization parameter, λ, controls the trade-\noﬀbetween ﬁtting and overﬁtting",
      "word_count": 226,
      "source_page": 180,
      "start_position": 64479,
      "end_position": 64704,
      "sentences_count": 12
    },
    {
      "chunk_id": 287,
      "text": "Furthermore, for convex-Lipschitz-bounded or convex-smooth-bounded\nproblems, the RLM rule with Tikhonov regularization leads to a stable learning\nalgorithm We discussed how the regularization parameter, λ, controls the trade-\noﬀbetween ﬁtting and overﬁtting Finally, we have shown that all learning prob-\nlems that are from the families of convex-Lipschitz-bounded and convex-smooth-\nbounded problems are learnable using the RLM rule The RLM paradigm is the\nbasis for many popular learning algorithms, including ridge regression (which we\ndiscussed in this chapter) and support vector machines (which will be discussed\nin Chapter 15) In the next chapter we will present Stochastic Gradient Descent, which gives us\na very practical alternative way to learn convex-Lipschitz-bounded and convex-\nsmooth-bounded problems and can also be used for eﬃciently implementing the\nRLM rule 13.6\nBibliographic Remarks\nStability is widely used in many mathematical contexts For example, the neces-\nsity of stability for so-called inverse problems to be well posed was ﬁrst recognized\nby Hadamard (1902) The idea of regularization and its relation to stability be-\ncame widely known through the works of Tikhonov (1943) and Phillips (1962).",
      "word_count": 179,
      "source_page": 180,
      "start_position": 64673,
      "end_position": 64851,
      "sentences_count": 8
    },
    {
      "chunk_id": 288,
      "text": "13.7 Exercises\n181\nIn the context of modern learning theory, the use of stability can be traced back\nat least to the work of Rogers & Wagner (1978), which noted that the sensitiv-\nity of a learning algorithm with regard to small changes in the sample controls\nthe variance of the leave-one-out estimate The authors used this observation to\nobtain generalization bounds for the k-nearest neighbor algorithm (see Chap-\nter 19) These results were later extended to other “local” learning algorithms\n(see Devroye, Gy¨orﬁ& Lugosi (1996) and references therein) In addition, practi-\ncal methods have been developed to introduce stability into learning algorithms,\nin particular the Bagging technique introduced by (Breiman 1996) Over the last decade, stability was studied as a generic condition for learnabil-\nity See (Kearns & Ron 1999, Bousquet & Elisseeﬀ2002, Kutin & Niyogi 2002,\nRakhlin, Mukherjee & Poggio 2005, Mukherjee, Niyogi, Poggio & Rifkin 2006) Our presentation follows the work of Shalev-Shwartz, Shamir, Srebro & Sridha-\nran (2010), who showed that stability is suﬃcient and necessary for learning They have also shown that all convex-Lipschitz-bounded learning problems are\nlearnable using RLM, even though for some convex-Lipschitz-bounded learning\nproblems uniform convergence does not hold in a strong sense 13.7\nExercises\n1 From Bounded Expected Risk to Agnostic PAC Learning: Let A be\nan algorithm that guarantees the following: If m ≥mH(ϵ) then for every\ndistribution D it holds that\nE\nS∼Dm[LD(A(S))] ≤min\nh∈H LD(h) + ϵ",
      "word_count": 239,
      "source_page": 181,
      "start_position": 64852,
      "end_position": 65090,
      "sentences_count": 10
    },
    {
      "chunk_id": 289,
      "text": "13.7\nExercises\n1 From Bounded Expected Risk to Agnostic PAC Learning: Let A be\nan algorithm that guarantees the following: If m ≥mH(ϵ) then for every\ndistribution D it holds that\nE\nS∼Dm[LD(A(S))] ≤min\nh∈H LD(h) + ϵ • Show that for every δ ∈(0, 1), if m ≥mH(ϵ δ) then with probability of at\nleast 1 −δ it holds that LD(A(S)) ≤minh∈H LD(h) + ϵ Hint: Observe that the random variable LD(A(S)) −minh∈H LD(h) is\nnonnegative and rely on Markov’s inequality • For every δ ∈(0, 1) let\nmH(ϵ, δ) = mH(ϵ/2)⌈log2(1/δ)⌉+\n\u0018log(4/δ) + log(⌈log2(1/δ)⌉)\nϵ2\n\u0019 Suggest a procedure that agnostic PAC learns the problem with sample\ncomplexity of mH(ϵ, δ), assuming that the loss function is bounded by\n1 Hint: Let k = ⌈log2(1/δ)⌉ Divide the data into k+1 chunks, where each\nof the ﬁrst k chunks is of size mH(ϵ/2) examples Train the ﬁrst k chunks\nusing A On the basis of the previous question argue that the probability\nthat for all of these chunks we have LD(A(S)) > minh∈H LD(h) + ϵ is\nat most 2−k ≤δ/2 Finally, use the last chunk as a validation set 2 Learnability without Uniform Convergence: Let B be the unit ball of",
      "word_count": 203,
      "source_page": 181,
      "start_position": 65053,
      "end_position": 65255,
      "sentences_count": 13
    },
    {
      "chunk_id": 290,
      "text": "182\nRegularization and Stability\nRd, let H = B, let Z = B × {0, 1}d, and let ℓ: Z × H →R be deﬁned as\nfollows:\nℓ(w, (x, α)) =\nd\nX\ni=1\nαi(xi −wi)2 This problem corresponds to an unsupervised learning task, meaning that we\ndo not try to predict the label of x Instead, what we try to do is to ﬁnd the\n“center of mass” of the distribution over B However, there is a twist, modeled\nby the vectors α Each example is a pair (x, α), where x is the instance x and\nα indicates which features of x are “active” and which are “turned oﬀ.” A\nhypothesis is a vector w representing the center of mass of the distribution,\nand the loss function is the squared Euclidean distance between x and w, but\nonly with respect to the “active” elements of x • Show that this problem is learnable using the RLM rule with a sample\ncomplexity that does not depend on d • Consider a distribution D over Z as follows: x is ﬁxed to be some x0, and\neach element of α is sampled to be either 1 or 0 with equal probability Show that the rate of uniform convergence of this problem grows with\nd Hint: Let m be a training set size",
      "word_count": 222,
      "source_page": 182,
      "start_position": 65256,
      "end_position": 65477,
      "sentences_count": 9
    },
    {
      "chunk_id": 291,
      "text": "Show that the rate of uniform convergence of this problem grows with\nd Hint: Let m be a training set size Show that if d ≫2m, then there is\na high probability of sampling a set of examples such that there exists\nsome j ∈[d] for which αj = 1 for all the examples in the training set Show that such a sample cannot be ϵ-representative Conclude that the\nsample complexity of uniform convergence must grow with log(d) • Conclude that if we take d to inﬁnity we obtain a problem that is learnable\nbut for which the uniform convergence property does not hold Compare\nto the fundamental theorem of statistical learning 3 Stability and Asymptotic ERM Are Suﬃcient for Learnability:\nWe say that a learning rule A is an AERM (Asymptotic Empirical Risk\nMinimizer) with rate ϵ(m) if for every distribution D it holds that\nE\nS∼Dm\n\u0014\nLS(A(S)) −min\nh∈H LS(h)\n\u0015\n≤ϵ(m) We say that a learning rule A learns a class H with rate ϵ(m) if for every\ndistribution D it holds that\nE\nS∼Dm\n\u0014\nLD(A(S)) −min\nh∈H LD(h)\n\u0015\n≤ϵ(m) Prove the following:\ntheorem 13.12\nIf a learning algorithm A is on-average-replace-one-stable\nwith rate ϵ1(m) and is an AERM with rate ϵ2(m), then it learns H with rate\nϵ1(m) + ϵ2(m).",
      "word_count": 217,
      "source_page": 182,
      "start_position": 65457,
      "end_position": 65673,
      "sentences_count": 11
    },
    {
      "chunk_id": 292,
      "text": "13.7 Exercises\n183\n4 Strong Convexity with Respect to General Norms:\nThroughout the section we used the ℓ2 norm In this exercise we generalize\nsome of the results to general norms Let ∥·∥be some arbitrary norm, and let f\nbe a strongly convex function with respect to this norm (see Deﬁnition 13.4) 1 Show that items 2–3 of Lemma 13.5 hold for every norm 2 (*) Give an example of a norm for which item 1 of Lemma 13.5 does not\nhold 3 Let R(w) be a function that is (2λ)-strongly convex with respect to some\nnorm ∥· ∥ Let A be an RLM rule with respect to R, namely,\nA(S) = argmin\nw\n(LS(w) + R(w)) Assume that for every z, the loss function ℓ(·, z) is ρ-Lipschitz with respect\nto the same norm, namely,\n∀z, ∀w, v,\nℓ(w, z) −ℓ(v, z) ≤ρ ∥w −v∥ Prove that A is on-average-replace-one-stable with rate 2ρ2\nλm 4 (*) Let q ∈(1, 2) and consider the ℓq-norm\n∥w∥q =\n d\nX\ni=1\n|wi|q\n!1/q It can be shown (see, for example, Shalev-Shwartz (2007)) that the function\nR(w) =\n1\n2(q −1)∥w∥2\nq\nis 1-strongly convex with respect to ∥w∥q Show that if q =\nlog(d)\nlog(d)−1 then\nR(w) is\n\u0010\n1\n3 log(d)\n\u0011\n-strongly convex with respect to the ℓ1 norm over Rd.",
      "word_count": 222,
      "source_page": 183,
      "start_position": 65674,
      "end_position": 65895,
      "sentences_count": 17
    },
    {
      "chunk_id": 293,
      "text": "14\nStochastic Gradient Descent\nRecall that the goal of learning is to minimize the risk function, LD(h) =\nEz∼D[ℓ(h, z)] We cannot directly minimize the risk function since it depends\non the unknown distribution D So far in the book, we have discussed learning\nmethods that depend on the empirical risk That is, we ﬁrst sample a training\nset S and deﬁne the empirical risk function LS(h) Then, the learner picks a\nhypothesis based on the value of LS(h) For example, the ERM rule tells us to\npick the hypothesis that minimizes LS(h) over the hypothesis class, H Or, in the\nprevious chapter, we discussed regularized risk minimization, in which we pick a\nhypothesis that jointly minimizes LS(h) and a regularization function over h In this chapter we describe and analyze a rather diﬀerent learning approach,\nwhich is called Stochastic Gradient Descent (SGD) As in Chapter 12 we will\nfocus on the important family of convex learning problems, and following the\nnotation in that chapter, we will refer to hypotheses as vectors w that come from\na convex hypothesis class, H In SGD, we try to minimize the risk function LD(w)\ndirectly using a gradient descent procedure Gradient descent is an iterative\noptimization procedure in which at each step we improve the solution by taking\na step along the negative of the gradient of the function to be minimized at\nthe current point",
      "word_count": 233,
      "source_page": 184,
      "start_position": 65896,
      "end_position": 66128,
      "sentences_count": 11
    },
    {
      "chunk_id": 294,
      "text": "In SGD, we try to minimize the risk function LD(w)\ndirectly using a gradient descent procedure Gradient descent is an iterative\noptimization procedure in which at each step we improve the solution by taking\na step along the negative of the gradient of the function to be minimized at\nthe current point Of course, in our case, we are minimizing the risk function,\nand since we do not know D we also do not know the gradient of LD(w) SGD\ncircumvents this problem by allowing the optimization procedure to take a step\nalong a random direction, as long as the expected value of the direction is the\nnegative of the gradient And, as we shall see, ﬁnding a random direction whose\nexpected value corresponds to the gradient is rather simple even though we do\nnot know the underlying distribution D The advantage of SGD, in the context of convex learning problems, over the\nregularized risk minimization learning rule is that SGD is an eﬃcient algorithm\nthat can be implemented in a few lines of code, yet still enjoys the same sample\ncomplexity as the regularized risk minimization rule The simplicity of SGD also\nallows us to use it in situations when it is not possible to apply methods that\nare based on the empirical risk, but this is beyond the scope of this book We start this chapter with the basic gradient descent algorithm and analyze its\nconvergence rate for convex-Lipschitz functions",
      "word_count": 242,
      "source_page": 184,
      "start_position": 66077,
      "end_position": 66318,
      "sentences_count": 8
    },
    {
      "chunk_id": 295,
      "text": "14.1 Gradient Descent\n185\nthe Stochastic Gradient Descent algorithm, along with several useful variants We show that SGD enjoys an expected convergence rate similar to the rate\nof gradient descent Finally, we turn to the applicability of SGD to learning\nproblems 14.1\nGradient Descent\nBefore we describe the stochastic gradient descent method, we would like to\ndescribe the standard gradient descent approach for minimizing a diﬀerentiable\nconvex function f(w) The gradient of a diﬀerentiable function f : Rd →R at w, denoted ∇f(w),\nis the vector of partial derivatives of f, namely, ∇f(w) =\n\u0010\n∂f(w)\n∂w[1] , , ∂f(w)\n∂w[d]\n\u0011 Gradient descent is an iterative algorithm We start with an initial value of w\n(say, w(1) = 0) Then, at each iteration, we take a step in the direction of the\nnegative of the gradient at the current point That is, the update step is\nw(t+1) = w(t) −η∇f(w(t)),\n(14.1)\nwhere η > 0 is a parameter to be discussed later Intuitively, since the gradi-\nent points in the direction of the greatest rate of increase of f around w(t),\nthe algorithm makes a small step in the opposite direction, thus decreasing the\nvalue of the function Eventually, after T iterations, the algorithm outputs the\naveraged vector, ¯w =\n1\nT\nPT\nt=1 w(t)",
      "word_count": 216,
      "source_page": 185,
      "start_position": 66380,
      "end_position": 66595,
      "sentences_count": 12
    },
    {
      "chunk_id": 296,
      "text": "Intuitively, since the gradi-\nent points in the direction of the greatest rate of increase of f around w(t),\nthe algorithm makes a small step in the opposite direction, thus decreasing the\nvalue of the function Eventually, after T iterations, the algorithm outputs the\naveraged vector, ¯w =\n1\nT\nPT\nt=1 w(t) The output could also be the last vector,\nw(T ), or the best performing vector, argmint∈[T ] f(w(t)), but taking the average\nturns out to be rather useful, especially when we generalize gradient descent to\nnondiﬀerentiable functions and to the stochastic case Another way to motivate gradient descent is by relying on Taylor approxima-\ntion The gradient of f at w yields the ﬁrst order Taylor approximation of f\naround w by f(u) ≈f(w) + ⟨u −w, ∇f(w)⟩ When f is convex, this approxi-\nmation lower bounds f, that is,\nf(u) ≥f(w) + ⟨u −w, ∇f(w)⟩ Therefore, for w close to w(t) we have that f(w) ≈f(w(t))+⟨w−w(t), ∇f(w(t))⟩ Hence we can minimize the approximation of f(w) However, the approximation\nmight become loose for w, which is far away from w(t) Therefore, we would like\nto minimize jointly the distance between w and w(t) and the approximation of\nf around w(t) If the parameter η controls the tradeoﬀbetween the two terms,\nwe obtain the update rule\nw(t+1) = argmin\nw\n1\n2∥w −w(t)∥2 + η\n\u0010\nf(w(t)) + ⟨w −w(t), ∇f(w(t))⟩\n\u0011",
      "word_count": 234,
      "source_page": 185,
      "start_position": 66543,
      "end_position": 66776,
      "sentences_count": 11
    },
    {
      "chunk_id": 297,
      "text": "186\nStochastic Gradient Descent\nFigure 14.1 An illustration of the gradient descent algorithm The function to be\nminimized is 1.25(x1 + 6)2 + (x2 −8)2 14.1.1\nAnalysis of GD for Convex-Lipschitz Functions\nTo analyze the convergence rate of the GD algorithm, we limit ourselves to\nthe case of convex-Lipschitz functions (as we have seen, many problems lend\nthemselves easily to this setting) Let w⋆be any vector and let B be an upper\nbound on ∥w⋆∥ It is convenient to think of w⋆as the minimizer of f(w), but\nthe analysis that follows holds for every w⋆ We would like to obtain an upper bound on the suboptimality of our solution\nwith respect to w⋆, namely, f( ¯w) −f(w⋆), where ¯w = 1\nT\nPT\nt=1 w(t) From the\ndeﬁnition of ¯w, and using Jensen’s inequality, we have that\nf( ¯w) −f(w⋆) = f\n \n1\nT\nT\nX\nt=1\nw(t) −f(w⋆)\n≤\n1\nT\nT\nX\nt=1\n\u0010\nf(w(t))\n\u0011\n−f(w⋆)\n= 1\nT\nT\nX\nt=1\n\u0010\nf(w(t)) −f(w⋆)\n\u0011 (14.2)\nFor every t, because of the convexity of f, we have that\nf(w(t)) −f(w⋆) ≤⟨w(t) −w⋆, ∇f(w(t))⟩ (14.3)\nCombining the preceding we obtain\nf( ¯w) −f(w⋆) ≤\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, ∇f(w(t))⟩ To bound the right-hand side we rely on the following lemma:",
      "word_count": 216,
      "source_page": 186,
      "start_position": 66802,
      "end_position": 67017,
      "sentences_count": 11
    },
    {
      "chunk_id": 298,
      "text": "14.1 Gradient Descent\n187\nlemma 14.1\nLet v1, , vT be an arbitrary sequence of vectors Any algorithm\nwith an initialization w(1) = 0 and an update rule of the form\nw(t+1) = w(t) −ηvt\n(14.4)\nsatisﬁes\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩≤∥w⋆∥2\n2η\n+ η\n2\nT\nX\nt=1\n∥vt∥2 (14.5)\nIn particular, for every B, ρ > 0, if for all t we have that ∥vt∥≤ρ and if we set\nη =\nq\nB2\nρ2 T , then for every w⋆with ∥w⋆∥≤B we have\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩≤B ρ\n√\nT Proof\nUsing algebraic manipulations (completing the square), we obtain:\n⟨w(t) −w⋆, vt⟩= 1\nη ⟨w(t) −w⋆, ηvt⟩\n= 1\n2η (−∥w(t) −w⋆−ηvt∥2 + ∥w(t) −w⋆∥2 + η2∥vt∥2)\n= 1\n2η (−∥w(t+1) −w⋆∥2 + ∥w(t) −w⋆∥2) + η\n2∥vt∥2,\nwhere the last equality follows from the deﬁnition of the update rule Summing\nthe equality over t, we have\nT\nX\nt=1\n⟨w(t)−w⋆, vt⟩=\n1\n2η\nT\nX\nt=1\n\u0010\n−∥w(t+1) −w⋆∥2 + ∥w(t) −w⋆∥2\u0011\n+ η\n2\nT\nX\nt=1\n∥vt∥2 (14.6)\nThe ﬁrst sum on the right-hand side is a telescopic sum that collapses to\n∥w(1) −w⋆∥2 −∥w(T +1) −w⋆∥2",
      "word_count": 197,
      "source_page": 187,
      "start_position": 67018,
      "end_position": 67214,
      "sentences_count": 7
    },
    {
      "chunk_id": 299,
      "text": "188\nStochastic Gradient Descent\nLemma 14.1 applies to the GD algorithm with vt = ∇f(w(t)) As we will\nshow later in Lemma 14.7, if f is ρ-Lipschitz, then ∥∇f(w(t))∥≤ρ We therefore\nsatisfy the lemma’s conditions and achieve the following corollary:\ncorollary 14.2\nLet f be a convex, ρ-Lipschitz function, and let w⋆∈argmin{w:∥w∥≤B} f(w) If we run the GD algorithm on f for T steps with η =\nq\nB2\nρ2 T , then the output\nvector ¯w satisﬁes\nf( ¯w) −f(w⋆) ≤B ρ\n√\nT Furthermore, for every ϵ > 0, to achieve f( ¯w)−f(w⋆) ≤ϵ, it suﬃces to run the\nGD algorithm for a number of iterations that satisﬁes\nT ≥B2ρ2\nϵ2 14.2\nSubgradients\nThe GD algorithm requires that the function f be diﬀerentiable We now gener-\nalize the discussion beyond diﬀerentiable functions We will show that the GD\nalgorithm can be applied to nondiﬀerentiable functions by using a so-called sub-\ngradient of f(w) at w(t), instead of the gradient To motivate the deﬁnition of subgradients, recall that for a convex function f,\nthe gradient at w deﬁnes the slope of a tangent that lies below f, that is,\n∀u,\nf(u) ≥f(w) + ⟨u −w, ∇f(w)⟩ (14.7)\nAn illustration is given on the left-hand side of Figure 14.2 The existence of a tangent that lies below f is an important property of convex\nfunctions, which is in fact an alternative characterization of convexity lemma 14.3\nLet S be an open convex set",
      "word_count": 243,
      "source_page": 188,
      "start_position": 67307,
      "end_position": 67549,
      "sentences_count": 12
    },
    {
      "chunk_id": 300,
      "text": "The existence of a tangent that lies below f is an important property of convex\nfunctions, which is in fact an alternative characterization of convexity lemma 14.3\nLet S be an open convex set A function f : S →R is convex iﬀ\nfor every w ∈S there exists v such that\n∀u ∈S,\nf(u) ≥f(w) + ⟨u −w, v⟩ (14.8)\nThe proof of this lemma can be found in many convex analysis textbooks (e.g.,\n(Borwein & Lewis 2006)) The preceding inequality leads us to the deﬁnition of\nsubgradients definition 14.4 (Subgradients)\nA vector v that satisﬁes Equation (14.8) is\ncalled a subgradient of f at w The set of subgradients of f at w is called the\ndiﬀerential set and denoted ∂f(w) An illustration of subgradients is given on the right-hand side of Figure 14.2 For scalar functions, a subgradient of a convex function f at w is a slope of a\nline that touches f at w and is not above f elsewhere.",
      "word_count": 165,
      "source_page": 188,
      "start_position": 67516,
      "end_position": 67680,
      "sentences_count": 9
    },
    {
      "chunk_id": 301,
      "text": "14.2 Subgradients\n189\nf(w)\nf(u)\nw\nu\nf(w) + ⟨u −w, ∇f(w)⟩\nFigure 14.2 Left: The right-hand side of Equation (14.7) is the tangent of f at w For\na convex function, the tangent lower bounds f Right: Illustration of several\nsubgradients of a nondiﬀerentiable convex function 14.2.1\nCalculating Subgradients\nHow do we construct subgradients of a given convex function If a function is\ndiﬀerentiable at a point w, then the diﬀerential set is trivial, as the following\nclaim shows claim 14.5\nIf f is diﬀerentiable at w then ∂f(w) contains a single element –\nthe gradient of f at w, ∇f(w) Example 14.1 (The Diﬀerential Set of the Absolute Function)\nConsider the\nabsolute value function f(x) = |x| Using Claim 14.5, we can easily construct\nthe diﬀerential set for the diﬀerentiable parts of f, and the only point that\nrequires special attention is x0 = 0 At that point, it is easy to verify that the\nsubdiﬀerential is the set of all numbers between −1 and 1 Hence:\n∂f(x) =\n\n\n\n\n\n\n\n{1}\nif x > 0\n{−1}\nif x < 0\n[−1, 1]\nif x = 0\nFor many practical uses, we do not need to calculate the whole set of subgra-\ndients at a given point, as one member of this set would suﬃce The following\nclaim shows how to construct a sub-gradient for pointwise maximum functions claim 14.6\nLet g(w) = maxi∈[r] gi(w) for r convex diﬀerentiable functions\ng1, , gr",
      "word_count": 250,
      "source_page": 189,
      "start_position": 67681,
      "end_position": 67930,
      "sentences_count": 13
    },
    {
      "chunk_id": 302,
      "text": "190\nStochastic Gradient Descent\nExample 14.2 (A Subgradient of the Hinge Loss)\nRecall the hinge loss function\nfrom Section 12.3, f(w) = max{0, 1 −y⟨w, x⟩} for some vector x and scalar y To calculate a subgradient of the hinge loss at some w we rely on the preceding\nclaim and obtain that the vector v deﬁned in the following is a subgradient of\nthe hinge loss at w:\nv =\n(\n0\nif 1 −y⟨w, x⟩≤0\n−yx\nif 1 −y⟨w, x⟩> 0\n14.2.2\nSubgradients of Lipschitz Functions\nRecall that a function f : A →R is ρ-Lipschitz if for all u, v ∈A\n|f(u) −f(v)| ≤ρ ∥u −v∥ The following lemma gives an equivalent deﬁnition using norms of subgradients lemma 14.7\nLet A be a convex open set and let f : A →R be a convex function Then, f is ρ-Lipschitz over A iﬀfor all w ∈A and v ∈∂f(w) we have that\n∥v∥≤ρ Proof\nAssume that for all v ∈∂f(w) we have that ∥v∥≤ρ Since v ∈∂f(w)\nwe have\nf(w) −f(u) ≤⟨v, w −u⟩ Bounding the right-hand side using Cauchy-Schwartz inequality we obtain\nf(w) −f(u) ≤⟨v, w −u⟩≤∥v∥∥w −u∥≤ρ ∥w −u∥ An analogous argument can show that f(u) −f(w) ≤ρ ∥w −u∥ Hence f is\nρ-Lipschitz Now assume that f is ρ-Lipschitz Choose some w ∈A, v ∈∂f(w) Since A\nis open, there exists ϵ > 0 such that u = w + ϵv/∥v∥belongs to A Therefore,\n⟨u −w, v⟩= ϵ∥v∥and ∥u −w∥= ϵ",
      "word_count": 247,
      "source_page": 190,
      "start_position": 67978,
      "end_position": 68224,
      "sentences_count": 14
    },
    {
      "chunk_id": 303,
      "text": "14.3 Stochastic Gradient Descent (SGD)\n191\nFigure 14.3 An illustration of the gradient descent algorithm (left) and the stochastic\ngradient descent algorithm (right) The function to be minimized is\n1.25(x + 6)2 + (y −8)2 For the stochastic case, the black line depicts the averaged\nvalue of w 14.3\nStochastic Gradient Descent (SGD)\nIn stochastic gradient descent we do not require the update direction to be based\nexactly on the gradient Instead, we allow the direction to be a random vector\nand only require that its expected value at each iteration will equal the gradient\ndirection Or, more generally, we require that the expected value of the random\nvector will be a subgradient of the function at the current vector Stochastic Gradient Descent (SGD) for minimizing\nf(w)\nparameters: Scalar η > 0, integer T > 0\ninitialize: w(1) = 0\nfor t = 1, 2, , T\nchoose vt at random from a distribution such that E[vt | w(t)] ∈∂f(w(t))\nupdate w(t+1) = w(t) −ηvt\noutput ¯w = 1\nT\nPT\nt=1 w(t)\nAn illustration of stochastic gradient descent versus gradient descent is given\nin Figure 14.3 As we will see in Section 14.5, in the context of learning problems,\nit is easy to ﬁnd a random vector whose expectation is a subgradient of the risk\nfunction 14.3.1\nAnalysis of SGD for Convex-Lipschitz-Bounded Functions\nRecall the bound we achieved for the GD algorithm in Corollary 14.2",
      "word_count": 236,
      "source_page": 191,
      "start_position": 68307,
      "end_position": 68542,
      "sentences_count": 10
    },
    {
      "chunk_id": 304,
      "text": "192\nStochastic Gradient Descent\nsubgradient of f at w(t), we can still derive a similar bound on the expected\noutput of stochastic gradient descent This is formalized in the following theorem theorem 14.8\nLet B, ρ > 0 Let f be a convex function and let w⋆∈argminw:∥w∥≤B f(w) Assume that SGD is run for T iterations with η =\nq\nB2\nρ2 T Assume also that for\nall t, ∥vt∥≤ρ with probability 1 Then,\nE [f( ¯w)] −f(w⋆) ≤B ρ\n√\nT Therefore, for any ϵ > 0, to achieve E[f( ¯w)] −f(w⋆) ≤ϵ, it suﬃces to run the\nSGD algorithm for a number of iterations that satisﬁes\nT ≥B2ρ2\nϵ2 Proof\nLet us introduce the notation v1:t to denote the sequence v1, , vt Taking expectation of Equation (14.2), we obtain\nE\nv1:T[f( ¯w) −f(w⋆)] ≤E\nv1:T\n\"\n1\nT\nT\nX\nt=1\n(f(w(t)) −f(w⋆))\n# Since Lemma 14.1 holds for any sequence v1, v2, ...vT , it applies to SGD as well By taking expectation of the bound in the lemma we have\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n≤B ρ\n√\nT (14.9)\nIt is left to show that\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n(f(w(t)) −f(w⋆))\n#\n≤E\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n,\n(14.10)\nwhich we will hereby prove",
      "word_count": 228,
      "source_page": 192,
      "start_position": 68572,
      "end_position": 68799,
      "sentences_count": 14
    },
    {
      "chunk_id": 305,
      "text": "By taking expectation of the bound in the lemma we have\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n≤B ρ\n√\nT (14.9)\nIt is left to show that\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n(f(w(t)) −f(w⋆))\n#\n≤E\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n,\n(14.10)\nwhich we will hereby prove Using the linearity of the expectation we have\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n= 1\nT\nT\nX\nt=1\nE\nv1:T[⟨w(t) −w⋆, vt⟩] Next, we recall the law of total expectation: For every two random variables α, β,\nand a function g, Eα[g(α)] = Eβ Eα[g(α)|β] Setting α = v1:t and β = v1:t−1 we\nget that\nE\nv1:T[⟨w(t) −w⋆, vt⟩] = E\nv1:t[⟨w(t) −w⋆, vt⟩]\n=\nE\nv1:t−1 E\nv1:t[⟨w(t) −w⋆, vt⟩| v1:t−1] Once we know v1:t−1, the value of w(t) is not random any more and therefore\nE\nv1:t−1 E\nv1:t[⟨w(t) −w⋆, vt⟩| v1:t−1] =\nE\nv1:t−1⟨w(t) −w⋆, E\nvt[vt | v1:t−1]⟩.",
      "word_count": 175,
      "source_page": 192,
      "start_position": 68736,
      "end_position": 68910,
      "sentences_count": 6
    },
    {
      "chunk_id": 306,
      "text": "14.4 Variants\n193\nSince w(t) only depends on v1:t−1 and SGD requires that Evt[vt | w(t)] ∈∂f(w(t))\nwe obtain that Evt[vt | v1:t−1] ∈∂f(w(t)) Thus,\nE\nv1: t−1⟨w(t) −w⋆, E\nvt[vt | v1: t−1]⟩≥\nE\nv1: t−1[f(w(t)) −f(w⋆)] Overall, we have shown that\nE\nv1:T[⟨w(t) −w⋆, vt⟩] ≥\nE\nv1:t−1[f(w(t)) −f(w⋆)]\n= E\nv1:T[f(w(t)) −f(w⋆)] Summing over t, dividing by T, and using the linearity of expectation, we get\nthat Equation (14.10) holds, which concludes our proof 14.4\nVariants\nIn this section we describe several variants of Stochastic Gradient Descent 14.4.1\nAdding a Projection Step\nIn the previous analyses of the GD and SGD algorithms, we required that the\nnorm of w⋆will be at most B, which is equivalent to requiring that w⋆is in the\nset H = {w : ∥w∥≤B} In terms of learning, this means restricting ourselves to\na B-bounded hypothesis class Yet any step we take in the opposite direction of\nthe gradient (or its expected direction) might result in stepping out of this bound,\nand there is even no guarantee that ¯w satisﬁes it We show in the following how\nto overcome this problem while maintaining the same convergence rate The basic idea is to add a projection step; namely, we will now have a two-step\nupdate rule, where we ﬁrst subtract a subgradient from the current value of w\nand then project the resulting vector onto H Formally,\n1 w(t+ 1\n2 ) = w(t) −ηvt\n2",
      "word_count": 242,
      "source_page": 193,
      "start_position": 68911,
      "end_position": 69152,
      "sentences_count": 12
    },
    {
      "chunk_id": 307,
      "text": "194\nStochastic Gradient Descent\nThen, for every u ∈H,\n∥w −u∥2 −∥v −u∥2 ≥0 Proof\nBy the convexity of H, for every α ∈(0, 1) we have that v+α(u−v) ∈H Therefore, from the optimality of v we obtain\n∥v −w∥2 ≤∥v + α(u −v) −w∥2\n= ∥v −w∥2 + 2α⟨v −w, u −v⟩+ α2∥u −v∥2 Rearranging, we obtain\n2⟨v −w, u −v⟩≥−α ∥u −v∥2 Taking the limit α →0 we get that\n⟨v −w, u −v⟩≥0 Therefore,\n∥w −u∥2 = ∥w −v + v −u∥2\n= ∥w −v∥2 + ∥v −u∥2 + 2⟨v −w, u −v⟩\n≥∥v −u∥2 Equipped with the preceding lemma, we can easily adapt the analysis of SGD\nto the case in which we add projection steps on a closed and convex set Simply\nnote that for every t,\n∥w(t+1) −w⋆∥2 −∥w(t) −w⋆∥2\n= ∥w(t+1) −w⋆∥2 −∥w(t+ 1\n2 ) −w⋆∥2 + ∥w(t+ 1\n2 ) −w⋆∥2 −∥w(t) −w⋆∥2\n≤∥w(t+ 1\n2 ) −w⋆∥2 −∥w(t) −w⋆∥2 Therefore, Lemma 14.1 holds when we add projection steps and hence the rest\nof the analysis follows directly 14.4.2\nVariable Step Size\nAnother variant of SGD is decreasing the step size as a function of t That is,\nrather than updating with a constant η, we use ηt For instance, we can set\nηt =\nB\nρ\n√\nt and achieve a bound similar to Theorem 14.8",
      "word_count": 226,
      "source_page": 194,
      "start_position": 69249,
      "end_position": 69474,
      "sentences_count": 12
    },
    {
      "chunk_id": 308,
      "text": "14.4 Variants\n195\n14.4.3\nOther Averaging Techniques\nWe have set the output vector to be ¯w =\n1\nT\nPT\nt=1 w(t) There are alternative\napproaches such as outputting w(t) for some random t ∈[t], or outputting the\naverage of w(t) over the last αT iterations, for some α ∈(0, 1) One can also take\na weighted average of the last few iterates These more sophisticated averaging\nschemes can improve the convergence speed in some situations, such as in the\ncase of strongly convex functions deﬁned in the following 14.4.4\nStrongly Convex Functions*\nIn this section we show a variant of SGD that enjoys a faster convergence rate for\nproblems in which the objective function is strongly convex (see Deﬁnition 13.4\nof strong convexity in the previous chapter) We rely on the following claim,\nwhich generalizes Lemma 13.5 claim 14.10\nIf f is λ-strongly convex then for every w, u and v ∈∂f(w) we\nhave\n⟨w −u, v⟩≥f(w) −f(u) + λ\n2 ∥w −u∥2 The proof is similar to the proof of Lemma 13.5 and is left as an exercise SGD for minimizing a λ-strongly convex function\nGoal: Solve minw∈H f(w)\nparameter: T\ninitialize: w(1) = 0\nfor t = 1, , T\nChoose a random vector vt s.t",
      "word_count": 209,
      "source_page": 195,
      "start_position": 69502,
      "end_position": 69710,
      "sentences_count": 10
    },
    {
      "chunk_id": 309,
      "text": "196\nStochastic Gradient Descent\nSince w(t+1) is the projection of w(t+ 1\n2 ) onto H, and w⋆∈H we have that\n∥w(t+ 1\n2 ) −w⋆∥2 ≥∥w(t+1) −w⋆∥2 Therefore,\n∥w(t) −w⋆∥2 −∥w(t+1) −w⋆∥2 ≥∥w(t) −w⋆∥2 −∥w(t+ 1\n2 ) −w⋆∥2\n= 2ηt⟨w(t) −w⋆, vt⟩−η2\nt ∥vt∥2 Taking expectation of both sides, rearranging, and using the assumption E[∥vt∥2] ≤\nρ2 yield Equation (14.12) Comparing Equation (14.11) and Equation (14.12) and\nsumming over t we obtain\nT\nX\nt=1\n(E[f(w(t))] −f(w⋆))\n≤E\n\" T\nX\nt=1\n\u0012∥w(t) −w⋆∥2 −∥w(t+1) −w⋆∥2\n2 ηt\n−λ\n2 ∥w(t) −w⋆∥2\n\u0013#\n+ ρ2\n2\nT\nX\nt=1\nηt Next, we use the deﬁnition ηt = 1/(λ t) and note that the ﬁrst sum on the\nright-hand side of the equation collapses to −λT∥w(T +1) −w⋆∥2 ≤0 Thus,\nT\nX\nt=1\n(E[f(w(t))] −f(w⋆)) ≤ρ2\n2 λ\nT\nX\nt=1\n1\nt ≤ρ2\n2 λ(1 + log(T)) The theorem follows from the preceding by dividing by T and using Jensen’s\ninequality Remark 14.3\nRakhlin, Shamir & Sridharan (2012) derived a convergence rate\nin which the log(T) term is eliminated for a variant of the algorithm in which\nwe output the average of the last T/2 iterates, ¯w = 2\nT\nPT\nt=T/2+1 w(t) Shamir &\nZhang (2013) have shown that Theorem 14.11 holds even if we output ¯w = w(T ) 14.5\nLearning with SGD\nWe have so far introduced and analyzed the SGD algorithm for general convex\nfunctions Now we shall consider its applicability to learning tasks",
      "word_count": 250,
      "source_page": 196,
      "start_position": 69826,
      "end_position": 70075,
      "sentences_count": 11
    },
    {
      "chunk_id": 310,
      "text": "14.5 Learning with SGD\n197\nLD(w), that is, a random vector whose conditional expected value is ∇LD(w(t)) We shall now see how such an estimate can be easily constructed For simplicity, let us ﬁrst consider the case of diﬀerentiable loss functions Hence the risk function LD is also diﬀerentiable The construction of the random\nvector vt will be as follows: First, sample z ∼D Then, deﬁne vt to be the\ngradient of the function ℓ(w, z) with respect to w, at the point w(t) Then, by\nthe linearity of the gradient we have\nE[vt|w(t)] = E\nz∼D[∇ℓ(w(t), z)] = ∇E\nz∼D[ℓ(w(t), z)] = ∇LD(w(t)) (14.13)\nThe gradient of the loss function ℓ(w, z) at w(t) is therefore an unbiased estimate\nof the gradient of the risk function LD(w(t)) and is easily constructed by sampling\na single fresh example z ∼D at each iteration t The same argument holds for nondiﬀerentiable loss functions We simply let\nvt be a subgradient of ℓ(w, z) at w(t) Then, for every u we have\nℓ(u, z) −ℓ(w(t), z) ≥⟨u −w(t), vt⟩ Taking expectation on both sides with respect to z ∼D and conditioned on the\nvalue of w(t) we obtain\nLD(u) −LD(w(t)) = E[ℓ(u, z) −ℓ(w(t), z)|w(t)]\n≥E[⟨u −w(t), vt⟩|w(t)]\n= ⟨u −w(t), E[vt|w(t)]⟩ It follows that E[vt|w(t)] is a subgradient of LD(w) at w(t) To summarize, the stochastic gradient descent framework for minimizing the\nrisk is as follows",
      "word_count": 235,
      "source_page": 197,
      "start_position": 70167,
      "end_position": 70401,
      "sentences_count": 14
    },
    {
      "chunk_id": 311,
      "text": "198\nStochastic Gradient Descent\nLD(w) with a number of iterations (i.e., number of examples)\nT ≥B2ρ2\nϵ2\nand with η =\nq\nB2\nρ2 T , then the output of SGD satisﬁes\nE [LD( ¯w)] ≤min\nw∈H LD(w) + ϵ It is interesting to note that the required sample complexity is of the same order\nof magnitude as the sample complexity guarantee we derived for regularized loss\nminimization In fact, the sample complexity of SGD is even better than what\nwe have derived for regularized loss minimization by a factor of 8 14.5.2\nAnalyzing SGD for Convex-Smooth Learning Problems\nIn the previous chapter we saw that the regularized loss minimization rule also\nlearns the class of convex-smooth-bounded learning problems We now show that\nthe SGD algorithm can be also used for such problems theorem 14.13\nAssume that for all z, the loss function ℓ(·, z) is convex, β-\nsmooth, and nonnegative Then, if we run the SGD algorithm for minimizing\nLD(w) we have that for every w⋆,\nE[LD( ¯w)] ≤\n1\n1 −ηβ\n\u0012\nLD(w⋆) + ∥w⋆∥2\n2η T\n\u0013 Proof\nRecall that if a function is β-smooth and nonnegative then it is self-\nbounded:\n∥∇f(w)∥2 ≤2βf(w) To analyze SGD for convex-smooth problems, let us deﬁne z1, , zT the random\nsamples of the SGD algorithm, let ft(·) = ℓ(·, zt), and note that vt = ∇ft(w(t)) For all t, ft is a convex function and therefore ft(w(t))−ft(w⋆) ≤⟨vt, w(t)−w⋆⟩",
      "word_count": 240,
      "source_page": 198,
      "start_position": 70498,
      "end_position": 70737,
      "sentences_count": 11
    },
    {
      "chunk_id": 312,
      "text": "14.5 Learning with SGD\n199\nto z1, , zT Clearly, E[ft(w⋆)] = LD(w⋆) In addition, using the same argument\nas in the proof of Theorem 14.8 we have that\nE\n\"\n1\nT\nT\nX\nt=1\nft(w(t))\n#\n= E\n\"\n1\nT\nT\nX\nt=1\nLD(w(t))\n#\n≥E[LD( ¯w)] Combining all we conclude our proof As a direct corollary we obtain:\ncorollary 14.14\nConsider a convex-smooth-bounded learning problem with\nparameters β, B Assume in addition that ℓ(0, z) ≤1 for all z ∈Z For every\nϵ > 0, set η =\n1\nβ(1+3/ϵ) Then, running SGD with T ≥12B2β/ϵ2 yields\nE[LD( ¯w)] ≤min\nw∈H LD(w) + ϵ 14.5.3\nSGD for Regularized Loss Minimization\nWe have shown that SGD enjoys the same worst-case sample complexity bound\nas regularized loss minimization However, on some distributions, regularized loss\nminimization may yield a better solution Therefore, in some cases we may want\nto solve the optimization problem associated with regularized loss minimization,\nnamely,1\nmin\nw\n\u0012λ\n2 ∥w∥2 + LS(w)\n\u0013 (14.14)\nSince we are dealing with convex learning problems in which the loss function is\nconvex, the preceding problem is also a convex optimization problem that can\nbe solved using SGD as well, as we shall see in this section Deﬁne f(w) = λ\n2 ∥w∥2 + LS(w) Note that f is a λ-strongly convex function;\ntherefore, we can apply the SGD variant given in Section 14.4.4 (with H = Rd)",
      "word_count": 239,
      "source_page": 199,
      "start_position": 70830,
      "end_position": 71068,
      "sentences_count": 15
    },
    {
      "chunk_id": 313,
      "text": "200\nStochastic Gradient Descent\nthat H = Rd and therefore the projection step does not matter) as follows\nw(t+1) = w(t) −1\nλ t\n\u0010\nλw(t) + vt\n\u0011\n=\n\u0012\n1 −1\nt\n\u0013\nw(t) −1\nλ tvt\n= t −1\nt\nw(t) −1\nλ tvt\n= t −1\nt\n\u0012t −2\nt −1w(t−1) −\n1\nλ (t −1)vt−1\n\u0013\n−1\nλ tvt\n= −1\nλ t\nt\nX\ni=1\nvi (14.15)\nIf we assume that the loss function is ρ-Lipschitz, it follows that for all t we have\n∥vt∥≤ρ and therefore ∥λw(t)∥≤ρ, which yields\n∥λw(t) + vt∥≤2ρ Theorem 14.11 therefore tells us that after performing T iterations we have that\nE[f( ¯w)] −f(w⋆) ≤4ρ2\nλ T (1 + log(T)) 14.6\nSummary\nWe have introduced the Gradient Descent and Stochastic Gradient Descent algo-\nrithms, along with several of their variants We have analyzed their convergence\nrate and calculated the number of iterations that would guarantee an expected\nobjective of at most ϵ plus the optimal objective Most importantly, we have\nshown that by using SGD we can directly minimize the risk function We do\nso by sampling a point i.i.d from D and using a subgradient of the loss of the\ncurrent hypothesis w(t) at this point as an unbiased estimate of the gradient (or\na subgradient) of the risk function This implies that a bound on the number of\niterations also yields a sample complexity bound",
      "word_count": 238,
      "source_page": 200,
      "start_position": 71148,
      "end_position": 71385,
      "sentences_count": 8
    },
    {
      "chunk_id": 314,
      "text": "14.8 Exercises\n201\nin the context of stochastic optimization See, for example, (Nemirovski & Yudin\n1978, Nesterov & Nesterov 2004, Nesterov 2005, Nemirovski, Juditsky, Lan &\nShapiro 2009, Shapiro, Dentcheva & Ruszczy´nski 2009) The bound we have derived for strongly convex function is due to Hazan,\nAgarwal & Kale (2007) As mentioned previously, improved bounds have been\nobtained in Rakhlin et al (2012) 14.8\nExercises\n1 Prove Claim 14.10 Hint: Extend the proof of Lemma 13.5 2 Prove Corollary 14.14 3 Perceptron as a subgradient descent algorithm: Let S = ((x1, y1), , (xm, ym)) ∈\n(Rd × {±1})m Assume that there exists w ∈Rd such that for every i ∈[m]\nwe have yi⟨w, xi⟩≥1, and let w⋆be a vector that has the minimal norm\namong all vectors that satisfy the preceding requirement Let R = maxi ∥xi∥ Deﬁne a function\nf(w) =\nmax\ni∈[m] (1 −yi ⟨w, xi⟩) • Show that minw:∥w∥≤∥w⋆∥f(w) = 0 and show that any w for which f(w) <\n1 separates the examples in S • Show how to calculate a subgradient of f • Describe and analyze the subgradient descent algorithm for this case Com-\npare the algorithm and the analysis to the Batch Perceptron algorithm\ngiven in Section 9.1.2 4 Variable step size (*): Prove an analog of Theorem 14.8 for SGD with a\nvariable step size, ηt =\nB\nρ\n√\nt.",
      "word_count": 230,
      "source_page": 201,
      "start_position": 71475,
      "end_position": 71704,
      "sentences_count": 22
    },
    {
      "chunk_id": 315,
      "text": "15\nSupport Vector Machines\nIn this chapter and the next we discuss a very useful machine learning tool: the\nsupport vector machine paradigm (SVM) for learning linear predictors in high\ndimensional feature spaces The high dimensionality of the feature space raises\nboth sample complexity and computational complexity challenges The SVM algorithmic paradigm tackles the sample complexity challenge by\nsearching for “large margin” separators Roughly speaking, a halfspace separates\na training set with a large margin if all the examples are not only on the correct\nside of the separating hyperplane but also far away from it Restricting the\nalgorithm to output a large margin separator can yield a small sample complexity\neven if the dimensionality of the feature space is high (and even inﬁnite) We\nintroduce the concept of margin and relate it to the regularized loss minimization\nparadigm as well as to the convergence rate of the Perceptron algorithm In the next chapter we will tackle the computational complexity challenge\nusing the idea of kernels 15.1\nMargin and Hard-SVM\nLet S = (x1, y1), , (xm, ym) be a training set of examples, where each xi ∈Rd\nand yi ∈{±1} We say that this training set is linearly separable, if there exists\na halfspace, (w, b), such that yi = sign(⟨w, xi⟩+ b) for all i Alternatively, this\ncondition can be rewritten as\n∀i ∈[m],\nyi(⟨w, xi⟩+ b) > 0",
      "word_count": 231,
      "source_page": 202,
      "start_position": 71705,
      "end_position": 71935,
      "sentences_count": 11
    },
    {
      "chunk_id": 316,
      "text": "15.1 Margin and Hard-SVM\n203\nx\nx\nWhile both the dashed-black and solid-green hyperplanes separate the four ex-\namples, our intuition would probably lead us to prefer the black hyperplane over\nthe green one One way to formalize this intuition is using the concept of margin The margin of a hyperplane with respect to a training set is deﬁned to be the\nminimal distance between a point in the training set and the hyperplane If a\nhyperplane has a large margin, then it will still separate the training set even if\nwe slightly perturb each instance We will see later on that the true error of a halfspace can be bounded in terms\nof the margin it has over the training sample (the larger the margin, the smaller\nthe error), regardless of the Euclidean dimension in which this halfspace resides Hard-SVM is the learning rule in which we return an ERM hyperplane that\nseparates the training set with the largest possible margin To deﬁne Hard-SVM\nformally, we ﬁrst express the distance between a point x to a hyperplane using\nthe parameters deﬁning the halfspace claim 15.1\nThe distance between a point x and the hyperplane deﬁned by\n(w, b) where ∥w∥= 1 is |⟨w, x⟩+ b| Proof\nThe distance between a point x and the hyperplane is deﬁned as\nmin{∥x −v∥: ⟨w, v⟩+ b = 0}",
      "word_count": 226,
      "source_page": 203,
      "start_position": 72017,
      "end_position": 72242,
      "sentences_count": 9
    },
    {
      "chunk_id": 317,
      "text": "claim 15.1\nThe distance between a point x and the hyperplane deﬁned by\n(w, b) where ∥w∥= 1 is |⟨w, x⟩+ b| Proof\nThe distance between a point x and the hyperplane is deﬁned as\nmin{∥x −v∥: ⟨w, v⟩+ b = 0} Taking v = x −(⟨w, x⟩+ b)w we have that\n⟨w, v⟩+ b = ⟨w, x⟩−(⟨w, x⟩+ b)∥w∥2 + b = 0,\nand\n∥x −v∥= |⟨w, x⟩+ b| ∥w∥= |⟨w, x⟩+ b| Hence, the distance is at most |⟨w, x⟩+ b| Next, take any other point u on the\nhyperplane, thus ⟨w, u⟩+ b = 0 We have\n∥x −u∥2 = ∥x −v + v −u∥2\n= ∥x −v∥2 + ∥v −u∥2 + 2⟨x −v, v −u⟩\n≥∥x −v∥2 + 2⟨x −v, v −u⟩\n= ∥x −v∥2 + 2(⟨w, x⟩+ b)⟨w, v −u⟩\n= ∥x −v∥2,\nwhere the last equality is because ⟨w, v⟩= ⟨w, u⟩= −b Hence, the distance",
      "word_count": 152,
      "source_page": 203,
      "start_position": 72201,
      "end_position": 72352,
      "sentences_count": 7
    },
    {
      "chunk_id": 318,
      "text": "204\nSupport Vector Machines\nbetween x and u is at least the distance between x and v, which concludes our\nproof On the basis of the preceding claim, the closest point in the training set to the\nseparating hyperplane is mini∈[m] |⟨w, xi⟩+ b| Therefore, the Hard-SVM rule is\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] |⟨w, xi⟩+ b|\ns.t ∀i, yi(⟨w, xi⟩+ b) > 0 Whenever there is a solution to the preceding problem (i.e., we are in the sepa-\nrable case), we can write an equivalent problem as follows (see Exercise 1):\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] yi(⟨w, xi⟩+ b) (15.1)\nNext, we give another equivalent formulation of the Hard-SVM rule as a quadratic\noptimization problem.1\nHard-SVM\ninput: (x1, y1), , (xm, ym)\nsolve:\n(w0, b0) = argmin\n(w,b)\n∥w∥2 s.t ∀i, yi(⟨w, xi⟩+ b) ≥1\n(15.2)\noutput: ˆw =\nw0\n∥w0∥, ˆb =\nb0\n∥w0∥\nThe lemma that follows shows that the output of hard-SVM is indeed the\nseparating hyperplane with the largest margin Intuitively, hard-SVM searches\nfor w of minimal norm among all the vectors that separate the data and for\nwhich |⟨w, xi⟩+ b| ≥1 for all i In other words, we enforce the margin to be 1,\nbut now the units in which we measure the margin scale with the norm of w Therefore, ﬁnding the largest margin halfspace boils down to ﬁnding w whose\nnorm is minimal Formally:\nlemma 15.2\nThe output of Hard-SVM is a solution of Equation (15.1)",
      "word_count": 243,
      "source_page": 204,
      "start_position": 72353,
      "end_position": 72595,
      "sentences_count": 12
    },
    {
      "chunk_id": 319,
      "text": "15.1 Margin and Hard-SVM\n205\nproblem given in Equation (15.2) Therefore, ∥w0∥≤∥w⋆\nγ⋆∥=\n1\nγ⋆ It follows that\nfor all i,\nyi(⟨ˆw, xi⟩+ ˆb) =\n1\n∥w0∥yi(⟨w0, xi⟩+ b0) ≥\n1\n∥w0∥≥γ⋆ Since ∥ˆw∥= 1 we obtain that ( ˆw,ˆb) is an optimal solution of Equation (15.1) 15.1.1\nThe Homogenous Case\nIt is often more convenient to consider homogenous halfspaces, namely, halfspaces\nthat pass through the origin and are thus deﬁned by sign(⟨w, x⟩), where the bias\nterm b is set to be zero Hard-SVM for homogenous halfspaces amounts to solving\nmin\nw\n∥w∥2\ns.t ∀i, yi⟨w, xi⟩≥1 (15.3)\nAs we discussed in Chapter 9, we can reduce the problem of learning nonhomogenous\nhalfspaces to the problem of learning homogenous halfspaces by adding one more\nfeature to each instance of xi, thus increasing the dimension to d + 1 Note, however, that the optimization problem given in Equation (15.2) does\nnot regularize the bias term b, while if we learn a homogenous halfspace in Rd+1\nusing Equation (15.3) then we regularize the bias term (i.e., the d+1 component\nof the weight vector) as well However, regularizing b usually does not make a\nsigniﬁcant diﬀerence to the sample complexity 15.1.2\nThe Sample Complexity of Hard-SVM\nRecall that the VC-dimension of halfspaces in Rd is d + 1 It follows that the\nsample complexity of learning halfspaces grows with the dimensionality of the\nproblem",
      "word_count": 232,
      "source_page": 205,
      "start_position": 72678,
      "end_position": 72909,
      "sentences_count": 12
    },
    {
      "chunk_id": 320,
      "text": "15.1.2\nThe Sample Complexity of Hard-SVM\nRecall that the VC-dimension of halfspaces in Rd is d + 1 It follows that the\nsample complexity of learning halfspaces grows with the dimensionality of the\nproblem Furthermore, the fundamental theorem of learning tells us that if the\nnumber of examples is signiﬁcantly smaller than d/ϵ then no algorithm can learn\nan ϵ-accurate halfspace This is problematic when d is very large To overcome this problem, we will make an additional assumption on the\nunderlying data distribution In particular, we will deﬁne a “separability with\nmargin γ” assumption and will show that if the data is separable with margin\nγ then the sample complexity is bounded from above by a function of 1/γ2 It\nfollows that even if the dimensionality is very large (or even inﬁnite), as long as\nthe data adheres to the separability with margin assumption we can still have a\nsmall sample complexity There is no contradiction to the lower bound given in\nthe fundamental theorem of learning because we are now making an additional\nassumption on the underlying data distribution Before we formally deﬁne the separability with margin assumption, there is a\nscaling issue we need to resolve Suppose that a training set S = (x1, y1), , (xm, ym)\nis separable with a margin γ, namely, the maximal objective value of Equa-\ntion (15.1) is at least γ Then, for any positive scalar α > 0, the training set",
      "word_count": 241,
      "source_page": 205,
      "start_position": 72876,
      "end_position": 73116,
      "sentences_count": 12
    },
    {
      "chunk_id": 321,
      "text": "206\nSupport Vector Machines\nS′ = (αx1, y1), , (αxm, ym) is separable with a margin of αγ That is, a sim-\nple scaling of the data can make it separable with an arbitrarily large margin It\nfollows that in order to give a meaningful deﬁnition of margin we must take into\naccount the scale of the examples as well One way to formalize this is using the\ndeﬁnition that follows definition 15.3\nLet D be a distribution over Rd × {±1} We say that D is\nseparable with a (γ, ρ)-margin if there exists (w⋆, b⋆) such that ∥w⋆∥= 1 and\nsuch that with probability 1 over the choice of (x, y) ∼D we have that y(⟨w⋆, x⟩+\nb⋆) ≥γ and ∥x∥≤ρ Similarly, we say that D is separable with a (γ, ρ)-margin\nusing a homogenous halfspace if the preceding holds with a halfspace of the form\n(w⋆, 0) In the advanced part of the book (Chapter 26), we will prove that the sample\ncomplexity of Hard-SVM depends on (ρ/γ)2 and is independent of the dimension\nd In particular, Theorem 26.13 in Section 26.3 states the following:\ntheorem 15.4\nLet D be a distribution over Rd ×{±1} that satisﬁes the (γ, ρ)-\nseparability with margin assumption using a homogenous halfspace Then, with\nprobability of at least 1 −δ over the choice of a training set of size m, the 0-1\nerror of the output of Hard-SVM is at most\nr\n4 (ρ/γ)2\nm\n+\nr\n2 log(2/δ)\nm",
      "word_count": 249,
      "source_page": 206,
      "start_position": 73117,
      "end_position": 73365,
      "sentences_count": 11
    },
    {
      "chunk_id": 322,
      "text": "In particular, Theorem 26.13 in Section 26.3 states the following:\ntheorem 15.4\nLet D be a distribution over Rd ×{±1} that satisﬁes the (γ, ρ)-\nseparability with margin assumption using a homogenous halfspace Then, with\nprobability of at least 1 −δ over the choice of a training set of size m, the 0-1\nerror of the output of Hard-SVM is at most\nr\n4 (ρ/γ)2\nm\n+\nr\n2 log(2/δ)\nm Remark 15.1 (Margin and the Perceptron)\nIn Section 9.1.2 we have described\nand analyzed the Perceptron algorithm for ﬁnding an ERM hypothesis with\nrespect to the class of halfspaces In particular, in Theorem 9.1 we upper bounded\nthe number of updates the Perceptron might make on a given training set It\ncan be shown (see Exercise 2) that the upper bound is exactly (ρ/γ)2, where ρ\nis the radius of examples and γ is the margin 15.2\nSoft-SVM and Norm Regularization\nThe Hard-SVM formulation assumes that the training set is linearly separable,\nwhich is a rather strong assumption Soft-SVM can be viewed as a relaxation of\nthe Hard-SVM rule that can be applied even if the training set is not linearly\nseparable The optimization problem in Equation (15.2) enforces the hard constraints\nyi(⟨w, xi⟩+ b) ≥1 for all i A natural relaxation is to allow the constraint to be\nviolated for some of the examples in the training set This can be modeled by\nintroducing nonnegative slack variables, ξ1,",
      "word_count": 240,
      "source_page": 206,
      "start_position": 73295,
      "end_position": 73534,
      "sentences_count": 10
    },
    {
      "chunk_id": 323,
      "text": "15.2 Soft-SVM and Norm Regularization\n207\nterms is controlled by a parameter λ This leads to the Soft-SVM optimization\nproblem:\nSoft-SVM\ninput: (x1, y1), , (xm, ym)\nparameter: λ > 0\nsolve:\nmin\nw,b,ξ\n \nλ∥w∥2 + 1\nm\nm\nX\ni=1\nξi s.t ∀i,\nyi(⟨w, xi⟩+ b) ≥1 −ξi and ξi ≥0\n(15.4)\noutput: w, b\nWe can rewrite Equation (15.4) as a regularized loss minimization problem Recall the deﬁnition of the hinge loss:\nℓhinge((w, b), (x, y)) = max{0, 1 −y(⟨w, x⟩+ b)} Given (w, b) and a training set S, the averaged hinge loss on S is denoted by\nLhinge\nS\n((w, b)) Now, consider the regularized loss minimization problem:\nmin\nw,b\n\u0010\nλ∥w∥2 + Lhinge\nS\n((w, b))\n\u0011 (15.5)\nclaim 15.5\nEquation (15.4) and Equation (15.5) are equivalent Proof\nFix some w, b and consider the minimization over ξ in Equation (15.4) Fix some i Since ξi must be nonnegative, the best assignment to ξi would be 0\nif yi(⟨w, xi⟩+ b) ≥1 and would be 1 −yi(⟨w, xi⟩+ b) otherwise In other words,\nξi = ℓhinge((w, b), (xi, yi)) for all i, and the claim follows We therefore see that Soft-SVM falls into the paradigm of regularized loss\nminimization that we studied in the previous chapter A Soft-SVM algorithm,\nthat is, a solution for Equation (15.5), has a bias toward low norm separators The objective function that we aim to minimize in Equation (15.5) penalizes not\nonly for training errors but also for large norm",
      "word_count": 250,
      "source_page": 207,
      "start_position": 73592,
      "end_position": 73841,
      "sentences_count": 16
    },
    {
      "chunk_id": 324,
      "text": "208\nSupport Vector Machines\n15.2.1\nThe Sample Complexity of Soft-SVM\nWe now analyze the sample complexity of Soft-SVM for the case of homogenous\nhalfspaces (namely, the output of Equation (15.6)) In Corollary 13.8 we derived\na generalization bound for the regularized loss minimization framework assuming\nthat the loss function is convex and Lipschitz We have already shown that the\nhinge loss is convex so it is only left to analyze the Lipschitzness of the hinge\nloss claim 15.6\nLet f(w) = max{0, 1 −y⟨w, x⟩} Then, f is ∥x∥-Lipschitz Proof\nIt is easy to verify that any subgradient of f at w is of the form αx where\n|α| ≤1 The claim now follows from Lemma 14.7 Corollary 13.8 therefore yields the following:\ncorollary 15.7\nLet D be a distribution over X × {0, 1}, where X = {x :\n∥x∥≤ρ} Consider running Soft-SVM (Equation (15.6)) on a training set S ∼\nDm and let A(S) be the solution of Soft-SVM Then, for every u,\nE\nS∼Dm[Lhinge\nD\n(A(S))] ≤Lhinge\nD\n(u) + λ∥u∥2 + 2ρ2\nλ m Furthermore, since the hinge loss upper bounds the 0−1 loss we also have\nE\nS∼Dm[L0−1\nD\n(A(S))] ≤Lhinge\nD\n(u) + λ∥u∥2 + 2ρ2\nλ m Last, for every B > 0, if we set λ =\nq\n2ρ2\nB2m then\nE\nS∼Dm[L0−1\nD\n(A(S))] ≤\nE\nS∼Dm[Lhinge\nD\n(A(S))] ≤\nmin\nw:∥w∥≤B Lhinge\nD\n(w) +\nr\n8ρ2B2\nm",
      "word_count": 238,
      "source_page": 208,
      "start_position": 73896,
      "end_position": 74133,
      "sentences_count": 12
    },
    {
      "chunk_id": 325,
      "text": "Furthermore, since the hinge loss upper bounds the 0−1 loss we also have\nE\nS∼Dm[L0−1\nD\n(A(S))] ≤Lhinge\nD\n(u) + λ∥u∥2 + 2ρ2\nλ m Last, for every B > 0, if we set λ =\nq\n2ρ2\nB2m then\nE\nS∼Dm[L0−1\nD\n(A(S))] ≤\nE\nS∼Dm[Lhinge\nD\n(A(S))] ≤\nmin\nw:∥w∥≤B Lhinge\nD\n(w) +\nr\n8ρ2B2\nm We therefore see that we can control the sample complexity of learning a half-\nspace as a function of the norm of that halfspace, independently of the Euclidean\ndimension of the space over which the halfspace is deﬁned This becomes highly\nsigniﬁcant when we learn via embeddings into high dimensional feature spaces,\nas we will consider in the next chapter Remark 15.2\nThe condition that X will contain vectors with a bounded norm\nfollows from the requirement that the loss function will be Lipschitz This is\nnot just a technicality As we discussed before, separation with large margin\nis meaningless without imposing a restriction on the scale of the instances In-\ndeed, without a constraint on the scale, we can always enlarge the margin by\nmultiplying all instances by a large scalar 15.2.2\nMargin and Norm-Based Bounds versus Dimension\nThe bounds we have derived for Hard-SVM and Soft-SVM do not depend on the\ndimension of the instance space Instead, the bounds depend on the norm of the",
      "word_count": 227,
      "source_page": 208,
      "start_position": 74074,
      "end_position": 74300,
      "sentences_count": 10
    },
    {
      "chunk_id": 326,
      "text": "15.2 Soft-SVM and Norm Regularization\n209\nexamples, ρ, the norm of the halfspace B (or equivalently the margin parameter\nγ) and, in the nonseparable case, the bounds also depend on the minimum hinge\nloss of all halfspaces of norm ≤B In contrast, the VC-dimension of the class of\nhomogenous halfspaces is d, which implies that the error of an ERM hypothesis\ndecreases as\np\nd/m does We now give an example in which ρ2B2 ≪d; hence\nthe bound given in Corollary 15.7 is much better than the VC bound Consider the problem of learning to classify a short text document according\nto its topic, say, whether the document is about sports or not We ﬁrst need to\nrepresent documents as vectors One simple yet eﬀective way is to use a bag-\nof-words representation That is, we deﬁne a dictionary of words and set the\ndimension d to be the number of words in the dictionary Given a document,\nwe represent it as a vector x ∈{0, 1}d, where xi = 1 if the i’th word in the\ndictionary appears in the document and xi = 0 otherwise Therefore, for this\nproblem, the value of ρ2 will be the maximal number of distinct words in a given\ndocument A halfspace for this problem assigns weights to words",
      "word_count": 216,
      "source_page": 209,
      "start_position": 74301,
      "end_position": 74516,
      "sentences_count": 10
    },
    {
      "chunk_id": 327,
      "text": "Therefore, for this\nproblem, the value of ρ2 will be the maximal number of distinct words in a given\ndocument A halfspace for this problem assigns weights to words It is natural to assume\nthat by assigning positive and negative weights to a few dozen words we will\nbe able to determine whether a given document is about sports or not with\nreasonable accuracy Therefore, for this problem, the value of B2 can be set to\nbe less than 100 Overall, it is reasonable to say that the value of B2ρ2 is smaller\nthan 10,000 On the other hand, a typical size of a dictionary is much larger than 10,000 For example, there are more than 100,000 distinct words in English We have\ntherefore shown a problem in which there can be an order of magnitude diﬀerence\nbetween learning a halfspace with the SVM rule and learning a halfspace using\nthe vanilla ERM rule Of course, it is possible to construct problems in which the SVM bound will\nbe worse than the VC bound When we use SVM, we in fact introduce another\nform of inductive bias – we prefer large margin halfspaces While this induc-\ntive bias can signiﬁcantly decrease our estimation error, it can also enlarge the\napproximation error 15.2.3\nThe Ramp Loss*\nThe margin-based bounds we have derived in Corollary 15.7 rely on the fact that\nwe minimize the hinge loss",
      "word_count": 234,
      "source_page": 209,
      "start_position": 74488,
      "end_position": 74721,
      "sentences_count": 12
    },
    {
      "chunk_id": 328,
      "text": "While this induc-\ntive bias can signiﬁcantly decrease our estimation error, it can also enlarge the\napproximation error 15.2.3\nThe Ramp Loss*\nThe margin-based bounds we have derived in Corollary 15.7 rely on the fact that\nwe minimize the hinge loss As we have shown in the previous subsection, the\nterm\np\nρ2B2/m can be much smaller than the corresponding term in the VC\nbound,\np\nd/m However, the approximation error in Corollary 15.7 is measured\nwith respect to the hinge loss while the approximation error in VC bounds is\nmeasured with respect to the 0−1 loss Since the hinge loss upper bounds the\n0−1 loss, the approximation error with respect to the 0−1 loss will never exceed\nthat of the hinge loss It is not possible to derive bounds that involve the estimation error term\np\nρ2B2/m for the 0−1 loss This follows from the fact that the 0−1 loss is scale",
      "word_count": 153,
      "source_page": 209,
      "start_position": 74681,
      "end_position": 74833,
      "sentences_count": 7
    },
    {
      "chunk_id": 329,
      "text": "210\nSupport Vector Machines\ninsensitive, and therefore there is no meaning to the norm of w or its margin\nwhen we measure error with the 0−1 loss However, it is possible to deﬁne a loss\nfunction that on one hand it is scale sensitive and thus enjoys the estimation\nerror\np\nρ2B2/m while on the other hand it is more similar to the 0−1 loss One\noption is the ramp loss, deﬁned as\nℓramp(w, (x, y)) = min{1, ℓhinge(w, (x, y))} = min{1 , max{0, 1 −y⟨w, x⟩}} The ramp loss penalizes mistakes in the same way as the 0−1 loss and does not\npenalize examples that are separated with margin The diﬀerence between the\nramp loss and the 0−1 loss is only with respect to examples that are correctly\nclassiﬁed but not with a signiﬁcant margin Generalization bounds for the ramp\nloss are given in the advanced part of this book (see Appendix 26.3) y⟨w, x⟩\nℓramp\nℓhinge\nℓ0−1\n1\n1\nThe reason SVM relies on the hinge loss and not on the ramp loss is that\nthe hinge loss is convex and, therefore, from the computational point of view,\nminimizing the hinge loss can be performed eﬃciently In contrast, the problem\nof minimizing the ramp loss is computationally intractable",
      "word_count": 211,
      "source_page": 210,
      "start_position": 74834,
      "end_position": 75044,
      "sentences_count": 8
    },
    {
      "chunk_id": 330,
      "text": "y⟨w, x⟩\nℓramp\nℓhinge\nℓ0−1\n1\n1\nThe reason SVM relies on the hinge loss and not on the ramp loss is that\nthe hinge loss is convex and, therefore, from the computational point of view,\nminimizing the hinge loss can be performed eﬃciently In contrast, the problem\nof minimizing the ramp loss is computationally intractable 15.3\nOptimality Conditions and “Support Vectors”*\nThe name “Support Vector Machine” stems from the fact that the solution of\nhard-SVM, w0, is supported by (i.e., is in the linear span of) the examples that\nare exactly at distance 1/∥w0∥from the separating hyperplane These vectors are\ntherefore called support vectors To see this, we rely on Fritz John optimality\nconditions theorem 15.8\nLet w0 be as deﬁned in Equation (15.3) and let I = {i :\n|⟨w0, xi⟩| = 1} Then, there exist coeﬃcients α1, , αm such that\nw0 =\nX\ni∈I\nαixi The examples {xi : i ∈I} are called support vectors The proof of this theorem follows by applying the following lemma to Equa-\ntion (15.3).",
      "word_count": 174,
      "source_page": 210,
      "start_position": 74989,
      "end_position": 75162,
      "sentences_count": 10
    },
    {
      "chunk_id": 331,
      "text": "15.4 Duality*\n211\nlemma 15.9 (Fritz John)\nSuppose that\nw⋆∈argmin\nw\nf(w)\ns.t ∀i ∈[m], gi(w) ≤0,\nwhere f, g1, , gm are diﬀerentiable Then, there exists α ∈Rm such that\n∇f(w⋆) + P\ni∈I αi∇gi(w⋆) = 0, where I = {i : gi(w⋆) = 0} 15.4\nDuality*\nHistorically, many of the properties of SVM have been obtained by considering\nthe dual of Equation (15.3) Our presentation of SVM does not rely on duality For completeness, we present in the following how to derive the dual of Equa-\ntion (15.3) We start by rewriting the problem in an equivalent form as follows Consider\nthe function\ng(w) =\nmax\nα∈Rm:α≥0\nm\nX\ni=1\nαi(1 −yi⟨w, xi⟩) =\n(\n0\nif ∀i, yi⟨w, xi⟩≥1\n∞\notherwise We can therefore rewrite Equation (15.3) as\nmin\nw\n\u0000∥w∥2 + g(w)\n\u0001 (15.7)\nRearranging the preceding we obtain that Equation (15.3) can be rewritten as\nthe problem\nmin\nw\nmax\nα∈Rm:α≥0\n \n1\n2∥w∥2 +\nm\nX\ni=1\nαi(1 −yi⟨w, xi⟩) (15.8)\nNow suppose that we ﬂip the order of min and max in the above equation This\ncan only decrease the objective value (see Exercise 4), and we have\nmin\nw\nmax\nα∈Rm:α≥0\n \n1\n2∥w∥2 +\nm\nX\ni=1\nαi(1 −yi⟨w, xi⟩) ≥\nmax\nα∈Rm:α≥0 min\nw\n \n1\n2∥w∥2 +\nm\nX\ni=1\nαi(1 −yi⟨w, xi⟩) The preceding inequality is called weak duality It turns out that in our case,\nstrong duality also holds; namely, the inequality holds with equality",
      "word_count": 245,
      "source_page": 211,
      "start_position": 75163,
      "end_position": 75407,
      "sentences_count": 16
    },
    {
      "chunk_id": 332,
      "text": "212\nSupport Vector Machines\nproblem with respect to w is unconstrained and the objective is diﬀerentiable;\nthus, at the optimum, the gradient equals zero:\nw −\nm\nX\ni=1\nαiyixi = 0\n⇒\nw =\nm\nX\ni=1\nαiyixi This shows us that the solution must be in the linear span of the examples, a\nfact we will use later to derive SVM with kernels Plugging the preceding into\nEquation (15.9) we obtain that the dual problem can be rewritten as\nmax\nα∈Rm:α≥0\n\n1\n2\n\n\n\n\n\nm\nX\ni=1\nαiyixi\n\n\n\n\n\n2\n+\nm\nX\ni=1\nαi\n\n1 −yi\n*X\nj\nαjyjxj, xi\n+\n\n\n (15.10)\nRearranging yields the dual problem\nmax\nα∈Rm:α≥0\n\n\nm\nX\ni=1\nαi −1\n2\nm\nX\ni=1\nm\nX\nj=1\nαiαjyiyj⟨xj, xi⟩\n\n (15.11)\nNote that the dual problem only involves inner products between instances and\ndoes not require direct access to speciﬁc elements within an instance This prop-\nerty is important when implementing SVM with kernels, as we will discuss in\nthe next chapter 15.5\nImplementing Soft-SVM Using SGD\nIn this section we describe a very simple algorithm for solving the optimization\nproblem of Soft-SVM, namely,\nmin\nw\n \nλ\n2 ∥w∥2 + 1\nm\nm\nX\ni=1\nmax{0, 1 −y⟨w, xi⟩} (15.12)\nWe rely on the SGD framework for solving regularized loss minimization prob-\nlems, as described in Section 14.5.3",
      "word_count": 231,
      "source_page": 212,
      "start_position": 75442,
      "end_position": 75672,
      "sentences_count": 8
    },
    {
      "chunk_id": 333,
      "text": "15.6 Summary\n213\nSGD for Solving Soft-SVM\ngoal: Solve Equation (15.12)\nparameter: T\ninitialize: θ(1) = 0\nfor t = 1, , T\nLet w(t) =\n1\nλ tθ(t)\nChoose i uniformly at random from [m]\nIf (yi⟨w(t), xi⟩< 1)\nSet θ(t+1) = θ(t) + yixi\nElse\nSet θ(t+1) = θ(t)\noutput: ¯w = 1\nT\nPT\nt=1 w(t)\n15.6\nSummary\nSVM is an algorithm for learning halfspaces with a certain type of prior knowl-\nedge, namely, preference for large margin Hard-SVM seeks the halfspace that\nseparates the data perfectly with the largest margin, whereas soft-SVM does\nnot assume separability of the data and allows the constraints to be violated to\nsome extent The sample complexity for both types of SVM is diﬀerent from the\nsample complexity of straightforward halfspace learning, as it does not depend\non the dimension of the domain but rather on parameters such as the maximal\nnorms of x and w The importance of dimension-independent sample complexity will be realized\nin the next chapter, where we will discuss the embedding of the given domain\ninto some high dimensional feature space as means for enriching our hypothesis\nclass Such a procedure raises computational and sample complexity problems The latter is solved by using SVM, whereas the former can be solved by using\nSVM with kernels, as we will see in the next chapter 15.7\nBibliographic Remarks\nSVMs have been introduced in (Cortes & Vapnik 1995, Boser, Guyon & Vapnik\n1992)",
      "word_count": 243,
      "source_page": 213,
      "start_position": 75757,
      "end_position": 75999,
      "sentences_count": 8
    },
    {
      "chunk_id": 334,
      "text": "214\nSupport Vector Machines\n15.8\nExercises\n1 Show that the hard-SVM rule, namely,\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] |⟨w, xi⟩+ b|\ns.t ∀i, yi(⟨w, xi⟩+ b) > 0,\nis equivalent to the following formulation:\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] yi(⟨w, xi⟩+ b) (15.13)\nHint: Deﬁne G = {(w, b) : ∀i, yi(⟨w, xi⟩+ b) > 0} 1 Show that\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] yi(⟨w, xi⟩+ b) ∈G\n2 Show that ∀(w, b) ∈G,\nmin\ni∈[m] yi(⟨w, xi⟩+ b) = min\ni∈[m] |⟨w, xi⟩+ b|\n2 Margin and the Perceptron Consider a training set that is linearly sep-\narable with a margin γ and such that all the instances are within a ball of\nradius ρ Prove that the maximal number of updates the Batch Perceptron\nalgorithm given in Section 9.1.2 will make when running on this training set\nis (ρ/γ)2 3 Hard versus soft SVM: Prove or refute the following claim:\nThere exists λ > 0 such that for every sample S of m > 1 examples, which\nis separable by the class of homogenous halfspaces, the hard-SVM and the\nsoft-SVM (with parameter λ) learning rules return exactly the same weight\nvector 4 Weak duality: Prove that for any function f of two vector variables x ∈\nX, y ∈Y, it holds that\nmin\nx∈X max\ny∈Y f(x, y) ≥max\ny∈Y min\nx∈X f(x, y).",
      "word_count": 223,
      "source_page": 214,
      "start_position": 76047,
      "end_position": 76269,
      "sentences_count": 13
    },
    {
      "chunk_id": 335,
      "text": "16\nKernel Methods\nIn the previous chapter we described the SVM paradigm for learning halfspaces\nin high dimensional feature spaces This enables us to enrich the expressive\npower of halfspaces by ﬁrst mapping the data into a high dimensional feature\nspace, and then learning a linear predictor in that space This is similar to the\nAdaBoost algorithm, which learns a composition of a halfspace over base hy-\npotheses While this approach greatly extends the expressiveness of halfspace\npredictors, it raises both sample complexity and computational complexity chal-\nlenges In the previous chapter we tackled the sample complexity issue using\nthe concept of margin In this chapter we tackle the computational complexity\nchallenge using the method of kernels We start the chapter by describing the idea of embedding the data into a high\ndimensional feature space We then introduce the idea of kernels A kernel is a\ntype of a similarity measure between instances The special property of kernel\nsimilarities is that they can be viewed as inner products in some Hilbert space\n(or Euclidean space of some high dimension) to which the instance space is vir-\ntually embedded We introduce the “kernel trick” that enables computationally\neﬃcient implementation of learning, without explicitly handling the high dimen-\nsional representation of the domain instances Kernel based learning algorithms,\nand in particular kernel-SVM, are very useful and popular machine learning\ntools",
      "word_count": 228,
      "source_page": 215,
      "start_position": 76270,
      "end_position": 76497,
      "sentences_count": 12
    },
    {
      "chunk_id": 336,
      "text": "We introduce the “kernel trick” that enables computationally\neﬃcient implementation of learning, without explicitly handling the high dimen-\nsional representation of the domain instances Kernel based learning algorithms,\nand in particular kernel-SVM, are very useful and popular machine learning\ntools Their success may be attributed both to being ﬂexible for accommodating\ndomain speciﬁc prior knowledge and to having a well developed set of eﬃcient\nimplementation algorithms 16.1\nEmbeddings into Feature Spaces\nThe expressive power of halfspaces is rather restricted – for example, the follow-\ning training set is not separable by a halfspace Let the domain be the real line; consider the domain points {−10, −9, −8, , 0,\n1, , 9, 10} where the labels are +1 for all x such that |x| > 2 and −1 otherwise To make the class of halfspaces more expressive, we can ﬁrst map the original\ninstance space into another space (possibly of a higher dimension) and then\nlearn a halfspace in that space For example, consider the example mentioned\npreviously Instead of learning a halfspace in the original representation let us\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 208,
      "source_page": 215,
      "start_position": 76458,
      "end_position": 76665,
      "sentences_count": 14
    },
    {
      "chunk_id": 337,
      "text": "216\nKernel Methods\nﬁrst deﬁne a mapping ψ : R →R2 as follows:\nψ(x) = (x, x2) We use the term feature space to denote the range of ψ After applying ψ the\ndata can be easily explained using the halfspace h(x) = sign(⟨w, ψ(x)⟩−b),\nwhere w = (0, 1) and b = 5 The basic paradigm is as follows:\n1 Given some domain set X and a learning task, choose a mapping ψ : X →F,\nfor some feature space F, that will usually be Rn for some n (however, the\nrange of such a mapping can be any Hilbert space, including such spaces of\ninﬁnite dimension, as we will show later) 2 Given a sequence of labeled examples, S = (x1, y1), , (xm, ym), create the\nimage sequence ˆS = (ψ(x1), y1), , (ψ(xm), ym) 3 Train a linear predictor h over ˆS 4 Predict the label of a test point, x, to be h(ψ(x)) Note that, for every probability distribution D over X × Y, we can readily\ndeﬁne its image probability distribution Dψ over F × Y by setting, for every\nsubset A ⊆F × Y, Dψ(A) = D(ψ−1(A)).1 It follows that for every predictor h\nover the feature space, LDψ(h) = LD(h ◦ψ), where h ◦ψ is the composition of h\nonto ψ",
      "word_count": 219,
      "source_page": 216,
      "start_position": 76666,
      "end_position": 76884,
      "sentences_count": 14
    },
    {
      "chunk_id": 338,
      "text": "Predict the label of a test point, x, to be h(ψ(x)) Note that, for every probability distribution D over X × Y, we can readily\ndeﬁne its image probability distribution Dψ over F × Y by setting, for every\nsubset A ⊆F × Y, Dψ(A) = D(ψ−1(A)).1 It follows that for every predictor h\nover the feature space, LDψ(h) = LD(h ◦ψ), where h ◦ψ is the composition of h\nonto ψ The success of this learning paradigm depends on choosing a good ψ for a\ngiven learning task: that is, a ψ that will make the image of the data distribution\n(close to being) linearly separable in the feature space, thus making the resulting\nalgorithm a good learner for a given task Picking such an embedding requires\nprior knowledge about that task However, often some generic mappings that\nenable us to enrich the class of halfspaces and extend its expressiveness are used One notable example is polynomial mappings, which are a generalization of the\nψ we have seen in the previous example Recall that the prediction of a standard halfspace classiﬁer on an instance x\nis based on the linear mapping x 7→⟨w, x⟩ We can generalize linear mappings\nto a polynomial mapping, x 7→p(x), where p is a multivariate polynomial of\ndegree k For simplicity, consider ﬁrst the case in which x is 1 dimensional In that case, p(x) = Pk\nj=0 wjxj, where w ∈Rk+1 is the vector of coeﬃcients\nof the polynomial we need to learn",
      "word_count": 250,
      "source_page": 216,
      "start_position": 76813,
      "end_position": 77062,
      "sentences_count": 10
    },
    {
      "chunk_id": 339,
      "text": "16.2 The Kernel Trick\n217\nAs before, we can rewrite p(x) = ⟨w, ψ(x)⟩where now ψ : Rn →Rd is such\nthat for every J ∈[n]r, r ≤k, the coordinate of ψ(x) associated with J is the\nmonomial Qr\ni=1 xJi Naturally, polynomial-based classiﬁers yield much richer hypothesis classes\nthan halfspaces We have seen at the beginning of this chapter an example in\nwhich the training set, in its original domain (X = R), cannot be separable\nby a halfspace, but after the embedding x 7→(x, x2) it is perfectly separable So, while the classiﬁer is always linear in the feature space, it can have highly\nnonlinear behavior on the original space from which instances were sampled In general, we can choose any feature mapping ψ that maps the original in-\nstances into some Hilbert space.2 The Euclidean space Rd is a Hilbert space for\nany ﬁnite d But there are also inﬁnite dimensional Hilbert spaces (as we shall\nsee later on in this chapter) The bottom line of this discussion is that we can enrich the class of halfspaces\nby ﬁrst applying a nonlinear mapping, ψ, that maps the instance space into some\nfeature space, and then learning a halfspace in that feature space However, if\nthe range of ψ is a high dimensional space we face two problems",
      "word_count": 220,
      "source_page": 217,
      "start_position": 77152,
      "end_position": 77371,
      "sentences_count": 8
    },
    {
      "chunk_id": 340,
      "text": "The bottom line of this discussion is that we can enrich the class of halfspaces\nby ﬁrst applying a nonlinear mapping, ψ, that maps the instance space into some\nfeature space, and then learning a halfspace in that feature space However, if\nthe range of ψ is a high dimensional space we face two problems First, the VC-\ndimension of halfspaces in Rn is n + 1, and therefore, if the range of ψ is very\nlarge, we need many more samples in order to learn a halfspace in the range\nof ψ Second, from the computational point of view, performing calculations in\nthe high dimensional space might be too costly In fact, even the representation\nof the vector w in the feature space can be unrealistic The ﬁrst issue can be\ntackled using the paradigm of large margin (or low norm predictors), as we\nalready discussed in the previous chapter in the context of the SVM algorithm In the following section we address the computational issue 16.2\nThe Kernel Trick\nWe have seen that embedding the input space into some high dimensional feature\nspace makes halfspace learning more expressive However, the computational\ncomplexity of such learning may still pose a serious hurdle – computing linear\nseparators over very high dimensional data may be computationally expensive The common solution to this concern is kernel based learning The term “kernels”\nis used in this context to describe inner products in the feature space",
      "word_count": 242,
      "source_page": 217,
      "start_position": 77317,
      "end_position": 77558,
      "sentences_count": 11
    },
    {
      "chunk_id": 341,
      "text": "The common solution to this concern is kernel based learning The term “kernels”\nis used in this context to describe inner products in the feature space Given\nan embedding ψ of some domain space X into some Hilbert space, we deﬁne\nthe kernel function K(x, x′) = ⟨ψ(x), ψ(x′)⟩ One can think of K as specifying\nsimilarity between instances and of the embedding ψ as mapping the domain set\n2 A Hilbert space is a vector space with an inner product, which is also complete A space is\ncomplete if all Cauchy sequences in the space converge In our case, the norm ∥w∥is deﬁned by the inner product\np\n⟨w, w⟩ The reason we require\nthe range of ψ to be in a Hilbert space is that projections in a Hilbert space are well\ndeﬁned In particular, if M is a linear subspace of a Hilbert space, then every x in the\nHilbert space can be written as a sum x = u + v where u ∈M and ⟨v, w⟩= 0 for all\nw ∈M We use this fact in the proof of the representer theorem given in the next section.",
      "word_count": 192,
      "source_page": 217,
      "start_position": 77533,
      "end_position": 77724,
      "sentences_count": 9
    },
    {
      "chunk_id": 342,
      "text": "218\nKernel Methods\nX into a space where these similarities are realized as inner products It turns\nout that many learning algorithms for halfspaces can be carried out just on the\nbasis of the values of the kernel function over pairs of domain points The main\nadvantage of such algorithms is that they implement linear separators in high\ndimensional feature spaces without having to specify points in that space or\nexpressing the embedding ψ explicitly The remainder of this section is devoted\nto constructing such algorithms In the previous chapter we saw that regularizing the norm of w yields a small\nsample complexity even if the dimensionality of the feature space is high Inter-\nestingly, as we show later, regularizing the norm of w is also helpful in overcoming\nthe computational problem To do so, ﬁrst note that all versions of the SVM op-\ntimization problem we have derived in the previous chapter are instances of the\nfollowing general problem:\nmin\nw (f (⟨w, ψ(x1)⟩, , ⟨w, ψ(xm)⟩) + R(∥w∥)),\n(16.2)\nwhere f : Rm →R is an arbitrary function and R : R+ →R is a monotoni-\ncally nondecreasing function For example, Soft-SVM for homogenous halfspaces\n(Equation (15.6)) can be derived from Equation (16.2) by letting R(a) = λa2 and\nf(a1, , am) = 1\nm\nP\ni max{0, 1−yiai} Similarly, Hard-SVM for nonhomogenous\nhalfspaces (Equation (15.2)) can be derived from Equation (16.2) by letting\nR(a) = a2 and letting f(a1,",
      "word_count": 242,
      "source_page": 218,
      "start_position": 77725,
      "end_position": 77966,
      "sentences_count": 11
    },
    {
      "chunk_id": 343,
      "text": ", am) = 1\nm\nP\ni max{0, 1−yiai} Similarly, Hard-SVM for nonhomogenous\nhalfspaces (Equation (15.2)) can be derived from Equation (16.2) by letting\nR(a) = a2 and letting f(a1, , am) be 0 if there exists b such that yi(ai+b) ≥1\nfor all i, and f(a1, , am) = ∞otherwise The following theorem shows that there exists an optimal solution of Equa-\ntion (16.2) that lies in the span of {ψ(x1), , ψ(xm)} theorem 16.1 (Representer Theorem)\nAssume that ψ is a mapping from X to\na Hilbert space Then, there exists a vector α ∈Rm such that w = Pm\ni=1 αiψ(xi)\nis an optimal solution of Equation (16.2) Proof\nLet w⋆be an optimal solution of Equation (16.2) Because w⋆is an\nelement of a Hilbert space, we can rewrite w⋆as\nw⋆=\nm\nX\ni=1\nαiψ(xi) + u,\nwhere ⟨u, ψ(xi)⟩= 0 for all i Set w = w⋆−u Clearly, ∥w⋆∥2 = ∥w∥2 + ∥u∥2,\nthus ∥w∥≤∥w⋆∥ Since R is nondecreasing we obtain that R(∥w∥) ≤R(∥w⋆∥) Additionally, for all i we have that\n⟨w, ψ(xi)⟩= ⟨w⋆−u, ψ(xi)⟩= ⟨w⋆, ψ(xi)⟩,\nhence\nf (⟨w, ψ(x1)⟩, , ⟨w, ψ(xm)⟩) = f (⟨w⋆, ψ(x1)⟩, , ⟨w⋆, ψ(xm)⟩) We have shown that the objective of Equation (16.2) at w cannot be larger\nthan the objective at w⋆and therefore w is also an optimal solution Since\nw = Pm\ni=1 αiψ(xi) we conclude our proof.",
      "word_count": 230,
      "source_page": 218,
      "start_position": 77937,
      "end_position": 78166,
      "sentences_count": 18
    },
    {
      "chunk_id": 344,
      "text": "16.2 The Kernel Trick\n219\nOn the basis of the representer theorem we can optimize Equation (16.2) with\nrespect to the coeﬃcients α instead of the coeﬃcients w as follows Writing\nw = Pm\nj=1 αjψ(xj) we have that for all i\n⟨w, ψ(xi)⟩=\n*X\nj\nαjψ(xj), ψ(xi)\n+\n=\nm\nX\nj=1\nαj⟨ψ(xj), ψ(xi)⟩ Similarly,\n∥w∥2 =\n*X\nj\nαjψ(xj),\nX\nj\nαjψ(xj)\n+\n=\nm\nX\ni,j=1\nαiαj⟨ψ(xi), ψ(xj)⟩ Let K(x, x′) = ⟨ψ(x), ψ(x′)⟩be a function that implements the kernel function\nwith respect to the embedding ψ Instead of solving Equation (16.2) we can solve\nthe equivalent problem\nmin\nα∈Rm f\n\n\nm\nX\nj=1\nαjK(xj, x1), ,\nm\nX\nj=1\nαjK(xj, xm)\n\n\n+ R\n\n\nv\nu\nu\nt\nm\nX\ni,j=1\nαiαjK(xj, xi)\n\n (16.3)\nTo solve the optimization problem given in Equation (16.3), we do not need any\ndirect access to elements in the feature space The only thing we should know is\nhow to calculate inner products in the feature space, or equivalently, to calculate\nthe kernel function In fact, to solve Equation (16.3) we solely need to know the\nvalue of the m × m matrix G s.t Gi,j = K(xi, xj), which is often called the\nGram matrix In particular, specifying the preceding to the Soft-SVM problem given in Equa-\ntion (15.6), we can rewrite the problem as\nmin\nα∈Rm\n \nλαT Gα + 1\nm\nm\nX\ni=1\nmax\n\b\n0, 1 −yi(Gα)i",
      "word_count": 247,
      "source_page": 219,
      "start_position": 78167,
      "end_position": 78413,
      "sentences_count": 11
    },
    {
      "chunk_id": 345,
      "text": "Gi,j = K(xi, xj), which is often called the\nGram matrix In particular, specifying the preceding to the Soft-SVM problem given in Equa-\ntion (15.6), we can rewrite the problem as\nmin\nα∈Rm\n \nλαT Gα + 1\nm\nm\nX\ni=1\nmax\n\b\n0, 1 −yi(Gα)i ,\n(16.4)\nwhere (Gα)i is the i’th element of the vector obtained by multiplying the Gram\nmatrix G by the vector α Note that Equation (16.4) can be written as quadratic\nprogramming and hence can be solved eﬃciently In the next section we describe\nan even simpler algorithm for solving Soft-SVM with kernels Once we learn the coeﬃcients α we can calculate the prediction on a new\ninstance by\n⟨w, ψ(x)⟩=\nm\nX\nj=1\nαj⟨ψ(xj), ψ(x)⟩=\nm\nX\nj=1\nαjK(xj, x) The advantage of working with kernels rather than directly optimizing w in\nthe feature space is that in some situations the dimension of the feature space",
      "word_count": 153,
      "source_page": 219,
      "start_position": 78368,
      "end_position": 78520,
      "sentences_count": 7
    },
    {
      "chunk_id": 346,
      "text": "220\nKernel Methods\nis extremely large while implementing the kernel function is very simple A few\nexamples are given in the following Example 16.1 (Polynomial Kernels)\nThe k degree polynomial kernel is deﬁned\nto be\nK(x, x′) = (1 + ⟨x, x′⟩)k Now we will show that this is indeed a kernel function That is, we will show\nthat there exists a mapping ψ from the original space to some higher dimensional\nspace for which K(x, x′) = ⟨ψ(x), ψ(x′)⟩ For simplicity, denote x0 = x′\n0 = 1 Then, we have\nK(x, x′) = (1 + ⟨x, x′⟩)k = (1 + ⟨x, x′⟩) · · · · · (1 + ⟨x, x′⟩)\n=\n\n\nn\nX\nj=0\nxjx′\nj\n\n· · · · ·\n\n\nn\nX\nj=0\nxjx′\nj\n\n\n=\nX\nJ∈{0,1,...,n}k\nk\nY\ni=1\nxJix′\nJi\n=\nX\nJ∈{0,1,...,n}k\nk\nY\ni=1\nxJi\nk\nY\ni=1\nx′\nJi Now, if we deﬁne ψ : Rn →R(n+1)k such that for J ∈{0, 1, , n}k there is an\nelement of ψ(x) that equals Qk\ni=1 xJi, we obtain that\nK(x, x′) = ⟨ψ(x), ψ(x′)⟩ Since ψ contains all the monomials up to degree k, a halfspace over the range\nof ψ corresponds to a polynomial predictor of degree k over the original space Hence, learning a halfspace with a k degree polynomial kernel enables us to learn\npolynomial predictors of degree k over the original space",
      "word_count": 243,
      "source_page": 220,
      "start_position": 78521,
      "end_position": 78763,
      "sentences_count": 11
    },
    {
      "chunk_id": 347,
      "text": "Since ψ contains all the monomials up to degree k, a halfspace over the range\nof ψ corresponds to a polynomial predictor of degree k over the original space Hence, learning a halfspace with a k degree polynomial kernel enables us to learn\npolynomial predictors of degree k over the original space Note that here the complexity of implementing K is O(n) while the dimension\nof the feature space is on the order of nk Example 16.2 (Gaussian Kernel)\nLet the original instance space be R and\nconsider the mapping ψ where for each nonnegative integer n ≥0 there exists\nan element ψ(x)n that equals\n1\n√\nn e−x2\n2 xn Then,\n⟨ψ(x), ψ(x′)⟩=\n∞\nX\nn=0\n\u0012 1\n√\nn e−x2\n2 xn\n\u0013 \u0012 1\n√\nn e−(x′)2\n2\n(x′)n\n\u0013\n= e−x2+(x′)2\n2\n∞\nX\nn=0\n\u0012(xx′)n\nn \u0013\n= e−∥x−x′∥2\n2 Here the feature space is of inﬁnite dimension while evaluating the kernel is very",
      "word_count": 159,
      "source_page": 220,
      "start_position": 78712,
      "end_position": 78870,
      "sentences_count": 10
    },
    {
      "chunk_id": 348,
      "text": "16.2 The Kernel Trick\n221\nsimple More generally, given a scalar σ > 0, the Gaussian kernel is deﬁned to\nbe\nK(x, x′) = e−∥x−x′∥2\n2 σ Intuitively, the Gaussian kernel sets the inner product in the feature space\nbetween x, x′ to be close to zero if the instances are far away from each other\n(in the original domain) and close to 1 if they are close σ is a parameter that\ncontrols the scale determining what we mean by “close.” It is easy to verify that\nK implements an inner product in a space in which for any n and any monomial\nof order k there exists an element of ψ(x) that equals\n1\n√\nn e−∥x∥2\n2\nQn\ni=1 xJi Hence, we can learn any polynomial predictor over the original space by using a\nGaussian kernel Recall that the VC-dimension of the class of all polynomial predictors is inﬁ-\nnite (see Exercise 12) There is no contradiction, because the sample complexity\nrequired to learn with Gaussian kernels depends on the margin in the feature\nspace, which will be large if we are lucky, but can in general be arbitrarily small The Gaussian kernel is also called the RBF kernel, for “Radial Basis Func-\ntions.”\n16.2.1\nKernels as a Way to Express Prior Knowledge\nAs we discussed previously, a feature mapping, ψ, may be viewed as expanding\nthe class of linear classiﬁers to a richer class (corresponding to linear classiﬁers\nover the feature space)",
      "word_count": 246,
      "source_page": 221,
      "start_position": 78871,
      "end_position": 79116,
      "sentences_count": 9
    },
    {
      "chunk_id": 349,
      "text": "There is no contradiction, because the sample complexity\nrequired to learn with Gaussian kernels depends on the margin in the feature\nspace, which will be large if we are lucky, but can in general be arbitrarily small The Gaussian kernel is also called the RBF kernel, for “Radial Basis Func-\ntions.”\n16.2.1\nKernels as a Way to Express Prior Knowledge\nAs we discussed previously, a feature mapping, ψ, may be viewed as expanding\nthe class of linear classiﬁers to a richer class (corresponding to linear classiﬁers\nover the feature space) However, as discussed in the book so far, the suitability\nof any hypothesis class to a given learning task depends on the nature of that\ntask One can therefore think of an embedding ψ as a way to express and utilize\nprior knowledge about the problem at hand For example, if we believe that\npositive examples can be distinguished by some ellipse, we can deﬁne ψ to be all\nthe monomials up to order 2, or use a degree 2 polynomial kernel As a more realistic example, consider the task of learning to ﬁnd a sequence of\ncharacters (“signature”) in a ﬁle that indicates whether it contains a virus or not Formally, let Xd be the set of all strings of length at most d over some alphabet\nset Σ",
      "word_count": 220,
      "source_page": 221,
      "start_position": 79027,
      "end_position": 79246,
      "sentences_count": 7
    },
    {
      "chunk_id": 350,
      "text": "As a more realistic example, consider the task of learning to ﬁnd a sequence of\ncharacters (“signature”) in a ﬁle that indicates whether it contains a virus or not Formally, let Xd be the set of all strings of length at most d over some alphabet\nset Σ The hypothesis class that one wishes to learn is H = {hv : v ∈Xd}, where,\nfor a string x ∈Xd, hv(x) is 1 iﬀv is a substring of x (and hv(x) = −1 otherwise) Let us show how using an appropriate embedding this class can be realized by\nlinear classiﬁers over the resulting feature space Consider a mapping ψ to a space\nRs where s = |Xd|, so that each coordinate of ψ(x) corresponds to some string v\nand indicates whether v is a substring of x (that is, for every x ∈Xd, ψ(x) is a\nvector in {0, 1}|Xd|) Note that the dimension of this feature space is exponential\nin d It is not hard to see that every member of the class H can be realized by\ncomposing a linear classiﬁer over ψ(x), and, moreover, by such a halfspace whose\nnorm is 1 and that attains a margin of 1 (see Exercise 1) Furthermore, for every\nx ∈X, ∥ψ(x)∥= O(d) So, overall, it is learnable using SVM with a sample",
      "word_count": 221,
      "source_page": 221,
      "start_position": 79199,
      "end_position": 79419,
      "sentences_count": 9
    },
    {
      "chunk_id": 351,
      "text": "222\nKernel Methods\ncomplexity that is polynomial in d However, the dimension of the feature space\nis exponential in d so a direct implementation of SVM over the feature space is\nproblematic Luckily, it is easy to calculate the inner product in the feature space\n(i.e., the kernel function) without explicitly mapping instances into the feature\nspace Indeed, K(x, x′) is simply the number of common substrings of x and x′,\nwhich can be easily calculated in time polynomial in d This example also demonstrates how feature mapping enables us to use halfspaces\nfor nonvectorial domains 16.2.2\nCharacterizing Kernel Functions*\nAs we have discussed in the previous section, we can think of the speciﬁcation of\nthe kernel matrix as a way to express prior knowledge Consider a given similarity\nfunction of the form K : X × X →R Is it a valid kernel function That is, does\nit represent an inner product between ψ(x) and ψ(x′) for some feature mapping\nψ The following lemma gives a suﬃcient and necessary condition lemma 16.2\nA symmetric function K : X × X →R implements an inner\nproduct in some Hilbert space if and only if it is positive semideﬁnite; namely,\nfor all x1, , xm, the Gram matrix, Gi,j = K(xi, xj), is a positive semideﬁnite\nmatrix Proof\nIt is trivial to see that if K implements an inner product in some Hilbert\nspace then the Gram matrix is positive semideﬁnite",
      "word_count": 240,
      "source_page": 222,
      "start_position": 79420,
      "end_position": 79659,
      "sentences_count": 13
    },
    {
      "chunk_id": 352,
      "text": ", xm, the Gram matrix, Gi,j = K(xi, xj), is a positive semideﬁnite\nmatrix Proof\nIt is trivial to see that if K implements an inner product in some Hilbert\nspace then the Gram matrix is positive semideﬁnite For the other direction,\ndeﬁne the space of functions over X as RX = {f : X →R} For each x ∈X\nlet ψ(x) be the function x 7→K(·, x) Deﬁne a vector space by taking all linear\ncombinations of elements of the form K(·, x) Deﬁne an inner product on this\nvector space to be\n*X\ni\nαiK(·, xi),\nX\nj\nβjK(·, x′\nj)\n+\n=\nX\ni,j\nαiβjK(xi, x′\nj) This is a valid inner product since it is symmetric (because K is symmetric), it is\nlinear (immediate), and it is positive deﬁnite (it is easy to see that K(x, x) ≥0\nwith equality only for ψ(x) being the zero function) Clearly,\n⟨ψ(x), ψ(x′)⟩= ⟨K(·, x), K(·, x′)⟩= K(x, x′),\nwhich concludes our proof 16.3\nImplementing Soft-SVM with Kernels\nNext, we turn to solving Soft-SVM with kernels While we could have designed\nan algorithm for solving Equation (16.4), there is an even simpler approach that",
      "word_count": 195,
      "source_page": 222,
      "start_position": 79622,
      "end_position": 79816,
      "sentences_count": 10
    },
    {
      "chunk_id": 353,
      "text": "16.3 Implementing Soft-SVM with Kernels\n223\ndirectly tackles the Soft-SVM optimization problem in the feature space,\nmin\nw\n \nλ\n2 ∥w∥2 + 1\nm\nm\nX\ni=1\nmax{0, 1 −y⟨w, ψ(xi)⟩} ,\n(16.5)\nwhile only using kernel evaluations The basic observation is that the vector w(t)\nmaintained by the SGD procedure we have described in Section 15.5 is always in\nthe linear span of {ψ(x1), , ψ(xm)} Therefore, rather than maintaining w(t)\nwe can maintain the corresponding coeﬃcients α Formally, let K be the kernel function, namely, for all x, x′, K(x, x′) =\n⟨ψ(x), ψ(x′)⟩ We shall maintain two vectors in Rm, corresponding to two vectors\nθ(t) and w(t) deﬁned in the SGD procedure of Section 15.5 That is, β(t) will be\na vector such that\nθ(t) =\nm\nX\nj=1\nβ(t)\nj ψ(xj)\n(16.6)\nand α(t) be such that\nw(t) =\nm\nX\nj=1\nα(t)\nj ψ(xj) (16.7)\nThe vectors β and α are updated according to the following procedure SGD for Solving Soft-SVM with Kernels\nGoal: Solve Equation (16.5)\nparameter: T\nInitialize: β(1) = 0\nfor t = 1,",
      "word_count": 182,
      "source_page": 223,
      "start_position": 79817,
      "end_position": 79998,
      "sentences_count": 10
    },
    {
      "chunk_id": 354,
      "text": "(16.7)\nThe vectors β and α are updated according to the following procedure SGD for Solving Soft-SVM with Kernels\nGoal: Solve Equation (16.5)\nparameter: T\nInitialize: β(1) = 0\nfor t = 1, , T\nLet α(t) =\n1\nλ tβ(t)\nChoose i uniformly at random from [m]\nFor all j ̸= i set β(t+1)\nj\n= β(t)\nj\nIf (yi\nPm\nj=1 α(t)\nj K(xj, xi) < 1)\nSet β(t+1)\ni\n= β(t)\ni\n+ yi\nElse\nSet β(t+1)\ni\n= β(t)\ni\nOutput: ¯w = Pm\nj=1 ¯αjψ(xj) where ¯α = 1\nT\nPT\nt=1 α(t)\nThe following lemma shows that the preceding implementation is equivalent\nto running the SGD procedure described in Section 15.5 on the feature space lemma 16.3\nLet ˆw be the output of the SGD procedure described in Sec-\ntion 15.5, when applied on the feature space, and let ¯w = Pm\nj=1 ¯αjψ(xj) be\nthe output of applying SGD with kernels Then ¯w = ˆw Proof\nWe will show that for every t Equation (16.6) holds, where θ(t) is the\nresult of running the SGD procedure described in Section 15.5 in the feature",
      "word_count": 190,
      "source_page": 223,
      "start_position": 79966,
      "end_position": 80155,
      "sentences_count": 6
    },
    {
      "chunk_id": 355,
      "text": "224\nKernel Methods\nspace By the deﬁnition of α(t) =\n1\nλ tβ(t) and w(t) =\n1\nλ tθ(t), this claim implies\nthat Equation (16.7) also holds, and the proof of our lemma will follow To prove\nthat Equation (16.6) holds we use a simple inductive argument For t = 1 the\nclaim trivially holds Assume it holds for t ≥1 Then,\nyi\nD\nw(t), ψ(xi)\nE\n= yi\n*X\nj\nα(t)\nj ψ(xj), ψ(xi)\n+\n= yi\nm\nX\nj=1\nα(t)\nj K(xj, xi) Hence, the condition in the two algorithms is equivalent and if we update θ we\nhave\nθ(t+1) = θ(t) + yiψ(xi) =\nm\nX\nj=1\nβ(t)\nj ψ(xj) + yiψ(xi) =\nm\nX\nj=1\nβ(t+1)\nj\nψ(xj),\nwhich concludes our proof 16.4\nSummary\nMappings from the given domain to some higher dimensional space, on which a\nhalfspace predictor is used, can be highly powerful We beneﬁt from a rich and\ncomplex hypothesis class, yet need to solve the problems of high sample and\ncomputational complexities In Chapter 10, we discussed the AdaBoost algo-\nrithm, which faces these challenges by using a weak learner: Even though we’re\nin a very high dimensional space, we have an “oracle” that bestows on us a\nsingle good coordinate to work with on each iteration In this chapter we intro-\nduced a diﬀerent approach, the kernel trick",
      "word_count": 227,
      "source_page": 224,
      "start_position": 80156,
      "end_position": 80382,
      "sentences_count": 11
    },
    {
      "chunk_id": 356,
      "text": "In Chapter 10, we discussed the AdaBoost algo-\nrithm, which faces these challenges by using a weak learner: Even though we’re\nin a very high dimensional space, we have an “oracle” that bestows on us a\nsingle good coordinate to work with on each iteration In this chapter we intro-\nduced a diﬀerent approach, the kernel trick The idea is that in order to ﬁnd a\nhalfspace predictor in the high dimensional space, we do not need to know the\nrepresentation of instances in that space, but rather the values of inner products\nbetween the mapped instances Calculating inner products between instances in\nthe high dimensional space without using their representation in that space is\ndone using kernel functions We have also shown how the SGD algorithm can be\nimplemented using kernels The ideas of feature mapping and the kernel trick allow us to use the framework\nof halfspaces and linear predictors for nonvectorial data We demonstrated how\nkernels can be used to learn predictors over the domain of strings We presented the applicability of the kernel trick in SVM However, the kernel\ntrick can be applied in many other algorithms A few examples are given as\nexercises This chapter ends the series of chapters on linear predictors and convex prob-\nlems The next two chapters deal with completely diﬀerent types of hypothesis\nclasses.",
      "word_count": 224,
      "source_page": 224,
      "start_position": 80326,
      "end_position": 80549,
      "sentences_count": 12
    },
    {
      "chunk_id": 357,
      "text": "16.5 Bibliographic Remarks\n225\n16.5\nBibliographic Remarks\nIn the context of SVM, the kernel-trick has been introduced in Boser et al (1992) See also Aizerman, Braverman & Rozonoer (1964) The observation that the\nkernel-trick can be applied whenever an algorithm only relies on inner products\nwas ﬁrst stated by Sch¨olkopf, Smola & M¨uller (1998) The proof of the representer\ntheorem is given in (Sch¨olkopf, Herbrich, Smola & Williamson 2000, Sch¨olkopf,\nHerbrich & Smola 2001) The conditions stated in Lemma 16.2 are simpliﬁcation\nof conditions due to Mercer Many useful kernel functions have been introduced\nin the literature for various applications We refer the reader to Sch¨olkopf &\nSmola (2002) 16.6\nExercises\n1 Consider the task of ﬁnding a sequence of characters in a ﬁle, as described\nin Section 16.2.1 Show that every member of the class H can be realized by\ncomposing a linear classiﬁer over ψ(x), whose norm is 1 and that attains a\nmargin of 1 2 Kernelized Perceptron: Show how to run the Perceptron algorithm while\nonly accessing the instances via the kernel function Hint: The derivation is\nsimilar to the derivation of implementing SGD with kernels 3 Kernel Ridge Regression: The ridge regression problem, with a feature\nmapping ψ, is the problem of ﬁnding a vector w that minimizes the function\nf(w) = λ ∥w∥2 + 1\n2m\nm\nX\ni=1\n(⟨w, ψ(xi)⟩−yi)2,\n(16.8)\nand then returning the predictor\nh(x) = ⟨w, x⟩ Show how to implement the ridge regression algorithm with kernels",
      "word_count": 247,
      "source_page": 225,
      "start_position": 80550,
      "end_position": 80796,
      "sentences_count": 17
    },
    {
      "chunk_id": 358,
      "text": "Kernel Ridge Regression: The ridge regression problem, with a feature\nmapping ψ, is the problem of ﬁnding a vector w that minimizes the function\nf(w) = λ ∥w∥2 + 1\n2m\nm\nX\ni=1\n(⟨w, ψ(xi)⟩−yi)2,\n(16.8)\nand then returning the predictor\nh(x) = ⟨w, x⟩ Show how to implement the ridge regression algorithm with kernels Hint: The representer theorem tells us that there exists a vector α ∈Rm\nsuch that Pm\ni=1 αiψ(xi) is a minimizer of Equation (16.8) 1 Let G be the Gram matrix with regard to S and K That is, Gij =\nK(xi, xj) Deﬁne g : Rm →R by\ng(α) = λ · αT Gα + 1\n2m\nm\nX\ni=1\n(⟨α, G·,i⟩−yi)2,\n(16.9)\nwhere G·,i is the i’th column of G Show that if α∗minimizes Equa-\ntion (16.9) then w∗= Pm\ni=1 α∗\ni ψ(xi) is a minimizer of f 2 Find a closed form expression for α∗ 4 Let N be any positive integer For every x, x′ ∈{1, , N} deﬁne\nK(x, x′) = min{x, x′}.",
      "word_count": 175,
      "source_page": 225,
      "start_position": 80741,
      "end_position": 80915,
      "sentences_count": 14
    },
    {
      "chunk_id": 359,
      "text": "226\nKernel Methods\nProve that K is a valid kernel; namely, ﬁnd a mapping ψ : {1, , N} →H\nwhere H is some Hilbert space, such that\n∀x, x′ ∈{1, , N}, K(x, x′) = ⟨ψ(x), ψ(x′)⟩ 5 A supermarket manager would like to learn which of his customers have babies\non the basis of their shopping carts Speciﬁcally, he sampled i.i.d customers,\nwhere for customer i, let xi ⊂{1, , d} denote the subset of items the\ncustomer bought, and let yi ∈{±1} be the label indicating whether this\ncustomer has a baby As prior knowledge, the manager knows that there are\nk items such that the label is determined to be 1 iﬀthe customer bought\nat least one of these k items Of course, the identity of these k items is not\nknown (otherwise, there was nothing to learn) In addition, according to the\nstore regulation, each customer can buy at most s items Help the manager to\ndesign a learning algorithm such that both its time complexity and its sample\ncomplexity are polynomial in s, k, and 1/ϵ 6 Let X be an instance set and let ψ be a feature mapping of X into some\nHilbert feature space V Let K : X × X →R be a kernel function that\nimplements inner products in the feature space V Consider the binary classiﬁcation algorithm that predicts the label of\nan unseen instance according to the class with the closest average",
      "word_count": 245,
      "source_page": 226,
      "start_position": 80916,
      "end_position": 81160,
      "sentences_count": 16
    },
    {
      "chunk_id": 360,
      "text": "17\nMulticlass, Ranking, and Complex\nPrediction Problems\nMulticlass categorization is the problem of classifying instances into one of several\npossible target classes That is, we are aiming at learning a predictor h : X →Y,\nwhere Y is a ﬁnite set of categories Applications include, for example, catego-\nrizing documents according to topic (X is the set of documents and Y is the set\nof possible topics) or determining which object appears in a given image (X is\nthe set of images and Y is the set of possible objects) The centrality of the multiclass learning problem has spurred the development\nof various approaches for tackling the task Perhaps the most straightforward\napproach is a reduction from multiclass classiﬁcation to binary classiﬁcation In\nSection 17.1 we discuss the most common two reductions as well as the main\ndrawback of the reduction approach We then turn to describe a family of linear predictors for multiclass problems Relying on the RLM and SGD frameworks from previous chapters, we describe\nseveral practical algorithms for multiclass prediction In Section 17.3 we show how to use the multiclass machinery for complex pre-\ndiction problems in which Y can be extremely large but has some structure on\nit This task is often called structured output learning",
      "word_count": 210,
      "source_page": 227,
      "start_position": 81259,
      "end_position": 81468,
      "sentences_count": 10
    },
    {
      "chunk_id": 361,
      "text": "In Section 17.3 we show how to use the multiclass machinery for complex pre-\ndiction problems in which Y can be extremely large but has some structure on\nit This task is often called structured output learning In particular, we demon-\nstrate this approach for the task of recognizing handwritten words, in which Y\nis the set of all possible strings of some bounded length (hence, the size of Y is\nexponential in the maximal length of a word) Finally, in Section 17.4 and Section 17.5 we discuss ranking problems in which\nthe learner should order a set of instances according to their “relevance.” A typ-\nical application is ordering results of a search engine according to their relevance\nto the query We describe several performance measures that are adequate for\nassessing the performance of ranking predictors and describe how to learn linear\npredictors for ranking problems eﬃciently 17.1\nOne-versus-All and All-Pairs\nThe simplest approach to tackle multiclass prediction problems is by reduction\nto binary classiﬁcation Recall that in multiclass prediction we would like to learn\na function h : X →Y Without loss of generality let us denote Y = {1, , k} In the One-versus-All method (a.k.a One-versus-Rest) we train k binary clas-\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 234,
      "source_page": 227,
      "start_position": 81432,
      "end_position": 81665,
      "sentences_count": 15
    },
    {
      "chunk_id": 362,
      "text": "228\nMulticlass, Ranking, and Complex Prediction Problems\nsiﬁers, each of which discriminates between one class and the rest of the classes That is, given a training set S = (x1, y1), , (xm, ym), where every yi is in Y, we\nconstruct k binary training sets, S1, , Sk, where Si = (x1, (−1)1[y1̸=i]), , (xm, (−1)1[ym̸=i]) In words, Si is the set of instances labeled 1 if their label in S was i, and −1\notherwise For every i ∈[k] we train a binary predictor hi : X →{±1} based on\nSi, hoping that hi(x) should equal 1 if and only if x belongs to class i Then,\ngiven h1, , hk, we construct a multiclass predictor using the rule\nh(x) ∈argmax\ni∈[k]\nhi(x) (17.1)\nWhen more than one binary hypothesis predicts “1” we should somehow decide\nwhich class to predict (e.g., we can arbitrarily decide to break ties by taking the\nminimal index in argmaxi hi(x)) A better approach can be applied whenever\neach hi hides additional information, which can be interpreted as the conﬁdence\nin the prediction y = i For example, this is the case in halfspaces, where the\nactual prediction is sign(⟨w, x⟩), but we can interpret ⟨w, x⟩as the conﬁdence\nin the prediction In such cases, we can apply the multiclass rule given in Equa-\ntion (17.1) on the real valued predictions A pseudocode of the One-versus-All\napproach is given in the following One-versus-All\ninput:\ntraining set S = (x1, y1),",
      "word_count": 247,
      "source_page": 228,
      "start_position": 81666,
      "end_position": 81912,
      "sentences_count": 15
    },
    {
      "chunk_id": 363,
      "text": "A pseudocode of the One-versus-All\napproach is given in the following One-versus-All\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nalgorithm for binary classiﬁcation A\nforeach i ∈Y\nlet Si = (x1, (−1)1[y1̸=i]), , (xm, (−1)1[ym̸=i])\nlet hi = A(Si)\noutput:\nthe multiclass hypothesis deﬁned by h(x) ∈argmaxi∈Y hi(x)\nAnother popular reduction is the All-Pairs approach, in which all pairs of\nclasses are compared to each other Formally, given a training set S = (x1, y1), , (xm, ym),\nwhere every yi is in [k], for every 1 ≤i < j ≤k we construct a binary training\nsequence, Si,j, containing all examples from S whose label is either i or j For\neach such an example, we set the binary label in Si,j to be +1 if the multiclass\nlabel in S is i and −1 if the multiclass label in S is j Next, we train a binary\nclassiﬁcation algorithm based on every Si,j to get hi,j Finally, we construct\na multiclass classiﬁer by predicting the class that had the highest number of\n“wins.” A pseudocode of the All-Pairs approach is given in the following.",
      "word_count": 188,
      "source_page": 228,
      "start_position": 81894,
      "end_position": 82081,
      "sentences_count": 9
    },
    {
      "chunk_id": 364,
      "text": "17.1 One-versus-All and All-Pairs\n229\nAll-Pairs\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nalgorithm for binary classiﬁcation A\nforeach i, j ∈Y s.t i < j\ninitialize Si,j to be the empty sequence\nfor t = 1, , m\nIf yt = i add (xt, 1) to Si,j\nIf yt = j add (xt, −1) to Si,j\nlet hi,j = A(Si,j)\noutput:\nthe multiclass hypothesis deﬁned by\nh(x) ∈argmaxi∈Y\n\u0010P\nj∈Y sign(j −i) hi,j(x)\n\u0011\nAlthough reduction methods such as the One-versus-All and All-Pairs are\nsimple and easy to construct from existing algorithms, their simplicity has a\nprice The binary learner is not aware of the fact that we are going to use its\noutput hypotheses for constructing a multiclass predictor, and this might lead\nto suboptimal results, as illustrated in the following example Example 17.1\nConsider a multiclass categorization problem in which the in-\nstance space is X = R2 and the label set is Y = {1, 2, 3} Suppose that instances\nof the diﬀerent classes are located in nonintersecting balls as depicted in the fol-\nlowing 1\n2\n3\nSuppose that the probability masses of classes 1, 2, 3 are 40%, 20%, and 40%,\nrespectively Consider the application of One-versus-All to this problem, and as-\nsume that the binary classiﬁcation algorithm used by One-versus-All is ERM\nwith respect to the hypothesis class of halfspaces",
      "word_count": 230,
      "source_page": 229,
      "start_position": 82082,
      "end_position": 82311,
      "sentences_count": 9
    },
    {
      "chunk_id": 365,
      "text": "1\n2\n3\nSuppose that the probability masses of classes 1, 2, 3 are 40%, 20%, and 40%,\nrespectively Consider the application of One-versus-All to this problem, and as-\nsume that the binary classiﬁcation algorithm used by One-versus-All is ERM\nwith respect to the hypothesis class of halfspaces Observe that for the prob-\nlem of discriminating between class 2 and the rest of the classes, the optimal\nhalfspace would be the all negative classiﬁer Therefore, the multiclass predic-\ntor constructed by One-versus-All might err on all the examples from class 2\n(this will be the case if the tie in the deﬁnition of h(x) is broken by the nu-\nmerical value of the class label) In contrast, if we choose hi(x) = ⟨wi, x⟩,\nwhere w1 =\n\u0010\n−1\n√\n2,\n1\n√\n2\n\u0011\n, w2 = (0, 1), and w3 =\n\u0010\n1\n√\n2,\n1\n√\n2\n\u0011\n, then the classi-\nﬁer deﬁned by h(x) = argmaxi hi(x) perfectly predicts all the examples We see",
      "word_count": 169,
      "source_page": 229,
      "start_position": 82264,
      "end_position": 82432,
      "sentences_count": 6
    },
    {
      "chunk_id": 366,
      "text": "230\nMulticlass, Ranking, and Complex Prediction Problems\nthat even though the approximation error of the class of predictors of the form\nh(x) = argmaxi⟨wi, x⟩is zero, the One-versus-All approach might fail to ﬁnd a\ngood predictor from this class 17.2\nLinear Multiclass Predictors\nIn light of the inadequacy of reduction methods, in this section we study a more\ndirect approach for learning multiclass predictors We describe the family of\nlinear multiclass predictors To motivate the construction of this family, recall\nthat a linear predictor for binary classiﬁcation (i.e., a halfspace) takes the form\nh(x) = sign(⟨w, x⟩) An equivalent way to express the prediction is as follows:\nh(x) = argmax\ny∈{±1}\n⟨w, yx⟩,\nwhere yx is the vector obtained by multiplying each element of x by y This representation leads to a natural generalization of halfspaces to multiclass\nproblems as follows Let Ψ : X × Y →Rd be a class-sensitive feature mapping That is, Ψ takes as input a pair (x, y) and maps it into a d dimensional feature\nvector Intuitively, we can think of the elements of Ψ(x, y) as score functions that\nassess how well the label y ﬁts the instance x We will elaborate on Ψ later on Given Ψ and a vector w ∈Rd, we can deﬁne a multiclass predictor, h : X →Y,\nas follows:\nh(x) = argmax\ny∈Y\n⟨w, Ψ(x, y)⟩",
      "word_count": 229,
      "source_page": 230,
      "start_position": 82433,
      "end_position": 82661,
      "sentences_count": 11
    },
    {
      "chunk_id": 367,
      "text": "We will elaborate on Ψ later on Given Ψ and a vector w ∈Rd, we can deﬁne a multiclass predictor, h : X →Y,\nas follows:\nh(x) = argmax\ny∈Y\n⟨w, Ψ(x, y)⟩ That is, the prediction of h for the input x is the label that achieves the highest\nweighted score, where weighting is according to the vector w Let W be some set of vectors in Rd, for example, W = {w ∈Rd : ∥w∥≤B},\nfor some scalar B > 0 Each pair (Ψ, W) deﬁnes a hypothesis class of multiclass\npredictors:\nHΨ,W = {x 7→argmax\ny∈Y\n⟨w, Ψ(x, y)⟩: w ∈W} Of course, the immediate question, which we discuss in the sequel, is how to\nconstruct a good Ψ Note that if Y = {±1} and we set Ψ(x, y) = yx and\nW = Rd, then HΨ,W becomes the hypothesis class of homogeneous halfspace\npredictors for binary classiﬁcation 17.2.1\nHow to Construct Ψ\nAs mentioned before, we can think of the elements of Ψ(x, y) as score functions\nthat assess how well the label y ﬁts the instance x Naturally, designing a good Ψ\nis similar to the problem of designing a good feature mapping (as we discussed in",
      "word_count": 203,
      "source_page": 230,
      "start_position": 82629,
      "end_position": 82831,
      "sentences_count": 9
    },
    {
      "chunk_id": 368,
      "text": "17.2 Linear Multiclass Predictors\n231\nChapter 16 and as we will discuss in more detail in Chapter 25) Two examples\nof useful constructions are given in the following The Multivector Construction:\nLet Y = {1, , k} and let X = Rn We deﬁne Ψ : X × Y →Rd, where d = nk,\nas follows\nΨ(x, y) = [ 0, , 0\n| {z }\n∈R(y−1)n\n, x1, , xn\n|\n{z\n}\n∈Rn\n, 0, , 0\n| {z }\n∈R(k−y)n\n] (17.2)\nThat is, Ψ(x, y) is composed of k vectors, each of which is of dimension n, where\nwe set all the vectors to be the all zeros vector except the y’th vector, which is\nset to be x It follows that we can think of w ∈Rnk as being composed of k\nweight vectors in Rn, that is, w = [w1; ; wk], hence the name multivec-\ntor construction By the construction we have that ⟨w, Ψ(x, y)⟩= ⟨wy, x⟩, and\ntherefore the multiclass prediction becomes\nh(x) = argmax\ny∈Y\n⟨wy, x⟩ A geometric illustration of the multiclass prediction over X = R2 is given in the\nfollowing w1\nw2\nw3\nw4\nTF-IDF:\nThe previous deﬁnition of Ψ(x, y) does not incorporate any prior knowledge\nabout the problem We next describe an example of a feature function Ψ that\ndoes incorporate prior knowledge Let X be a set of text documents and Y be a\nset of possible topics",
      "word_count": 244,
      "source_page": 231,
      "start_position": 82832,
      "end_position": 83075,
      "sentences_count": 16
    },
    {
      "chunk_id": 369,
      "text": "We next describe an example of a feature function Ψ that\ndoes incorporate prior knowledge Let X be a set of text documents and Y be a\nset of possible topics Let d be a size of a dictionary of words For each word in the\ndictionary, whose corresponding index is j, let TF(j, x) be the number of times\nthe word corresponding to j appears in the document x This quantity is called\nTerm-Frequency Additionally, let DF(j, y) be the number of times the word\ncorresponding to j appears in documents in our training set that are not about\ntopic y This quantity is called Document-Frequency and measures whether word\nj is frequent in other topics Now, deﬁne Ψ : X × Y →Rd to be such that\nΨj(x, y) = TF(j, x) log\n\u0010\nm\nDF (j,y)\n\u0011\n,\nwhere m is the total number of documents in our training set The preced-\ning quantity is called term-frequency-inverse-document-frequency or TF-IDF for",
      "word_count": 163,
      "source_page": 231,
      "start_position": 83045,
      "end_position": 83207,
      "sentences_count": 9
    },
    {
      "chunk_id": 370,
      "text": "232\nMulticlass, Ranking, and Complex Prediction Problems\nshort Intuitively, Ψj(x, y) should be large if the word corresponding to j ap-\npears a lot in the document x but does not appear at all in documents that are\nnot on topic y If this is the case, we tend to believe that the document x is on\ntopic y Note that unlike the multivector construction described previously, in\nthe current construction the dimension of Ψ does not depend on the number of\ntopics (i.e., the size of Y) 17.2.2\nCost-Sensitive Classiﬁcation\nSo far we used the zero-one loss as our performance measure of the quality of\nh(x) That is, the loss of a hypothesis h on an example (x, y) is 1 if h(x) ̸= y and\n0 otherwise In some situations it makes more sense to penalize diﬀerent levels\nof loss for diﬀerent mistakes For example, in object recognition tasks, it is less\nsevere to predict that an image of a tiger contains a cat than predicting that\nthe image contains a whale This can be modeled by specifying a loss function,\n∆: Y × Y →R+, where for every pair of labels, y′, y, the loss of predicting\nthe label y′ when the correct label is y is deﬁned to be ∆(y′, y) We assume\nthat ∆(y, y) = 0 Note that the zero-one loss can be easily modeled by setting\n∆(y′, y) = 1[y′̸=y]",
      "word_count": 237,
      "source_page": 232,
      "start_position": 83208,
      "end_position": 83444,
      "sentences_count": 11
    },
    {
      "chunk_id": 371,
      "text": "We assume\nthat ∆(y, y) = 0 Note that the zero-one loss can be easily modeled by setting\n∆(y′, y) = 1[y′̸=y] 17.2.3\nERM\nWe have deﬁned the hypothesis class HΨ,W and speciﬁed a loss function ∆ To\nlearn the class with respect to the loss function, we can apply the ERM rule with\nrespect to this class That is, we search for a multiclass hypothesis h ∈HΨ,W ,\nparameterized by a vector w, that minimizes the empirical risk with respect to\n∆,\nLS(h) = 1\nm\nm\nX\ni=1\n∆(h(xi), yi) We now show that when W = Rd and we are in the realizable case, then it is\npossible to solve the ERM problem eﬃciently using linear programming Indeed,\nin the realizable case, we need to ﬁnd a vector w ∈Rd that satisﬁes\n∀i ∈[m],\nyi = argmax\ny∈Y\n⟨w, Ψ(xi, y)⟩ Equivalently, we need that w will satisfy the following set of linear inequalities\n∀i ∈[m], ∀y ∈Y \\ {yi},\n⟨w, Ψ(xi, yi)⟩> ⟨w, Ψ(xi, y)⟩ Finding w that satisﬁes the preceding set of linear equations amounts to solving\na linear program As in the case of binary classiﬁcation, it is also possible to use a generalization\nof the Perceptron algorithm for solving the ERM problem See Exercise 2 In the nonrealizable case, solving the ERM problem is in general computa-\ntionally hard We tackle this diﬃculty using the method of convex surrogate",
      "word_count": 236,
      "source_page": 232,
      "start_position": 83423,
      "end_position": 83658,
      "sentences_count": 13
    },
    {
      "chunk_id": 372,
      "text": "17.2 Linear Multiclass Predictors\n233\nloss functions (see Section 12.3) In particular, we generalize the hinge loss to\nmulticlass problems 17.2.4\nGeneralized Hinge Loss\nRecall that in binary classiﬁcation, the hinge loss is deﬁned to be max{0, 1 −\ny⟨w, x⟩} We now generalize the hinge loss to multiclass predictors of the form\nhw(x) = argmax\ny′∈Y\n⟨w, Ψ(x, y′)⟩ Recall that a surrogate convex loss should upper bound the original nonconvex\nloss, which in our case is ∆(hw(x), y) To derive an upper bound on ∆(hw(x), y)\nwe ﬁrst note that the deﬁnition of hw(x) implies that\n⟨w, Ψ(x, y)⟩≤⟨w, Ψ(x, hw(x))⟩ Therefore,\n∆(hw(x), y) ≤∆(hw(x), y) + ⟨w, Ψ(x, hw(x)) −Ψ(x, y)⟩ Since hw(x) ∈Y we can upper bound the right-hand side of the preceding by\nmax\ny′∈Y (∆(y′, y) + ⟨w, Ψ(x, y′) −Ψ(x, y)⟩)\ndef\n=\nℓ(w, (x, y)) (17.3)\nWe use the term “generalized hinge loss” to denote the preceding expression As\nwe have shown, ℓ(w, (x, y)) ≥∆(hw(x), y) Furthermore, equality holds when-\never the score of the correct label is larger than the score of any other label, y′,\nby at least ∆(y′, y), namely,\n∀y′ ∈Y \\ {y},\n⟨w, Ψ(x, y)⟩≥⟨w, Ψ(x, y′)⟩+ ∆(y′, y)",
      "word_count": 203,
      "source_page": 233,
      "start_position": 83659,
      "end_position": 83861,
      "sentences_count": 11
    },
    {
      "chunk_id": 373,
      "text": "As\nwe have shown, ℓ(w, (x, y)) ≥∆(hw(x), y) Furthermore, equality holds when-\never the score of the correct label is larger than the score of any other label, y′,\nby at least ∆(y′, y), namely,\n∀y′ ∈Y \\ {y},\n⟨w, Ψ(x, y)⟩≥⟨w, Ψ(x, y′)⟩+ ∆(y′, y) It is also immediate to see that ℓ(w, (x, y)) is a convex function with respect to w\nsince it is a maximum over linear functions of w (see Claim 12.5 in Chapter 12),\nand that ℓ(w, (x, y)) is ρ-Lipschitz with ρ = maxy′∈Y ∥Ψ(x, y′) −Ψ(x, y)∥ Remark 17.2\nWe use the name “generalized hinge loss” since in the binary\ncase, when Y = {±1}, if we set Ψ(x, y) = yx\n2 , then the generalized hinge loss\nbecomes the vanilla hinge loss for binary classiﬁcation,\nℓ(w, (x, y)) = max{0, 1 −y⟨w, x⟩} Geometric Intuition:\nThe feature function Ψ : X × Y →Rd maps each x into |Y| vectors in Rd The value of ℓ(w, (x, y)) will be zero if there exists a direction w such that\nwhen projecting the |Y| vectors onto this direction we obtain that each vector is\nrepresented by the scalar ⟨w, Ψ(x, y)⟩, and we can rank the diﬀerent points on\nthe basis of these scalars so that\n• The point corresponding to the correct y is top-ranked",
      "word_count": 226,
      "source_page": 233,
      "start_position": 83815,
      "end_position": 84040,
      "sentences_count": 6
    },
    {
      "chunk_id": 374,
      "text": "234\nMulticlass, Ranking, and Complex Prediction Problems\n• For each y′ ̸= y, the diﬀerence between ⟨w, Ψ(x, y)⟩and ⟨w, Ψ(x, y′)⟩is larger\nthan the loss of predicting y′ instead of y The diﬀerence ⟨w, Ψ(x, y)⟩−\n⟨w, Ψ(x, y′)⟩is also referred to as the “margin” (see Section 15.1) This is illustrated in the following ﬁgure:\nw\nΨ(x, y)\nΨ(x, y′)\nΨ(x, y′′)\n≥∆(y, y′)\n≥\n∆(y, y′′)\n17.2.5\nMulticlass SVM and SGD\nOnce we have deﬁned the generalized hinge loss, we obtain a convex-Lipschitz\nlearning problem and we can apply our general techniques for solving such prob-\nlems In particular, the RLM technique we have studied in Chapter 13 yields the\nmulticlass SVM rule:\nMulticlass SVM\ninput: (x1, y1), , (xm, ym)\nparameters:\nregularization parameter λ > 0\nloss function ∆: Y × Y →R+\nclass-sensitive feature mapping Ψ : X × Y →Rd\nsolve:\nmin\nw∈Rd\n \nλ∥w∥2 + 1\nm\nm\nX\ni=1\nmax\ny′∈Y (∆(y′, yi) + ⟨w, Ψ(xi, y′) −Ψ(xi, yi)⟩) output the predictor hw(x) = argmaxy∈Y⟨w, Ψ(x, y)⟩\nWe can solve the optimization problem associated with multiclass SVM us-\ning generic convex optimization algorithms (or using the method described in\nSection 15.5) Let us analyze the risk of the resulting hypothesis The analysis\nseamlessly follows from our general analysis for convex-Lipschitz problems given\nin Chapter 13",
      "word_count": 221,
      "source_page": 234,
      "start_position": 84041,
      "end_position": 84261,
      "sentences_count": 8
    },
    {
      "chunk_id": 375,
      "text": "17.2 Linear Multiclass Predictors\n235\nConsider running Multiclass SVM with λ =\nq\n2ρ2\nB2m on a training set S ∼Dm\nand let hw be the output of Multiclass SVM Then,\nE\nS∼Dm[L∆\nD(hw)] ≤\nE\nS∼Dm[Lg−hinge\nD\n(w)] ≤\nmin\nu:∥u∥≤B Lg−hinge\nD\n(u) +\nr\n8ρ2B2\nm\n,\nwhere L∆\nD(h) = E(x,y)∼D[∆(h(x), y)] and Lg−hinge\nD\n(w) = E(x,y)∼D[ℓ(w, (x, y))]\nwith ℓbeing the generalized hinge-loss as deﬁned in Equation (17.3) We can also apply the SGD learning framework for minimizing Lg−hinge\nD\n(w) as\ndescribed in Chapter 14 Recall Claim 14.6, which dealt with subgradients of max\nfunctions In light of this claim, in order to ﬁnd a subgradient of the generalized\nhinge loss all we need to do is to ﬁnd y ∈Y that achieves the maximum in the\ndeﬁnition of the generalized hinge loss This yields the following algorithm:\nSGD for Multiclass Learning\nparameters:\nScalar η > 0, integer T > 0\nloss function ∆: Y × Y →R+\nclass-sensitive feature mapping Ψ : X × Y →Rd\ninitialize: w(1) = 0 ∈Rd\nfor t = 1, 2,",
      "word_count": 184,
      "source_page": 235,
      "start_position": 84324,
      "end_position": 84507,
      "sentences_count": 6
    },
    {
      "chunk_id": 376,
      "text": "In light of this claim, in order to ﬁnd a subgradient of the generalized\nhinge loss all we need to do is to ﬁnd y ∈Y that achieves the maximum in the\ndeﬁnition of the generalized hinge loss This yields the following algorithm:\nSGD for Multiclass Learning\nparameters:\nScalar η > 0, integer T > 0\nloss function ∆: Y × Y →R+\nclass-sensitive feature mapping Ψ : X × Y →Rd\ninitialize: w(1) = 0 ∈Rd\nfor t = 1, 2, , T\nsample (x, y) ∼D\nﬁnd ˆy ∈argmaxy′∈Y\n\u0000∆(y′, y) + ⟨w(t), Ψ(x, y′) −Ψ(x, y)⟩\n\u0001\nset vt = Ψ(x, ˆy) −Ψ(x, y)\nupdate w(t+1) = w(t) −ηvt\noutput ¯w = 1\nT\nPT\nt=1 w(t)\nOur general analysis of SGD given in Corollary 14.12 immediately implies:\ncorollary 17.2\nLet D be a distribution over X × Y, let Ψ : X × Y →Rd,\nand assume that for all x ∈X and y ∈Y we have ∥Ψ(x, y)∥≤ρ/2 Let B > 0 Then, for every ϵ > 0, if we run SGD for multiclass learning with a number of\niterations (i.e., number of examples)\nT ≥B2ρ2\nϵ2\nand with η =\nq\nB2\nρ2 T , then the output of SGD satisﬁes\nE\nS∼Dm[L∆\nD(h ¯w)] ≤\nE\nS∼Dm[Lg−hinge\nD\n( ¯w)] ≤\nmin\nu:∥u∥≤B Lg−hinge\nD\n(u) + ϵ",
      "word_count": 225,
      "source_page": 235,
      "start_position": 84426,
      "end_position": 84650,
      "sentences_count": 5
    },
    {
      "chunk_id": 377,
      "text": "236\nMulticlass, Ranking, and Complex Prediction Problems\n17.3\nStructured Output Prediction\nStructured output prediction problems are multiclass problems in which Y is\nvery large but is endowed with a predeﬁned structure The structure plays a\nkey role in constructing eﬃcient algorithms To motivate structured learning\nproblems, consider the problem of optical character recognition (OCR) Suppose\nwe receive an image of some handwritten word and would like to predict which\nword is written in the image To simplify the setting, suppose we know how to\nsegment the image into a sequence of images, each of which contains a patch of\nthe image corresponding to a single letter Therefore, X is the set of sequences\nof images and Y is the set of sequences of letters Note that the size of Y grows\nexponentially with the maximal length of a word An example of an image x\ncorresponding to the label y = “workable” is given in the following To tackle structure prediction we can rely on the family of linear predictors\ndescribed in the previous section In particular, we need to deﬁne a reasonable\nloss function for the problem, ∆, as well as a good class-sensitive feature mapping,\nΨ By “good” we mean a feature mapping that will lead to a low approximation\nerror for the class of linear predictors with respect to Ψ and ∆ Once we do this,\nwe can rely, for example, on the SGD learning algorithm deﬁned in the previous\nsection",
      "word_count": 244,
      "source_page": 236,
      "start_position": 84735,
      "end_position": 84978,
      "sentences_count": 12
    },
    {
      "chunk_id": 378,
      "text": "By “good” we mean a feature mapping that will lead to a low approximation\nerror for the class of linear predictors with respect to Ψ and ∆ Once we do this,\nwe can rely, for example, on the SGD learning algorithm deﬁned in the previous\nsection However, the huge size of Y poses several challenges:\n1 To apply the multiclass prediction we need to solve a maximization problem\nover Y How can we predict eﬃciently when Y is so large 2 How do we train w eﬃciently In particular, to apply the SGD rule we again\nneed to solve a maximization problem over Y 3 How can we avoid overﬁtting In the previous section we have already shown that the sample complexity of\nlearning a linear multiclass predictor does not depend explicitly on the number\nof classes We just need to make sure that the norm of the range of Ψ is not too\nlarge This will take care of the overﬁtting problem To tackle the computational\nchallenges we rely on the structure of the problem, and deﬁne the functions Ψ and\n∆so that calculating the maximization problems in the deﬁnition of hw and in\nthe SGD algorithm can be performed eﬃciently In the following we demonstrate\none way to achieve these goals for the OCR task mentioned previously To simplify the presentation, let us assume that all the words in Y are of length\nr and that the number of diﬀerent letters in our alphabet is q",
      "word_count": 248,
      "source_page": 236,
      "start_position": 84933,
      "end_position": 85180,
      "sentences_count": 16
    },
    {
      "chunk_id": 379,
      "text": "17.3 Structured Output Prediction\n237\nwords (i.e., sequences of letters) in Y We deﬁne the function ∆(y′, y) to be the\naverage number of letters that are diﬀerent in y′ and y, namely, 1\nr\nPr\ni=1 1[yi̸=y′\ni] Next, let us deﬁne a class-sensitive feature mapping Ψ(x, y) It will be conve-\nnient to think about x as a matrix of size n × r, where n is the number of pixels\nin each image, and r is the number of images in the sequence The j’th column\nof x corresponds to the j’th image in the sequence (encoded as a vector of gray\nlevel values of pixels) The dimension of the range of Ψ is set to be d = n q + q2 The ﬁrst nq feature functions are “type 1” features and take the form:\nΨi,j,1(x, y) = 1\nr\nr\nX\nt=1\nxi,t 1[yt=j] That is, we sum the value of the i’th pixel only over the images for which y\nassigns the letter j The triple index (i, j, 1) indicates that we are dealing with\nfeature (i, j) of type 1 Intuitively, such features can capture pixels in the image\nwhose gray level values are indicative of a certain letter The second type of\nfeatures take the form\nΨi,j,2(x, y) = 1\nr\nr\nX\nt=2\n1[yt=i] 1[yt−1=j] That is, we sum the number of times the letter i follows the letter j",
      "word_count": 240,
      "source_page": 237,
      "start_position": 85187,
      "end_position": 85426,
      "sentences_count": 12
    },
    {
      "chunk_id": 380,
      "text": "The second type of\nfeatures take the form\nΨi,j,2(x, y) = 1\nr\nr\nX\nt=2\n1[yt=i] 1[yt−1=j] That is, we sum the number of times the letter i follows the letter j Intuitively,\nthese features can capture rules like “It is likely to see the pair ‘qu’ in a word”\nor “It is unlikely to see the pair ‘rz’ in a word.” Of course, some of these features\nwill not be very useful, so the goal of the learning process is to assign weights to\nfeatures by learning the vector w, so that the weighted score will give us a good\nprediction via\nhw(x) = argmax\ny∈Y\n⟨w, Ψ(x, y)⟩ It is left to show how to solve the optimization problem in the deﬁnition\nof hw(x) eﬃciently, as well as how to solve the optimization problem in the\ndeﬁnition of ˆy in the SGD algorithm We can do this by applying a dynamic\nprogramming procedure We describe the procedure for solving the maximization\nin the deﬁnition of hw and leave as an exercise the maximization problem in the\ndeﬁnition of ˆy in the SGD algorithm To derive the dynamic programming procedure, let us ﬁrst observe that we\ncan write\nΨ(x, y) =\nr\nX\nt=1\nφ(x, yt, yt−1),\nfor an appropriate φ : X × [q] × [q] ∪{0} →Rd, and for simplicity we assume\nthat y0 is always equal to 0 Indeed, each feature function Ψi,j,1 can be written\nin terms of\nφi,j,1(x, yt, yt−1) = xi,t 1[yt=j],",
      "word_count": 250,
      "source_page": 237,
      "start_position": 85394,
      "end_position": 85643,
      "sentences_count": 8
    },
    {
      "chunk_id": 381,
      "text": "238\nMulticlass, Ranking, and Complex Prediction Problems\nwhile the feature function Ψi,j,2 can be written in terms of\nφi,j,2(x, yt, yt−1) = 1[yt=i] 1[yt−1=j] Therefore, the prediction can be written as\nhw(x) = argmax\ny∈Y\nr\nX\nt=1\n⟨w, φ(x, yt, yt−1)⟩ (17.4)\nIn the following we derive a dynamic programming procedure that solves every\nproblem of the form given in Equation (17.4) The procedure will maintain a\nmatrix M ∈Rq,r such that\nMs,τ =\nmax\n(y1,...,yτ ):yτ =s\nτ\nX\nt=1\n⟨w, φ(x, yt, yt−1)⟩ Clearly, the maximum of ⟨w, Ψ(x, y)⟩equals maxs Ms,r Furthermore, we can\ncalculate M in a recursive manner:\nMs,τ = max\ns′\n(Ms′,τ−1 + ⟨w, φ(x, s, s′)⟩) (17.5)\nThis yields the following procedure:\nDynamic Programming for Calculating hw(x) as Given\nin Equation (17.4)\ninput: a matrix x ∈Rn,r and a vector w\ninitialize:\nforeach s ∈[q]\nMs,1 = ⟨w, φ(x, s, −1)⟩\nfor τ = 2, , r\nforeach s ∈[q]\nset Ms,τ as in Equation (17.5)\nset Is,τ to be the s′ that maximizes Equation (17.5)\nset yt = argmaxs Ms,r\nfor τ = r, r −1, , 2\nset yτ−1 = Iyτ ,τ\noutput: y = (y1, , yr)\n17.4\nRanking\nRanking is the problem of ordering a set of instances according to their “rele-\nvance.” A typical application is ordering results of a search engine according to\ntheir relevance to the query Another example is a system that monitors elec-\ntronic transactions and should alert for possible fraudulent transactions",
      "word_count": 249,
      "source_page": 238,
      "start_position": 85644,
      "end_position": 85892,
      "sentences_count": 11
    },
    {
      "chunk_id": 382,
      "text": "17.4 Ranking\n239\nX of arbitrary length A ranking hypothesis, h, is a function that receives a\nsequence of instances ¯x = (x1, , xr) ∈X ∗, and returns a permutation of [r] It is more convenient to let the output of h be a vector y ∈Rr, where by\nsorting the elements of y we obtain the permutation over [r] We denote by\nπ(y) the permutation over [r] induced by y For example, for r = 5, the vector\ny = (2, 1, 6, −1, 0.5) induces the permutation π(y) = (4, 3, 5, 1, 2) That is,\nif we sort y in an ascending order, then we obtain the vector (−1, 0.5, 1, 2, 6) Now, π(y)i is the position of yi in the sorted vector (−1, 0.5, 1, 2, 6) This\nnotation reﬂects that the top-ranked instances are those that achieve the highest\nvalues in π(y) In the notation of our PAC learning model, the examples domain is Z =\nS∞\nr=1(X r × Rr), and the hypothesis class, H, is some set of ranking hypotheses We next turn to describe loss functions for ranking There are many possible ways\nto deﬁne such loss functions, and here we list a few examples In all the examples\nwe deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞\nr=1(Rr × Rr) →R+ • 0–1 Ranking loss: ∆(y′, y) is zero if y and y′ induce exactly the same\nranking and ∆(y′, y) = 1 otherwise",
      "word_count": 249,
      "source_page": 239,
      "start_position": 85922,
      "end_position": 86170,
      "sentences_count": 14
    },
    {
      "chunk_id": 383,
      "text": "In all the examples\nwe deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞\nr=1(Rr × Rr) →R+ • 0–1 Ranking loss: ∆(y′, y) is zero if y and y′ induce exactly the same\nranking and ∆(y′, y) = 1 otherwise That is, ∆(y′, y) = 1[π(y′)̸=π(y)] Such\na loss function is almost never used in practice as it does not distinguish\nbetween the case in which π(y′) is almost equal to π(y) and the case in\nwhich π(y′) is completely diﬀerent from π(y) • Kendall-Tau Loss: We count the number of pairs (i, j) that are in diﬀerent\norder in the two permutations This can be written as\n∆(y′, y) =\n2\nr(r −1)\nr−1\nX\ni=1\nr\nX\nj=i+1\n1[sign(y′\ni−y′\nj)̸=sign(yi−yj)] This loss function is more useful than the 0–1 loss as it reﬂects the level of\nsimilarity between the two rankings • Normalized Discounted Cumulative Gain (NDCG): This measure em-\nphasizes the correctness at the top of the list by using a monotonically\nnondecreasing discount function D : N →R+ We ﬁrst deﬁne a discounted\ncumulative gain measure:\nG(y′, y) =\nr\nX\ni=1\nD(π(y′)i) yi In words, if we interpret yi as a score of the “true relevance” of item i, then\nwe take a weighted sum of the relevance of the elements, while the weight\nof yi is determined on the basis of the position of i in π(y′)",
      "word_count": 238,
      "source_page": 239,
      "start_position": 86127,
      "end_position": 86364,
      "sentences_count": 10
    },
    {
      "chunk_id": 384,
      "text": "240\nMulticlass, Ranking, and Complex Prediction Problems\nWe can easily see that ∆(y′, y) ∈[0, 1] and that ∆(y′, y) = 0 whenever\nπ(y′) = π(y) A typical way to deﬁne the discount function is by\nD(i) =\n(\n1\nlog2(r−i+2)\nif i ∈{r −k + 1, , r}\n0\notherwise\nwhere k < r is a parameter This means that we care more about elements\nthat are ranked higher, and we completely ignore elements that are not at\nthe top-k ranked elements The NDCG measure is often used to evaluate\nthe performance of search engines since in such applications it makes sense\ncompletely to ignore elements that are not at the top of the ranking Once we have a hypothesis class and a ranking loss function, we can learn a\nranking function using the ERM rule However, from the computational point of\nview, the resulting optimization problem might be hard to solve We next discuss\nhow to learn linear predictors for ranking 17.4.1\nLinear Predictors for Ranking\nA natural way to deﬁne a ranking function is by projecting the instances onto\nsome vector w and then outputting the resulting scalars as our representation\nof the ranking function That is, assuming that X ⊂Rd, for every w ∈Rd we\ndeﬁne a ranking function\nhw((x1, , xr)) = (⟨w, x1⟩, , ⟨w, xr⟩)",
      "word_count": 222,
      "source_page": 240,
      "start_position": 86425,
      "end_position": 86646,
      "sentences_count": 12
    },
    {
      "chunk_id": 385,
      "text": ", xr)) = (⟨w, x1⟩, , ⟨w, xr⟩) (17.6)\nAs we discussed in Chapter 16, we can also apply a feature mapping that maps\ninstances into some feature space and then takes the inner products with w in the\nfeature space For simplicity, we focus on the simpler form as in Equation (17.6) Given some W ⊂Rd, we can now deﬁne the hypothesis class HW = {hw :\nw ∈W} Once we have deﬁned this hypothesis class, and have chosen a ranking\nloss function, we can apply the ERM rule as follows: Given a training set, S =\n(¯x1, y1), , (¯xm, ym), where each (¯xi, yi) is in (X × R)ri, for some ri ∈N, we\nshould search w ∈W that minimizes the empirical loss, Pm\ni=1 ∆(hw(¯xi), yi) As in the case of binary classiﬁcation, for many loss functions this problem is\ncomputationally hard, and we therefore turn to describe convex surrogate loss\nfunctions We describe the surrogates for the Kendall tau loss and for the NDCG\nloss A Hinge Loss for the Kendall Tau Loss Function:\nWe can think of the Kendall tau loss as an average of 0−1 losses for each pair In particular, for every (i, j) we can rewrite\n1[sign(y′\ni−y′\nj)̸=sign(yi−yj)] = 1[sign(yi−yj)(y′\ni−y′\nj)≤0].",
      "word_count": 212,
      "source_page": 240,
      "start_position": 86639,
      "end_position": 86850,
      "sentences_count": 11
    },
    {
      "chunk_id": 386,
      "text": "17.4 Ranking\n241\nIn our case, y′\ni −y′\nj = ⟨w, xi −xj⟩ It follows that we can use the hinge loss upper\nbound as follows:\n1[sign(yi−yj)(y′\ni−y′\nj)≤0] ≤max {0, 1 −sign (yi −yj) ⟨w, xi −xj⟩} Taking the average over the pairs we obtain the following surrogate convex loss\nfor the Kendall tau loss function:\n∆(hw(¯x), y) ≤\n2\nr(r −1)\nr−1\nX\ni=1\nr\nX\nj=i+1\nmax {0, 1 −sign(yi −yj) ⟨w, xi −xj⟩} The right-hand side is convex with respect to w and upper bounds the Kendall\ntau loss It is also a ρ-Lipschitz function with parameter ρ ≤maxi,j ∥xi −xj∥ A Hinge Loss for the NDCG Loss Function:\nThe NDCG loss function depends on the predicted ranking vector y′ ∈Rr via\nthe permutation it induces To derive a surrogate loss function we ﬁrst make\nthe following observation Let V be the set of all permutations of [r] encoded as\nvectors; namely, each v ∈V is a vector in [r]r such that for all i ̸= j we have\nvi ̸= vj Then (see Exercise 4),\nπ(y′) = argmax\nv∈V\nr\nX\ni=1\nvi y′\ni (17.7)\nLet us denote Ψ(¯x, v) = Pr\ni=1 vixi; it follows that\nπ(hw(¯x)) = argmax\nv∈V\nr\nX\ni=1\nvi⟨w, xi⟩\n= argmax\nv∈V\n*\nw,\nr\nX\ni=1\nvixi\n+\n= argmax\nv∈V\n⟨w, Ψ(¯x, v)⟩",
      "word_count": 229,
      "source_page": 241,
      "start_position": 86851,
      "end_position": 87079,
      "sentences_count": 10
    },
    {
      "chunk_id": 387,
      "text": "Then (see Exercise 4),\nπ(y′) = argmax\nv∈V\nr\nX\ni=1\nvi y′\ni (17.7)\nLet us denote Ψ(¯x, v) = Pr\ni=1 vixi; it follows that\nπ(hw(¯x)) = argmax\nv∈V\nr\nX\ni=1\nvi⟨w, xi⟩\n= argmax\nv∈V\n*\nw,\nr\nX\ni=1\nvixi\n+\n= argmax\nv∈V\n⟨w, Ψ(¯x, v)⟩ On the basis of this observation, we can use the generalized hinge loss for cost-\nsensitive multiclass classiﬁcation as a surrogate loss function for the NDCG loss\nas follows:\n∆(hw(¯x), y) ≤∆(hw(¯x), y) + ⟨w, Ψ(¯x, π(hw(¯x)))⟩−⟨w, Ψ(¯x, π(y))⟩\n≤max\nv∈V\n[∆(v, y) + ⟨w, Ψ(¯x, v)⟩−⟨w, Ψ(¯x, π(y))⟩]\n= max\nv∈V\n\"\n∆(v, y) +\nr\nX\ni=1\n(vi −π(y)i) ⟨w, xi⟩\n# (17.8)\nThe right-hand side is a convex function with respect to w We can now solve the learning problem using SGD as described in Section 17.2.5 The main computational bottleneck is calculating a subgradient of the loss func-\ntion, which is equivalent to ﬁnding v that achieves the maximum in Equa-\ntion (17.8) (see Claim 14.6) Using the deﬁnition of the NDCG loss, this is",
      "word_count": 181,
      "source_page": 241,
      "start_position": 87028,
      "end_position": 87208,
      "sentences_count": 7
    },
    {
      "chunk_id": 388,
      "text": "242\nMulticlass, Ranking, and Complex Prediction Problems\nequivalent to solving the problem\nargmin\nv∈V\nr\nX\ni=1\n(αivi + βi D(vi)),\nwhere αi = −⟨w, xi⟩and βi = yi/G(y, y) We can think of this problem a little\nbit diﬀerently by deﬁning a matrix A ∈Rr,r where\nAi,j = jαi + D(j) βi Now, let us think about each j as a “worker,” each i as a “task,” and Ai,j as\nthe cost of assigning task i to worker j With this view, the problem of ﬁnding\nv becomes the problem of ﬁnding an assignment of the tasks to workers of\nminimal cost This problem is called “the assignment problem” and can be solved\neﬃciently One particular algorithm is the “Hungarian method” (Kuhn 1955) Another way to solve the assignment problem is using linear programming To\ndo so, let us ﬁrst write the assignment problem as\nargmin\nB∈Rr,r\n+\nr\nX\ni,j=1\nAi,jBi,j\n(17.9)\ns.t ∀i ∈[r],\nr\nX\nj=1\nBi,j = 1\n∀j ∈[r],\nr\nX\ni=1\nBi,j = 1\n∀i, j,\nBi,j ∈{0, 1}\nA matrix B that satisﬁes the constraints in the preceding optimization problem\nis called a permutation matrix This is because the constraints guarantee that\nthere is at most a single entry of each row that equals 1 and a single entry of each\ncolumn that equals 1 Therefore, the matrix B corresponds to the permutation\nv ∈V deﬁned by vi = j for the single index j that satisﬁes Bi,j = 1",
      "word_count": 248,
      "source_page": 242,
      "start_position": 87209,
      "end_position": 87456,
      "sentences_count": 11
    },
    {
      "chunk_id": 389,
      "text": "This is because the constraints guarantee that\nthere is at most a single entry of each row that equals 1 and a single entry of each\ncolumn that equals 1 Therefore, the matrix B corresponds to the permutation\nv ∈V deﬁned by vi = j for the single index j that satisﬁes Bi,j = 1 The preceding optimization is still not a linear program because of the com-\nbinatorial constraint Bi,j ∈{0, 1} However, as it turns out, this constraint is\nredundant – if we solve the optimization problem while simply omitting the\ncombinatorial constraint, then we are still guaranteed that there is an optimal\nsolution that will satisfy this constraint This is formalized later Denote ⟨A, B⟩= P\ni,j Ai,jBi,j Then, Equation (17.9) is the problem of mini-\nmizing ⟨A, B⟩such that B is a permutation matrix A matrix B ∈Rr,r is called doubly stochastic if all elements of B are non-\nnegative, the sum of each row of B is 1, and the sum of each column of B is 1 Therefore, solving Equation (17.9) without the constraints Bi,j ∈{0, 1} is the\nproblem\nargmin\nB∈Rr,r ⟨A, B⟩s.t B is a doubly stochastic matrix (17.10)",
      "word_count": 197,
      "source_page": 242,
      "start_position": 87402,
      "end_position": 87598,
      "sentences_count": 11
    },
    {
      "chunk_id": 390,
      "text": "17.5 Bipartite Ranking and Multivariate Performance Measures\n243\nThe following claim states that every doubly stochastic matrix is a convex\ncombination of permutation matrices claim 17.3 ((Birkhoﬀ1946, Von Neumann 1953))\nThe set of doubly stochastic\nmatrices in Rr,r is the convex hull of the set of permutation matrices in Rr,r On the basis of the claim, we easily obtain the following:\nlemma 17.4\nThere exists an optimal solution of Equation (17.10) that is also\nan optimal solution of Equation (17.9) Proof\nLet B be a solution of Equation (17.10) Then, by Claim 17.3, we can\nwrite B = P\ni γiCi, where each Ci is a permutation matrix, each γi > 0, and\nP\ni γi = 1 Since all the Ci are also doubly stochastic, we clearly have that\n⟨A, B⟩≤⟨A, Ci⟩for every i We claim that there is some i for which ⟨A, B⟩=\n⟨A, Ci⟩ This must be true since otherwise, if for every i ⟨A, B⟩< ⟨A, Ci⟩, we\nwould have that\n⟨A, B⟩=\n*\nA,\nX\ni\nγiCi\n+\n=\nX\ni\nγi⟨A, Ci⟩>\nX\ni\nγi⟨A, B⟩= ⟨A, B⟩,\nwhich cannot hold We have thus shown that some permutation matrix, Ci,\nsatisﬁes ⟨A, B⟩= ⟨A, Ci⟩ But, since for every other permutation matrix C we\nhave ⟨A, B⟩≤⟨A, C⟩we conclude that Ci is an optimal solution of both Equa-\ntion (17.9) and Equation (17.10) 17.5\nBipartite Ranking and Multivariate Performance Measures\nIn the previous section we described the problem of ranking",
      "word_count": 247,
      "source_page": 243,
      "start_position": 87599,
      "end_position": 87845,
      "sentences_count": 11
    },
    {
      "chunk_id": 391,
      "text": "But, since for every other permutation matrix C we\nhave ⟨A, B⟩≤⟨A, C⟩we conclude that Ci is an optimal solution of both Equa-\ntion (17.9) and Equation (17.10) 17.5\nBipartite Ranking and Multivariate Performance Measures\nIn the previous section we described the problem of ranking We used a vector\ny ∈Rr for representing an order over the elements x1, , xr If all elements in y\nare diﬀerent from each other, then y speciﬁes a full order over [r] However, if two\nelements of y attain the same value, yi = yj for i ̸= j, then y can only specify a\npartial order over [r] In such a case, we say that xi and xj are of equal relevance\naccording to y In the extreme case, y ∈{±1}r, which means that each xi is\neither relevant or nonrelevant This setting is often called “bipartite ranking.” For\nexample, in the fraud detection application mentioned in the previous section,\neach transaction is labeled as either fraudulent (yi = 1) or benign (yi = −1) Seemingly, we can solve the bipartite ranking problem by learning a binary\nclassiﬁer, applying it on each instance, and putting the positive ones at the top\nof the ranked list However, this may lead to poor results as the goal of a binary\nlearner is usually to minimize the zero-one loss (or some surrogate of it), while the\ngoal of a ranker might be signiﬁcantly diﬀerent To illustrate this, consider again\nthe problem of fraud detection",
      "word_count": 249,
      "source_page": 243,
      "start_position": 87801,
      "end_position": 88049,
      "sentences_count": 12
    },
    {
      "chunk_id": 392,
      "text": "244\nMulticlass, Ranking, and Complex Prediction Problems\nproblem stems from the inadequacy of the zero-one loss for what we are really\ninterested in A more adequate performance measure should take into account\nthe predictions over the entire set of instances For example, in the previous\nsection we have deﬁned the NDCG loss, which emphasizes the correctness of the\ntop-ranked items In this section we describe additional loss functions that are\nspeciﬁcally adequate for bipartite ranking problems As in the previous section, we are given a sequence of instances, ¯x = (x1, , xr),\nand we predict a ranking vector y′ ∈Rr The feedback vector is y ∈{±1}r We\ndeﬁne a loss that depends on y′ and y and depends on a threshold θ ∈R This\nthreshold transforms the vector y′ ∈Rr into the vector (sign(y′\ni−θ), , sign(y′\nr−\nθ)) ∈{±1}r Usually, the value of θ is set to be 0 However, as we will see, we\nsometimes set θ while taking into account additional constraints on the problem The loss functions we deﬁne in the following depend on the following 4 num-\nbers:\nTrue positives: a = |{i : yi = +1 ∧sign(y′\ni −θ) = +1}|\nFalse positives: b = |{i : yi = −1 ∧sign(y′\ni −θ) = +1}|\nFalse negatives: c = |{i : yi = +1 ∧sign(y′\ni −θ) = −1}|\nTrue negatives: d = |{i : yi = −1 ∧sign(y′\ni −θ) = −1}|\n(17.11)\nThe recall (a.k.a",
      "word_count": 244,
      "source_page": 244,
      "start_position": 88097,
      "end_position": 88340,
      "sentences_count": 13
    },
    {
      "chunk_id": 393,
      "text": "However, as we will see, we\nsometimes set θ while taking into account additional constraints on the problem The loss functions we deﬁne in the following depend on the following 4 num-\nbers:\nTrue positives: a = |{i : yi = +1 ∧sign(y′\ni −θ) = +1}|\nFalse positives: b = |{i : yi = −1 ∧sign(y′\ni −θ) = +1}|\nFalse negatives: c = |{i : yi = +1 ∧sign(y′\ni −θ) = −1}|\nTrue negatives: d = |{i : yi = −1 ∧sign(y′\ni −θ) = −1}|\n(17.11)\nThe recall (a.k.a sensitivity) of a prediction vector is the fraction of true\npositives y′ “catches,” namely,\na\na+c The precision is the fraction of correct\npredictions among the positive labels we predict, namely,\na\na+b The speciﬁcity\nis the fraction of true negatives that our predictor “catches,” namely,\nd\nd+b Note that as we decrease θ the recall increases (attaining the value 1 when\nθ = −∞) On the other hand, the precision and the speciﬁcity usually decrease\nas we decrease θ Therefore, there is a tradeoﬀbetween precision and recall, and\nwe can control it by changing θ The loss functions deﬁned in the following use\nvarious techniques for combining both the precision and recall • Averaging sensitivity and speciﬁcity: This measure is the average of the\nsensitivity and speciﬁcity, namely, 1\n2\n\u0010\na\na+c +\nd\nd+b\n\u0011 This is also the accuracy\non positive examples averaged with the accuracy on negative examples",
      "word_count": 246,
      "source_page": 244,
      "start_position": 88248,
      "end_position": 88493,
      "sentences_count": 11
    },
    {
      "chunk_id": 394,
      "text": "• Averaging sensitivity and speciﬁcity: This measure is the average of the\nsensitivity and speciﬁcity, namely, 1\n2\n\u0010\na\na+c +\nd\nd+b\n\u0011 This is also the accuracy\non positive examples averaged with the accuracy on negative examples Here, we set θ = 0 and the corresponding loss function is ∆(y′, y) =\n1 −1\n2\n\u0010\na\na+c +\nd\nd+b\n\u0011 • F1-score: The F1 score is the harmonic mean of the precision and recall:\n2\n1\nPrecision +\n1\nRecall Its maximal value (of 1) is obtained when both precision\nand recall are 1, and its minimal value (of 0) is obtained whenever one of\nthem is 0 (even if the other one is 1) The F1 score can be written using\nthe numbers a, b, c as follows; F1 =\n2a\n2a+b+c Again, we set θ = 0, and the\nloss function becomes ∆(y′, y) = 1 −F1 • Fβ-score: It is like F1 score, but we attach β2 times more importance to\nrecall than to precision, that is,\n1+β2\n1\nPrecision +β2\n1\nRecall It can also be written as",
      "word_count": 187,
      "source_page": 244,
      "start_position": 88454,
      "end_position": 88640,
      "sentences_count": 9
    },
    {
      "chunk_id": 395,
      "text": "17.5 Bipartite Ranking and Multivariate Performance Measures\n245\nFβ =\n(1+β2)a\n(1+β2)a+b+β2c Again, we set θ = 0, and the loss function becomes\n∆(y′, y) = 1 −Fβ • Recall at k: We measure the recall while the prediction must contain at most\nk positive labels That is, we should set θ so that a + b ≤k This is conve-\nnient, for example, in the application of a fraud detection system, where a\nbank employee can only handle a small number of suspicious transactions • Precision at k: We measure the precision while the prediction must contain\nat least k positive labels That is, we should set θ so that a + b ≥k The measures deﬁned previously are often referred to as multivariate perfor-\nmance measures Note that these measures are highly diﬀerent from the average\nzero-one loss, which in the preceding notation equals\nb+d\na+b+c+d In the aforemen-\ntioned example of fraud detection, when 99.9% of the examples are negatively\nlabeled, the zero-one loss of predicting that all the examples are negatives is\n0.1% In contrast, the recall of such prediction is 0 and hence the F1 score is also\n0, which means that the corresponding loss will be 1 17.5.1\nLinear Predictors for Bipartite Ranking\nWe next describe how to train linear predictors for bipartite ranking As in the\nprevious section, a linear predictor for ranking is deﬁned to be\nhw(¯x) = (⟨w, x1⟩, , ⟨w, xr⟩)",
      "word_count": 241,
      "source_page": 245,
      "start_position": 88641,
      "end_position": 88881,
      "sentences_count": 14
    },
    {
      "chunk_id": 396,
      "text": "As in the\nprevious section, a linear predictor for ranking is deﬁned to be\nhw(¯x) = (⟨w, x1⟩, , ⟨w, xr⟩) The corresponding loss function is one of the multivariate performance measures\ndescribed before The loss function depends on y′ = hw(¯x) via the binary vector\nit induces, which we denote by\nb(y′) = (sign(y′\n1 −θ), , sign(y′\nr −θ)) ∈{±1}r (17.12)\nAs in the previous section, to facilitate an eﬃcient algorithm we derive a convex\nsurrogate loss function on ∆ The derivation is similar to the derivation of the\ngeneralized hinge loss for the NDCG ranking loss, as described in the previous\nsection Our ﬁrst observation is that for all the values of θ deﬁned before, there is some\nV ⊆{±1}r such that b(y′) can be rewritten as\nb(y′) = argmax\nv∈V\nr\nX\ni=1\nviy′\ni (17.13)\nThis is clearly true for the case θ = 0 if we choose V = {±1}r The two measures\nfor which θ is not taken to be 0 are precision at k and recall at k For precision\nat k we can take V to be the set V≥k, containing all vectors in {±1}r whose\nnumber of ones is at least k For recall at k, we can take V to be V≤k, which is\ndeﬁned analogously See Exercise 5.",
      "word_count": 220,
      "source_page": 245,
      "start_position": 88861,
      "end_position": 89080,
      "sentences_count": 13
    },
    {
      "chunk_id": 397,
      "text": "246\nMulticlass, Ranking, and Complex Prediction Problems\nOnce we have deﬁned b as in Equation (17.13), we can easily derive a convex\nsurrogate loss as follows Assuming that y ∈V , we have that\n∆(hw(¯x), y) = ∆(b(hw(¯x)), y)\n≤∆(b(hw(¯x)), y) +\nr\nX\ni=1\n(bi(hw(¯x)) −yi)⟨w, xi⟩\n≤max\nv∈V\n\"\n∆(v, y) +\nr\nX\ni=1\n(vi −yi) ⟨w, xi⟩\n# (17.14)\nThe right-hand side is a convex function with respect to w We can now solve the learning problem using SGD as described in Section 17.2.5 The main computational bottleneck is calculating a subgradient of the loss func-\ntion, which is equivalent to ﬁnding v that achieves the maximum in Equa-\ntion (17.14) (see Claim 14.6) In the following we describe how to ﬁnd this maximizer eﬃciently for any\nperformance measure that can be written as a function of the numbers a, b, c, d\ngiven in Equation (17.11), and for which the set V contains all elements in {±1}r\nfor which the values of a, b satisfy some constraints For example, for “recall at\nk” the set V is all vectors for which a + b ≤k The idea is as follows For any a, b ∈[r], let\n¯Ya,b = {v : |{i : vi = 1 ∧yi = 1}| = a ∧|{i : vi = 1 ∧yi = −1}| = b } Any vector v ∈V falls into ¯Ya,b for some a, b ∈[r]",
      "word_count": 238,
      "source_page": 246,
      "start_position": 89081,
      "end_position": 89318,
      "sentences_count": 10
    },
    {
      "chunk_id": 398,
      "text": "For any a, b ∈[r], let\n¯Ya,b = {v : |{i : vi = 1 ∧yi = 1}| = a ∧|{i : vi = 1 ∧yi = −1}| = b } Any vector v ∈V falls into ¯Ya,b for some a, b ∈[r] Furthermore, if ¯Ya,b ∩V\nis not empty for some a, b ∈[r] then ¯Ya,b ∩V = ¯Ya,b Therefore, we can search\nwithin each ¯Ya,b that has a nonempty intersection with V separately, and then\ntake the optimal value The key observation is that once we are searching only\nwithin ¯Ya,b, the value of ∆is ﬁxed so we only need to maximize the expression\nmax\nv∈¯\nYa,b\nr\nX\ni=1\nvi⟨w, xi⟩ Suppose the examples are sorted so that ⟨w, x1⟩≥· · · ≥⟨w, xr⟩ Then, it is\neasy to verify that we would like to set vi to be positive for the smallest indices\ni Doing this, with the constraint on a, b, amounts to setting vi = 1 for the a\ntop ranked positive examples and for the b top-ranked negative examples This\nyields the following procedure.",
      "word_count": 181,
      "source_page": 246,
      "start_position": 89276,
      "end_position": 89456,
      "sentences_count": 9
    },
    {
      "chunk_id": 399,
      "text": "17.6 Summary\n247\nSolving Equation (17.14)\ninput:\n(x1, , xr), (y1, , yr), w, V, ∆\nassumptions:\n∆is a function of a, b, c, d\nV contains all vectors for which f(a, b) = 1 for some function f\ninitialize:\nP = |{i : yi = 1}|, N = |{i : yi = −1}|\nµ = (⟨w, x1⟩, , ⟨w, xr⟩), α⋆= −∞\nsort examples so that µ1 ≥µ2 ≥· · · ≥µr\nlet i1, , iP be the (sorted) indices of the positive examples\nlet j1, , jN be the (sorted) indices of the negative examples\nfor a = 0, 1, , P\nc = P −a\nfor b = 0, 1, , N such that f(a, b) = 1\nd = N −b\ncalculate ∆using a, b, c, d\nset v1, , vr s.t vi1 = · · · = via = vj1 = · · · = vjb = 1\nand the rest of the elements of v equal −1\nset α = ∆+ Pr\ni=1 viµi\nif α ≥α⋆\nα⋆= α, v⋆= v\noutput v⋆\n17.6\nSummary\nMany real world supervised learning problems can be cast as learning a multiclass\npredictor We started the chapter by introducing reductions of multiclass learning\nto binary learning We then described and analyzed the family of linear predictors\nfor multiclass learning We have shown how this family can be used even if the\nnumber of classes is extremely large, as long as we have an adequate structure\non the problem",
      "word_count": 250,
      "source_page": 247,
      "start_position": 89457,
      "end_position": 89706,
      "sentences_count": 13
    },
    {
      "chunk_id": 400,
      "text": "248\nMulticlass, Ranking, and Complex Prediction Problems\nin (Daniely et al 2011, Daniely, Sabato & Shwartz 2012) See also Chapter 29,\nin which we analyze the sample complexity of multiclass learning Direct approaches to multiclass learning with linear predictors have been stud-\nied in (Vapnik 1998, Weston & Watkins 1999, Crammer & Singer 2001) In par-\nticular, the multivector construction is due to Crammer & Singer (2001) Collins (2000) has shown how to apply the Perceptron algorithm for structured\noutput problems See also Collins (2002) A related approach is discriminative\nlearning of conditional random ﬁelds; see Laﬀerty, McCallum & Pereira (2001) Structured output SVM has been studied in (Weston, Chapelle, Vapnik, Elisseeﬀ\n& Sch¨olkopf 2002, Taskar, Guestrin & Koller 2003, Tsochantaridis, Hofmann,\nJoachims & Altun 2004) The dynamic procedure we have presented for calculating the prediction hw(x)\nin the structured output section is similar to the forward-backward variables\ncalculated by the Viterbi procedure in HMMs (see, for instance, (Rabiner &\nJuang 1986)) More generally, solving the maximization problem in structured\noutput is closely related to the problem of inference in graphical models (see, for\nexample, Koller & Friedman (2009)) Chapelle, Le & Smola (2007) proposed to learn a ranking function with respect\nto the NDCG loss using ideas from structured output learning They also ob-\nserved that the maximization problem in the deﬁnition of the generalized hinge\nloss is equivalent to the assignment problem Agarwal & Roth (2005) analyzed the sample complexity of bipartite ranking",
      "word_count": 245,
      "source_page": 248,
      "start_position": 89786,
      "end_position": 90030,
      "sentences_count": 14
    },
    {
      "chunk_id": 401,
      "text": "They also ob-\nserved that the maximization problem in the deﬁnition of the generalized hinge\nloss is equivalent to the assignment problem Agarwal & Roth (2005) analyzed the sample complexity of bipartite ranking Joachims (2005) studied the applicability of structured output SVM to bipartite\nranking with multivariate performance measures 17.8\nExercises\n1 Consider a set S of examples in Rn×[k] for which there exist vectors µ1, , µk\nsuch that every example (x, y) ∈S falls within a ball centered at µy whose\nradius is r ≥1 Assume also that for every i ̸= j, ∥µi −µj∥≥4r Con-\nsider concatenating each instance by the constant 1 and then applying the\nmultivector construction, namely,\nΨ(x, y) = [\n0, , 0\n| {z }\n∈R(y−1)(n+1)\n, x1, , xn, 1\n|\n{z\n}\n∈Rn+1\n,\n0, , 0\n| {z }\n∈R(k−y)(n+1)\n] Show that there exists a vector w ∈Rk(n+1) such that ℓ(w, (x, y)) = 0 for\nevery (x, y) ∈S Hint: Observe that for every example (x, y) ∈S we can write x = µy + v for\nsome ∥v∥≤r Now, take w = [w1, , wk], where wi = [µi , −∥µi∥2/2] 2 Multiclass Perceptron: Consider the following algorithm:",
      "word_count": 202,
      "source_page": 248,
      "start_position": 89998,
      "end_position": 90199,
      "sentences_count": 17
    },
    {
      "chunk_id": 402,
      "text": "17.8 Exercises\n249\nMulticlass Batch Perceptron\nInput:\nA training set (x1, y1), , (xm, ym)\nA class-sensitive feature mapping Ψ : X × Y →Rd\nInitialize: w(1) = (0, , 0) ∈Rd\nFor t = 1, 2, If (∃i and y ̸= yi s.t ⟨w(t), Ψ(xi, yi)⟩≤⟨w(t), Ψ(xi, y)⟩) then\nw(t+1) = w(t) + Ψ(xi, yi) −Ψ(xi, y)\nelse\noutput w(t)\nProve the following:\ntheorem 17.5\nAssume that there exists w⋆such that for all i and for all\ny ̸= yi it holds that ⟨w⋆, Ψ(xi, yi)⟩≥⟨w⋆, Ψ(xi, y)⟩+1 Let R = maxi,y ∥Ψ(xi, yi)−\nΨ(xi, y)∥ Then, the multiclass Perceptron algorithm stops after at most (R∥w⋆∥)2\niterations, and when it stops it holds that ∀i ∈[m], yi = argmaxy ⟨w(t), Ψ(xi, y)⟩ 3 Generalize the dynamic programming procedure given in Section 17.3 for solv-\ning the maximization problem given in the deﬁnition of ˆh in the SGD proce-\ndure for multiclass prediction You can assume that ∆(y′, y) = Pr\nt=1 δ(y′\nt, yt)\nfor some arbitrary function δ 4 Prove that Equation (17.7) holds 5 Show that the two deﬁnitions of π as deﬁned in Equation (17.12) and Equa-\ntion (17.13) are indeed equivalent for all the multivariate performance mea-\nsures.",
      "word_count": 203,
      "source_page": 249,
      "start_position": 90200,
      "end_position": 90402,
      "sentences_count": 14
    },
    {
      "chunk_id": 403,
      "text": "18\nDecision Trees\nA decision tree is a predictor, h : X →Y, that predicts the label associated with\nan instance x by traveling from a root node of a tree to a leaf For simplicity\nwe focus on the binary classiﬁcation setting, namely, Y = {0, 1}, but decision\ntrees can be applied for other prediction problems as well At each node on the\nroot-to-leaf path, the successor child is chosen on the basis of a splitting of the\ninput space Usually, the splitting is based on one of the features of x or on a\npredeﬁned set of splitting rules A leaf contains a speciﬁc label An example of\na decision tree for the papayas example (described in Chapter 2) is given in the\nfollowing:\nColor not-tasty\nother\nSoftness not-tasty\nother\ntasty\ngives slightly to palm pressure\npale green to pale yellow\nTo check if a given papaya is tasty or not, the decision tree ﬁrst examines\nthe color of the Papaya If this color is not in the range pale green to pale\nyellow, then the tree immediately predicts that the papaya is not tasty without\nadditional tests Otherwise, the tree turns to examine the softness of the papaya If the softness level of the papaya is such that it gives slightly to palm pressure,\nthe decision tree predicts that the papaya is tasty",
      "word_count": 227,
      "source_page": 250,
      "start_position": 90403,
      "end_position": 90629,
      "sentences_count": 11
    },
    {
      "chunk_id": 404,
      "text": "18.1 Sample Complexity\n251\n18.1\nSample Complexity\nA popular splitting rule at internal nodes of the tree is based on thresholding the\nvalue of a single feature That is, we move to the right or left child of the node on\nthe basis of 1[xi<θ], where i ∈[d] is the index of the relevant feature and θ ∈R\nis the threshold In such cases, we can think of a decision tree as a splitting of\nthe instance space, X = Rd, into cells, where each leaf of the tree corresponds\nto one cell It follows that a tree with k leaves can shatter a set of k instances Hence, if we allow decision trees of arbitrary size, we obtain a hypothesis class\nof inﬁnite VC dimension Such an approach can easily lead to overﬁtting To avoid overﬁtting, we can rely on the minimum description length (MDL)\nprinciple described in Chapter 7, and aim at learning a decision tree that on one\nhand ﬁts the data well while on the other hand is not too large For simplicity, we will assume that X = {0, 1}d In other words, each instance\nis a vector of d bits In that case, thresholding the value of a single feature\ncorresponds to a splitting rule of the form 1[xi=1] for some i = [d]",
      "word_count": 220,
      "source_page": 251,
      "start_position": 90687,
      "end_position": 90906,
      "sentences_count": 10
    },
    {
      "chunk_id": 405,
      "text": "In other words, each instance\nis a vector of d bits In that case, thresholding the value of a single feature\ncorresponds to a splitting rule of the form 1[xi=1] for some i = [d] For instance,\nwe can model the “papaya decision tree” earlier by assuming that a papaya is\nparameterized by a two-dimensional bit vector x ∈{0, 1}2, where the bit x1\nrepresents whether the color is pale green to pale yellow or not, and the bit x2\nrepresents whether the softness is gives slightly to palm pressure or not With\nthis representation, the node Color can be replaced with 1[x1=1], and the node\nSoftness can be replaced with 1[x2=1] While this is a big simpliﬁcation, the\nalgorithms and analysis we provide in the following can be extended to more\ngeneral cases With the aforementioned simplifying assumption, the hypothesis class becomes\nﬁnite, but is still very large In particular, any classiﬁer from {0, 1}d to {0, 1}\ncan be represented by a decision tree with 2d leaves and depth of d + 1 (see\nExercise 1) Therefore, the VC dimension of the class is 2d, which means that\nthe number of examples we need to PAC learn the hypothesis class grows with\n2d Unless d is very small, this is a huge number of examples To overcome this obstacle, we rely on the MDL scheme described in Chapter 7 The underlying prior knowledge is that we should prefer smaller trees over larger\ntrees",
      "word_count": 245,
      "source_page": 251,
      "start_position": 90872,
      "end_position": 91116,
      "sentences_count": 13
    },
    {
      "chunk_id": 406,
      "text": "252\nDecision Trees\nOverall, there are d + 3 options, hence we need log2(d + 3) bits to describe each\nblock Assuming each internal node has two children,1 it is not hard to show that\nthis is a preﬁx-free encoding of the tree, and that the description length of a tree\nwith n nodes is (n + 1) log2(d + 3) By Theorem 7.7 we have that with probability of at least 1 −δ over a sample\nof size m, for every n and every decision tree h ∈H with n nodes it holds that\nLD(h) ≤LS(h) +\nr\n(n + 1) log2(d + 3) + log(2/δ)\n2m (18.1)\nThis bound performs a tradeoﬀ: on the one hand, we expect larger, more complex\ndecision trees to have a smaller training risk, LS(h), but the respective value of\nn will be larger On the other hand, smaller decision trees will have a smaller\nvalue of n, but LS(h) might be larger Our hope (or prior knowledge) is that we\ncan ﬁnd a decision tree with both low empirical risk, LS(h), and a number of\nnodes n not too high Our bound indicates that such a tree will have low true\nrisk, LD(h) 18.2\nDecision Tree Algorithms\nThe bound on LD(h) given in Equation (18.1) suggests a learning rule for decision\ntrees – search for a tree that minimizes the right-hand side of Equation (18.1)",
      "word_count": 233,
      "source_page": 252,
      "start_position": 91233,
      "end_position": 91465,
      "sentences_count": 8
    },
    {
      "chunk_id": 407,
      "text": "Our bound indicates that such a tree will have low true\nrisk, LD(h) 18.2\nDecision Tree Algorithms\nThe bound on LD(h) given in Equation (18.1) suggests a learning rule for decision\ntrees – search for a tree that minimizes the right-hand side of Equation (18.1) Unfortunately, it turns out that solving this problem is computationally hard.2\nConsequently, practical decision tree learning algorithms are based on heuristics\nsuch as a greedy approach, where the tree is constructed gradually, and locally\noptimal decisions are made at the construction of each node Such algorithms\ncannot guarantee to return the globally optimal decision tree but tend to work\nreasonably well in practice A general framework for growing a decision tree is as follows We start with\na tree with a single leaf (the root) and assign this leaf a label according to a\nmajority vote among all labels over the training set We now perform a series of\niterations On each iteration, we examine the eﬀect of splitting a single leaf We\ndeﬁne some “gain” measure that quantiﬁes the improvement due to this split Then, among all possible splits, we either choose the one that maximizes the\ngain and perform it, or choose not to split the leaf at all In the following we provide a possible implementation It is based on a popular\ndecision tree algorithm known as “ID3” (short for “Iterative Dichotomizer 3”)",
      "word_count": 231,
      "source_page": 252,
      "start_position": 91421,
      "end_position": 91651,
      "sentences_count": 12
    },
    {
      "chunk_id": 408,
      "text": "18.2 Decision Tree Algorithms\n253\nand therefore all splitting rules are of the form 1[xi=1] for some feature i ∈[d] We discuss the case of real valued features in Section 18.2.3 The algorithm works by recursive calls, with the initial call being ID3(S, [d]),\nand returns a decision tree In the pseudocode that follows, we use a call to a\nprocedure Gain(S, i), which receives a training set S and an index i and evaluates\nthe gain of a split of the tree according to the ith feature We describe several\ngain measures in Section 18.2.1 ID3(S, A)\nInput: training set S, feature subset A ⊆[d]\nif all examples in S are labeled by 1, return a leaf 1\nif all examples in S are labeled by 0, return a leaf 0\nif A = ∅, return a leaf whose value = majority of labels in S\nelse :\nLet j = argmaxi∈A Gain(S, i)\nif all examples in S have the same label\nReturn a leaf whose value = majority of labels in S\nelse\nLet T1 be the tree returned by ID3({(x, y) ∈S : xj = 1}, A \\ {j}) Let T2 be the tree returned by ID3({(x, y) ∈S : xj = 0}, A \\ {j}) Return the tree:\nxj = 1 T2\nT1\n18.2.1\nImplementations of the Gain Measure\nDiﬀerent algorithms use diﬀerent implementations of Gain(S, i) Here we present\nthree",
      "word_count": 236,
      "source_page": 253,
      "start_position": 91721,
      "end_position": 91956,
      "sentences_count": 10
    },
    {
      "chunk_id": 409,
      "text": "T2\nT1\n18.2.1\nImplementations of the Gain Measure\nDiﬀerent algorithms use diﬀerent implementations of Gain(S, i) Here we present\nthree We use the notation PS[F] to denote the probability that an event holds\nwith respect to the uniform distribution over S Train Error: The simplest deﬁnition of gain is the decrease in training error Formally, let C(a) = min{a, 1−a} Note that the training error before splitting on\nfeature i is C(PS[y = 1]), since we took a majority vote among labels Similarly,\nthe error after splitting on feature i is\nP\nS[xi = 1] C(P\nS[y = 1|xi = 1]) + P\nS[xi = 0]C(P\nS[y = 1|xi = 0]) Therefore, we can deﬁne Gain to be the diﬀerence between the two, namely,\nGain(S, i) := C(P\nS[y = 1])\n−\n\u0010\nP\nS[xi = 1] C(P\nS[y = 1|xi = 1]) + P\nS[xi = 0]C(P\nS[y = 1|xi = 0])\n\u0011\n.",
      "word_count": 155,
      "source_page": 253,
      "start_position": 91937,
      "end_position": 92091,
      "sentences_count": 8
    },
    {
      "chunk_id": 410,
      "text": "254\nDecision Trees\nInformation Gain: Another popular gain measure that is used in the ID3\nand C4.5 algorithms of Quinlan (1993) is the information gain The information\ngain is the diﬀerence between the entropy of the label before and after the split,\nand is achieved by replacing the function C in the previous expression by the\nentropy function,\nC(a) = −a log(a) −(1 −a) log(1 −a) Gini Index:\nYet another deﬁnition of a gain, which is used by the CART\nalgorithm of Breiman, Friedman, Olshen & Stone (1984), is the Gini index,\nC(a) = 2a(1 −a) Both the information gain and the Gini index are smooth and concave upper\nbounds of the train error These properties can be advantageous in some situa-\ntions (see, for example, Kearns & Mansour (1996)) 18.2.2\nPruning\nThe ID3 algorithm described previously still suﬀers from a big problem: The\nreturned tree will usually be very large Such trees may have low empirical risk,\nbut their true risk will tend to be high – both according to our theoretical\nanalysis, and in practice One solution is to limit the number of iterations of ID3,\nleading to a tree with a bounded number of nodes Another common solution is\nto prune the tree after it is built, hoping to reduce it to a much smaller tree,\nbut still with a similar empirical error",
      "word_count": 226,
      "source_page": 254,
      "start_position": 92092,
      "end_position": 92317,
      "sentences_count": 9
    },
    {
      "chunk_id": 411,
      "text": "One solution is to limit the number of iterations of ID3,\nleading to a tree with a bounded number of nodes Another common solution is\nto prune the tree after it is built, hoping to reduce it to a much smaller tree,\nbut still with a similar empirical error Theoretically, according to the bound in\nEquation (18.1), if we can make n much smaller without increasing LS(h) by\nmuch, we are likely to get a decision tree with a smaller true risk Usually, the pruning is performed by a bottom-up walk on the tree Each node\nmight be replaced with one of its subtrees or with a leaf, based on some bound\nor estimate of LD(h) (for example, the bound in Equation (18.1)) A pseudocode\nof a common template is given in the following Generic Tree Pruning Procedure\ninput:\nfunction f(T, m) (bound/estimate for the generalization error\nof a decision tree T, based on a sample of size m),\ntree T foreach node j in a bottom-up walk on T (from leaves to root):\nﬁnd T ′ which minimizes f(T ′, m), where T ′ is any of the following:\nthe current tree after replacing node j with a leaf 1 the current tree after replacing node j with a leaf 0 the current tree after replacing node j with its left subtree the current tree after replacing node j with its right subtree the current tree let T := T ′.",
      "word_count": 242,
      "source_page": 254,
      "start_position": 92269,
      "end_position": 92510,
      "sentences_count": 13
    },
    {
      "chunk_id": 412,
      "text": "18.3 Random Forests\n255\n18.2.3\nThreshold-Based Splitting Rules for Real-Valued Features\nIn the previous section we have described an algorithm for growing a decision\ntree assuming that the features are binary and the splitting rules are of the\nform 1[xi=1] We now extend this result to the case of real-valued features and\nthreshold-based splitting rules, namely, 1[xi<θ] Such splitting rules yield decision\nstumps, and we have studied them in Chapter 10 The basic idea is to reduce the problem to the case of binary features as\nfollows Let x1, , xm be the instances of the training set For each real-valued\nfeature i, sort the instances so that x1,i ≤· · · ≤xm,i Deﬁne a set of thresholds\nθ0,i, , θm+1,i such that θj,i ∈(xj,i, xj+1,i) (where we use the convention x0,i =\n−∞and xm+1,i = ∞) Finally, for each i and j we deﬁne the binary feature\n1[xi<θj,i] Once we have constructed these binary features, we can run the ID3\nprocedure described in the previous section It is easy to verify that for any\ndecision tree with threshold-based splitting rules over the original real-valued\nfeatures there exists a decision tree over the constructed binary features with\nthe same training error and the same number of nodes If the original number of real-valued features is d and the number of examples\nis m, then the number of constructed binary features becomes dm Calculating\nthe Gain of each feature might therefore take O(dm2) operations",
      "word_count": 243,
      "source_page": 255,
      "start_position": 92511,
      "end_position": 92753,
      "sentences_count": 14
    },
    {
      "chunk_id": 413,
      "text": "If the original number of real-valued features is d and the number of examples\nis m, then the number of constructed binary features becomes dm Calculating\nthe Gain of each feature might therefore take O(dm2) operations However, using\na more clever implementation, the runtime can be reduced to O(dm log(m)) The\nidea is similar to the implementation of ERM for decision stumps as described\nin Section 10.1.1 18.3\nRandom Forests\nAs mentioned before, the class of decision trees of arbitrary size has inﬁnite VC\ndimension We therefore restricted the size of the decision tree Another way\nto reduce the danger of overﬁtting is by constructing an ensemble of trees In\nparticular, in the following we describe the method of random forests, introduced\nby Breiman (2001) A random forest is a classiﬁer consisting of a collection of decision trees, where\neach tree is constructed by applying an algorithm A on the training set S and\nan additional random vector, θ, where θ is sampled i.i.d from some distribution The prediction of the random forest is obtained by a majority vote over the\npredictions of the individual trees To specify a particular random forest, we need to deﬁne the algorithm A and\nthe distribution over θ There are many ways to do this and here we describe one\nparticular option We generate θ as follows First, we take a random subsample\nfrom S with replacements; namely, we sample a new training set S′ of size m′\nusing the uniform distribution over S",
      "word_count": 250,
      "source_page": 255,
      "start_position": 92718,
      "end_position": 92967,
      "sentences_count": 15
    },
    {
      "chunk_id": 414,
      "text": "256\nDecision Trees\nthe algorithm A grows a decision tree (e.g., using the ID3 algorithm) based on\nthe sample S′, where at each splitting stage of the algorithm, the algorithm is\nrestricted to choosing a feature that maximizes Gain from the set It Intuitively,\nif k is small, this restriction may prevent overﬁtting 18.4\nSummary\nDecision trees are very intuitive predictors Typically, if a human programmer\ncreates a predictor it will look like a decision tree We have shown that the VC\ndimension of decision trees with k leaves is k and proposed the MDL paradigm\nfor learning decision trees The main problem with decision trees is that they\nare computationally hard to learn; therefore we described several heuristic pro-\ncedures for training them 18.5\nBibliographic Remarks\nMany algorithms for learning decision trees (such as ID3 and C4.5) have been\nderived by Quinlan (1986) The CART algorithm is due to Breiman et al (1984) Random forests were introduced by Breiman (2001) For additional reading we\nrefer the reader to (Hastie, Tibshirani & Friedman 2001, Rokach 2007) The proof of the hardness of training decision trees is given in Hyaﬁl & Rivest\n(1976) 18.6\nExercises\n1 1 Show that any binary classiﬁer h : {0, 1}d 7→{0, 1} can be implemented\nas a decision tree of height at most d + 1, with internal nodes of the form\n(xi = 0?) for some i ∈{1, , d} 2",
      "word_count": 237,
      "source_page": 256,
      "start_position": 93007,
      "end_position": 93243,
      "sentences_count": 17
    },
    {
      "chunk_id": 415,
      "text": "19\nNearest Neighbor\nNearest Neighbor algorithms are among the simplest of all machine learning\nalgorithms The idea is to memorize the training set and then to predict the\nlabel of any new instance on the basis of the labels of its closest neighbors in\nthe training set The rationale behind such a method is based on the assumption\nthat the features that are used to describe the domain points are relevant to\ntheir labelings in a way that makes close-by points likely to have the same label Furthermore, in some situations, even when the training set is immense, ﬁnding\na nearest neighbor can be done extremely fast (for example, when the training\nset is the entire Web and distances are based on links) Note that, in contrast with the algorithmic paradigms that we have discussed\nso far, like ERM, SRM, MDL, or RLM, that are determined by some hypothesis\nclass, H, the Nearest Neighbor method ﬁgures out a label on any test point\nwithout searching for a predictor within some predeﬁned class of functions In this chapter we describe Nearest Neighbor methods for classiﬁcation and\nregression problems We analyze their performance for the simple case of binary\nclassiﬁcation and discuss the eﬃciency of implementing these methods 19.1\nk Nearest Neighbors\nThroughout the entire chapter we assume that our instance domain, X, is en-\ndowed with a metric function ρ That is, ρ : X ×X →R is a function that returns\nthe distance between any two elements of X",
      "word_count": 249,
      "source_page": 258,
      "start_position": 93451,
      "end_position": 93699,
      "sentences_count": 9
    },
    {
      "chunk_id": 416,
      "text": "19.1\nk Nearest Neighbors\nThroughout the entire chapter we assume that our instance domain, X, is en-\ndowed with a metric function ρ That is, ρ : X ×X →R is a function that returns\nthe distance between any two elements of X For example, if X = Rd then ρ can\nbe the Euclidean distance, ρ(x, x′) = ∥x −x′∥=\nqPd\ni=1(xi −x′\ni)2 Let S = (x1, y1), , (xm, ym) be a sequence of training examples For each\nx ∈X, let π1(x), , πm(x) be a reordering of {1, , m} according to their\ndistance to x, ρ(x, xi) That is, for all i < m,\nρ(x, xπi(x)) ≤ρ(x, xπi+1(x)) For a number k, the k-NN rule for binary classiﬁcation is deﬁned as follows:\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 156,
      "source_page": 258,
      "start_position": 93657,
      "end_position": 93812,
      "sentences_count": 14
    },
    {
      "chunk_id": 417,
      "text": "19.2 Analysis\n259\nFigure 19.1 An illustration of the decision boundaries of the 1-NN rule The points\ndepicted are the sample points, and the predicted label of any new point will be the\nlabel of the sample point in the center of the cell it belongs to These cells are called a\nVoronoi Tessellation of the space k-NN\ninput: a training sample S = (x1, y1), , (xm, ym)\noutput: for every point x ∈X,\nreturn the majority label among {yπi(x) : i ≤k}\nWhen k = 1, we have the 1-NN rule:\nhS(x) = yπ1(x) A geometric illustration of the 1-NN rule is given in Figure 19.1 For regression problems, namely, Y = R, one can deﬁne the prediction to be\nthe average target of the k nearest neighbors That is, hS(x) = 1\nk\nPk\ni=1 yπi(x) More generally, for some function φ : (X × Y)k →Y, the k-NN rule with respect\nto φ is:\nhS(x) = φ\n\u0000(xπ1(x), yπ1(x)), , (xπk(x), yπk(x))\n\u0001 (19.1)\nIt is easy to verify that we can cast the prediction by majority of labels (for\nclassiﬁcation) or by the averaged target (for regression) as in Equation (19.1) by\nan appropriate choice of φ The generality can lead to other rules; for example, if\nY = R, we can take a weighted average of the targets according to the distance\nfrom x:\nhS(x) =\nk\nX\ni=1\nρ(x, xπi(x))\nPk\nj=1 ρ(x, xπj(x))\nyπi(x)",
      "word_count": 242,
      "source_page": 259,
      "start_position": 93813,
      "end_position": 94054,
      "sentences_count": 12
    },
    {
      "chunk_id": 418,
      "text": "260\nNearest Neighbor\ngoes to inﬁnity, and the rate of convergence depends on the underlying distribu-\ntion As we have argued in Section 7.4, this type of analysis is not satisfactory One would like to learn from ﬁnite training samples and to understand the gen-\neralization performance as a function of the size of such ﬁnite training sets and\nclear prior assumptions on the data distribution We therefore provide a ﬁnite-\nsample analysis of the 1-NN rule, showing how the error decreases as a function\nof m and how it depends on properties of the distribution We will also explain\nhow the analysis can be generalized to k-NN rules for arbitrary values of k In\nparticular, the analysis speciﬁes the number of examples required to achieve a\ntrue error of 2LD(h⋆) + ϵ, where h⋆is the Bayes optimal hypothesis, assuming\nthat the labeling rule is “well behaved” (in a sense we will deﬁne later) 19.2.1\nA Generalization Bound for the 1-NN Rule\nWe now analyze the true error of the 1-NN rule for binary classiﬁcation with\nthe 0-1 loss, namely, Y = {0, 1} and ℓ(h, (x, y)) = 1[h(x)̸=y] We also assume\nthroughout the analysis that X = [0, 1]d and ρ is the Euclidean distance We start by introducing some notation Let D be a distribution over X × Y",
      "word_count": 222,
      "source_page": 260,
      "start_position": 94093,
      "end_position": 94314,
      "sentences_count": 10
    },
    {
      "chunk_id": 419,
      "text": "We start by introducing some notation Let D be a distribution over X × Y Let DX denote the induced marginal distribution over X and let η : Rd →R be\nthe conditional probability1 over the labels, that is,\nη(x) = P[y = 1|x] Recall that the Bayes optimal rule (that is, the hypothesis that minimizes LD(h)\nover all functions) is\nh⋆(x) = 1[η(x)>1/2] We assume that the conditional probability function η is c-Lipschitz for some\nc > 0: Namely, for all x, x′ ∈X, |η(x)−η(x′)| ≤c ∥x−x′∥ In other words, this\nassumption means that if two vectors are close to each other then their labels\nare likely to be the same The following lemma applies the Lipschitzness of the conditional probability\nfunction to upper bound the true error of the 1-NN rule as a function of the\nexpected distance between each test instance and its nearest neighbor in the\ntraining set lemma 19.1\nLet X = [0, 1]d, Y = {0, 1}, and D be a distribution over X × Y\nfor which the conditional probability function, η, is a c-Lipschitz function Let\nS = (x1, y1), , (xm, ym) be an i.i.d sample and let hS be its corresponding\n1-NN hypothesis Let h⋆be the Bayes optimal rule for η Then,\nE\nS∼Dm[LD(hS)] ≤2 LD(h⋆) + c\nE\nS∼Dm,x∼D[∥x −xπ1(x)∥] 1 Formally, P[y = 1|x] = limδ→0\nD({(x′,1):x′∈B(x,δ)})\nD({(x′,y):x′∈B(x,δ),y∈Y}), where B(x, δ) is a ball of radius δ\ncentered around x.",
      "word_count": 242,
      "source_page": 260,
      "start_position": 94300,
      "end_position": 94541,
      "sentences_count": 14
    },
    {
      "chunk_id": 420,
      "text": "19.2 Analysis\n261\nProof\nSince LD(hS) = E(x,y)∼D[1[hS(x)̸=y]], we obtain that ES[LD(hS)] is the\nprobability to sample a training set S and an additional example (x, y), such\nthat the label of π1(x) is diﬀerent from y In other words, we can ﬁrst sample\nm unlabeled examples, Sx = (x1, , xm), according to DX , and an additional\nunlabeled example, x ∼DX , then ﬁnd π1(x) to be the nearest neighbor of x in\nSx, and ﬁnally sample y ∼η(x) and yπ1(x) ∼η(π1(x)) It follows that\nE\nS[LD(hS)] =\nE\nSx∼Dm\nX ,x∼DX ,y∼η(x),y′∼η(π1(x))[1[y̸=y′]]\n=\nE\nSx∼Dm\nX ,x∼DX\n\u0014\nP\ny∼η(x),y′∼η(π1(x))[y ̸= y′]\n\u0015 (19.2)\nWe next upper bound Py∼η(x),y′∼η(x′)[y ̸= y′] for any two domain points x, x′:\nP\ny∼η(x),y′∼η(x′)[y ̸= y′] = η(x′)(1 −η(x)) + (1 −η(x′))η(x)\n= (η(x) −η(x) + η(x′))(1 −η(x))\n+ (1 −η(x) + η(x) −η(x′))η(x)\n= 2η(x)(1 −η(x)) + (η(x) −η(x′))(2η(x) −1) Using |2η(x) −1| ≤1 and the assumption that η is c-Lipschitz, we obtain that\nthe probability is at most:\nP\ny∼η(x),y′∼η(x′)[y ̸= y′] ≤2η(x)(1 −η(x)) + c ∥x −x′∥ Plugging this into Equation (19.2) we conclude that\nE\nS[LD(hS)] ≤E\nx[2η(x)(1 −η(x))] + c E\nS,x[∥x −xπ1(x)∥] Finally, the error of the Bayes optimal classiﬁer is\nLD(h⋆) = E\nx[min{η(x), 1 −η(x)}] ≥E\nx[η(x)(1 −η(x))] Combining the preceding two inequalities concludes our proof The next step is to bound the expected distance between a random x and its\nclosest element in S We ﬁrst need the following general probability lemma",
      "word_count": 250,
      "source_page": 261,
      "start_position": 94542,
      "end_position": 94791,
      "sentences_count": 11
    },
    {
      "chunk_id": 421,
      "text": "262\nNearest Neighbor\nProof\nFrom the linearity of expectation, we can rewrite:\nE\nS\n\n\nX\ni:Ci∩S=∅\nP[Ci]\n\n=\nr\nX\ni=1\nP[Ci] E\nS\n\u0002\n1[Ci∩S=∅]\n\u0003 Next, for each i we have\nE\nS\n\u0002\n1[Ci∩S=∅]\n\u0003\n= P\nS[Ci ∩S = ∅] = (1 −P[Ci])m ≤e−P[Ci] m Combining the preceding two equations we get\nE\nS\n\n\nX\ni:Ci∩S=∅\nP[Ci]\n\n≤\nr\nX\ni=1\nP[Ci] e−P[Ci] m ≤r max\ni\nP[Ci] e−P[Ci] m Finally, by a standard calculus, maxa ae−ma ≤\n1\nme and this concludes the proof Equipped with the preceding lemmas we are now ready to state and prove the\nmain result of this section – an upper bound on the expected error of the 1-NN\nlearning rule theorem 19.3\nLet X = [0, 1]d, Y = {0, 1}, and D be a distribution over X ×Y\nfor which the conditional probability function, η, is a c-Lipschitz function Let\nhS denote the result of applying the 1-NN rule to a sample S ∼Dm Then,\nE\nS∼Dm[LD(hS)] ≤2 LD(h⋆) + 4 c\n√\nd m−\n1\nd+1 Proof\nFix some ϵ = 1/T, for some integer T, let r = T d and let C1, , Cr be the\ncover of the set X using boxes of length ϵ: Namely, for every (α1, , αd) ∈[T]d,\nthere exists a set Ci of the form {x : ∀j, xj ∈[(αj −1)/T, αj/T]}",
      "word_count": 239,
      "source_page": 262,
      "start_position": 94865,
      "end_position": 95103,
      "sentences_count": 11
    },
    {
      "chunk_id": 422,
      "text": "19.2 Analysis\n263\nSince the number of boxes is r = (1/ϵ)d we get that\nE\nS,x[∥x −xπ1(x)∥] ≤\n√\nd\n\u0010\n2d ϵ−d\nm e\n+ ϵ\n\u0011 Combining the preceding with Lemma 19.1 we obtain that\nE\nS[LD(hS)] ≤2 LD(h⋆) + c\n√\nd\n\u0010\n2d ϵ−d\nm e\n+ ϵ\n\u0011 Finally, setting ϵ = 2 m−1/(d+1) and noting that\n2d ϵ−d\nm e\n+ ϵ = 2d 2−d md/(d+1)\nm e\n+ 2 m−1/(d+1)\n= m−1/(d+1)(1/e + 2) ≤4m−1/(d+1)\nwe conclude our proof The theorem implies that if we ﬁrst ﬁx the data-generating distribution and\nthen let m go to inﬁnity, then the error of the 1-NN rule converges to twice the\nBayes error The analysis can be generalized to larger values of k, showing that\nthe expected error of the k-NN rule converges to (1 +\np\n8/k) times the error of\nthe Bayes classiﬁer This is formalized in Theorem 19.5, whose proof is left as a\nguided exercise 19.2.2\nThe “Curse of Dimensionality”\nThe upper bound given in Theorem 19.3 grows with c (the Lipschitz coeﬃcient\nof η) and with d, the Euclidean dimension of the domain set X In fact, it is easy\nto see that a necessary condition for the last term in Theorem 19.3 to be smaller\nthan ϵ is that m ≥(4 c\n√\nd/ϵ)d+1 That is, the size of the training set should\nincrease exponentially with the dimension",
      "word_count": 239,
      "source_page": 263,
      "start_position": 95204,
      "end_position": 95442,
      "sentences_count": 9
    },
    {
      "chunk_id": 423,
      "text": "In fact, it is easy\nto see that a necessary condition for the last term in Theorem 19.3 to be smaller\nthan ϵ is that m ≥(4 c\n√\nd/ϵ)d+1 That is, the size of the training set should\nincrease exponentially with the dimension The following theorem tells us that\nthis is not just an artifact of our upper bound, but, for some distributions, this\namount of examples is indeed necessary for learning with the NN rule theorem 19.4\nFor any c > 1, and every learning rule, L, there exists a\ndistribution over [0, 1]d × {0, 1}, such that η(x) is c-Lipschitz, the Bayes error of\nthe distribution is 0, but for sample sizes m ≤(c + 1)d/2, the true error of the\nrule L is greater than 1/4 Proof\nFix any values of c and d Let Gd\nc be the grid on [0, 1]d with distance of\n1/c between points on the grid That is, each point on the grid is of the form\n(a1/c, , ad/c) where ai is in {0, , c −1, c} Note that, since any two distinct\npoints on this grid are at least 1/c apart, any function η : GD\nC →[0, 1] is a\nc-Lipschitz function It follows that the set of all c-Lipschitz functions over Gd\nc\ncontains the set of all binary valued functions over that domain",
      "word_count": 230,
      "source_page": 263,
      "start_position": 95399,
      "end_position": 95628,
      "sentences_count": 11
    },
    {
      "chunk_id": 424,
      "text": "264\nNearest Neighbor\nThe exponential dependence on the dimension is known as the curse of di-\nmensionality As we saw, the 1-NN rule might fail if the number of examples is\nsmaller than Ω((c+1)d) Therefore, while the 1-NN rule does not restrict itself to\na predeﬁned set of hypotheses, it still relies on some prior knowledge – its success\ndepends on the assumption that the dimension and the Lipschitz constant of the\nunderlying distribution, η, are not too high 19.3\nEﬃcient Implementation*\nNearest Neighbor is a learning-by-memorization type of rule It requires the\nentire training data set to be stored, and at test time, we need to scan the entire\ndata set in order to ﬁnd the neighbors The time of applying the NN rule is\ntherefore Θ(d m) This leads to expensive computation at test time When d is small, several results from the ﬁeld of computational geometry have\nproposed data structures that enable to apply the NN rule in time o(dO(1) log(m)) However, the space required by these data structures is roughly mO(d), which\nmakes these methods impractical for larger values of d To overcome this problem, it was suggested to improve the search method by\nallowing an approximate search Formally, an r-approximate search procedure is\nguaranteed to retrieve a point within distance of at most r times the distance\nto the nearest neighbor Three popular approximate algorithms for NN are the\nkd-tree, balltrees, and locality-sensitive hashing (LSH)",
      "word_count": 240,
      "source_page": 264,
      "start_position": 95679,
      "end_position": 95918,
      "sentences_count": 12
    },
    {
      "chunk_id": 425,
      "text": "Formally, an r-approximate search procedure is\nguaranteed to retrieve a point within distance of at most r times the distance\nto the nearest neighbor Three popular approximate algorithms for NN are the\nkd-tree, balltrees, and locality-sensitive hashing (LSH) We refer the reader, for\nexample, to (Shakhnarovich, Darrell & Indyk 2006) 19.4\nSummary\nThe k-NN rule is a very simple learning algorithm that relies on the assumption\nthat “things that look alike must be alike.” We formalized this intuition using\nthe Lipschitzness of the conditional probability We have shown that with a suf-\nﬁciently large training set, the risk of the 1-NN is upper bounded by twice the\nrisk of the Bayes optimal rule We have also derived a lower bound that shows\nthe “curse of dimensionality” – the required sample size might increase expo-\nnentially with the dimension As a result, NN is usually performed in practice\nafter a dimensionality reduction preprocessing step We discuss dimensionality\nreduction techniques later on in Chapter 23 19.5\nBibliographic Remarks\nCover & Hart (1967) gave the ﬁrst analysis of 1-NN, showing that its risk con-\nverges to twice the Bayes optimal error under mild conditions Following a lemma\ndue to Stone (1977), Devroye & Gy¨orﬁ(1985) have shown that the k-NN rule",
      "word_count": 207,
      "source_page": 264,
      "start_position": 95881,
      "end_position": 96087,
      "sentences_count": 10
    },
    {
      "chunk_id": 426,
      "text": "19.6 Exercises\n265\nis consistent (with respect to the hypothesis class of all functions from Rd to\n{0, 1}) A good presentation of the analysis is given in the book of Devroye et al (1996) Here, we give a ﬁnite sample guarantee that explicitly underscores the\nprior assumption on the distribution See Section 7.4 for a discussion on con-\nsistency results Finally, Gottlieb, Kontorovich & Krauthgamer (2010) derived\nanother ﬁnite sample bound for NN that is more similar to VC bounds 19.6\nExercises\nIn this exercise we will prove the following theorem for the k-NN rule theorem 19.5\nLet X = [0, 1]d, Y = {0, 1}, and D be a distribution over X ×Y\nfor which the conditional probability function, η, is a c-Lipschitz function Let hS\ndenote the result of applying the k-NN rule to a sample S ∼Dm, where k ≥10 Let h⋆be the Bayes optimal hypothesis Then,\nE\nS[LD(hS)] ≤\n \n1 +\nr\n8\nk LD(h⋆) +\n\u0010\n6 c\n√\nd + k\n\u0011\nm−1/(d+1) 1 Prove the following lemma lemma 19.6\nLet C1, , Cr be a collection of subsets of some domain set,\nX Let S be a sequence of m points sampled i.i.d according to some probability\ndistribution, D over X Then, for every k ≥2,\nE\nS∼Dm\n\n\nX\ni:|Ci∩S|<k\nP[Ci]\n\n\n≤2rk\nm Hints:\n• Show that\nE\nS\n\n\nX\ni:|Ci∩S|<k\nP[Ci]\n\n=\nr\nX\ni=1\nP[Ci] P\nS [|Ci ∩S| < k]",
      "word_count": 248,
      "source_page": 265,
      "start_position": 96088,
      "end_position": 96335,
      "sentences_count": 20
    },
    {
      "chunk_id": 427,
      "text": "266\nNearest Neighbor\n2 We use the notation y ∼p as a shorthand for “y is a Bernoulli random variable\nwith expected value p.” Prove the following lemma:\nlemma 19.7\nLet k ≥10 and let Z1, , Zk be independent Bernoulli random\nvariables with P[Zi = 1] = pi Denote p = 1\nk\nP\ni pi and p′ = 1\nk\nPk\ni=1 Zi Show\nthat\nE\nZ1,...,Zk P\ny∼p[y ̸= 1[p′>1/2]] ≤\n \n1 +\nr\n8\nk P\ny∼p[y ̸= 1[p>1/2]] Hints:\nW.l.o.g assume that p ≤1/2 Then, Py∼p[y ̸= 1[p>1/2]] = p Let y′ = 1[p′>1/2] • Show that\nE\nZ1,...,Zk P\ny∼p[y ̸= y′] −p =\nP\nZ1,...,Zk[p′ > 1/2](1 −2p) • Use Chernoﬀ’s bound (Lemma B.3) to show that\nP[p′ > 1/2] ≤e−k p h( 1\n2p −1),\nwhere\nh(a) = (1 + a) log(1 + a) −a • To conclude the proof of the lemma, you can rely on the following inequality\n(without proving it): For every p ∈[0, 1/2] and k ≥10:\n(1 −2p) e−k p + k\n2 (log(2p)+1) ≤\nr\n8\nk p 3 Fix some p, p′ ∈[0, 1] and y′ ∈{0, 1} Show that\nP\ny∼p[y ̸= y′] ≤\nP\ny∼p′[y ̸= y′] + |p −p′| 4 Conclude the proof of the theorem according to the following steps:\n• As in the proof of Theorem 19.3, six some ϵ > 0 and let C1, , Cr be the\ncover of the set X using boxes of length ϵ",
      "word_count": 249,
      "source_page": 266,
      "start_position": 96425,
      "end_position": 96673,
      "sentences_count": 19
    },
    {
      "chunk_id": 428,
      "text": "Conclude the proof of the theorem according to the following steps:\n• As in the proof of Theorem 19.3, six some ϵ > 0 and let C1, , Cr be the\ncover of the set X using boxes of length ϵ For each x, x′ in the same\nbox we have ∥x −x′∥≤\n√\nd ϵ Otherwise, ∥x −x′∥≤2\n√\nd Show that\nE\nS[LD(hS)] ≤E\nS\n\n\nX\ni:|Ci∩S|<k\nP[Ci]\n\n\n+ max\ni\nP\nS,(x,y)\nh\nhS(x) ̸= y | ∀j ∈[k], ∥x −xπj(x)∥≤ϵ\n√\nd\ni (19.3)\n• Bound the ﬁrst summand using Lemma 19.6 • To bound the second summand, let us ﬁx S|x and x such that all the k\nneighbors of x in S|x are at distance of at most ϵ\n√\nd from x W.l.o.g\nassume that the k NN are x1, , xk Denote pi = η(xi) and let p =\n1\nk\nP\ni pi Use Exercise 3 to show that\nE\ny1,...,yj\nP\ny∼η(x)[hS(x) ̸= y] ≤\nE\ny1,...,yj P\ny∼p[hS(x) ̸= y] + |p −η(x)|.",
      "word_count": 178,
      "source_page": 266,
      "start_position": 96633,
      "end_position": 96810,
      "sentences_count": 11
    },
    {
      "chunk_id": 429,
      "text": "20\nNeural Networks\nAn artiﬁcial neural network is a model of computation inspired by the structure\nof neural networks in the brain In simpliﬁed models of the brain, it consists of\na large number of basic computing devices (neurons) that are connected to each\nother in a complex communication network, through which the brain is able to\ncarry out highly complex computations Artiﬁcial neural networks are formal\ncomputation constructs that are modeled after this computation paradigm Learning with neural networks was proposed in the mid-20th century It yields\nan eﬀective learning paradigm and has recently been shown to achieve cutting-\nedge performance on several learning tasks A neural network can be described as a directed graph whose nodes correspond\nto neurons and edges correspond to links between them Each neuron receives\nas input a weighted sum of the outputs of the neurons connected to its incoming\nedges We focus on feedforward networks in which the underlying graph does not\ncontain cycles In the context of learning, we can deﬁne a hypothesis class consisting of neural\nnetwork predictors, where all the hypotheses share the underlying graph struc-\nture of the network and diﬀer in the weights over edges As we will show in\nSection 20.3, every predictor over n variables that can be implemented in time\nT(n) can also be expressed as a neural network predictor of size O(T(n)2), where\nthe size of the network is the number of nodes in it",
      "word_count": 241,
      "source_page": 268,
      "start_position": 96941,
      "end_position": 97181,
      "sentences_count": 10
    },
    {
      "chunk_id": 430,
      "text": "In the context of learning, we can deﬁne a hypothesis class consisting of neural\nnetwork predictors, where all the hypotheses share the underlying graph struc-\nture of the network and diﬀer in the weights over edges As we will show in\nSection 20.3, every predictor over n variables that can be implemented in time\nT(n) can also be expressed as a neural network predictor of size O(T(n)2), where\nthe size of the network is the number of nodes in it It follows that the family\nof hypothesis classes of neural networks of polynomial size can suﬃce for all\npractical learning tasks, in which our goal is to learn predictors which can be\nimplemented eﬃciently Furthermore, in Section 20.4 we will show that the sam-\nple complexity of learning such hypothesis classes is also bounded in terms of the\nsize of the network Hence, it seems that this is the ultimate learning paradigm\nwe would want to adapt, in the sense that it both has a polynomial sample com-\nplexity and has the minimal approximation error among all hypothesis classes\nconsisting of eﬃciently implementable predictors The caveat is that the problem of training such hypothesis classes of neural net-\nwork predictors is computationally hard This will be formalized in Section 20.5 A widely used heuristic for training neural networks relies on the SGD frame-\nwork we studied in Chapter 14 There, we have shown that SGD is a successful\nlearner if the loss function is convex",
      "word_count": 245,
      "source_page": 268,
      "start_position": 97102,
      "end_position": 97346,
      "sentences_count": 9
    },
    {
      "chunk_id": 431,
      "text": "20.1 Feedforward Neural Networks\n269\nhope it will ﬁnd a reasonable solution (as happens to be the case in several\npractical tasks) In Section 20.6 we describe how to implement SGD for neural\nnetworks In particular, the most complicated operation is the calculation of the\ngradient of the loss function with respect to the parameters of the network We\npresent the backpropagation algorithm that eﬃciently calculates the gradient 20.1\nFeedforward Neural Networks\nThe idea behind neural networks is that many neurons can be joined together\nby communication links to carry out complex computations It is common to\ndescribe the structure of a neural network as a graph whose nodes are the neurons\nand each (directed) edge in the graph links the output of some neuron to the\ninput of another neuron We will restrict our attention to feedforward network\nstructures in which the underlying graph does not contain cycles A feedforward neural network is described by a directed acyclic graph, G =\n(V, E), and a weight function over the edges, w : E →R Nodes of the graph\ncorrespond to neurons Each single neuron is modeled as a simple scalar func-\ntion, σ : R →R We will focus on three possible functions for σ: the sign\nfunction, σ(a) = sign(a), the threshold function, σ(a) = 1[a>0], and the sig-\nmoid function, σ(a) = 1/(1 + exp(−a)), which is a smooth approximation to the\nthreshold function We call σ the “activation” function of the neuron",
      "word_count": 246,
      "source_page": 269,
      "start_position": 97394,
      "end_position": 97639,
      "sentences_count": 12
    },
    {
      "chunk_id": 432,
      "text": "We will focus on three possible functions for σ: the sign\nfunction, σ(a) = sign(a), the threshold function, σ(a) = 1[a>0], and the sig-\nmoid function, σ(a) = 1/(1 + exp(−a)), which is a smooth approximation to the\nthreshold function We call σ the “activation” function of the neuron Each edge\nin the graph links the output of some neuron to the input of another neuron The input of a neuron is obtained by taking a weighted sum of the outputs of\nall the neurons connected to it, where the weighting is according to w To simplify the description of the calculation performed by the network, we\nfurther assume that the network is organized in layers That is, the set of nodes\ncan be decomposed into a union of (nonempty) disjoint subsets, V = ·∪T\nt=0Vt,\nsuch that every edge in E connects some node in Vt−1 to some node in Vt, for\nsome t ∈[T] The bottom layer, V0, is called the input layer It contains n + 1\nneurons, where n is the dimensionality of the input space For every i ∈[n], the\noutput of neuron i in V0 is simply xi The last neuron in V0 is the “constant”\nneuron, which always outputs 1 We denote by vt,i the ith neuron of the tth layer\nand by ot,i(x) the output of vt,i when the network is fed with the input vector x",
      "word_count": 235,
      "source_page": 269,
      "start_position": 97591,
      "end_position": 97825,
      "sentences_count": 11
    },
    {
      "chunk_id": 433,
      "text": "270\nNeural Networks\nand\not+1,j(x) = σ (at+1,j(x)) That is, the input to vt+1,j is a weighted sum of the outputs of the neurons in Vt\nthat are connected to vt+1,j, where weighting is according to w, and the output\nof vt+1,j is simply the application of the activation function σ on its input Layers V1, , VT −1 are often called hidden layers The top layer, VT , is called\nthe output layer In simple prediction problems the output layer contains a single\nneuron whose output is the output of the network We refer to T as the number of layers in the network (excluding V0), or the\n“depth” of the network The size of the network is |V | The “width” of the\nnetwork is maxt |Vt| An illustration of a layered feedforward neural network of\ndepth 2, size 10, and width 5, is given in the following Note that there is a\nneuron in the hidden layer that has no incoming edges This neuron will output\nthe constant σ(0) v0,1\nx1\nv0,2\nx2\nv0,3\nx3\nv0,4\nconstant\nv1,1\nv1,2\nv1,3\nv1,4\nv1,5\nv2,1\nOutput\nHidden\nlayer\n(V1)\nInput\nlayer\n(V0)\nOutput\nlayer\n(V2)\n20.2\nLearning Neural Networks\nOnce we have speciﬁed a neural network by (V, E, σ, w), we obtain a function\nhV,E,σ,w : R|V0|−1 →R|VT | Any set of such functions can serve as a hypothesis\nclass for learning",
      "word_count": 234,
      "source_page": 270,
      "start_position": 97917,
      "end_position": 98150,
      "sentences_count": 14
    },
    {
      "chunk_id": 434,
      "text": "20.3 The Expressive Power of Neural Networks\n271\nThat is, the parameters specifying a hypothesis in the hypothesis class are the\nweights over the edges of the network We can now study the approximation error, estimation error, and optimization\nerror of such hypothesis classes In Section 20.3 we study the approximation\nerror of HV,E,σ by studying what type of functions hypotheses in HV,E,σ can\nimplement, in terms of the size of the underlying graph In Section 20.4 we\nstudy the estimation error of HV,E,σ, for the case of binary classiﬁcation (i.e.,\nVT = 1 and σ is the sign function), by analyzing its VC dimension Finally, in\nSection 20.5 we show that it is computationally hard to learn the class HV,E,σ,\neven if the underlying graph is small, and in Section 20.6 we present the most\ncommonly used heuristic for training HV,E,σ 20.3\nThe Expressive Power of Neural Networks\nIn this section we study the expressive power of neural networks, namely, what\ntype of functions can be implemented using a neural network More concretely,\nwe will ﬁx some architecture, V, E, σ, and will study what functions hypotheses\nin HV,E,σ can implement, as a function of the size of V We start the discussion with studying which type of Boolean functions (i.e.,\nfunctions from {±1}n to {±1}) can be implemented by HV,E,sign",
      "word_count": 222,
      "source_page": 271,
      "start_position": 98225,
      "end_position": 98446,
      "sentences_count": 8
    },
    {
      "chunk_id": 435,
      "text": "More concretely,\nwe will ﬁx some architecture, V, E, σ, and will study what functions hypotheses\nin HV,E,σ can implement, as a function of the size of V We start the discussion with studying which type of Boolean functions (i.e.,\nfunctions from {±1}n to {±1}) can be implemented by HV,E,sign Observe that\nfor every computer in which real numbers are stored using b bits, whenever we\ncalculate a function f : Rn →R on such a computer we in fact calculate a\nfunction g : {±1}nb →{±1}b Therefore, studying which Boolean functions can\nbe implemented by HV,E,sign can tell us which functions can be implemented on\na computer that stores real numbers using b bits We begin with a simple claim, showing that without restricting the size of the\nnetwork, every Boolean function can be implemented using a neural network of\ndepth 2 claim 20.1\nFor every n, there exists a graph (V, E) of depth 2, such that\nHV,E,sign contains all functions from {±1}n to {±1} Proof\nWe construct a graph with |V0| = n + 1, |V1| = 2n + 1, and |V2| = 1 Let\nE be all possible edges between adjacent layers Now, let f : {±1}n →{±1}\nbe some Boolean function We need to show that we can adjust the weights so\nthat the network will implement f Let u1, , uk be all vectors in {±1}n on\nwhich f outputs 1",
      "word_count": 237,
      "source_page": 271,
      "start_position": 98397,
      "end_position": 98633,
      "sentences_count": 12
    },
    {
      "chunk_id": 436,
      "text": "272\nNeural Networks\nthe functions gi(x), and therefore can be written as\nf(x) = sign\n k\nX\ni=1\ngi(x) + k −1 ,\nwhich concludes our proof The preceding claim shows that neural networks can implement any Boolean\nfunction However, this is a very weak property, as the size of the resulting\nnetwork might be exponentially large In the construction given at the proof of\nClaim 20.1, the number of nodes in the hidden layer is exponentially large This\nis not an artifact of our proof, as stated in the following theorem theorem 20.2\nFor every n, let s(n) be the minimal integer such that there\nexists a graph (V, E) with |V | = s(n) such that the hypothesis class HV,E,sign\ncontains all the functions from {0, 1}n to {0, 1} Then, s(n) is exponential in n Similar results hold for HV,E,σ where σ is the sigmoid function Proof\nSuppose that for some (V, E) we have that HV,E,sign contains all functions\nfrom {0, 1}n to {0, 1} It follows that it can shatter the set of m = 2n vectors in\n{0, 1}n and hence the VC dimension of HV,E,sign is 2n On the other hand, the\nVC dimension of HV,E,sign is bounded by O(|E| log(|E|)) ≤O(|V |3), as we will\nshow in the next section This implies that |V | ≥Ω(2n/3), which concludes our\nproof for the case of networks with the sign activation function The proof for\nthe sigmoid case is analogous",
      "word_count": 246,
      "source_page": 272,
      "start_position": 98712,
      "end_position": 98957,
      "sentences_count": 14
    },
    {
      "chunk_id": 437,
      "text": "This implies that |V | ≥Ω(2n/3), which concludes our\nproof for the case of networks with the sign activation function The proof for\nthe sigmoid case is analogous Remark 20.1\nIt is possible to derive a similar theorem for HV,E,σ for any σ, as\nlong as we restrict the weights so that it is possible to express every weight using\na number of bits which is bounded by a universal constant We can even con-\nsider hypothesis classes where diﬀerent neurons can employ diﬀerent activation\nfunctions, as long as the number of allowed activation functions is also ﬁnite Which functions can we express using a network of polynomial size The pre-\nceding claim tells us that it is impossible to express all Boolean functions using\na network of polynomial size On the positive side, in the following we show\nthat all Boolean functions that can be calculated in time O(T(n)) can also be\nexpressed by a network of size O(T(n)2) theorem 20.3\nLet T : N →N and for every n, let Fn be the set of functions\nthat can be implemented using a Turing machine using runtime of at most T(n) Then, there exist constants b, c ∈R+ such that for every n, there is a graph\n(Vn, En) of size at most c T(n)2 + b such that HVn,En,sign contains Fn The proof of this theorem relies on the relation between the time complexity\nof programs and their circuit complexity (see, for example, Sipser (2006))",
      "word_count": 247,
      "source_page": 272,
      "start_position": 98930,
      "end_position": 99176,
      "sentences_count": 10
    },
    {
      "chunk_id": 438,
      "text": "20.3 The Expressive Power of Neural Networks\n273\nimplement conjunctions, disjunctions, and negation of their inputs Circuit com-\nplexity measures the size of Boolean circuits required to calculate functions The\nrelation between time complexity and circuit complexity can be seen intuitively\nas follows We can model each step of the execution of a computer program as a\nsimple operation on its memory state Therefore, the neurons at each layer of the\nnetwork will reﬂect the memory state of the computer at the corresponding time,\nand the translation to the next layer of the network involves a simple calculation\nthat can be carried out by the network To relate Boolean circuits to networks\nwith the sign activation function, we need to show that we can implement the\noperations of conjunction, disjunction, and negation, using the sign activation\nfunction Clearly, we can implement the negation operator using the sign activa-\ntion function The following lemma shows that the sign activation function can\nalso implement conjunctions and disjunctions of its inputs lemma 20.4\nSuppose that a neuron v, that implements the sign activation\nfunction, has k incoming edges, connecting it to neurons whose outputs are in\n{±1} Then, by adding one more edge, linking a “constant” neuron to v, and\nby adjusting the weights on the edges to v, the output of v can implement the\nconjunction or the disjunction of its inputs",
      "word_count": 230,
      "source_page": 273,
      "start_position": 99193,
      "end_position": 99422,
      "sentences_count": 10
    },
    {
      "chunk_id": 439,
      "text": "lemma 20.4\nSuppose that a neuron v, that implements the sign activation\nfunction, has k incoming edges, connecting it to neurons whose outputs are in\n{±1} Then, by adding one more edge, linking a “constant” neuron to v, and\nby adjusting the weights on the edges to v, the output of v can implement the\nconjunction or the disjunction of its inputs Proof\nSimply observe that if f : {±1}k →{±1} is the conjunction func-\ntion, f(x) = ∧ixi, then it can be written as f(x) = sign\n\u0010\n1 −k + Pk\ni=1 xi\n\u0011 Similarly, the disjunction function, f(x) = ∨ixi, can be written as f(x) =\nsign\n\u0010\nk −1 + Pk\ni=1 xi\n\u0011 So far we have discussed Boolean functions In Exercise 1 we show that neural\nnetworks are universal approximators That is, for every ﬁxed precision param-\neter, ϵ > 0, and every Lipschitz function f : [−1, 1]n →[−1, 1], it is possible to\nconstruct a network such that for every input x ∈[−1, 1]n, the network outputs\na number between f(x) −ϵ and f(x) + ϵ However, as in the case of Boolean\nfunctions, the size of the network here again cannot be polynomial in n This is\nformalized in the following theorem, whose proof is a direct corollary of Theo-\nrem 20.2 and is left as an exercise theorem 20.5\nFix some ϵ ∈(0, 1)",
      "word_count": 234,
      "source_page": 273,
      "start_position": 99361,
      "end_position": 99594,
      "sentences_count": 10
    },
    {
      "chunk_id": 440,
      "text": "274\nNeural Networks\nLet us start with a depth 2 network, namely, a network with a single hidden\nlayer Each neuron in the hidden layer implements a halfspace predictor Then,\nthe single neuron at the output layer applies a halfspace on top of the binary\noutputs of the neurons in the hidden layer As we have shown before, a halfspace\ncan implement the conjunction function Therefore, such networks contain all\nhypotheses which are an intersection of k −1 halfspaces, where k is the number\nof neurons in the hidden layer; namely, they can express all convex polytopes\nwith k −1 faces An example of an intersection of 5 halfspaces is given in the\nfollowing We have shown that a neuron in layer V2 can implement a function that\nindicates whether x is in some convex polytope By adding one more layer, and\nletting the neuron in the output layer implement the disjunction of its inputs,\nwe get a network that computes the union of polytopes An illustration of such\na function is given in the following 20.4\nThe Sample Complexity of Neural Networks\nNext we discuss the sample complexity of learning the class HV,E,σ Recall that\nthe fundamental theorem of learning tells us that the sample complexity of learn-\ning a hypothesis class of binary classiﬁers depends on its VC dimension There-\nfore, we focus on calculating the VC dimension of hypothesis classes of the form\nHV,E,σ, where the output layer of the graph contains a single neuron",
      "word_count": 248,
      "source_page": 274,
      "start_position": 99681,
      "end_position": 99928,
      "sentences_count": 12
    },
    {
      "chunk_id": 441,
      "text": "20.4 The Sample Complexity of Neural Networks\n275\nProof\nTo simplify the notation throughout the proof, let us denote the hy-\npothesis class by H Recall the deﬁnition of the growth function, τH(m), from\nSection 6.5.1 This function measures maxC⊂X:|C|=m |HC|, where HC is the re-\nstriction of H to functions from C to {0, 1} We can naturally extend the deﬁ-\nnition for a set of functions from X to some ﬁnite set Y, by letting HC be the\nrestriction of H to functions from C to Y, and keeping the deﬁnition of τH(m)\nintact Our neural network is deﬁned by a layered graph Let V0, , VT be the layers\nof the graph Fix some t ∈[T] By assigning diﬀerent weights on the edges\nbetween Vt−1 and Vt, we obtain diﬀerent functions from R|Vt−1| →{±1}|Vt| Let\nH(t) be the class of all possible such mappings from R|Vt−1| →{±1}|Vt| Then,\nH can be written as a composition, H = H(T ) ◦ ◦H(1) In Exercise 4 we show\nthat the growth function of a composition of hypothesis classes is bounded by\nthe products of the growth functions of the individual classes Therefore,\nτH(m) ≤\nT\nY\nt=1\nτH(t)(m) In addition, each H(t) can be written as a product of function classes, H(t) =\nH(t,1) × · · · × H(t,|Vt|), where each H(t,j) is all functions from layer t −1 to {±1}\nthat the jth neuron of layer t can implement",
      "word_count": 242,
      "source_page": 275,
      "start_position": 99982,
      "end_position": 100223,
      "sentences_count": 15
    },
    {
      "chunk_id": 442,
      "text": "Therefore,\nτH(m) ≤\nT\nY\nt=1\nτH(t)(m) In addition, each H(t) can be written as a product of function classes, H(t) =\nH(t,1) × · · · × H(t,|Vt|), where each H(t,j) is all functions from layer t −1 to {±1}\nthat the jth neuron of layer t can implement In Exercise 3 we bound product\nclasses, and this yields\nτH(t)(m) ≤\n|Vt|\nY\ni=1\nτH(t,i)(m) Let dt,i be the number of edges that are headed to the ith neuron of layer t Since the neuron is a homogenous halfspace hypothesis and the VC dimension\nof homogenous halfspaces is the dimension of their input, we have by Sauer’s\nlemma that\nτH(t,i)(m) ≤\n\u0010\nem\ndt,i\n\u0011dt,i\n≤(em)dt,i Overall, we obtained that\nτH(m) ≤(em)\nP\nt,i dt,i = (em)|E| Now, assume that there are m shattered points Then, we must have τH(m) =\n2m, from which we obtain\n2m ≤(em)|E|\n⇒\nm ≤|E| log(em)/ log(2) The claim follows by Lemma A.2 Next, we consider HV,E,σ, where σ is the sigmoid function Surprisingly, it\nturns out that the VC dimension of HV,E,σ is lower bounded by Ω(|E|2) (see\nExercise 5.) That is, the VC dimension is the number of tunable parameters\nsquared It is also possible to upper bound the VC dimension by O(|V |2 |E|2),\nbut the proof is beyond the scope of this book In any case, since in practice",
      "word_count": 230,
      "source_page": 275,
      "start_position": 100174,
      "end_position": 100403,
      "sentences_count": 13
    },
    {
      "chunk_id": 443,
      "text": "276\nNeural Networks\nwe only consider networks in which the weights have a short representation as\nﬂoating point numbers with O(1) bits, by using the discretization trick we easily\nobtain that such networks have a VC dimension of O(|E|), even if we use the\nsigmoid activation function 20.5\nThe Runtime of Learning Neural Networks\nIn the previous sections we have shown that the class of neural networks with an\nunderlying graph of polynomial size can express all functions that can be imple-\nmented eﬃciently, and that the sample complexity has a favorable dependence\non the size of the network In this section we turn to the analysis of the time\ncomplexity of training neural networks We ﬁrst show that it is NP hard to implement the ERM rule with respect to\nHV,E,sign even for networks with a single hidden layer that contain just 4 neurons\nin the hidden layer theorem 20.7\nLet k ≥3 For every n, let (V, E) be a layered graph with n\ninput nodes, k + 1 nodes at the (single) hidden layer, where one of them is the\nconstant neuron, and a single output node Then, it is NP hard to implement the\nERM rule with respect to HV,E,sign The proof relies on a reduction from the k-coloring problem and is left as\nExercise 6",
      "word_count": 220,
      "source_page": 276,
      "start_position": 100404,
      "end_position": 100623,
      "sentences_count": 8
    },
    {
      "chunk_id": 444,
      "text": "Then, it is NP hard to implement the\nERM rule with respect to HV,E,sign The proof relies on a reduction from the k-coloring problem and is left as\nExercise 6 One way around the preceding hardness result could be that for the purpose\nof learning, it may suﬃce to ﬁnd a predictor h ∈H with low empirical error,\nnot necessarily an exact ERM However, it turns out that even the task of ﬁnd-\ning weights that result in close-to-minimal empirical error is computationally\ninfeasible (see (Bartlett & Ben-David 2002)) One may also wonder whether it may be possible to change the architecture\nof the network so as to circumvent the hardness result That is, maybe ERM\nwith respect to the original network structure is computationally hard but ERM\nwith respect to some other, larger, network may be implemented eﬃciently (see\nChapter 8 for examples of such cases) Another possibility is to use other acti-\nvation functions (such as sigmoids, or any other type of eﬃciently computable\nactivation functions) There is a strong indication that all of such approaches\nare doomed to fail Indeed, under some cryptographic assumption, the problem\nof learning intersections of halfspaces is known to be hard even in the repre-\nsentation independent model of learning (see Klivans & Sherstov (2006)) This\nimplies that, under the same cryptographic assumption, any hypothesis class\nwhich contains intersections of halfspaces cannot be learned eﬃciently",
      "word_count": 233,
      "source_page": 276,
      "start_position": 100594,
      "end_position": 100826,
      "sentences_count": 10
    },
    {
      "chunk_id": 445,
      "text": "20.6 SGD and Backpropagation\n277\nhope it will ﬁnd a reasonable solution (as happens to be the case in several\npractical tasks) 20.6\nSGD and Backpropagation\nThe problem of ﬁnding a hypothesis in HV,E,σ with a low risk amounts to the\nproblem of tuning the weights over the edges In this section we show how to\napply a heuristic search for good weights using the SGD algorithm Throughout\nthis section we assume that σ is the sigmoid function, σ(a) = 1/(1 + e−a), but\nthe derivation holds for any diﬀerentiable scalar function Since E is a ﬁnite set, we can think of the weight function as a vector w ∈R|E| Suppose the network has n input neurons and k output neurons, and denote by\nhw : Rn →Rk the function calculated by the network if the weight function is\ndeﬁned by w Let us denote by ∆(hw(x), y) the loss of predicting hw(x) when\nthe target is y ∈Y For concreteness, we will take ∆to be the squared loss,\n∆(hw(x), y) = 1\n2∥hw(x) −y∥2; however, similar derivation can be obtained for\nevery diﬀerentiable function Finally, given a distribution D over the examples\ndomain, Rn × Rk, let LD(w) be the risk of the network, namely,\nLD(w) =\nE\n(x,y)∼D [∆(hw(x), y)] Recall the SGD algorithm for minimizing the risk function LD(w) We repeat\nthe pseudocode from Chapter 14 with a few modiﬁcations, which are relevant\nto the neural network application because of the nonconvexity of the objective\nfunction",
      "word_count": 249,
      "source_page": 277,
      "start_position": 100880,
      "end_position": 101128,
      "sentences_count": 11
    },
    {
      "chunk_id": 446,
      "text": "Recall the SGD algorithm for minimizing the risk function LD(w) We repeat\nthe pseudocode from Chapter 14 with a few modiﬁcations, which are relevant\nto the neural network application because of the nonconvexity of the objective\nfunction First, while in Chapter 14 we initialized w to be the zero vector, here\nwe initialize w to be a randomly chosen vector with values close to zero This\nis because an initialization with the zero vector will lead all hidden neurons to\nhave the same weights (if the network is a full layered network) In addition,\nthe hope is that if we repeat the SGD procedure several times, where each time\nwe initialize the process with a new random vector, one of the runs will lead\nto a good local minimum Second, while a ﬁxed step size, η, is guaranteed to\nbe good enough for convex problems, here we utilize a variable step size, ηt, as\ndeﬁned in Section 14.4.2 Because of the nonconvexity of the loss function, the\nchoice of the sequence ηt is more signiﬁcant, and it is tuned in practice by a trial\nand error manner Third, we output the best performing vector on a validation\nset In addition, it is sometimes helpful to add regularization on the weights,\nwith parameter λ That is, we try to minimize LD(w) + λ\n2 ∥w∥2 Finally, the\ngradient does not have a closed form solution Instead, it is implemented using\nthe backpropagation algorithm, which will be described in the sequel.",
      "word_count": 249,
      "source_page": 277,
      "start_position": 101092,
      "end_position": 101340,
      "sentences_count": 12
    },
    {
      "chunk_id": 447,
      "text": "278\nNeural Networks\nSGD for Neural Networks\nparameters:\nnumber of iterations τ\nstep size sequence η1, η2, , ητ\nregularization parameter λ > 0\ninput:\nlayered graph (V, E)\ndiﬀerentiable activation function σ : R →R\ninitialize:\nchoose w(1) ∈R|E| at random\n(from a distribution s.t w(1) is close enough to 0)\nfor i = 1, 2, , τ\nsample (x, y) ∼D\ncalculate gradient vi = backpropagation(x, y, w, (V, E), σ)\nupdate w(i+1) = w(i) −ηi(vi + λw(i))\noutput:\n¯w is the best performing w(i) on a validation set\nBackpropagation\ninput:\nexample (x, y), weight vector w, layered graph (V, E),\nactivation function σ : R →R\ninitialize:\ndenote layers of the graph V0, , VT where Vt = {vt,1, , vt,kt}\ndeﬁne Wt,i,j as the weight of (vt,j, vt+1,i)\n(where we set Wt,i,j = 0 if (vt,j, vt+1,i) /∈E)\nforward:\nset o0 = x\nfor t = 1, , T\nfor i = 1, , kt\nset at,i = Pkt−1\nj=1 Wt−1,i,j ot−1,j\nset ot,i = σ(at,i)\nbackward:\nset δT = oT −y\nfor t = T −1, T −2, , 1\nfor i = 1, , kt\nδt,i = Pkt+1\nj=1 Wt,j,i δt+1,j σ′(at+1,j)\noutput:\nforeach edge (vt−1,j, vt,i) ∈E\nset the partial derivative to δt,i σ′(at,i) ot−1,j",
      "word_count": 212,
      "source_page": 278,
      "start_position": 101341,
      "end_position": 101552,
      "sentences_count": 10
    },
    {
      "chunk_id": 448,
      "text": "20.6 SGD and Backpropagation\n279\nExplaining How Backpropagation Calculates the Gradient:\nWe next explain how the backpropagation algorithm calculates the gradient of\nthe loss function on an example (x, y) with respect to the vector w Let us ﬁrst\nrecall a few deﬁnitions from vector calculus Each element of the gradient is\nthe partial derivative with respect to the variable in w corresponding to one of\nthe edges of the network Recall the deﬁnition of a partial derivative Given a\nfunction f : Rn →R, the partial derivative with respect to the ith variable at w\nis obtained by ﬁxing the values of w1, , wi−1, wi+1, wn, which yields the scalar\nfunction g : R →R deﬁned by g(a) = f((w1, , wi−1, wi + a, wi+1, , wn)),\nand then taking the derivative of g at 0 For a function with multiple outputs,\nf : Rn →Rm, the Jacobian of f at w ∈Rn, denoted Jw(f), is the m × n matrix\nwhose i, j element is the partial derivative of fi : Rn →R w.r.t its jth variable\nat w Note that if m = 1 then the Jacobian matrix is the gradient of the function\n(represented as a row vector) Two examples of Jacobian calculations, which we\nwill later use, are as follows • Let f(w) = Aw for A ∈Rm,n Then Jw(f) = A",
      "word_count": 229,
      "source_page": 279,
      "start_position": 101553,
      "end_position": 101781,
      "sentences_count": 14
    },
    {
      "chunk_id": 449,
      "text": "• Let f(w) = Aw for A ∈Rm,n Then Jw(f) = A • For every n, we use the notation σ to denote the function from Rn to Rn\nwhich applies the sigmoid function element-wise That is, α = σ(θ) means\nthat for every i we have αi = σ(θi) =\n1\n1+exp(−θi) It is easy to verify\nthat Jθ(σ) is a diagonal matrix whose (i, i) entry is σ′(θi), where σ′ is\nthe derivative function of the (scalar) sigmoid function, namely, σ′(θi) =\n1\n(1+exp(θi))(1+exp(−θi)) We also use the notation diag(σ′(θ)) to denote this\nmatrix The chain rule for taking the derivative of a composition of functions can be\nwritten in terms of the Jacobian as follows Given two functions f : Rn →Rm\nand g : Rk →Rn, we have that the Jacobian of the composition function,\n(f ◦g) : Rk →Rm, at w, is\nJw(f ◦g) = Jg(w)(f)Jw(g) For example, for g(w) = Aw, where A ∈Rn,k, we have that\nJw(σ ◦g) = diag(σ′(Aw)) A To describe the backpropagation algorithm, let us ﬁrst decompose V into the\nlayers of the graph, V = ·∪T\nt=0Vt For every t, let us write Vt = {vt,1, , vt,kt},\nwhere kt = |Vt| In addition, for every t denote Wt ∈Rkt+1,kt a matrix which\ngives a weight to every potential edge between Vt and Vt+1 If the edge exists in\nE then we set Wt,i,j to be the weight, according to w, of the edge (vt,j, vt+1,i)",
      "word_count": 247,
      "source_page": 279,
      "start_position": 101770,
      "end_position": 102016,
      "sentences_count": 14
    },
    {
      "chunk_id": 450,
      "text": "280\nNeural Networks\nNext, we discuss how to calculate the partial derivatives with respect to the\nedges from Vt−1 to Vt, namely, with respect to the elements in Wt−1 Since we\nﬁx all other weights of the network, it follows that the outputs of all the neurons\nin Vt−1 are ﬁxed numbers which do not depend on the weights in Wt−1 Denote\nthe corresponding vector by ot−1 In addition, let us denote by ℓt : Rkt →R the\nloss function of the subnetwork deﬁned by layers Vt, , VT as a function of the\noutputs of the neurons in Vt The input to the neurons of Vt can be written as\nat = Wt−1ot−1 and the output of the neurons of Vt is ot = σ(at) That is, for\nevery j we have ot,j = σ(at,j) We obtain that the loss, as a function of Wt−1,\ncan be written as\ngt(Wt−1) = ℓt(ot) = ℓt(σ(at)) = ℓt(σ(Wt−1ot−1)) It would be convenient to rewrite this as follows Let wt−1 ∈Rkt−1kt be the\ncolumn vector obtained by concatenating the rows of Wt−1 and then taking the\ntranspose of the resulting long vector Deﬁne by Ot−1 the kt × (kt−1kt) matrix\nOt−1 =\n\n\n\n\n\n\n\no⊤\nt−1\n0\n· · ·\n0\n0\no⊤\nt−1\n· · ·\n0 0\n0\n· · ·\no⊤\nt−1\n\n\n\n\n\n\n (20.2)\nThen, Wt−1ot−1 = Ot−1wt−1, so we can also write\ngt(wt−1) = ℓt(σ(Ot−1 wt−1))",
      "word_count": 249,
      "source_page": 280,
      "start_position": 102087,
      "end_position": 102335,
      "sentences_count": 13
    },
    {
      "chunk_id": 451,
      "text": "0\n0\n· · ·\no⊤\nt−1\n\n\n\n\n\n\n (20.2)\nThen, Wt−1ot−1 = Ot−1wt−1, so we can also write\ngt(wt−1) = ℓt(σ(Ot−1 wt−1)) Therefore, applying the chain rule, we obtain that\nJwt−1(gt) = Jσ(Ot−1wt−1)(ℓt) diag(σ′(Ot−1wt−1)) Ot−1 Using our notation we have ot = σ(Ot−1wt−1) and at = Ot−1wt−1, which yields\nJwt−1(gt) = Jot(ℓt) diag(σ′(at)) Ot−1 Let us also denote δt = Jot(ℓt) Then, we can further rewrite the preceding as\nJwt−1(gt) =\n\u0000δt,1 σ′(at,1) o⊤\nt−1 , ,\nδt,kt σ′(at,kt) o⊤\nt−1\n\u0001 (20.3)\nIt is left to calculate the vector δt = Jot(ℓt) for every t This is the gradient\nof ℓt at ot We calculate this in a recursive manner First observe that for the\nlast layer we have that ℓT (u) = ∆(u, y), where ∆is the loss function Since we\nassume that ∆(u, y) = 1\n2∥u−y∥2 we obtain that Ju(ℓT ) = (u−y) In particular,\nδT = JoT (ℓT ) = (oT −y) Next, note that\nℓt(u) = ℓt+1(σ(Wtu)) Therefore, by the chain rule,\nJu(ℓt) = Jσ(Wtu)(ℓt+1)diag(σ′(Wtu))Wt.",
      "word_count": 177,
      "source_page": 280,
      "start_position": 102308,
      "end_position": 102484,
      "sentences_count": 15
    },
    {
      "chunk_id": 452,
      "text": "20.7 Summary\n281\nIn particular,\nδt = Jot(ℓt) = Jσ(Wtot)(ℓt+1)diag(σ′(Wtot))Wt\n= Jot+1(ℓt+1)diag(σ′(at+1))Wt\n= δt+1 diag(σ′(at+1))Wt In summary, we can ﬁrst calculate the vectors {at, ot} from the bottom of\nthe network to its top Then, we calculate the vectors {δt} from the top of\nthe network back to its bottom Once we have all of these vectors, the partial\nderivatives are easily obtained using Equation (20.3) We have thus shown that\nthe pseudocode of backpropagation indeed calculates the gradient 20.7\nSummary\nNeural networks over graphs of size s(n) can be used to describe hypothesis\nclasses of all predictors that can be implemented in runtime of O(\np\ns(n)) We\nhave also shown that their sample complexity depends polynomially on s(n)\n(speciﬁcally, it depends on the number of edges in the network) Therefore, classes\nof neural network hypotheses seem to be an excellent choice Regrettably, the\nproblem of training the network on the basis of training data is computationally\nhard We have presented the SGD framework as a heuristic approach for training\nneural networks and described the backpropagation algorithm which eﬃciently\ncalculates the gradient of the loss function with respect to the weights over the\nedges 20.8\nBibliographic Remarks\nNeural networks were extensively studied in the 1980s and early 1990s, but with\nmixed empirical success In recent years, a combination of algorithmic advance-\nments, as well as increasing computational power and data size, has led to a\nbreakthrough in the eﬀectiveness of neural networks",
      "word_count": 243,
      "source_page": 281,
      "start_position": 102485,
      "end_position": 102727,
      "sentences_count": 12
    },
    {
      "chunk_id": 453,
      "text": "20.8\nBibliographic Remarks\nNeural networks were extensively studied in the 1980s and early 1990s, but with\nmixed empirical success In recent years, a combination of algorithmic advance-\nments, as well as increasing computational power and data size, has led to a\nbreakthrough in the eﬀectiveness of neural networks In particular, “deep net-\nworks” (i.e., networks of more than 2 layers) have shown very impressive practical\nperformance on a variety of domains A few examples include convolutional net-\nworks (Lecun & Bengio 1995), restricted Boltzmann machines (Hinton, Osindero\n& Teh 2006), auto-encoders (Ranzato, Huang, Boureau & Lecun 2007, Bengio &\nLeCun 2007, Collobert & Weston 2008, Lee, Grosse, Ranganath & Ng 2009, Le,\nRanzato, Monga, Devin, Corrado, Chen, Dean & Ng 2012), and sum-product\nnetworks (Livni, Shalev-Shwartz & Shamir 2013, Poon & Domingos 2011) See\nalso (Bengio 2009) and the references therein The expressive power of neural networks and the relation to circuit complexity\nhave been extensively studied in (Parberry 1994) For the analysis of the sample\ncomplexity of neural networks we refer the reader to (Anthony & Bartlet 1999) Our proof technique of Theorem 20.6 is due to Kakade and Tewari lecture notes.",
      "word_count": 193,
      "source_page": 281,
      "start_position": 102680,
      "end_position": 102872,
      "sentences_count": 8
    },
    {
      "chunk_id": 454,
      "text": "282\nNeural Networks\nKlivans & Sherstov (2006) have shown that for any c > 0, intersections of nc\nhalfspaces over {±1}n are not eﬃciently PAC learnable, even if we allow repre-\nsentation independent learning This hardness result relies on the cryptographic\nassumption that there is no polynomial time solution to the unique-shortest-\nvector problem As we have argued, this implies that there cannot be an eﬃcient\nalgorithm for training neural networks, even if we allow larger networks or other\nactivation functions that can be implemented eﬃciently The backpropagation algorithm has been introduced in Rumelhart, Hinton &\nWilliams (1986) 20.9\nExercises\n1 Neural Networks are universal approximators: Let f : [−1, 1]n →\n[−1, 1] be a ρ-Lipschitz function Fix some ϵ > 0 Construct a neural net-\nwork N : [−1, 1]n →[−1, 1], with the sigmoid activation function, such that\nfor every x ∈[−1, 1]n it holds that |f(x) −N(x)| ≤ϵ Hint: Similarly to the proof of Theorem 19.3, partition [−1, 1]n into small\nboxes Use the Lipschitzness of f to show that it is approximately constant\nat each box Finally, show that a neural network can ﬁrst decide which box\nthe input vector belongs to, and then predict the averaged value of f at that\nbox 2 Prove Theorem 20.5 Hint: For every f : {−1, 1}n →{−1, 1} construct a 1-Lipschitz function\ng : [−1, 1]n →[−1, 1] such that if you can approximate g then you can express\nf 3",
      "word_count": 244,
      "source_page": 282,
      "start_position": 102873,
      "end_position": 103116,
      "sentences_count": 15
    },
    {
      "chunk_id": 455,
      "text": "Hint: For every f : {−1, 1}n →{−1, 1} construct a 1-Lipschitz function\ng : [−1, 1]n →[−1, 1] such that if you can approximate g then you can express\nf 3 Growth function of product: For i = 1, 2, let Fi be a set of functions from\nX to Yi Deﬁne H = F1 × F2 to be the Cartesian product class That is, for\nevery f1 ∈F1 and f2 ∈F2, there exists h ∈H such that h(x) = (f1(x), f2(x)) Prove that τH(m) ≤τF1(m) τF2(m) 4 Growth function of composition: Let F1 be a set of functions from X\nto Z and let F2 be a set of functions from Z to Y Let H = F2 ◦F1 be the\ncomposition class That is, for every f1 ∈F1 and f2 ∈F2, there exists h ∈H\nsuch that h(x) = f2(f1(x)) Prove that τH(m) ≤τF2(m)τF1(m) 5 VC of sigmoidal networks: In this exercise we show that there is a graph\n(V, E) such that the VC dimension of the class of neural networks over these\ngraphs with the sigmoid activation function is Ω(|E|2) Note that for every ϵ >\n0, the sigmoid activation function can approximate the threshold activation\nfunction, 1[P\ni xi], up to accuracy ϵ To simplify the presentation, throughout\nthe exercise we assume that we can exactly implement the activation function\n1[P\ni xi>0] using a sigmoid activation function Fix some n 1",
      "word_count": 238,
      "source_page": 282,
      "start_position": 103085,
      "end_position": 103322,
      "sentences_count": 17
    },
    {
      "chunk_id": 456,
      "text": "20.9 Exercises\n283\nif we feed the network with the real number 0.x1x2 xn, then the output\nof the network will be x Hint: Denote α = 0.x1x2 xn and observe that 10kα −0.5 is at least 0.5\nif xk = 1 and is at most −0.3 if xk = −1 2 Construct a network, N2, with O(n) weights, which implements a function\nfrom [n] to {0, 1}n such that N2(i) = ei for all i That is, upon receiving\nthe input i, the network outputs the vector of all zeros except 1 at the i’th\nneuron 3 Let α1, , αn be n real numbers such that every αi is of the form 0.a(i)\n1 a(i)\n2 a(i)\nn ,\nwith a(i)\nj\n∈{0, 1} Construct a network, N3, with O(n) weights, which im-\nplements a function from [n] to R, and satisﬁes N2(i) = αi for every i ∈[n] 4 Combine N1, N3 to obtain a network that receives i ∈[n] and output a(i) 5 Construct a network N4 that receives (i, j) ∈[n] × [n] and outputs a(i)\nj Hint: Observe that the AND function over {0, 1}2 can be calculated using\nO(1) weights 6 Conclude that there is a graph with O(n) weights such that the VC di-\nmension of the resulting hypothesis class is n2 6 Prove Theorem 20.7 Hint: The proof is similar to the hardness of learning intersections of halfs-\npaces – see Exercise 32 in Chapter 8.",
      "word_count": 245,
      "source_page": 283,
      "start_position": 103349,
      "end_position": 103593,
      "sentences_count": 22
    },
    {
      "chunk_id": 457,
      "text": "21\nOnline Learning\nIn this chapter we describe a diﬀerent model of learning, which is called online\nlearning Previously, we studied the PAC learning model, in which the learner\nﬁrst receives a batch of training examples, uses the training set to learn a hy-\npothesis, and only when learning is completed uses the learned hypothesis for\npredicting the label of new examples In our papayas learning problem, this\nmeans that we should ﬁrst buy a bunch of papayas and taste them all Then, we\nuse all of this information to learn a prediction rule that determines the taste\nof new papayas In contrast, in online learning there is no separation between a\ntraining phase and a prediction phase Instead, each time we buy a papaya, it\nis ﬁrst considered a test example since we should predict whether it is going to\ntaste good Then, after taking a bite from the papaya, we know the true label,\nand the same papaya can be used as a training example that can help us improve\nour prediction mechanism for future papayas Concretely, online learning takes place in a sequence of consecutive rounds On each online round, the learner ﬁrst receives an instance (the learner buys\na papaya and knows its shape and color, which form the instance) Then, the\nlearner is required to predict a label (is the papaya tasty?)",
      "word_count": 227,
      "source_page": 287,
      "start_position": 103594,
      "end_position": 103820,
      "sentences_count": 10
    },
    {
      "chunk_id": 458,
      "text": "On each online round, the learner ﬁrst receives an instance (the learner buys\na papaya and knows its shape and color, which form the instance) Then, the\nlearner is required to predict a label (is the papaya tasty?) At the end of the\nround, the learner obtains the correct label (he tastes the papaya and then knows\nwhether it is tasty or not) Finally, the learner uses this information to improve\nhis future predictions To analyze online learning, we follow a similar route to our study of PAC\nlearning We start with online binary classiﬁcation problems We consider both\nthe realizable case, in which we assume, as prior knowledge, that all the labels are\ngenerated by some hypothesis from a given hypothesis class, and the unrealizable\ncase, which corresponds to the agnostic PAC learning model In particular, we\npresent an important algorithm called Weighted-Majority Next, we study online\nlearning problems in which the loss function is convex Finally, we present the\nPerceptron algorithm as an example of the use of surrogate convex loss functions\nin the online learning model Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 208,
      "source_page": 287,
      "start_position": 103783,
      "end_position": 103990,
      "sentences_count": 15
    },
    {
      "chunk_id": 459,
      "text": "288\nOnline Learning\n21.1\nOnline Classiﬁcation in the Realizable Case\nOnline learning is performed in a sequence of consecutive rounds, where at round\nt the learner is given an instance, xt, taken from an instance domain X, and is\nrequired to provide its label We denote the predicted label by pt After predicting\nthe label, the correct label, yt ∈{0, 1}, is revealed to the learner The learner’s\ngoal is to make as few prediction mistakes as possible during this process The\nlearner tries to deduce information from previous rounds so as to improve its\npredictions on future rounds Clearly, learning is hopeless if there is no correlation between past and present\nrounds Previously in the book, we studied the PAC model in which we assume\nthat past and present examples are sampled i.i.d from the same distribution\nsource In the online learning model we make no statistical assumptions regard-\ning the origin of the sequence of examples The sequence is allowed to be deter-\nministic, stochastic, or even adversarially adaptive to the learner’s own behavior\n(as in the case of spam e-mail ﬁltering) Naturally, an adversary can make the\nnumber of prediction mistakes of our online learning algorithm arbitrarily large For example, the adversary can present the same instance on each online round,\nwait for the learner’s prediction, and provide the opposite label as the correct\nlabel To make nontrivial statements we must further restrict the problem The real-\nizability assumption is one possible natural restriction",
      "word_count": 247,
      "source_page": 288,
      "start_position": 103991,
      "end_position": 104237,
      "sentences_count": 14
    },
    {
      "chunk_id": 460,
      "text": "To make nontrivial statements we must further restrict the problem The real-\nizability assumption is one possible natural restriction In the realizable case, we\nassume that all the labels are generated by some hypothesis, h⋆: X →Y Fur-\nthermore, h⋆is taken from a hypothesis class H, which is known to the learner This is analogous to the PAC learning model we studied in Chapter 3 With this\nrestriction on the sequence, the learner should make as few mistakes as possible,\nassuming that both h⋆and the sequence of instances can be chosen by an ad-\nversary For an online learning algorithm, A, we denote by MA(H) the maximal\nnumber of mistakes A might make on a sequence of examples which is labeled by\nsome h⋆∈H We emphasize again that both h⋆and the sequence of instances\ncan be chosen by an adversary A bound on MA(H) is called a mistake-bound and\nwe will study how to design algorithms for which MA(H) is minimal Formally:\ndefinition 21.1 (Mistake Bounds, Online Learnability)\nLet H be a hypoth-\nesis class and let A be an online learning algorithm Given any sequence S =\n(x1, h⋆(y1)), , (xT , h⋆(yT )), where T is any integer and h⋆∈H, let MA(S) be\nthe number of mistakes A makes on the sequence S We denote by MA(H) the\nsupremum of MA(S) over all sequences of the above form A bound of the form\nMA(H) ≤B < ∞is called a mistake bound",
      "word_count": 243,
      "source_page": 288,
      "start_position": 104219,
      "end_position": 104461,
      "sentences_count": 14
    },
    {
      "chunk_id": 461,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n289\ntional aspect of learning, and do not restrict the algorithms to be eﬃcient In\nSection 21.3 and Section 21.4 we study eﬃcient online learning algorithms To simplify the presentation, we start with the case of a ﬁnite hypothesis class,\nnamely, |H| < ∞ In PAC learning, we identiﬁed ERM as a good learning algorithm, in the sense\nthat if H is learnable then it is learnable by the rule ERMH A natural learning\nrule for online learning is to use (at any online round) any ERM hypothesis,\nnamely, any hypothesis which is consistent with all past examples Consistent\ninput: A ﬁnite hypothesis class H\ninitialize: V1 = H\nfor t = 1, 2, receive xt\nchoose any h ∈Vt\npredict pt = h(xt)\nreceive true label yt = h⋆(xt)\nupdate Vt+1 = {h ∈Vt : h(xt) = yt}\nThe Consistent algorithm maintains a set, Vt, of all the hypotheses which\nare consistent with (x1, y1), , (xt−1, yt−1) This set is often called the version\nspace It then picks any hypothesis from Vt and predicts according to this hy-\npothesis Obviously, whenever Consistent makes a prediction mistake, at least one\nhypothesis is removed from Vt Therefore, after making M mistakes we have\n|Vt| ≤|H| −M Since Vt is always nonempty (by the realizability assumption it\ncontains h⋆) we have 1 ≤|Vt| ≤|H| −M Rearranging, we obtain the following:\ncorollary 21.2\nLet H be a ﬁnite hypothesis class",
      "word_count": 245,
      "source_page": 289,
      "start_position": 104523,
      "end_position": 104767,
      "sentences_count": 14
    },
    {
      "chunk_id": 462,
      "text": "290\nOnline Learning\ntheorem 21.3\nLet H be a ﬁnite hypothesis class The Halving algorithm\nenjoys the mistake bound MHalving(H) ≤log2(|H|) Proof\nWe simply note that whenever the algorithm errs we have |Vt+1| ≤|Vt|/2,\n(hence the name Halving) Therefore, if M is the total number of mistakes, we\nhave\n1 ≤|VT +1| ≤|H| 2−M Rearranging this inequality we conclude our proof Of course, Halving’s mistake bound is much better than Consistent’s mistake\nbound We already see that online learning is diﬀerent from PAC learning—while\nin PAC, any ERM hypothesis is good, in online learning choosing an arbitrary\nERM hypothesis is far from being optimal 21.1.1\nOnline Learnability\nWe next take a more general approach, and aim at characterizing online learn-\nability In particular, we target the following question: What is the optimal online\nlearning algorithm for a given hypothesis class H We present a dimension of hypothesis classes that characterizes the best achiev-\nable mistake bound This measure was proposed by Nick Littlestone and we\ntherefore refer to it as Ldim(H) To motivate the deﬁnition of Ldim it is convenient to view the online learning\nprocess as a game between two players: the learner versus the environment On\nround t of the game, the environment picks an instance xt, the learner predicts a\nlabel pt ∈{0, 1}, and ﬁnally the environment outputs the true label, yt ∈{0, 1} Suppose that the environment wants to make the learner err on the ﬁrst T rounds\nof the game",
      "word_count": 246,
      "source_page": 290,
      "start_position": 104884,
      "end_position": 105129,
      "sentences_count": 14
    },
    {
      "chunk_id": 463,
      "text": "On\nround t of the game, the environment picks an instance xt, the learner predicts a\nlabel pt ∈{0, 1}, and ﬁnally the environment outputs the true label, yt ∈{0, 1} Suppose that the environment wants to make the learner err on the ﬁrst T rounds\nof the game Then, it must output yt = 1 −pt, and the only question is how it\nshould choose the instances xt in such a way that ensures that for some h⋆∈H\nwe have yt = h⋆(xt) for all t ∈[T] A strategy for an adversarial environment can be formally described as a\nbinary tree, as follows Each node of the tree is associated with an instance from\nX Initially, the environment presents to the learner the instance associated with\nthe root of the tree Then, if the learner predicts pt = 1 the environment will\ndeclare that this is a wrong prediction (i.e., yt = 0) and will traverse to the right\nchild of the current node If the learner predicts pt = 0 then the environment\nwill set yt = 1 and will traverse to the left child This process will continue and\nat each round, the environment will present the instance associated with the\ncurrent node Formally, consider a complete binary tree of depth T (we deﬁne the depth of\nthe tree as the number of edges in a path from the root to a leaf)",
      "word_count": 236,
      "source_page": 290,
      "start_position": 105081,
      "end_position": 105316,
      "sentences_count": 10
    },
    {
      "chunk_id": 464,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n291\nv1\nv2\nv3\nh1\nh2\nh3\nh4\nv1\n0\n0\n1\n1\nv2\n0\n1\n∗\n∗\nv3\n∗\n∗\n0\n1\nFigure 21.1 An illustration of a shattered tree of depth 2 The dashed path\ncorresponds to the sequence of examples ((v1, 1), (v3, 0)) The tree is shattered by\nH = {h1, h2, h3, h4}, where the predictions of each hypothesis in H on the instances\nv1, v2, v3 is given in the table (the ’*’ mark means that hj(vi) can be either 1 or 0) round t, we go to the left child of it if yt = 0 or to the right child if yt = 1 That\nis, it+1 = 2it+yt Unraveling the recursion we obtain it = 2t−1+Pt−1\nj=1 yj 2t−1−j The preceding strategy for the environment succeeds only if for every (y1, , yT )\nthere exists h ∈H such that yt = h(xt) for all t ∈[T] This leads to the following\ndeﬁnition definition 21.4 (H Shattered Tree)\nA shattered tree of depth d is a sequence\nof instances v1, , v2d−1 in X such that for every labeling (y1, , yd) ∈{0, 1}d\nthere exists h ∈H such that for all t ∈[d] we have h(vit) = yt where it =\n2t−1 + Pt−1\nj=1 yj 2t−1−j An illustration of a shattered tree of depth 2 is given in Figure 21.1",
      "word_count": 238,
      "source_page": 291,
      "start_position": 105374,
      "end_position": 105611,
      "sentences_count": 13
    },
    {
      "chunk_id": 465,
      "text": ", yd) ∈{0, 1}d\nthere exists h ∈H such that for all t ∈[d] we have h(vit) = yt where it =\n2t−1 + Pt−1\nj=1 yj 2t−1−j An illustration of a shattered tree of depth 2 is given in Figure 21.1 definition 21.5 (Littlestone’s Dimension (Ldim))\nLdim(H) is the maximal\ninteger T such that there exists a shattered tree of depth T, which is shattered\nby H The deﬁnition of Ldim and the discussion above immediately imply the fol-\nlowing:\nlemma 21.6\nNo algorithm can have a mistake bound strictly smaller than\nLdim(H); namely, for every algorithm, A, we have MA(H) ≥Ldim(H) Proof\nLet T = Ldim(H) and let v1, , v2T −1 be a sequence that satisﬁes the\nrequirements in the deﬁnition of Ldim If the environment sets xt = vit and\nyt = 1−pt for all t ∈[T], then the learner makes T mistakes while the deﬁnition\nof Ldim implies that there exists a hypothesis h ∈H such that yt = h(xt) for all\nt Let us now give several examples Example 21.2\nLet H be a ﬁnite hypothesis class Clearly, any tree that is shat-\ntered by H has depth of at most log2(|H|) Therefore, Ldim(H) ≤log2(|H|) Another way to conclude this inequality is by combining Lemma 21.6 with The-\norem 21.3 Example 21.3\nLet X = {1, , d} and H = {h1, , hd} where hj(x) = 1 iﬀ",
      "word_count": 235,
      "source_page": 291,
      "start_position": 105570,
      "end_position": 105804,
      "sentences_count": 15
    },
    {
      "chunk_id": 466,
      "text": "292\nOnline Learning\nx = j Then, it is easy to show that Ldim(H) = 1 while |H| = d can be arbitrarily\nlarge Therefore, this example shows that Ldim(H) can be signiﬁcantly smaller\nthan log2(|H|) Example 21.4\nLet X = [0, 1] and H = {x 7→1[x<a] : a ∈[0, 1]}; namely, H is\nthe class of thresholds on the interval [0, 1] Then, Ldim(H) = ∞ To see this,\nconsider the tree\n1/2\n1/4\n1/8\n3/8\n3/4\n5/8\n7/8\nThis tree is shattered by H And, because of the density of the reals, this tree\ncan be made arbitrarily deep Lemma 21.6 states that Ldim(H) lower bounds the mistake bound of any\nalgorithm Interestingly, there is a standard algorithm whose mistake bound\nmatches this lower bound The algorithm is similar to the Halving algorithm Recall that the prediction of Halving is made according to a majority vote of\nthe hypotheses which are consistent with previous examples We denoted this\nset by Vt Put another way, Halving partitions Vt into two sets: V +\nt\n= {h ∈Vt :\nh(xt) = 1} and V −\nt\n= {h ∈Vt : h(xt) = 0} It then predicts according to the\nlarger of the two groups The rationale behind this prediction is that whenever\nHalving makes a mistake it ends up with |Vt+1| ≤0.5 |Vt|",
      "word_count": 224,
      "source_page": 292,
      "start_position": 105805,
      "end_position": 106028,
      "sentences_count": 15
    },
    {
      "chunk_id": 467,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n293\nlemma 21.7\nSOA enjoys the mistake bound MSOA(H) ≤Ldim(H) Proof\nIt suﬃces to prove that whenever the algorithm makes a prediction mis-\ntake we have Ldim(Vt+1) ≤Ldim(Vt) −1 We prove this claim by assuming the\ncontrary, that is, Ldim(Vt+1) = Ldim(Vt) If this holds true, then the deﬁnition\nof pt implies that Ldim(V (r)\nt\n) = Ldim(Vt) for both r = 1 and r = 0 But, then\nwe can construct a shaterred tree of depth Ldim(Vt) + 1 for the class Vt, which\nleads to the desired contradiction Combining Lemma 21.7 and Lemma 21.6 we obtain:\ncorollary 21.8\nLet H be any hypothesis class Then, the standard optimal\nalgorithm enjoys the mistake bound MSOA(H) = Ldim(H) and no other algorithm\ncan have MA(H) < Ldim(H) Comparison to VC Dimension\nIn the PAC learning model, learnability is characterized by the VC dimension of\nthe class H Recall that the VC dimension of a class H is the maximal number\nd such that there are instances x1, , xd that are shattered by H That is, for\nany sequence of labels (y1, , yd) ∈{0, 1}d there exists a hypothesis h ∈H\nthat gives exactly this sequence of labels The following theorem relates the VC\ndimension to the Littlestone dimension theorem 21.9\nFor any class H, VCdim(H) ≤Ldim(H), and there are classes\nfor which strict inequality holds Furthermore, the gap can be arbitrarily larger Proof\nWe ﬁrst prove that VCdim(H) ≤Ldim(H)",
      "word_count": 249,
      "source_page": 293,
      "start_position": 106136,
      "end_position": 106384,
      "sentences_count": 16
    },
    {
      "chunk_id": 468,
      "text": "294\nOnline Learning\n21.2\nOnline Classiﬁcation in the Unrealizable Case\nIn the previous section we studied online learnability in the realizable case We\nnow consider the unrealizable case Similarly to the agnostic PAC model, we\nno longer assume that all labels are generated by some h⋆∈H, but we require\nthe learner to be competitive with the best ﬁxed predictor from H This is\ncaptured by the regret of the algorithm, which measures how “sorry” the learner\nis, in retrospect, not to have followed the predictions of some hypothesis h ∈H Formally, the regret of an algorithm A relative to h when running on a sequence\nof T examples is deﬁned as\nRegretA(h, T) =\nsup\n(x1,y1),...,(xT ,yT )\n\" T\nX\nt=1\n|pt −yt| −\nT\nX\nt=1\n|h(xt) −yt|\n#\n,\n(21.1)\nand the regret of the algorithm relative to a hypothesis class H is\nRegretA(H, T) = sup\nh∈H\nRegretA(h, T) (21.2)\nWe restate the learner’s goal as having the lowest possible regret relative to H An interesting question is whether we can derive an algorithm with low regret,\nmeaning that RegretA(H, T) grows sublinearly with the number of rounds, T,\nwhich implies that the diﬀerence between the error rate of the learner and the\nbest hypothesis in H tends to zero as T goes to inﬁnity We ﬁrst show that this is an impossible mission—no algorithm can obtain a\nsublinear regret bound even if |H| = 2",
      "word_count": 240,
      "source_page": 294,
      "start_position": 106487,
      "end_position": 106726,
      "sentences_count": 8
    },
    {
      "chunk_id": 469,
      "text": "An interesting question is whether we can derive an algorithm with low regret,\nmeaning that RegretA(H, T) grows sublinearly with the number of rounds, T,\nwhich implies that the diﬀerence between the error rate of the learner and the\nbest hypothesis in H tends to zero as T goes to inﬁnity We ﬁrst show that this is an impossible mission—no algorithm can obtain a\nsublinear regret bound even if |H| = 2 Indeed, consider H = {h0, h1}, where h0\nis the function that always returns 0 and h1 is the function that always returns\n1 An adversary can make the number of mistakes of any online algorithm be\nequal to T, by simply waiting for the learner’s prediction and then providing\nthe opposite label as the true label In contrast, for any sequence of true labels,\ny1, , yT , let b be the majority of labels in y1, , yT , then the number of\nmistakes of hb is at most T/2 Therefore, the regret of any online algorithm\nmight be at least T −T/2 = T/2, which is not sublinear in T This impossibility\nresult is attributed to Cover (Cover 1965) To sidestep Cover’s impossibility result, we must further restrict the power\nof the adversarial environment We do so by allowing the learner to randomize\nhis predictions Of course, this by itself does not circumvent Cover’s impossibil-\nity result, since in deriving this result we assumed nothing about the learner’s\nstrategy",
      "word_count": 244,
      "source_page": 294,
      "start_position": 106655,
      "end_position": 106898,
      "sentences_count": 12
    },
    {
      "chunk_id": 470,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n295\non round t is\nP[ˆyt ̸= yt] = |pt −yt| Put another way, instead of having the predictions of the learner being in {0, 1}\nwe allow them to be in [0, 1], and interpret pt ∈[0, 1] as the probability to predict\nthe label 1 on round t With this assumption it is possible to derive a low regret algorithm In partic-\nular, we will prove the following theorem theorem 21.10\nFor every hypothesis class H, there exists an algorithm for\nonline classiﬁcation, whose predictions come from [0, 1], that enjoys the regret\nbound\n∀h ∈H,\nT\nX\nt=1\n|pt−yt|−\nT\nX\nt=1\n|h(xt)−yt| ≤\np\n2 min{log(|H|) , Ldim(H) log(eT)} T Furthermore, no algorithm can achieve an expected regret bound smaller than\nΩ\n\u0010p\nLdim(H) T\n\u0011 We will provide a constructive proof of the upper bound part of the preceding\ntheorem The proof of the lower bound part can be found in (Ben-David, Pal, &\nShalev-Shwartz 2009) The proof of Theorem 21.10 relies on the Weighted-Majority algorithm for\nlearning with expert advice This algorithm is important by itself and we dedicate\nthe next subsection to it 21.2.1\nWeighted-Majority\nWeighted-majority is an algorithm for the problem of prediction with expert ad-\nvice In this online learning problem, on round t the learner has to choose the\nadvice of d given experts",
      "word_count": 231,
      "source_page": 295,
      "start_position": 107010,
      "end_position": 107240,
      "sentences_count": 12
    },
    {
      "chunk_id": 471,
      "text": "21.2.1\nWeighted-Majority\nWeighted-majority is an algorithm for the problem of prediction with expert ad-\nvice In this online learning problem, on round t the learner has to choose the\nadvice of d given experts We also allow the learner to randomize his choice by\ndeﬁning a distribution over the d experts, that is, picking a vector w(t) ∈[0, 1]d,\nwith P\ni w(t)\ni\n= 1, and choosing the ith expert with probability w(t)\ni After the\nlearner chooses an expert, it receives a vector of costs, vt ∈[0, 1]d, where vt,i\nis the cost of following the advice of the ith expert If the learner’s predic-\ntions are randomized, then its loss is deﬁned to be the averaged cost, namely,\nP\ni w(t)\ni vt,i = ⟨w(t), vt⟩ The algorithm assumes that the number of rounds T is\ngiven In Exercise 4 we show how to get rid of this dependence using the doubling\ntrick.",
      "word_count": 156,
      "source_page": 295,
      "start_position": 107207,
      "end_position": 107362,
      "sentences_count": 7
    },
    {
      "chunk_id": 472,
      "text": "296\nOnline Learning\nWeighted-Majority\ninput: number of experts, d ; number of rounds, T\nparameter: η =\np\n2 log(d)/T\ninitialize: ˜w(1) = (1, , 1)\nfor t = 1, 2, set w(t) = ˜w(t)/Zt where Zt = P\ni ˜w(t)\ni\nchoose expert i at random according to P[i] = w(t)\ni\nreceive costs of all experts vt ∈[0, 1]d\npay cost ⟨w(t), vt⟩\nupdate rule ∀i, ˜w(t+1)\ni\n= ˜w(t)\ni e−ηvt,i\nThe following theorem is key for analyzing the regret bound of Weighted-\nMajority theorem 21.11\nAssuming that T > 2 log(d), the Weighted-Majority algo-\nrithm enjoys the bound\nT\nX\nt=1\n⟨w(t), vt⟩−min\ni∈[d]\nT\nX\nt=1\nvt,i ≤\np\n2 log(d) T Proof\nWe have:\nlog Zt+1\nZt\n= log\nX\ni\n˜w(t)\ni\nZt\ne−ηvt,i = log\nX\ni\nw(t)\ni\ne−ηvt,i Using the inequality e−a ≤1 −a + a2/2, which holds for all a ∈(0, 1), and the\nfact that P\ni w(t)\ni\n= 1, we obtain\nlog Zt+1\nZt\n≤log\nX\ni\nw(t)\ni\n\u00001 −ηvt,i + η2v2\nt,i/2\n\u0001\n= log\n\u00001 −\nX\ni\nw(t)\ni\n\u0000ηvt,i −η2v2\nt,i/2\n\u0001\n|\n{z\n}\ndef\n= b\n\u0001 Next, note that b ∈(0, 1)",
      "word_count": 204,
      "source_page": 296,
      "start_position": 107363,
      "end_position": 107566,
      "sentences_count": 7
    },
    {
      "chunk_id": 473,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n297\nSumming this inequality over t we get\nlog(ZT +1) −log(Z1) =\nT\nX\nt=1\nlog Zt+1\nZt\n≤−η\nT\nX\nt=1\n⟨w(t), vt⟩+ T η2\n2 (21.3)\nNext, we lower bound ZT +1 For each i, we can rewrite ˜w(T +1)\ni\n= e−η P\nt vt,i and\nwe get that\nlog ZT +1 = log\n X\ni\ne−η P\nt vt,i ≥log\n\u0010\nmax\ni\ne−η P\nt vt,i\u0011\n= −η min\ni\nX\nt\nvt,i Combining the preceding with Equation (21.3) and using the fact that log(Z1) =\nlog(d) we get that\n−η min\ni\nX\nt\nvt,i −log(d) ≤\n−η\nT\nX\nt=1\n⟨w(t), vt⟩+ T η2\n2\n,\nwhich can be rearranged as follows:\nT\nX\nt=1\n⟨w(t), vt⟩−min\ni\nX\nt\nvt,i ≤log(d)\nη\n+ η T\n2 Plugging the value of η into the equation concludes our proof Proof of Theorem 21.10\nEquipped with the Weighted-Majority algorithm and Theorem 21.11, we are\nready to prove Theorem 21.10 We start with the simpler case, in which H is\na ﬁnite class, and let us write H = {h1, , hd} In this case, we can refer to\neach hypothesis, hi, as an expert, whose advice is to predict hi(xt), and whose\ncost is vt,i = |hi(xt) −yt|",
      "word_count": 220,
      "source_page": 297,
      "start_position": 107622,
      "end_position": 107841,
      "sentences_count": 10
    },
    {
      "chunk_id": 474,
      "text": ", hd} In this case, we can refer to\neach hypothesis, hi, as an expert, whose advice is to predict hi(xt), and whose\ncost is vt,i = |hi(xt) −yt| The prediction of the algorithm will therefore be\npt = P\ni w(t)\ni hi(xt) ∈[0, 1], and the loss is\n|pt −yt| =\n\f\f\f\f\f\nd\nX\ni=1\nw(t)\ni hi(xt) −yt\n\f\f\f\f\f =\n\f\f\f\f\f\nd\nX\ni=1\nw(t)\ni (hi(xt) −yt) Now, if yt = 1, then for all i, hi(xt) −yt ≤0 Therefore, the above equals to\nP\ni w(t)\ni |hi(xt) −yt| If yt = 0 then for all i, hi(xt) −yt ≥0, and the above also\nequals P\ni w(t)\ni |hi(xt) −yt| All in all, we have shown that\n|pt −yt| =\nd\nX\ni=1\nw(t)\ni |hi(xt) −yt| = ⟨w(t), vt⟩ Furthermore, for each i, P\nt vt,i is exactly the number of mistakes hypothesis hi\nmakes Applying Theorem 21.11 we obtain",
      "word_count": 154,
      "source_page": 297,
      "start_position": 107813,
      "end_position": 107966,
      "sentences_count": 9
    },
    {
      "chunk_id": 475,
      "text": "298\nOnline Learning\ncorollary 21.12\nLet H be a ﬁnite hypothesis class There exists an algorithm\nfor online classiﬁcation, whose predictions come from [0, 1], that enjoys the regret\nbound\nT\nX\nt=1\n|pt −yt| −min\nh∈H\nT\nX\nt=1\n|h(xt) −yt| ≤\np\n2 log(|H|) T Next, we consider the case of a general hypothesis class Previously, we con-\nstructed an expert for each individual hypothesis However, if H is inﬁnite this\nleads to a vacuous bound The main idea is to construct a set of experts in a\nmore sophisticated way The challenge is how to deﬁne a set of experts that, on\none hand, is not excessively large and, on the other hand, contains experts that\ngive accurate predictions We construct the set of experts so that for each hypothesis h ∈H and every\nsequence of instances, x1, x2, , xT , there exists at least one expert in the set\nwhich behaves exactly as h on these instances For each L ≤Ldim(H) and each\nsequence 1 ≤i1 < i2 < · · · < iL ≤T we deﬁne an expert The expert simulates\nthe game between SOA (presented in the previous section) and the environment\non the sequence of instances x1, x2, , xT assuming that SOA makes a mistake\nprecisely in rounds i1, i2, , iL The expert is deﬁned by the following algorithm Expert(i1, i2,",
      "word_count": 231,
      "source_page": 298,
      "start_position": 107967,
      "end_position": 108197,
      "sentences_count": 15
    },
    {
      "chunk_id": 476,
      "text": "The expert is deﬁned by the following algorithm Expert(i1, i2, , iL)\ninput A hypothesis class H ; Indices i1 < i2 < · · · < iL\ninitialize: V1 = H\nfor t = 1, 2, , T\nreceive xt\nfor r ∈{0, 1} let V (r)\nt\n= {h ∈Vt : h(xt) = r}\ndeﬁne ˜yt = argmaxr Ldim\n\u0010\nV (r)\nt\n\u0011\n(in case of a tie set ˜yt = 0)\nif\nt ∈{i1, i2, , iL}\npredict ˆyt = 1 −˜yt\nelse\npredict ˆyt = ˜yt\nupdate Vt+1 = V (ˆyt)\nt\nNote that each such expert can give us predictions at every round t while only\nobserving the instances x1, , xt Our generic online learning algorithm is now\nan application of the Weighted-Majority algorithm with these experts To analyze the algorithm we ﬁrst note that the number of experts is\nd =\nLdim(H)\nX\nL=0\n\u0012T\nL\n\u0013 (21.4)\nIt can be shown that when T ≥Ldim(H)+2, the right-hand side of the equation\nis bounded by (eT/Ldim(H))Ldim(H) (the proof can be found in Lemma A.5).",
      "word_count": 182,
      "source_page": 298,
      "start_position": 108188,
      "end_position": 108369,
      "sentences_count": 9
    },
    {
      "chunk_id": 477,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n299\nTheorem 21.11 tells us that the expected number of mistakes of Weighted-Majority\nis at most the number of mistakes of the best expert plus\np\n2 log(d) T We will\nnext show that the number of mistakes of the best expert is at most the number\nof mistakes of the best hypothesis in H The following key lemma shows that,\non any sequence of instances, for each hypothesis h ∈H there exists an expert\nwith the same behavior lemma 21.13\nLet H be any hypothesis class with Ldim(H) < ∞ Let x1, x2, , xT\nbe any sequence of instances For any h ∈H, there exists L ≤Ldim(H) and in-\ndices 1 ≤i1 < i2 < · · · < iL ≤T such that when running Expert(i1, i2, , iL)\non the sequence x1, x2, , xT , the expert predicts h(xt) on each online round\nt = 1, 2, , T Proof\nFix h ∈H and the sequence x1, x2, , xT We must construct L and the\nindices i1, i2, , iL Consider running SOA on the input (x1, h(x1)), (x2, h(x2)), ., (xT , h(xT )) SOA makes at most Ldim(H) mistakes on such input We deﬁne\nL to be the number of mistakes made by SOA and we deﬁne {i1, i2, , iL} to\nbe the set of rounds in which SOA made the mistakes Now, consider the Expert(i1, i2, , iL) running on the sequence x1, x2,",
      "word_count": 250,
      "source_page": 299,
      "start_position": 108370,
      "end_position": 108619,
      "sentences_count": 21
    },
    {
      "chunk_id": 478,
      "text": "Now, consider the Expert(i1, i2, , iL) running on the sequence x1, x2, , xT By construction, the set Vt maintained by Expert(i1, i2, , iL) equals the set Vt\nmaintained by SOA when running on the sequence (x1, h(x1)), , (xT , h(xT )) The predictions of SOA diﬀer from the predictions of h if and only if the round is\nin {i1, i2, , iL} Since Expert(i1, i2, , iL) predicts exactly like SOA if t is not\nin {i1, i2, , iL} and the opposite of SOAs’ predictions if t is in {i1, i2, , iL},\nwe conclude that the predictions of the expert are always the same as the pre-\ndictions of h The previous lemma holds in particular for the hypothesis in H that makes the\nleast number of mistakes on the sequence of examples, and we therefore obtain\nthe following:\ncorollary 21.14\nLet (x1, y1), (x2, y2), , (xT , yT ) be a sequence of examples\nand let H be a hypothesis class with Ldim(H) < ∞ There exists L ≤Ldim(H)\nand indices 1 ≤i1 < i2 < · · · < iL ≤T, such that Expert(i1, i2, , iL) makes\nat most as many mistakes as the best h ∈H does, namely,\nmin\nh∈H\nT\nX\nt=1\n|h(xt) −yt|\nmistakes on the sequence of examples Together with Theorem 21.11, the upper bound part of Theorem 21.10 is\nproven.",
      "word_count": 236,
      "source_page": 299,
      "start_position": 108607,
      "end_position": 108842,
      "sentences_count": 17
    },
    {
      "chunk_id": 479,
      "text": "300\nOnline Learning\n21.3\nOnline Convex Optimization\nIn Chapter 12 we studied convex learning problems and showed learnability\nresults for these problems in the agnostic PAC learning framework In this section\nwe show that similar learnability results hold for convex problems in the online\nlearning framework In particular, we consider the following problem Online Convex Optimization\ndeﬁnitions:\nhypothesis class H ; domain Z ; loss function ℓ: H × Z →R\nassumptions:\nH is convex\n∀z ∈Z, ℓ(·, z) is a convex function\nfor t = 1, 2, , T\nlearner predicts a vector w(t) ∈H\nenvironment responds with zt ∈Z\nlearner suﬀers loss ℓ(w(t), zt)\nAs in the online classiﬁcation problem, we analyze the regret of the algorithm Recall that the regret of an online algorithm with respect to a competing hy-\npothesis, which here will be some vector w⋆∈H, is deﬁned as\nRegretA(w⋆, T) =\nT\nX\nt=1\nℓ(w(t), zt) −\nT\nX\nt=1\nℓ(w⋆, zt) (21.5)\nAs before, the regret of the algorithm relative to a set of competing vectors, H,\nis deﬁned as\nRegretA(H, T) = sup\nw⋆∈H\nRegretA(w⋆, T) In Chapter 14 we have shown that Stochastic Gradient Descent solves convex\nlearning problems in the agnostic PAC model We now show that a very similar\nalgorithm, Online Gradient Descent, solves online convex learning problems Online Gradient Descent\nparameter: η > 0\ninitialize: w(1) = 0\nfor t = 1, 2,",
      "word_count": 235,
      "source_page": 300,
      "start_position": 108843,
      "end_position": 109077,
      "sentences_count": 10
    },
    {
      "chunk_id": 480,
      "text": "21.4 The Online Perceptron Algorithm\n301\ntheorem 21.15\nThe Online Gradient Descent algorithm enjoys the following\nregret bound for every w⋆∈H,\nRegretA(w⋆, T) ≤∥w⋆∥2\n2η\n+ η\n2\nT\nX\nt=1\n∥vt∥2 If we further assume that ft is ρ-Lipschitz for all t, then setting η = 1/\n√\nT yields\nRegretA(w⋆, T) ≤1\n2(∥w⋆∥2 + ρ2)\n√\nT If we further assume that H is B-bounded and we set η =\nB\nρ\n√\nT then\nRegretA(H, T) ≤B ρ\n√\nT Proof\nThe analysis is similar to the analysis of Stochastic Gradient Descent\nwith projections Using the projection lemma, the deﬁnition of w(t+ 1\n2 ), and the\ndeﬁnition of subgradients, we have that for every t,\n∥w(t+1) −w⋆∥2 −∥w(t) −w⋆∥2\n= ∥w(t+1) −w⋆∥2 −∥w(t+ 1\n2 ) −w⋆∥2 + ∥w(t+ 1\n2 ) −w⋆∥2 −∥w(t) −w⋆∥2\n≤∥w(t+ 1\n2 ) −w⋆∥2 −∥w(t) −w⋆∥2\n= ∥w(t) −ηvt −w⋆∥2 −∥w(t) −w⋆∥2\n= −2η⟨w(t) −w⋆, vt⟩+ η2∥vt∥2\n≤−2η(ft(w(t)) −ft(w⋆)) + η2∥vt∥2 Summing over t and observing that the left-hand side is a telescopic sum we\nobtain that\n∥w(T +1) −w⋆∥2 −∥w(1) −w⋆∥2 ≤−2η\nT\nX\nt=1\n(ft(w(t)) −ft(w⋆)) + η2\nT\nX\nt=1\n∥vt∥2 Rearranging the inequality and using the fact that w(1) = 0, we get that\nT\nX\nt=1\n(ft(w(t)) −ft(w⋆)) ≤∥w(1) −w⋆∥2 −∥w(T +1) −w⋆∥2\n2η\n+ η\n2\nT\nX\nt=1\n∥vt∥2\n≤∥w⋆∥2\n2η\n+ η\n2\nT\nX\nt=1\n∥vt∥2 This proves the ﬁrst bound in the theorem",
      "word_count": 243,
      "source_page": 301,
      "start_position": 109111,
      "end_position": 109353,
      "sentences_count": 8
    },
    {
      "chunk_id": 481,
      "text": "302\nOnline Learning\nw ∈Rd} In Section 9.1.2 we have presented the batch version of the Perceptron,\nwhich aims to solve the ERM problem with respect to H We now present an\nonline version of the Perceptron algorithm Let X = Rd, Y = {−1, 1} On round t, the learner receives a vector xt ∈Rd The learner maintains a weight vector w(t) ∈Rd and predicts pt = sign(⟨w(t), xt⟩) Then, it receives yt ∈Y and pays 1 if pt ̸= yt and 0 otherwise The goal of the learner is to make as few prediction mistakes as possible In\nSection 21.1 we characterized the optimal algorithm and showed that the best\nachievable mistake bound depends on the Littlestone dimension of the class We show later that if d ≥2 then Ldim(H) = ∞, which implies that we have\nno hope of making few prediction mistakes Indeed, consider the tree for which\nv1 = ( 1\n2, 1, 0, , 0), v2 = ( 1\n4, 1, 0, , 0), v3 = ( 3\n4, 1, 0, , 0), etc Because of\nthe density of the reals, this tree is shattered by the subset of H which contains\nall hypotheses that are parametrized by w of the form w = (−1, a, 0, , 0), for\na ∈[0, 1] We conclude that indeed Ldim(H) = ∞ To sidestep this impossibility result, the Perceptron algorithm relies on the\ntechnique of surrogate convex losses (see Section 12.3)",
      "word_count": 245,
      "source_page": 302,
      "start_position": 109399,
      "end_position": 109643,
      "sentences_count": 18
    },
    {
      "chunk_id": 482,
      "text": "We conclude that indeed Ldim(H) = ∞ To sidestep this impossibility result, the Perceptron algorithm relies on the\ntechnique of surrogate convex losses (see Section 12.3) This is also closely related\nto the notion of margin we studied in Chapter 15 A weight vector w makes a mistake on an example (x, y) whenever the sign of\n⟨w, x⟩does not equal y Therefore, we can write the 0−1 loss function as follows\nℓ(w, (x, y)) = 1[y⟨w,x⟩≤0] On rounds on which the algorithm makes a prediction mistake, we shall use the\nhinge-loss as a surrogate convex loss function\nft(w) = max{0, 1 −yt⟨w, xt⟩} The hinge-loss satisﬁes the two conditions:\n• ft is a convex function\n• For all w, ft(w) ≥ℓ(w, (xt, yt)) In particular, this holds for w(t) On rounds on which the algorithm is correct, we shall deﬁne ft(w) = 0 Clearly,\nft is convex in this case as well Furthermore, ft(w(t)) = ℓ(w(t), (xt, yt)) = 0 Remark 21.5\nIn Section 12.3 we used the same surrogate loss function for all the\nexamples In the online model, we allow the surrogate to depend on the speciﬁc\nround It can even depend on w(t) Our ability to use a round speciﬁc surrogate\nstems from the worst-case type of analysis we employ in online learning Let us now run the Online Gradient Descent algorithm on the sequence of\nfunctions, f1, , fT , with the hypothesis class being all vectors in Rd (hence,\nthe projection step is vacuous)",
      "word_count": 250,
      "source_page": 302,
      "start_position": 109618,
      "end_position": 109867,
      "sentences_count": 17
    },
    {
      "chunk_id": 483,
      "text": "21.4 The Online Perceptron Algorithm\n303\nfunction and we can take vt = 0 Otherwise, it is easy to verify that vt = −ytxt\nis in ∂ft(w(t)) We therefore obtain the update rule\nw(t+1) =\n(\nw(t)\nif yt⟨w(t), xt⟩> 0\nw(t) + ηytxt\notherwise\nDenote by M the set of rounds in which sign(⟨w(t), xt⟩) ̸= yt Note that on\nround t, the prediction of the Perceptron can be rewritten as\npt = sign(⟨w(t), xt⟩) = sign\n \nη\nX\ni∈M:i<t\nyi ⟨xi, xt⟩ This form implies that the predictions of the Perceptron algorithm and the set\nM do not depend on the actual value of η as long as η > 0 We have therefore\nobtained the Perceptron algorithm:\nPerceptron\ninitialize: w1 = 0\nfor t = 1, 2, , T\nreceive xt\npredict pt = sign(⟨w(t), xt⟩)\nif yt⟨w(t), xt⟩≤0\nw(t+1) = w(t) + ytxt\nelse\nw(t+1) = w(t)\nTo analyze the Perceptron, we rely on the analysis of Online Gradient De-\nscent given in the previous section In our case, the subgradient of ft we use\nin the Perceptron is vt = −1[yt⟨w(t),xt⟩≤0] yt xt Indeed, the Perceptron’s update\nis w(t+1) = w(t) −vt, and as discussed before this is equivalent to w(t+1) =\nw(t) −ηvt for every η > 0 Therefore, Theorem 21.15 tells us that\nT\nX\nt=1\nft(w(t)) −\nT\nX\nt=1\nft(w⋆) ≤1\n2η ∥w⋆∥2\n2 + η\n2\nT\nX\nt=1\n∥vt∥2\n2",
      "word_count": 241,
      "source_page": 303,
      "start_position": 109901,
      "end_position": 110141,
      "sentences_count": 10
    },
    {
      "chunk_id": 484,
      "text": "304\nOnline Learning\ntheorem 21.16\nSuppose that the Perceptron algorithm runs on a sequence\n(x1, y1), , (xT , yT ) and let R = maxt ∥xt∥ Let M be the rounds on which the\nPerceptron errs and let ft(w) = 1[t∈M] [1 −yt⟨w, xt⟩]+ Then, for every w⋆\n|M| ≤\nX\nt\nft(w⋆) + R ∥w⋆∥\nsX\nt\nft(w⋆) + R2 ∥w⋆∥2 In particular, if there exists w⋆such that yt⟨w⋆, xt⟩≥1 for all t then\n|M| ≤R2 ∥w⋆∥2 Proof\nThe theorem follows from Equation (21.6) and the following claim: Given\nx, b, c ∈R+, the inequality x −b √x −c ≤0 implies that x ≤c + b2 + b √c The\nlast claim can be easily derived by analyzing the roots of the convex parabola\nQ(y) = y2 −by −c The last assumption of Theorem 21.16 is called separability with large margin\n(see Chapter 15) That is, there exists w⋆that not only satisﬁes that the point\nxt lies on the correct side of the halfspace, it also guarantees that xt is not too\nclose to the decision boundary More speciﬁcally, the distance from xt to the\ndecision boundary is at least γ = 1/∥w⋆∥and the bound becomes (R/γ)2 When the separability assumption does not hold, the bound involves the term\n[1 −yt⟨w⋆, xt⟩]+ which measures how much the separability with margin require-\nment is violated",
      "word_count": 226,
      "source_page": 304,
      "start_position": 110204,
      "end_position": 110429,
      "sentences_count": 11
    },
    {
      "chunk_id": 485,
      "text": "More speciﬁcally, the distance from xt to the\ndecision boundary is at least γ = 1/∥w⋆∥and the bound becomes (R/γ)2 When the separability assumption does not hold, the bound involves the term\n[1 −yt⟨w⋆, xt⟩]+ which measures how much the separability with margin require-\nment is violated As a last remark we note that there can be cases in which there exists some\nw⋆that makes zero errors on the sequence but the Perceptron will make many\nerrors Indeed, this is a direct consequence of the fact that Ldim(H) = ∞ The\nway we sidestep this impossibility result is by assuming more on the sequence of\nexamples – the bound in Theorem 21.16 will be meaningful only if the cumulative\nsurrogate loss, P\nt ft(w⋆) is not excessively large 21.5\nSummary\nIn this chapter we have studied the online learning model Many of the results\nwe derived for the PAC learning model have an analog in the online model First,\nwe have shown that a combinatorial dimension, the Littlestone dimension, char-\nacterizes online learnability To show this, we introduced the SOA algorithm (for\nthe realizable case) and the Weighted-Majority algorithm (for the unrealizable\ncase) We have also studied online convex optimization and have shown that\nonline gradient descent is a successful online learner whenever the loss function\nis convex and Lipschitz Finally, we presented the online Perceptron algorithm\nas a combination of online gradient descent and the concept of surrogate convex\nloss functions.",
      "word_count": 242,
      "source_page": 304,
      "start_position": 110383,
      "end_position": 110624,
      "sentences_count": 11
    },
    {
      "chunk_id": 486,
      "text": "21.6 Bibliographic Remarks\n305\n21.6\nBibliographic Remarks\nThe Standard Optimal Algorithm was derived by the seminal work of Lit-\ntlestone (1988) A generalization to the nonrealizable case, as well as other\nvariants like margin-based Littlestone’s dimension, were derived in (Ben-David\net al 2009) Characterizations of online learnability beyond classiﬁcation have\nbeen obtained in (Abernethy, Bartlett, Rakhlin & Tewari 2008, Rakhlin, Srid-\nharan & Tewari 2010, Daniely et al 2011) The Weighted-Majority algorithm is\ndue to (Littlestone & Warmuth 1994) and (Vovk 1990) The term “online convex programming” was introduced by Zinkevich (2003)\nbut this setting was introduced some years earlier by Gordon (1999) The Per-\nceptron dates back to Rosenblatt (Rosenblatt 1958) An analysis for the re-\nalizable case (with margin assumptions) appears in (Agmon 1954, Minsky &\nPapert 1969) Freund and Schapire (Freund & Schapire 1999) presented an anal-\nysis for the unrealizable case with a squared-hinge-loss based on a reduction to\nthe realizable case A direct analysis for the unrealizable case with the hinge-loss\nwas given by Gentile (Gentile 2003) For additional information we refer the reader to Cesa-Bianchi & Lugosi (2006)\nand Shalev-Shwartz (2011) 21.7\nExercises\n1 Find a hypothesis class H and a sequence of examples on which Consistent\nmakes |H| −1 mistakes 2 Find a hypothesis class H and a sequence of examples on which the mistake\nbound of the Halving algorithm is tight 3 Let d ≥2, X = {1, , d} and let H = {hj : j ∈[d]}, where hj(x) = 1[x=j]",
      "word_count": 250,
      "source_page": 305,
      "start_position": 110625,
      "end_position": 110874,
      "sentences_count": 19
    },
    {
      "chunk_id": 487,
      "text": "Let d ≥2, X = {1, , d} and let H = {hj : j ∈[d]}, where hj(x) = 1[x=j] Calculate MHalving(H) (i.e., derive lower and upper bounds on MHalving(H),\nand prove that they are equal) 4 The Doubling Trick:\nIn Theorem 21.15, the parameter η depends on the time horizon T In this\nexercise we show how to get rid of this dependence by a simple trick Consider an algorithm that enjoys a regret bound of the form α\n√\nT, but\nits parameters require the knowledge of T The doubling trick, described in\nthe following, enables us to convert such an algorithm into an algorithm that\ndoes not need to know the time horizon The idea is to divide the time into\nperiods of increasing size and run the original algorithm on each period The Doubling Trick\ninput: algorithm A whose parameters depend on the time horizon\nfor m = 0, 1, 2, run A on the 2m rounds t = 2m, , 2m+1 −1",
      "word_count": 167,
      "source_page": 305,
      "start_position": 110855,
      "end_position": 111021,
      "sentences_count": 12
    },
    {
      "chunk_id": 488,
      "text": "306\nOnline Learning\nShow that if the regret of A on each period of 2m rounds is at most α\n√\n2m,\nthen the total regret is at most\n√\n2\n√\n2 −1 α\n√\nT 5 Online-to-batch Conversions: In this exercise we demonstrate how a suc-\ncessful online learning algorithm can be used to derive a successful PAC\nlearner as well Consider a PAC learning problem for binary classiﬁcation parameterized\nby an instance domain, X, and a hypothesis class, H Suppose that there exists\nan online learning algorithm, A, which enjoys a mistake bound MA(H) < ∞ Consider running this algorithm on a sequence of T examples which are sam-\npled i.i.d from a distribution D over the instance space X, and are labeled by\nsome h⋆∈H Suppose that for every round t, the prediction of the algorithm\nis based on a hypothesis ht : X →{0, 1} Show that\nE[LD(hr)] ≤MA(H)\nT\n,\nwhere the expectation is over the random choice of the instances as well as a\nrandom choice of r according to the uniform distribution over [T] Hint: Use similar arguments to the ones appearing in the proof of Theo-\nrem 14.8.",
      "word_count": 197,
      "source_page": 306,
      "start_position": 111022,
      "end_position": 111218,
      "sentences_count": 10
    },
    {
      "chunk_id": 489,
      "text": "22\nClustering\nClustering is one of the most widely used techniques for exploratory data anal-\nysis Across all disciplines, from social sciences to biology to computer science,\npeople try to get a ﬁrst intuition about their data by identifying meaningful\ngroups among the data points For example, computational biologists cluster\ngenes on the basis of similarities in their expression in diﬀerent experiments; re-\ntailers cluster customers, on the basis of their customer proﬁles, for the purpose\nof targeted marketing; and astronomers cluster stars on the basis of their spacial\nproximity The ﬁrst point that one should clarify is, naturally, what is clustering In-\ntuitively, clustering is the task of grouping a set of objects such that similar\nobjects end up in the same group and dissimilar objects are separated into dif-\nferent groups Clearly, this description is quite imprecise and possibly ambiguous Quite surprisingly, it is not at all clear how to come up with a more rigorous\ndeﬁnition There are several sources for this diﬃculty One basic problem is that the\ntwo objectives mentioned in the earlier statement may in many cases contradict\neach other Mathematically speaking, similarity (or proximity) is not a transi-\ntive relation, while cluster sharing is an equivalence relation and, in particular,\nit is a transitive relation More concretely, it may be the case that there is a\nlong sequence of objects, x1,",
      "word_count": 228,
      "source_page": 307,
      "start_position": 111219,
      "end_position": 111446,
      "sentences_count": 11
    },
    {
      "chunk_id": 490,
      "text": "Mathematically speaking, similarity (or proximity) is not a transi-\ntive relation, while cluster sharing is an equivalence relation and, in particular,\nit is a transitive relation More concretely, it may be the case that there is a\nlong sequence of objects, x1, , xm such that each xi is very similar to its two\nneighbors, xi−1 and xi+1, but x1 and xm are very dissimilar If we wish to make\nsure that whenever two elements are similar they share the same cluster, then\nwe must put all of the elements of the sequence in the same cluster However,\nin that case, we end up with dissimilar elements (x1 and xm) sharing a cluster,\nthus violating the second requirement To illustrate this point further, suppose that we would like to cluster the points\nin the following picture into two clusters A clustering algorithm that emphasizes not separating close-by points (e.g., the\nSingle Linkage algorithm that will be described in Section 22.1) will cluster this\ninput by separating it horizontally according to the two lines:\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 202,
      "source_page": 307,
      "start_position": 111405,
      "end_position": 111606,
      "sentences_count": 11
    },
    {
      "chunk_id": 491,
      "text": "308\nClustering\nIn contrast, a clustering method that emphasizes not having far-away points\nshare the same cluster (e.g., the 2-means algorithm that will be described in\nSection 22.1) will cluster the same input by dividing it vertically into the right-\nhand half and the left-hand half:\nAnother basic problem is the lack of “ground truth” for clustering, which is a\ncommon problem in unsupervised learning So far in the book, we have mainly\ndealt with supervised learning (e.g., the problem of learning a classiﬁer from\nlabeled training data) The goal of supervised learning is clear – we wish to\nlearn a classiﬁer which will predict the labels of future examples as accurately\nas possible Furthermore, a supervised learner can estimate the success, or the\nrisk, of its hypotheses using the labeled training data by computing the empirical\nloss In contrast, clustering is an unsupervised learning problem; namely, there\nare no labels that we try to predict Instead, we wish to organize the data in\nsome meaningful way As a result, there is no clear success evaluation procedure\nfor clustering In fact, even on the basis of full knowledge of the underlying data\ndistribution, it is not clear what is the “correct” clustering for that data or how\nto evaluate a proposed clustering Consider, for example, the following set of points in R2:\nand suppose we are required to cluster them into two clusters We have two\nhighly justiﬁable solutions:",
      "word_count": 239,
      "source_page": 308,
      "start_position": 111607,
      "end_position": 111845,
      "sentences_count": 10
    },
    {
      "chunk_id": 492,
      "text": "Clustering\n309\nThis phenomenon is not just artiﬁcial but occurs in real applications A given\nset of objects can be clustered in various diﬀerent meaningful ways This may\nbe due to having diﬀerent implicit notions of distance (or similarity) between\nobjects, for example, clustering recordings of speech by the accent of the speaker\nversus clustering them by content, clustering movie reviews by movie topic versus\nclustering them by the review sentiment, clustering paintings by topic versus\nclustering them by style, and so on To summarize, there may be several very diﬀerent conceivable clustering so-\nlutions for a given data set As a result, there is a wide variety of clustering\nalgorithms that, on some input data, will output very diﬀerent clusterings A Clustering Model:\nClustering tasks can vary in terms of both the type of input they have and the\ntype of outcome they are expected to compute For concreteness, we shall focus\non the following common setup:\nInput — a set of elements, X, and a distance function over it That is, a function\nd : X × X →R+ that is symmetric, satisﬁes d(x, x) = 0 for all x ∈X\nand often also satisﬁes the triangle inequality Alternatively, the function\ncould be a similarity function s : X × X →[0, 1] that is symmetric\nand satisﬁes s(x, x) = 1 for all x ∈X Additionally, some clustering\nalgorithms also require an input parameter k (determining the number\nof required clusters)",
      "word_count": 244,
      "source_page": 309,
      "start_position": 111846,
      "end_position": 112089,
      "sentences_count": 10
    },
    {
      "chunk_id": 493,
      "text": "Alternatively, the function\ncould be a similarity function s : X × X →[0, 1] that is symmetric\nand satisﬁes s(x, x) = 1 for all x ∈X Additionally, some clustering\nalgorithms also require an input parameter k (determining the number\nof required clusters) Output — a partition of the domain set X into subsets That is, C = (C1, Ck)\nwhere Sk\ni=1 Ci = X and for all i ̸= j, Ci ∩Cj = ∅ In some situations the\nclustering is “soft,” namely, the partition of X into the diﬀerent clusters\nis probabilistic where the output is a function assigning to each domain\npoint, x ∈X, a vector (p1(x), , pk(x)), where pi(x) = P[x ∈Ci] is\nthe probability that x belongs to cluster Ci Another possible output is\na clustering dendrogram (from Greek dendron = tree, gramma = draw-\ning), which is a hierarchical tree of domain subsets, having the singleton\nsets in its leaves, and the full domain as its root We shall discuss this\nformulation in more detail in the following.",
      "word_count": 176,
      "source_page": 309,
      "start_position": 112046,
      "end_position": 112221,
      "sentences_count": 9
    },
    {
      "chunk_id": 494,
      "text": "310\nClustering\nIn the following we survey some of the most popular clustering methods In\nthe last section of this chapter we return to the high level discussion of what is\nclustering 22.1\nLinkage-Based Clustering Algorithms\nLinkage-based clustering is probably the simplest and most straightforward paradigm\nof clustering These algorithms proceed in a sequence of rounds They start from\nthe trivial clustering that has each data point as a single-point cluster Then,\nrepeatedly, these algorithms merge the “closest” clusters of the previous cluster-\ning Consequently, the number of clusters decreases with each such round If kept\ngoing, such algorithms would eventually result in the trivial clustering in which\nall of the domain points share one large cluster Two parameters, then, need to\nbe determined to deﬁne such an algorithm clearly First, we have to decide how\nto measure (or deﬁne) the distance between clusters, and, second, we have to\ndetermine when to stop merging Recall that the input to a clustering algorithm\nis a between-points distance function, d There are many ways of extending d to\na measure of distance between domain subsets (or clusters) The most common\nways are\n1 Single Linkage clustering, in which the between-clusters distance is deﬁned\nby the minimum distance between members of the two clusters, namely,\nD(A, B)\ndef\n=\nmin{d(x, y) : x ∈A, y ∈B}\n2",
      "word_count": 224,
      "source_page": 310,
      "start_position": 112222,
      "end_position": 112445,
      "sentences_count": 14
    },
    {
      "chunk_id": 495,
      "text": "The most common\nways are\n1 Single Linkage clustering, in which the between-clusters distance is deﬁned\nby the minimum distance between members of the two clusters, namely,\nD(A, B)\ndef\n=\nmin{d(x, y) : x ∈A, y ∈B}\n2 Average Linkage clustering, in which the distance between two clusters is\ndeﬁned to be the average distance between a point in one of the clusters and\na point in the other, namely,\nD(A, B)\ndef\n=\n1\n|A||B|\nX\nx∈A, y∈B\nd(x, y)\n3 Max Linkage clustering, in which the distance between two clusters is deﬁned\nas the maximum distance between their elements, namely,\nD(A, B)\ndef\n=\nmax{d(x, y) : x ∈A, y ∈B} The linkage-based clustering algorithms are agglomerative in the sense that they\nstart from data that is completely fragmented and keep building larger and\nlarger clusters as they proceed Without employing a stopping rule, the outcome\nof such an algorithm can be described by a clustering dendrogram: that is, a tree\nof domain subsets, having the singleton sets in its leaves, and the full domain as\nits root For example, if the input is the elements X = {a, b, c, d, e} ⊂R2 with\nthe Euclidean distance as depicted on the left, then the resulting dendrogram is\nthe one depicted on the right:",
      "word_count": 217,
      "source_page": 310,
      "start_position": 112407,
      "end_position": 112623,
      "sentences_count": 7
    },
    {
      "chunk_id": 496,
      "text": "22.2 k-Means and Other Cost Minimization Clusterings\n311\nb\nc\nd\ne\na\n{a}\n{b}\n{c}\n{d}\n{e}\n{b, c}\n{d, e}\n{b, c, d, e}\n{a, b, c, d, e}\nThe single linkage algorithm is closely related to Kruskal’s algorithm for ﬁnding\na minimal spanning tree on a weighted graph Indeed, consider the full graph\nwhose vertices are elements of X and the weight of an edge (x, y) is the distance\nd(x, y) Each merge of two clusters performed by the single linkage algorithm\ncorresponds to a choice of an edge in the aforementioned graph It is also possible\nto show that the set of edges the single linkage algorithm chooses along its run\nforms a minimal spanning tree If one wishes to turn a dendrogram into a partition of the space (a clustering),\none needs to employ a stopping criterion Common stopping criteria include\n• Fixed number of clusters – ﬁx some parameter, k, and stop merging clusters\nas soon as the number of clusters is k • Distance upper bound – ﬁx some r ∈R+ Stop merging as soon as all the\nbetween-clusters distances are larger than r We can also set r to be\nα max{d(x, y) : x, y ∈X} for some α < 1",
      "word_count": 211,
      "source_page": 311,
      "start_position": 112624,
      "end_position": 112834,
      "sentences_count": 9
    },
    {
      "chunk_id": 497,
      "text": "Stop merging as soon as all the\nbetween-clusters distances are larger than r We can also set r to be\nα max{d(x, y) : x, y ∈X} for some α < 1 In that case the stopping\ncriterion is called “scaled distance upper bound.”\n22.2\nk-Means and Other Cost Minimization Clusterings\nAnother popular approach to clustering starts by deﬁning a cost function over a\nparameterized set of possible clusterings and the goal of the clustering algorithm\nis to ﬁnd a partitioning (clustering) of minimal cost Under this paradigm, the\nclustering task is turned into an optimization problem The objective function\nis a function from pairs of an input, (X, d), and a proposed clustering solution\nC = (C1, , Ck), to positive real numbers Given such an objective function,\nwhich we denote by G, the goal of a clustering algorithm is deﬁned as ﬁnding, for\na given input (X, d), a clustering C so that G((X, d), C) is minimized In order\nto reach that goal, one has to apply some appropriate search algorithm As it turns out, most of the resulting optimization problems are NP-hard, and\nsome are even NP-hard to approximate Consequently, when people talk about,\nsay, k-means clustering, they often refer to some particular common approxima-\ntion algorithm rather than the cost function or the corresponding exact solution\nof the minimization problem Many common objective functions require the number of clusters, k, as a",
      "word_count": 237,
      "source_page": 311,
      "start_position": 112803,
      "end_position": 113039,
      "sentences_count": 11
    },
    {
      "chunk_id": 498,
      "text": "312\nClustering\nparameter In practice, it is often up to the user of the clustering algorithm to\nchoose the parameter k that is most suitable for the given clustering problem In the following we describe some of the most common objective functions • The k-means objective function is one of the most popular clustering\nobjectives In k-means the data is partitioned into disjoint sets C1, , Ck\nwhere each Ci is represented by a centroid µi It is assumed that the input\nset X is embedded in some larger metric space (X ′, d) (so that X ⊆X ′)\nand centroids are members of X ′ The k-means objective function measures\nthe squared distance between each point in X to the centroid of its cluster The centroid of Ci is deﬁned to be\nµi(Ci) = argmin\nµ∈X ′\nX\nx∈Ci\nd(x, µ)2 Then, the k-means objective is\nGk−means((X, d), (C1, , Ck)) =\nk\nX\ni=1\nX\nx∈Ci\nd(x, µi(Ci))2 This can also be rewritten as\nGk−means((X, d), (C1, , Ck)) =\nmin\nµ1,...µk∈X ′\nk\nX\ni=1\nX\nx∈Ci\nd(x, µi)2 (22.1)\nThe k-means objective function is relevant, for example, in digital com-\nmunication tasks, where the members of X may be viewed as a collection\nof signals that have to be transmitted While X may be a very large set\nof real valued vectors, digital transmission allows transmitting of only a\nﬁnite number of bits for each signal",
      "word_count": 240,
      "source_page": 312,
      "start_position": 113040,
      "end_position": 113279,
      "sentences_count": 15
    },
    {
      "chunk_id": 499,
      "text": "(22.1)\nThe k-means objective function is relevant, for example, in digital com-\nmunication tasks, where the members of X may be viewed as a collection\nof signals that have to be transmitted While X may be a very large set\nof real valued vectors, digital transmission allows transmitting of only a\nﬁnite number of bits for each signal One way to achieve good transmis-\nsion under such constraints is to represent each member of X by a “close”\nmember of some ﬁnite set µ1, µk, and replace the transmission of any\nx ∈X by transmitting the index of the closest µi The k-means objective\ncan be viewed as a measure of the distortion created by such a transmission\nrepresentation scheme • The k-medoids objective function is similar to the k-means objective,\nexcept that it requires the cluster centroids to be members of the input\nset The objective function is deﬁned by\nGK−medoid((X, d), (C1, , Ck)) =\nmin\nµ1,...µk∈X\nk\nX\ni=1\nX\nx∈Ci\nd(x, µi)2 • The k-median objective function is quite similar to the k-medoids objec-\ntive, except that the “distortion” between a data point and the centroid\nof its cluster is measured by distance, rather than by the square of the\ndistance:\nGK−median((X, d), (C1, , Ck)) =\nmin\nµ1,...µk∈X\nk\nX\ni=1\nX\nx∈Ci\nd(x, µi).",
      "word_count": 220,
      "source_page": 312,
      "start_position": 113222,
      "end_position": 113441,
      "sentences_count": 10
    },
    {
      "chunk_id": 500,
      "text": "22.2 k-Means and Other Cost Minimization Clusterings\n313\nAn example where such an objective makes sense is the facility location\nproblem Consider the task of locating k ﬁre stations in a city One can\nmodel houses as data points and aim to place the stations so as to minimize\nthe average distance between a house and its closest ﬁre station The previous examples can all be viewed as center-based objectives The so-\nlution to such a clustering problem is determined by a set of cluster centers,\nand the clustering assigns each instance to the center closest to it More gener-\nally, center-based objective is determined by choosing some monotonic function\nf : R+ →R+ and then deﬁning\nGf((X, d), (C1, Ck)) =\nmin\nµ1,...µk∈X ′\nk\nX\ni=1\nX\nx∈Ci\nf(d(x, µi)),\nwhere X ′ is either X or some superset of X Some objective functions are not center based For example, the sum of in-\ncluster distances (SOD)\nGSOD((X, d), (C1, Ck)) =\nk\nX\ni=1\nX\nx,y∈Ci\nd(x, y)\nand the MinCut objective that we shall discuss in Section 22.3 are not center-\nbased objectives 22.2.1\nThe k-Means Algorithm\nThe k-means objective function is quite popular in practical applications of clus-\ntering However, it turns out that ﬁnding the optimal k-means solution is of-\nten computationally infeasible (the problem is NP-hard, and even NP-hard to\napproximate to within some constant)",
      "word_count": 232,
      "source_page": 313,
      "start_position": 113442,
      "end_position": 113673,
      "sentences_count": 12
    },
    {
      "chunk_id": 501,
      "text": "22.2.1\nThe k-Means Algorithm\nThe k-means objective function is quite popular in practical applications of clus-\ntering However, it turns out that ﬁnding the optimal k-means solution is of-\nten computationally infeasible (the problem is NP-hard, and even NP-hard to\napproximate to within some constant) As an alternative, the following simple\niterative algorithm is often used, so often that, in many cases, the term k-means\nClustering refers to the outcome of this algorithm rather than to the cluster-\ning that minimizes the k-means objective cost We describe the algorithm with\nrespect to the Euclidean distance function d(x, y) = ∥x −y∥ k-Means\ninput: X ⊂Rn ; Number of clusters k\ninitialize: Randomly choose initial centroids µ1, , µk\nrepeat until convergence\n∀i ∈[k] set Ci = {x ∈X : i = argminj ∥x −µj∥}\n(break ties in some arbitrary manner)\n∀i ∈[k] update µi =\n1\n|Ci|\nP\nx∈Ci x\nlemma 22.1\nEach iteration of the k-means algorithm does not increase the\nk-means objective function (as given in Equation (22.1)).",
      "word_count": 170,
      "source_page": 313,
      "start_position": 113629,
      "end_position": 113798,
      "sentences_count": 6
    },
    {
      "chunk_id": 502,
      "text": "314\nClustering\nProof\nTo simplify the notation, let us use the shorthand G(C1, , Ck) for the\nk-means objective, namely,\nG(C1, , Ck) =\nmin\nµ1,...,µk∈Rn\nk\nX\ni=1\nX\nx∈Ci\n∥x −µi∥2 (22.2)\nIt is convenient to deﬁne µ(Ci) =\n1\n|Ci|\nP\nx∈Ci x and note that µ(Ci) = argminµ∈Rn P\nx∈Ci ∥x−\nµ∥2 Therefore, we can rewrite the k-means objective as\nG(C1, , Ck) =\nk\nX\ni=1\nX\nx∈Ci\n∥x −µ(Ci)∥2 (22.3)\nConsider the update at iteration t of the k-means algorithm Let C(t−1)\n1\n, , C(t−1)\nk\nbe the previous partition, let µ(t−1)\ni\n= µ(C(t−1)\ni\n), and let C(t)\n1 , , C(t)\nk\nbe the\nnew partition assigned at iteration t Using the deﬁnition of the objective as\ngiven in Equation (22.2) we clearly have that\nG(C(t)\n1 , , C(t)\nk ) ≤\nk\nX\ni=1\nX\nx∈C(t)\ni\n∥x −µ(t−1)\ni\n∥2 (22.4)\nIn addition, the deﬁnition of the new partition (C(t)\n1 , , C(t)\nk ) implies that it\nminimizes the expression Pk\ni=1\nP\nx∈Ci ∥x −µ(t−1)\ni\n∥2 over all possible partitions\n(C1, , Ck) Hence,\nk\nX\ni=1\nX\nx∈C(t)\ni\n∥x −µ(t−1)\ni\n∥2 ≤\nk\nX\ni=1\nX\nx∈C(t−1)\ni\n∥x −µ(t−1)\ni\n∥2 (22.5)\nUsing Equation (22.3) we have that the right-hand side of Equation (22.5) equals\nG(C(t−1)\n1\n, , C(t−1)\nk\n) Combining this with Equation (22.4) and Equation (22.5),\nwe obtain that G(C(t)\n1 ,",
      "word_count": 247,
      "source_page": 314,
      "start_position": 113799,
      "end_position": 114045,
      "sentences_count": 19
    },
    {
      "chunk_id": 503,
      "text": ", C(t−1)\nk\n) Combining this with Equation (22.4) and Equation (22.5),\nwe obtain that G(C(t)\n1 , , C(t)\nk ) ≤G(C(t−1)\n1\n, , C(t−1)\nk\n), which concludes our\nproof While the preceding lemma tells us that the k-means objective is monotonically\nnonincreasing, there is no guarantee on the number of iterations the k-means al-\ngorithm needs in order to reach convergence Furthermore, there is no nontrivial\nlower bound on the gap between the value of the k-means objective of the al-\ngorithm’s output and the minimum possible value of that objective function In\nfact, k-means might converge to a point which is not even a local minimum (see\nExercise 2) To improve the results of k-means it is often recommended to repeat\nthe procedure several times with diﬀerent randomly chosen initial centroids (e.g.,\nwe can choose the initial centroids to be random points from the data).",
      "word_count": 150,
      "source_page": 314,
      "start_position": 114028,
      "end_position": 114177,
      "sentences_count": 8
    },
    {
      "chunk_id": 504,
      "text": "22.3 Spectral Clustering\n315\n22.3\nSpectral Clustering\nOften, a convenient way to represent the relationships between points in a data\nset X = {x1, , xm} is by a similarity graph; each vertex represents a data\npoint xi, and every two vertices are connected by an edge whose weight is their\nsimilarity, Wi,j = s(xi, xj), where W ∈Rm,m For example, we can set Wi,j =\nexp(−d(xi, xj)2/σ2), where d(·, ·) is a distance function and σ is a parameter The clustering problem can now be formulated as follows: We want to ﬁnd a\npartition of the graph such that the edges between diﬀerent groups have low\nweights and the edges within a group have high weights In the clustering objectives described previously, the focus was on one side\nof our intuitive deﬁnition of clustering – making sure that points in the same\ncluster are similar We now present objectives that focus on the other requirement\n– points separated into diﬀerent clusters should be nonsimilar 22.3.1\nGraph Cut\nGiven a graph represented by a similarity matrix W, the simplest and most\ndirect way to construct a partition of the graph is to solve the mincut problem,\nwhich chooses a partition C1, , Ck that minimizes the objective\ncut(C1, , Ck) =\nk\nX\ni=1\nX\nr∈Ci,s/∈Ci\nWr,s For k = 2, the mincut problem can be solved eﬃciently However, in practice it\noften does not lead to satisfactory partitions",
      "word_count": 239,
      "source_page": 315,
      "start_position": 114178,
      "end_position": 114416,
      "sentences_count": 11
    },
    {
      "chunk_id": 505,
      "text": "For k = 2, the mincut problem can be solved eﬃciently However, in practice it\noften does not lead to satisfactory partitions The problem is that in many cases,\nthe solution of mincut simply separates one individual vertex from the rest of the\ngraph Of course, this is not what we want to achieve in clustering, as clusters\nshould be reasonably large groups of points Several solutions to this problem have been suggested The simplest solution\nis to normalize the cut and deﬁne the normalized mincut objective as follows:\nRatioCut(C1, , Ck) =\nk\nX\ni=1\n1\n|Ci|\nX\nr∈Ci,s/∈Ci\nWr,s The preceding objective assumes smaller values if the clusters are not too small Unfortunately, introducing this balancing makes the problem computationally\nhard to solve Spectral clustering is a way to relax the problem of minimizing\nRatioCut 22.3.2\nGraph Laplacian and Relaxed Graph Cuts\nThe main mathematical object for spectral clustering is the graph Laplacian\nmatrix There are several diﬀerent deﬁnitions of graph Laplacian in the literature,\nand in the following we describe one particular deﬁnition.",
      "word_count": 176,
      "source_page": 315,
      "start_position": 114395,
      "end_position": 114570,
      "sentences_count": 12
    },
    {
      "chunk_id": 506,
      "text": "316\nClustering\ndefinition 22.2 (Unnormalized Graph Laplacian)\nThe unnormalized graph\nLaplacian is the m × m matrix L = D −W where D is a diagonal matrix with\nDi,i = Pm\nj=1 Wi,j The matrix D is called the degree matrix The following lemma underscores the relation between RatioCut and the Lapla-\ncian matrix lemma 22.3\nLet C1, , Ck be a clustering and let H ∈Rm,k be the matrix\nsuch that\nHi,j =\n1\n√\n|Cj| 1[i∈Cj] Then, the columns of H are orthonormal to each other and\nRatioCut(C1, , Ck) = trace(H⊤L H) Proof\nLet h1, , hk be the columns of H The fact that these vectors are\northonormal is immediate from the deﬁnition Next, by standard algebraic ma-\nnipulations, it can be shown that trace(H⊤L H) = Pk\ni=1 h⊤\ni Lhi and that for\nany vector v we have\nv⊤Lv = 1\n2\n X\nr\nDr,rv2\nr −2\nX\nr,s\nvrvsWr,s +\nX\ns\nDs,sv2\ns = 1\n2\nX\nr,s\nWr,s(vr −vs)2 Applying this with v = hi and noting that (hi,r −hi,s)2 is nonzero only if\nr ∈Ci, s /∈Ci or the other way around, we obtain that\nh⊤\ni Lhi =\n1\n|Ci|\nX\nr∈Ci,s/∈Ci\nWr,s Therefore, to minimize RatioCut we can search for a matrix H whose columns\nare orthonormal and such that each Hi,j is either 0 or 1/\np\n|Cj| Unfortunately,\nthis is an integer programming problem which we cannot solve eﬃciently",
      "word_count": 243,
      "source_page": 316,
      "start_position": 114571,
      "end_position": 114813,
      "sentences_count": 15
    },
    {
      "chunk_id": 507,
      "text": "22.4 Information Bottleneck*\n317\n22.3.3\nUnnormalized Spectral Clustering\nUnnormalized Spectral Clustering\nInput: W ∈Rm,m ; Number of clusters k\nInitialize: Compute the unnormalized graph Laplacian L\nLet U ∈Rm,k be the matrix whose columns are the eigenvectors of L\ncorresponding to the k smallest eigenvalues\nLet v1, , vm be the rows of U\nCluster the points v1, , vm using k-means\nOutput: Clusters C1, , CK of the k-means algorithm\nThe spectral clustering algorithm starts with ﬁnding the matrix H of the k\neigenvectors corresponding to the smallest eigenvalues of the graph Laplacian\nmatrix It then represents points according to the rows of H It is due to the\nproperties of the graph Laplacians that this change of representation is useful In many situations, this change of representation enables the simple k-means\nalgorithm to detect the clusters seamlessly Intuitively, if H is as deﬁned in\nLemma 22.3 then each point in the new representation is an indicator vector\nwhose value is nonzero only on the element corresponding to the cluster it belongs\nto 22.4\nInformation Bottleneck*\nThe information bottleneck method is a clustering technique introduced by\nTishby, Pereira, and Bialek It relies on notions from information theory To\nillustrate the method, consider the problem of clustering text documents where\neach document is represented as a bag-of-words; namely, each document is a\nvector x = {0, 1}n, where n is the size of the dictionary and xi = 1 iﬀthe word\ncorresponding to index i appears in the document",
      "word_count": 249,
      "source_page": 317,
      "start_position": 114883,
      "end_position": 115131,
      "sentences_count": 11
    },
    {
      "chunk_id": 508,
      "text": "It relies on notions from information theory To\nillustrate the method, consider the problem of clustering text documents where\neach document is represented as a bag-of-words; namely, each document is a\nvector x = {0, 1}n, where n is the size of the dictionary and xi = 1 iﬀthe word\ncorresponding to index i appears in the document Given a set of m documents,\nwe can interpret the bag-of-words representation of the m documents as a joint\nprobability over a random variable x, indicating the identity of a document (thus\ntaking values in [m]), and a random variable y, indicating the identity of a word\nin the dictionary (thus taking values in [n]) With this interpretation, the information bottleneck refers to the identity of\na clustering as another random variable, denoted C, that takes values in [k]\n(where k will be set by the method as well) Once we have formulated x, y, C\nas random variables, we can use tools from information theory to express a\nclustering objective In particular, the information bottleneck objective is\nmin\np(C|x) I(x; C) −βI(C; y) ,\nwhere I(·; ·) is the mutual information between two random variables,1 β is a\n1 That is, given a probability function, p over the pairs (x, C),",
      "word_count": 210,
      "source_page": 317,
      "start_position": 115074,
      "end_position": 115283,
      "sentences_count": 6
    },
    {
      "chunk_id": 509,
      "text": "318\nClustering\nparameter, and the minimization is over all possible probabilistic assignments of\npoints to clusters Intuitively, we would like to achieve two contradictory goals On one hand, we would like the mutual information between the identity of\nthe document and the identity of the cluster to be as small as possible This\nreﬂects the fact that we would like a strong compression of the original data On\nthe other hand, we would like high mutual information between the clustering\nvariable and the identity of the words, which reﬂects the goal that the “relevant”\ninformation about the document (as reﬂected by the words that appear in the\ndocument) is retained This generalizes the classical notion of minimal suﬃcient\nstatistics2 used in parametric statistics to arbitrary distributions Solving the optimization problem associated with the information bottleneck\nprinciple is hard in the general case Some of the proposed methods are similar\nto the EM principle, which we will discuss in Chapter 24 22.5\nA High Level View of Clustering\nSo far, we have mainly listed various useful clustering tools However, some fun-\ndamental questions remain unaddressed First and foremost, what is clustering What is it that distinguishes a clustering algorithm from any arbitrary function\nthat takes an input space and outputs a partition of that space Are there any\nbasic properties of clustering that are independent of any speciﬁc algorithm or\ntask One method for addressing such questions is via an axiomatic approach",
      "word_count": 241,
      "source_page": 318,
      "start_position": 115284,
      "end_position": 115524,
      "sentences_count": 14
    },
    {
      "chunk_id": 510,
      "text": "Are there any\nbasic properties of clustering that are independent of any speciﬁc algorithm or\ntask One method for addressing such questions is via an axiomatic approach There\nhave been several attempts to provide an axiomatic deﬁnition of clustering Let\nus demonstrate this approach by presenting the attempt made by Kleinberg\n(2003) Consider a clustering function, F, that takes as input any ﬁnite domain X\nwith a dissimilarity function d over its pairs and returns a partition of X Consider the following three properties of such a function:\nScale Invariance (SI) For any domain set X, dissimilarity function d, and\nany α > 0, the following should hold: F(X, d) = F(X, αd) (where\n(αd)(x, y)\ndef\n= α d(x, y)) Richness (Ri) For any ﬁnite X and every partition C = (C1, Ck) of X (into\nnonempty subsets) there exists some dissimilarity function d over X such\nthat F(X, d) = C I(x; C) = P\na\nP\nb p(a, b) log\n\u0010\np(a,b)\np(a)p(b)\n\u0011\n, where the sum is over all values x can take and all\nvalues C can take",
      "word_count": 184,
      "source_page": 318,
      "start_position": 115498,
      "end_position": 115681,
      "sentences_count": 9
    },
    {
      "chunk_id": 511,
      "text": "22.5 A High Level View of Clustering\n319\nConsistency (Co) If d and d′ are dissimilarity functions over X, such that\nfor every x, y ∈X, if x, y belong to the same cluster in F(X, d) then\nd′(x, y) ≤d(x, y) and if x, y belong to diﬀerent clusters in F(X, d) then\nd′(x, y) ≥d(x, y), then F(X, d) = F(X, d′) A moment of reﬂection reveals that the Scale Invariance is a very natural\nrequirement – it would be odd to have the result of a clustering function depend\non the units used to measure between-point distances The Richness requirement\nbasically states that the outcome of the clustering function is fully controlled by\nthe function d, which is also a very intuitive feature The third requirement,\nConsistency, is the only requirement that refers to the basic (informal) deﬁnition\nof clustering – we wish that similar points will be clustered together and that\ndissimilar points will be separated to diﬀerent clusters, and therefore, if points\nthat already share a cluster become more similar, and points that are already\nseparated become even less similar to each other, the clustering function should\nhave even stronger “support” of its previous clustering decisions However, Kleinberg (2003) has shown the following “impossibility” result:\ntheorem 22.4\nThere exists no function, F, that satisﬁes all the three proper-\nties: Scale Invariance, Richness, and Consistency Proof\nAssume, by way of contradiction, that some F does satisfy all three\nproperties",
      "word_count": 243,
      "source_page": 319,
      "start_position": 115760,
      "end_position": 116002,
      "sentences_count": 6
    },
    {
      "chunk_id": 512,
      "text": "However, Kleinberg (2003) has shown the following “impossibility” result:\ntheorem 22.4\nThere exists no function, F, that satisﬁes all the three proper-\nties: Scale Invariance, Richness, and Consistency Proof\nAssume, by way of contradiction, that some F does satisfy all three\nproperties Pick some domain set X with at least three points By Richness,\nthere must be some d1 such that F(X, d1) = {{x} : x ∈X} and there also exists\nsome d2 such that F(X, d2) ̸= F(X, d1) Let α ∈R+ be such that for every x, y ∈X, αd2(x, y) ≥d1(x, y) Let d3 =\nαd2 Consider F(X, d3) By the Scale Invariance property of F, we should have\nF(X, d3) = F(X, d2) On the other hand, since all distinct x, y ∈X reside in\ndiﬀerent clusters w.r.t F(X, d1), and d3(x, y) ≥d1(x, y), the Consistency of F\nimplies that F(X, d3) = F(X, d1) This is a contradiction, since we chose d1, d2\nso that F(X, d2) ̸= F(X, d1) It is important to note that there is no single “bad property” among the three\nproperties For every pair of the the three axioms, there exist natural clustering\nfunctions that satisfy the two properties in that pair (one can even construct such\nexamples just by varying the stopping criteria for the Single Linkage clustering\nfunction)",
      "word_count": 222,
      "source_page": 319,
      "start_position": 115961,
      "end_position": 116182,
      "sentences_count": 13
    },
    {
      "chunk_id": 513,
      "text": "It is important to note that there is no single “bad property” among the three\nproperties For every pair of the the three axioms, there exist natural clustering\nfunctions that satisfy the two properties in that pair (one can even construct such\nexamples just by varying the stopping criteria for the Single Linkage clustering\nfunction) On the other hand, Kleinberg shows that any clustering algorithm\nthat minimizes any center-based objective function inevitably fails the consis-\ntency property (yet, the k-sum-of-in-cluster-distances minimization clustering\ndoes satisfy Consistency) The Kleinberg impossibility result can be easily circumvented by varying the\nproperties For example, if one wishes to discuss clustering functions that have\na ﬁxed number-of-clusters parameter, then it is natural to replace Richness by\nk-Richness (namely, the requirement that every partition of the domain into k\nsubsets is attainable by the clustering function) k-Richness, Scale Invariance\nand Consistency all hold for the k-means clustering and are therefore consistent.",
      "word_count": 154,
      "source_page": 319,
      "start_position": 116128,
      "end_position": 116281,
      "sentences_count": 6
    },
    {
      "chunk_id": 514,
      "text": "320\nClustering\nAlternatively, one can relax the Consistency property For example, say that two\nclusterings C = (C1, Ck) and C′ = (C′\n1, C′\nl) are compatible if for every\nclusters Ci ∈C and C′\nj ∈C′, either Ci ⊆C′\nj or C′\nj ⊆Ci or Ci ∩C′\nj = ∅(it is\nworthwhile noting that for every dendrogram, every two clusterings that are ob-\ntained by trimming that dendrogram are compatible) “Reﬁnement Consistency”\nis the requirement that, under the assumptions of the Consistency property, the\nnew clustering F(X, d′) is compatible with the old clustering F(X, d) Many\ncommon clustering functions satisfy this requirement as well as Scale Invariance\nand Richness Furthermore, one can come up with many other, diﬀerent, prop-\nerties of clustering functions that sound intuitive and desirable and are satisﬁed\nby some common clustering functions There are many ways to interpret these results We suggest to view it as indi-\ncating that there is no “ideal” clustering function Every clustering function will\ninevitably have some “undesirable” properties The choice of a clustering func-\ntion for any given task must therefore take into account the speciﬁc properties\nof that task There is no generic clustering solution, just as there is no clas-\nsiﬁcation algorithm that will learn every learnable task (as the No-Free-Lunch\ntheorem shows) Clustering, just like classiﬁcation prediction, must take into\naccount some prior knowledge about the speciﬁc task at hand",
      "word_count": 236,
      "source_page": 320,
      "start_position": 116282,
      "end_position": 116517,
      "sentences_count": 13
    },
    {
      "chunk_id": 515,
      "text": "There is no generic clustering solution, just as there is no clas-\nsiﬁcation algorithm that will learn every learnable task (as the No-Free-Lunch\ntheorem shows) Clustering, just like classiﬁcation prediction, must take into\naccount some prior knowledge about the speciﬁc task at hand 22.6\nSummary\nClustering is an unsupervised learning problem, in which we wish to partition\na set of points into “meaningful” subsets We presented several clustering ap-\nproaches including linkage-based algorithms, the k-means family, spectral clus-\ntering, and the information bottleneck We discussed the diﬃculty of formalizing\nthe intuitive meaning of clustering 22.7\nBibliographic Remarks\nThe k-means algorithm is sometimes named Lloyd’s algorithm, after Stuart\nLloyd, who proposed the method in 1957 For a more complete overview of\nspectral clustering we refer the reader to the excellent tutorial by Von Luxburg\n(2007) The information bottleneck method was introduced by Tishby, Pereira\n& Bialek (1999) For an additional discussion on the axiomatic approach see\nAckerman & Ben-David (2008) 22.8\nExercises\n1 Suboptimality of k-Means:\nFor every parameter t > 1, show that there\nexists an instance of the k-means problem for which the k-means algorithm",
      "word_count": 186,
      "source_page": 320,
      "start_position": 116475,
      "end_position": 116660,
      "sentences_count": 11
    },
    {
      "chunk_id": 516,
      "text": "22.8 Exercises\n321\n(might) ﬁnd a solution whose k-means objective is at least t · OPT, where\nOPT is the minimum k-means objective 2 k-Means Might Not Necessarily Converge to a Local Minimum:\nShow that the k-means algorithm might converge to a point which is not\na local minimum Hint: Suppose that k = 2 and the sample points are\n{1, 2, 3, 4} ⊂R suppose we initialize the k-means with the centers {2, 4};\nand suppose we break ties in the deﬁnition of Ci by assigning i to be the\nsmallest value in argminj ∥x −µj∥ 3 Given a metric space (X, d), where |X| < ∞, and k ∈N, we would like to ﬁnd\na partition of X into C1, , Ck which minimizes the expression\nGk−diam((X, d), (C1, , Ck)) = max\nj∈[d] diam(Cj),\nwhere diam(Cj) = maxx,x′∈Cj d(x, x′) (we use the convention diam(Cj) = 0\nif |Cj| < 2) Similarly to the k-means objective, it is NP-hard to minimize the k-\ndiam objective Fortunately, we have a very simple approximation algorithm:\nInitially, we pick some x ∈X and set µ1 = x Then, the algorithm iteratively\nsets\n∀j ∈{2, , k}, µj = argmax\nx∈X\nmin\ni∈[j−1] d(x, µi) Finally, we set\n∀i ∈[k], Ci = {x ∈X : i = argmin\nj∈[k]\nd(x, µj)} Prove that the algorithm described is a 2-approximation algorithm That\nis, if we denote its output by ˆC1, , ˆCk, and denote the optimal solution by\nC∗\n1,",
      "word_count": 248,
      "source_page": 321,
      "start_position": 116661,
      "end_position": 116908,
      "sentences_count": 16
    },
    {
      "chunk_id": 517,
      "text": "322\nClustering\nProve that for every k > 1 the k-diam clustering function deﬁned in the\nprevious exercise is not a center-based clustering function Hint: Given a clustering input (X, d), with |X| > 2, consider the eﬀect of\nadding many close-by points to some (but not all) of the members of X, on\neither the k-diam clustering or any given center-based clustering 5 Recall that we discussed three clustering “properties”: Scale Invariance, Rich-\nness, and Consistency Consider the Single Linkage clustering algorithm 1 Find which of the three properties is satisﬁed by Single Linkage with the\nFixed Number of Clusters (any ﬁxed nonzero number) stopping rule 2 Find which of the three properties is satisﬁed by Single Linkage with the\nDistance Upper Bound (any ﬁxed nonzero upper bound) stopping rule 3 Show that for any pair of these properties there exists a stopping criterion\nfor Single Linkage clustering, under which these two axioms are satisﬁed 6 Given some number k, let k-Richness be the following requirement:\nFor any ﬁnite X and every partition C = (C1, Ck) of X (into nonempty subsets)\nthere exists some dissimilarity function d over X such that F(X, d) = C Prove that, for every number k, there exists a clustering function that\nsatisﬁes the three properties: Scale Invariance, k-Richness, and Consistency.",
      "word_count": 218,
      "source_page": 322,
      "start_position": 117036,
      "end_position": 117253,
      "sentences_count": 15
    },
    {
      "chunk_id": 518,
      "text": "23\nDimensionality Reduction\nDimensionality reduction is the process of taking data in a high dimensional\nspace and mapping it into a new space whose dimensionality is much smaller This process is closely related to the concept of (lossy) compression in infor-\nmation theory There are several reasons to reduce the dimensionality of the\ndata First, high dimensional data impose computational challenges Moreover,\nin some situations high dimensionality might lead to poor generalization abili-\nties of the learning algorithm (for example, in Nearest Neighbor classiﬁers the\nsample complexity increases exponentially with the dimension—see Chapter 19) Finally, dimensionality reduction can be used for interpretability of the data, for\nﬁnding meaningful structure of the data, and for illustration purposes In this chapter we describe popular methods for dimensionality reduction In\nthose methods, the reduction is performed by applying a linear transformation\nto the original data That is, if the original data is in Rd and we want to embed\nit into Rn (n < d) then we would like to ﬁnd a matrix W ∈Rn,d that induces\nthe mapping x 7→Wx A natural criterion for choosing W is in a way that will\nenable a reasonable recovery of the original x It is not hard to show that in\ngeneral, exact recovery of x from Wx is impossible (see Exercise 1) The ﬁrst method we describe is called Principal Component Analysis (PCA)",
      "word_count": 229,
      "source_page": 323,
      "start_position": 117254,
      "end_position": 117482,
      "sentences_count": 12
    },
    {
      "chunk_id": 519,
      "text": "It is not hard to show that in\ngeneral, exact recovery of x from Wx is impossible (see Exercise 1) The ﬁrst method we describe is called Principal Component Analysis (PCA) In PCA, both the compression and the recovery are performed by linear transfor-\nmations and the method ﬁnds the linear transformations for which the diﬀerences\nbetween the recovered vectors and the original vectors are minimal in the least\nsquared sense Next, we describe dimensionality reduction using random matrices W We\nderive an important lemma, often called the “Johnson-Lindenstrauss lemma,”\nwhich analyzes the distortion caused by such a random dimensionality reduction\ntechnique Last, we show how one can reduce the dimension of all sparse vectors using\nagain a random matrix This process is known as Compressed Sensing In this\ncase, the recovery process is nonlinear but can still be implemented eﬃciently\nusing linear programming We conclude by underscoring the underlying “prior assumptions” behind PCA\nand compressed sensing, which can help us understand the merits and pitfalls of\nthe two methods Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 199,
      "source_page": 323,
      "start_position": 117452,
      "end_position": 117650,
      "sentences_count": 14
    },
    {
      "chunk_id": 520,
      "text": "324\nDimensionality Reduction\n23.1\nPrincipal Component Analysis (PCA)\nLet x1, , xm be m vectors in Rd We would like to reduce the dimensional-\nity of these vectors using a linear transformation A matrix W ∈Rn,d, where\nn < d, induces a mapping x 7→Wx, where Wx ∈Rn is the lower dimensionality\nrepresentation of x Then, a second matrix U ∈Rd,n can be used to (approxi-\nmately) recover each original vector x from its compressed version That is, for\na compressed vector y = Wx, where y is in the low dimensional space Rn, we\ncan construct ˜x = Uy, so that ˜x is the recovered version of x and resides in the\noriginal high dimensional space Rd In PCA, we ﬁnd the compression matrix W and the recovering matrix U so\nthat the total squared distance between the original and recovered vectors is\nminimal; namely, we aim at solving the problem\nargmin\nW ∈Rn,d,U∈Rd,n\nm\nX\ni=1\n∥xi −UWxi∥2\n2 (23.1)\nTo solve this problem we ﬁrst show that the optimal solution takes a speciﬁc\nform lemma 23.1\nLet (U, W) be a solution to Equation (23.1) Then the columns of\nU are orthonormal (namely, U ⊤U is the identity matrix of Rn) and W = U ⊤ Proof\nFix any U, W and consider the mapping x 7→UWx The range of this\nmapping, R = {UWx : x ∈Rd}, is an n dimensional linear subspace of Rd",
      "word_count": 239,
      "source_page": 324,
      "start_position": 117651,
      "end_position": 117889,
      "sentences_count": 12
    },
    {
      "chunk_id": 521,
      "text": "Proof\nFix any U, W and consider the mapping x 7→UWx The range of this\nmapping, R = {UWx : x ∈Rd}, is an n dimensional linear subspace of Rd Let\nV ∈Rd,n be a matrix whose columns form an orthonormal basis of this subspace,\nnamely, the range of V is R and V ⊤V = I Therefore, each vector in R can be\nwritten as V y where y ∈Rn For every x ∈Rd and y ∈Rn we have\n∥x −V y∥2\n2 = ∥x∥2 + y⊤V ⊤V y −2y⊤V ⊤x = ∥x∥2 + ∥y∥2 −2y⊤(V ⊤x),\nwhere we used the fact that V ⊤V is the identity matrix of Rn Minimizing the\npreceding expression with respect to y by comparing the gradient with respect\nto y to zero gives that y = V ⊤x Therefore, for each x we have that\nV V ⊤x = argmin\n˜x∈R\n∥x −˜x∥2\n2 In particular this holds for x1, , xm and therefore we can replace U, W by\nV, V ⊤and by that do not increase the objective\nm\nX\ni=1\n∥xi −UWxi∥2\n2 ≥\nm\nX\ni=1\n∥xi −V V ⊤xi∥2\n2 Since this holds for every U, W the proof of the lemma follows On the basis of the preceding lemma, we can rewrite the optimization problem\ngiven in Equation (23.1) as follows:\nargmin\nU∈Rd,n:U ⊤U=I\nm\nX\ni=1\n∥xi −UU ⊤xi∥2\n2 (23.2)",
      "word_count": 236,
      "source_page": 324,
      "start_position": 117860,
      "end_position": 118095,
      "sentences_count": 12
    },
    {
      "chunk_id": 522,
      "text": "23.1 Principal Component Analysis (PCA)\n325\nWe further simplify the optimization problem by using the following elementary\nalgebraic manipulations For every x ∈Rd and a matrix U ∈Rd,n such that\nU ⊤U = I we have\n∥x −UU ⊤x∥2 = ∥x∥2 −2x⊤UU ⊤x + x⊤UU ⊤UU ⊤x\n= ∥x∥2 −x⊤UU ⊤x\n= ∥x∥2 −trace(U ⊤xx⊤U),\n(23.3)\nwhere the trace of a matrix is the sum of its diagonal entries Since the trace is\na linear operator, this allows us to rewrite Equation (23.2) as follows:\nargmax\nU∈Rd,n:U ⊤U=I\ntrace\n \nU ⊤\nm\nX\ni=1\nxix⊤\ni U (23.4)\nLet A = Pm\ni=1 xix⊤\ni The matrix A is symmetric and therefore it can be\nwritten using its spectral decomposition as A = VDV ⊤, where D is diagonal and\nV ⊤V = VV ⊤= I Here, the elements on the diagonal of D are the eigenvalues of\nA and the columns of V are the corresponding eigenvectors We assume without\nloss of generality that D1,1 ≥D2,2 ≥· · · ≥Dd,d Since A is positive semideﬁnite\nit also holds that Dd,d ≥0 We claim that the solution to Equation (23.4) is\nthe matrix U whose columns are the n eigenvectors of A corresponding to the\nlargest n eigenvalues theorem 23.2\nLet x1, , xm be arbitrary vectors in Rd, let A = Pm\ni=1 xix⊤\ni ,\nand let u1, , un be n eigenvectors of the matrix A corresponding to the largest\nn eigenvalues of A",
      "word_count": 247,
      "source_page": 325,
      "start_position": 118096,
      "end_position": 118342,
      "sentences_count": 12
    },
    {
      "chunk_id": 523,
      "text": ", xm be arbitrary vectors in Rd, let A = Pm\ni=1 xix⊤\ni ,\nand let u1, , un be n eigenvectors of the matrix A corresponding to the largest\nn eigenvalues of A Then, the solution to the PCA optimization problem given\nin Equation (23.1) is to set U to be the matrix whose columns are u1, , un\nand to set W = U ⊤ Proof\nLet VDV ⊤be the spectral decomposition of A Fix some matrix U ∈Rd,n\nwith orthonormal columns and let B = V ⊤U Then, VB = VV ⊤U = U It\nfollows that\nU ⊤AU = B⊤V ⊤VDV ⊤VB = B⊤DB,\nand therefore\ntrace(U ⊤AU ) =\nd\nX\nj=1\nDj,j\nn\nX\ni=1\nB2\nj,i Note that B⊤B = U ⊤VV ⊤U = U ⊤U = I Therefore, the columns of B are\nalso orthonormal, which implies that Pd\nj=1\nPn\ni=1 B2\nj,i = n In addition, let ˜B ∈\nRd,d be a matrix such that its ﬁrst n columns are the columns of B and in\naddition ˜B⊤˜B = I Then, for every j we have Pd\ni=1 ˜B2\nj,i = 1, which implies that\nPn\ni=1 B2\nj,i ≤1 It follows that:\ntrace(U ⊤AU) ≤\nmax\nβ∈[0,1]d : ∥β∥1≤n\nd\nX\nj=1\nDj,jβj .",
      "word_count": 216,
      "source_page": 325,
      "start_position": 118308,
      "end_position": 118523,
      "sentences_count": 13
    },
    {
      "chunk_id": 524,
      "text": "326\nDimensionality Reduction\nIt is not hard to verify (see Exercise 2) that the right-hand side equals to\nPn\nj=1 Dj,j We have therefore shown that for every matrix U ∈Rd,n with or-\nthonormal columns it holds that trace(U ⊤AU) ≤Pn\nj=1 Dj,j On the other hand,\nif we set U to be the matrix whose columns are the n leading eigenvectors of A\nwe obtain that trace(U ⊤AU) = Pn\nj=1 Dj,j, and this concludes our proof Remark 23.1\nThe proof of Theorem 23.2 also tells us that the value of the\nobjective of Equation (23.4) is Pn\ni=1 Di,i Combining this with Equation (23.3)\nand noting that Pm\ni=1 ∥xi∥2 = trace(A) = Pd\ni=1 Di,i we obtain that the optimal\nobjective value of Equation (23.1) is Pd\ni=n+1 Di,i Remark 23.2\nIt is a common practice to “center” the examples before applying\nPCA That is, we ﬁrst calculate µ =\n1\nm\nPm\ni=1 xi and then apply PCA on the\nvectors (x1 −µ), , (xm −µ) This is also related to the interpretation of PCA\nas variance maximization (see Exercise 4) 23.1.1\nA More Eﬃcient Solution for the Case d ≫m\nIn some situations the original dimensionality of the data is much larger than\nthe number of examples m The computational complexity of calculating the\nPCA solution as described previously is O(d3) (for calculating eigenvalues of A)\nplus O(md2) (for constructing the matrix A)",
      "word_count": 238,
      "source_page": 326,
      "start_position": 118524,
      "end_position": 118761,
      "sentences_count": 11
    },
    {
      "chunk_id": 525,
      "text": "23.1.1\nA More Eﬃcient Solution for the Case d ≫m\nIn some situations the original dimensionality of the data is much larger than\nthe number of examples m The computational complexity of calculating the\nPCA solution as described previously is O(d3) (for calculating eigenvalues of A)\nplus O(md2) (for constructing the matrix A) We now show a simple trick that\nenables us to calculate the PCA solution more eﬃciently when d ≫m Recall that the matrix A is deﬁned to be Pm\ni=1 xix⊤\ni It is convenient to rewrite\nA = X⊤X where X ∈Rm,d is a matrix whose ith row is x⊤\ni Consider the\nmatrix B = XX⊤ That is, B ∈Rm,m is the matrix whose i, j element equals\n⟨xi, xj⟩ Suppose that u is an eigenvector of B: That is, Bu = λu for some\nλ ∈R Multiplying the equality by X⊤and using the deﬁnition of B we obtain\nX⊤XX⊤u = λX⊤u But, using the deﬁnition of A, we get that A(X⊤u) =\nλ(X⊤u) Thus,\nX⊤u\n∥X⊤u∥is an eigenvector of A with eigenvalue of λ We can therefore calculate the PCA solution by calculating the eigenvalues of\nB instead of A The complexity is O(m3) (for calculating eigenvalues of B) and\nm2d (for constructing the matrix B) Remark 23.3\nThe previous discussion also implies that to calculate the PCA\nsolution we only need to know how to calculate inner products between vectors",
      "word_count": 237,
      "source_page": 326,
      "start_position": 118709,
      "end_position": 118945,
      "sentences_count": 14
    },
    {
      "chunk_id": 526,
      "text": "23.1 Principal Component Analysis (PCA)\n327\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\nFigure 23.1 A set of vectors in R2 (blue x’s) and their reconstruction after\ndimensionality reduction to R1 using PCA (red circles) PCA\ninput\nA matrix of m examples X ∈Rm,d\nnumber of components n\nif (m > d)\nA = X⊤X\nLet u1, , un be the eigenvectors of A with largest eigenvalues\nelse\nB = XX⊤\nLet v1, , vn be the eigenvectors of B with largest eigenvalues\nfor i = 1, , n set ui =\n1\n∥X⊤vi∥X⊤vi\noutput: u1, , un\nTo illustrate how PCA works, let us generate vectors in R2 that approximately\nreside on a line, namely, on a one dimensional subspace of R2 For example,\nsuppose that each example is of the form (x, x + y) where x is chosen uniformly\nat random from [−1, 1] and y is sampled from a Gaussian distribution with mean\n0 and standard deviation of 0.1 Suppose we apply PCA on this data Then, the\neigenvector corresponding to the largest eigenvalue will be close to the vector\n(1/\n√\n2, 1/\n√\n2) When projecting a point (x, x + y) on this principal component\nwe will obtain the scalar 2x+y\n√\n2 The reconstruction of the original vector will be\n((x + y/2), (x + y/2)) In Figure 23.1 we depict the original versus reconstructed\ndata",
      "word_count": 241,
      "source_page": 327,
      "start_position": 118983,
      "end_position": 119223,
      "sentences_count": 12
    },
    {
      "chunk_id": 527,
      "text": "328\nDimensionality Reduction\nx\nx\nx\nx x\nx\nx\no\no\no\no o\no\no\n*\n*\n*\n***\n*\n+ +\n+ + +\n+ +\nFigure 23.2 Images of faces extracted from the Yale data set Top-Left: the original\nimages in R50x50 Top-Right: the images after dimensionality reduction to R10 and\nreconstruction Middle row: an enlarged version of one of the images before and after\nPCA Bottom: The images after dimensionality reduction to R2 The diﬀerent marks\nindicate diﬀerent individuals Some images of faces are depicted on the top-left side of Figure 23.2 Using\nPCA, we reduced the dimensionality to R10 and reconstructed back to the orig-\ninal dimension, which is 502 The resulting reconstructed images are depicted\non the top-right side of Figure 23.2 Finally, on the bottom of Figure 23.2 we\ndepict a 2 dimensional representation of the images As can be seen, even from a\n2 dimensional representation of the images we can still roughly separate diﬀerent\nindividuals.",
      "word_count": 165,
      "source_page": 328,
      "start_position": 119266,
      "end_position": 119430,
      "sentences_count": 11
    },
    {
      "chunk_id": 528,
      "text": "23.2 Random Projections\n329\n23.2\nRandom Projections\nIn this section we show that reducing the dimension by using a random linear\ntransformation leads to a simple compression scheme with a surprisingly low\ndistortion The transformation x 7→Wx, when W is a random matrix, is often\nreferred to as a random projection In particular, we provide a variant of a famous\nlemma due to Johnson and Lindenstrauss, showing that random projections do\nnot distort Euclidean distances too much Let x1, x2 be two vectors in Rd A matrix W does not distort too much the\ndistance between x1 and x2 if the ratio\n∥Wx1 −Wx2∥\n∥x1 −x2∥\nis close to 1 In other words, the distances between x1 and x2 before and after\nthe transformation are almost the same To show that ∥Wx1 −Wx2∥is not too\nfar away from ∥x1 −x2∥it suﬃces to show that W does not distort the norm of\nthe diﬀerence vector x = x1 −x2 Therefore, from now on we focus on the ratio\n∥W x∥\n∥x∥ We start with analyzing the distortion caused by applying a random projection\nto a single vector lemma 23.3\nFix some x ∈Rd Let W ∈Rn,d be a random matrix such that\neach Wi,j is an independent normal random variable Then, for every ϵ ∈(0, 3)\nwe have\nP\n\" \f\f\f\f\f\n∥(1/√n)Wx∥2\n∥x∥2\n−1\n\f\f\f\f\f > ϵ\n#\n≤2 e−ϵ2n/6 Proof\nWithout loss of generality we can assume that ∥x∥2 = 1",
      "word_count": 239,
      "source_page": 329,
      "start_position": 119431,
      "end_position": 119669,
      "sentences_count": 13
    },
    {
      "chunk_id": 529,
      "text": "Then, for every ϵ ∈(0, 3)\nwe have\nP\n\" \f\f\f\f\f\n∥(1/√n)Wx∥2\n∥x∥2\n−1\n\f\f\f\f\f > ϵ\n#\n≤2 e−ϵ2n/6 Proof\nWithout loss of generality we can assume that ∥x∥2 = 1 Therefore, an\nequivalent inequality is\nP\n\u0002\n(1 −ϵ)n ≤∥Wx∥2 ≤(1 + ϵ)n\n\u0003\n≥1 −2e−ϵ2n/6 Let wi be the ith row of W The random variable ⟨wi, x⟩is a weighted sum of\nd independent normal random variables and therefore it is normally distributed\nwith zero mean and variance P\nj x2\nj = ∥x∥2 = 1 Therefore, the random vari-\nable ∥Wx∥2 = Pn\ni=1(⟨wi, x⟩)2 has a χ2\nn distribution The claim now follows\ndirectly from a measure concentration property of χ2 random variables stated in\nLemma B.12 given in Section B.7 The Johnson-Lindenstrauss lemma follows from this using a simple union\nbound argument lemma 23.4 (Johnson-Lindenstrauss Lemma)\nLet Q be a ﬁnite set of vectors\nin Rd Let δ ∈(0, 1) and n be an integer such that\nϵ =\nr\n6 log(2|Q|/δ)\nn\n≤3.",
      "word_count": 168,
      "source_page": 329,
      "start_position": 119640,
      "end_position": 119807,
      "sentences_count": 10
    },
    {
      "chunk_id": 530,
      "text": "330\nDimensionality Reduction\nThen, with probability of at least 1−δ over a choice of a random matrix W ∈Rn,d\nsuch that each element of W is distributed normally with zero mean and variance\nof 1/n we have\nsup\nx∈Q\n\f\f\f\f\n∥Wx∥2\n∥x∥2\n−1\n\f\f\f\f < ϵ Proof\nCombining Lemma 23.3 and the union bound we have that for every\nϵ ∈(0, 3):\nP\n\u0014\nsup\nx∈Q\n\f\f\f\f\n∥Wx∥2\n∥x∥2\n−1\n\f\f\f\f > ϵ\n\u0015\n≤2 |Q| e−ϵ2n/6 Let δ denote the right-hand side of the inequality; thus we obtain that\nϵ =\nr\n6 log(2|Q|/δ)\nn Interestingly, the bound given in Lemma 23.4 does not depend on the original\ndimension of x In fact, the bound holds even if x is in an inﬁnite dimensional\nHilbert space 23.3\nCompressed Sensing\nCompressed sensing is a dimensionality reduction technique which utilizes a prior\nassumption that the original vector is sparse in some basis To motivate com-\npressed sensing, consider a vector x ∈Rd that has at most s nonzero elements That is,\n∥x∥0\ndef\n= |{i : xi ̸= 0}| ≤s Clearly, we can compress x by representing it using s (index,value) pairs Fur-\nthermore, this compression is lossless – we can reconstruct x exactly from the s\n(index,value) pairs Now, lets take one step forward and assume that x = Uα,\nwhere α is a sparse vector, ∥α∥0 ≤s, and U is a ﬁxed orthonormal matrix That\nis, x has a sparse representation in another basis",
      "word_count": 241,
      "source_page": 330,
      "start_position": 119808,
      "end_position": 120048,
      "sentences_count": 12
    },
    {
      "chunk_id": 531,
      "text": "Now, lets take one step forward and assume that x = Uα,\nwhere α is a sparse vector, ∥α∥0 ≤s, and U is a ﬁxed orthonormal matrix That\nis, x has a sparse representation in another basis It turns out that many nat-\nural vectors are (at least approximately) sparse in some representation In fact,\nthis assumption underlies many modern compression schemes For example, the\nJPEG-2000 format for image compression relies on the fact that natural images\nare approximately sparse in a wavelet basis Can we still compress x into roughly s numbers Well, one simple way to do\nthis is to multiply x by U ⊤, which yields the sparse vector α, and then represent\nα by its s (index,value) pairs However, this requires us ﬁrst to “sense” x, to\nstore it, and then to multiply it by U ⊤ This raises a very natural question: Why\ngo to so much eﬀort to acquire all the data when most of what we get will be\nthrown away Cannot we just directly measure the part that will not end up\nbeing thrown away?",
      "word_count": 183,
      "source_page": 330,
      "start_position": 120012,
      "end_position": 120194,
      "sentences_count": 10
    },
    {
      "chunk_id": 532,
      "text": "23.3 Compressed Sensing\n331\nCompressed sensing is a technique that simultaneously acquires and com-\npresses the data The key result is that a random linear transformation can\ncompress x without losing information The number of measurements needed is\norder of s log(d) That is, we roughly acquire only the important information\nabout the signal As we will see later, the price we pay is a slower reconstruction\nphase In some situations, it makes sense to save time in compression even at\nthe price of a slower reconstruction For example, a security camera should sense\nand compress a large amount of images while most of the time we do not need to\ndecode the compressed data at all Furthermore, in many practical applications,\ncompression by a linear transformation is advantageous because it can be per-\nformed eﬃciently in hardware For example, a team led by Baraniuk and Kelly\nhas proposed a camera architecture that employs a digital micromirror array to\nperform optical calculations of a linear transformation of an image In this case,\nobtaining each compressed measurement is as easy as obtaining a single raw\nmeasurement Another important application of compressed sensing is medical\nimaging, in which requiring fewer measurements translates to less radiation for\nthe patient Informally, the main premise of compressed sensing is the following three “sur-\nprising” results:\n1",
      "word_count": 221,
      "source_page": 331,
      "start_position": 120195,
      "end_position": 120415,
      "sentences_count": 12
    },
    {
      "chunk_id": 533,
      "text": "Another important application of compressed sensing is medical\nimaging, in which requiring fewer measurements translates to less radiation for\nthe patient Informally, the main premise of compressed sensing is the following three “sur-\nprising” results:\n1 It is possible to reconstruct any sparse signal fully if it was compressed by\nx 7→Wx, where W is a matrix which satisﬁes a condition called the Re-\nstricted Isoperimetric Property (RIP) A matrix that satisﬁes this property is\nguaranteed to have a low distortion of the norm of any sparse representable\nvector 2 The reconstruction can be calculated in polynomial time by solving a linear\nprogram 3 A random n × d matrix is likely to satisfy the RIP condition provided that n\nis greater than an order of s log(d) Formally,\ndefinition 23.5 (RIP)\nA matrix W ∈Rn,d is (ϵ, s)-RIP if for all x ̸= 0 s.t ∥x∥0 ≤s we have\n\f\f\f\f\n∥Wx∥2\n2\n∥x∥2\n2\n−1\n\f\f\f\f ≤ϵ The ﬁrst theorem establishes that RIP matrices yield a lossless compression\nscheme for sparse vectors It also provides a (noneﬃcient) reconstruction scheme theorem 23.6\nLet ϵ < 1 and let W be a (ϵ, 2s)-RIP matrix Let x be a vector\ns.t ∥x∥0 ≤s, let y = Wx be the compression of x, and let\n˜x ∈argmin\nv:W v=y\n∥v∥0\nbe a reconstructed vector Then, ˜x = x.",
      "word_count": 224,
      "source_page": 331,
      "start_position": 120380,
      "end_position": 120603,
      "sentences_count": 16
    },
    {
      "chunk_id": 534,
      "text": "332\nDimensionality Reduction\nProof\nWe assume, by way of contradiction, that ˜x ̸= x Since x satisﬁes the\nconstraints in the optimization problem that deﬁnes ˜x we clearly have that\n∥˜x∥0 ≤∥x∥0 ≤s Therefore, ∥x −˜x∥0 ≤2s and we can apply the RIP in-\nequality on the vector x −˜x But, since W(x −˜x) = 0 we get that |0 −1| ≤ϵ,\nwhich leads to a contradiction The reconstruction scheme given in Theorem 23.6 seems to be noneﬃcient\nbecause we need to minimize a combinatorial objective (the sparsity of v) Quite\nsurprisingly, it turns out that we can replace the combinatorial objective, ∥v∥0,\nwith a convex objective, ∥v∥1, which leads to a linear programming problem that\ncan be solved eﬃciently This is stated formally in the following theorem theorem 23.7\nAssume that the conditions of Theorem 23.6 holds and that\nϵ <\n1\n1+\n√\n2 Then,\nx =\nargmin\nv:W v=y\n∥v∥0 =\nargmin\nv:W v=y\n∥v∥1 In fact, we will prove a stronger result, which holds even if x is not a sparse\nvector theorem 23.8\nLet ϵ <\n1\n1+\n√\n2 and let W be a (ϵ, 2s)-RIP matrix Let x be an\narbitrary vector and denote\nxs ∈argmin\nv:∥v∥0≤s\n∥x −v∥1 That is, xs is the vector which equals x on the s largest elements of x and equals\n0 elsewhere Let y = Wx be the compression of x and let\nx⋆∈argmin\nv:W v=y\n∥v∥1\nbe the reconstructed vector",
      "word_count": 245,
      "source_page": 332,
      "start_position": 120604,
      "end_position": 120848,
      "sentences_count": 14
    },
    {
      "chunk_id": 535,
      "text": "That is, xs is the vector which equals x on the s largest elements of x and equals\n0 elsewhere Let y = Wx be the compression of x and let\nx⋆∈argmin\nv:W v=y\n∥v∥1\nbe the reconstructed vector Then,\n∥x⋆−x∥2 ≤21 + ρ\n1 −ρ s−1/2∥x −xs∥1,\nwhere ρ =\n√\n2ϵ/(1 −ϵ) Note that in the special case that x = xs we get an exact recovery, x⋆= x, so\nTheorem 23.7 is a special case of Theorem 23.8 The proof of Theorem 23.8 is\ngiven in Section 23.3.1 Finally, the third result tells us that random matrices with n ≥Ω(s log(d)) are\nlikely to be RIP In fact, the theorem shows that multiplying a random matrix\nby an orthonormal matrix also provides an RIP matrix This is important for\ncompressing signals of the form x = Uα where x is not sparse but α is sparse In that case, if W is a random matrix and we compress using y = Wx then this\nis the same as compressing α by y = (WU)α and since WU is also RIP we can\nreconstruct α (and thus also x) from y.",
      "word_count": 193,
      "source_page": 332,
      "start_position": 120810,
      "end_position": 121002,
      "sentences_count": 9
    },
    {
      "chunk_id": 536,
      "text": "23.3 Compressed Sensing\n333\ntheorem 23.9\nLet U be an arbitrary ﬁxed d × d orthonormal matrix, let ϵ, δ\nbe scalars in (0, 1), let s be an integer in [d], and let n be an integer that satisﬁes\nn ≥100 s log(40d/(δ ϵ))\nϵ2 Let W ∈Rn,d be a matrix s.t each element of W is distributed normally with\nzero mean and variance of 1/n Then, with proabability of at least 1 −δ over the\nchoice of W, the matrix WU is (ϵ, s)-RIP 23.3.1\nProofs*\nProof of Theorem 23.8\nWe follow a proof due to Cand`es (2008) Let h = x⋆−x Given a vector v and a set of indices I we denote by vI the\nvector whose ith element is vi if i ∈I and 0 otherwise The ﬁrst trick we use is to partition the set of indices [d] = {1, , d} into\ndisjoint sets of size s That is, we will write [d] = T0 ·∪T1 ·∪T2 Td/s−1 where\nfor all i, |Ti| = s, and we assume for simplicity that d/s is an integer We deﬁne\nthe partition as follows In T0 we put the s indices corresponding to the s largest\nelements in absolute values of x (ties are broken arbitrarily) Let T c\n0 = [d] \\ T0 Next, T1 will be the s indices corresponding to the s largest elements in absolute\nvalue of hT c\n0 Let T0,1 = T0 ∪T1 and T c\n0,1 = [d]\\T0,1",
      "word_count": 249,
      "source_page": 333,
      "start_position": 121003,
      "end_position": 121251,
      "sentences_count": 16
    },
    {
      "chunk_id": 537,
      "text": "Next, T1 will be the s indices corresponding to the s largest elements in absolute\nvalue of hT c\n0 Let T0,1 = T0 ∪T1 and T c\n0,1 = [d]\\T0,1 Next, T2 will correspond to\nthe s largest elements in absolute value of hT c\n0,1 And, we will construct T3, T4, in the same way To prove the theorem we ﬁrst need the following lemma, which shows that\nRIP also implies approximate orthogonality lemma 23.10\nLet W be an (ϵ, 2s)-RIP matrix Then, for any two disjoint sets\nI, J, both of size at most s, and for any vector u we have that ⟨WuI, WuJ⟩≤\nϵ∥uI∥2 ∥uJ∥2 Proof\nW.l.o.g assume ∥uI∥2 = ∥uJ∥2 = 1 ⟨WuI, WuJ⟩= ∥WuI + WuJ∥2\n2 −∥WuI −WuJ∥2\n2\n4 But, since |J ∪I| ≤2s we get from the RIP condition that ∥WuI + WuJ∥2\n2 ≤\n(1+ϵ)(∥uI∥2\n2 +∥uJ∥2\n2) = 2(1+ϵ) and that −∥WuI −WuJ∥2\n2 ≤−(1−ϵ)(∥uI∥2\n2 +\n∥uJ∥2\n2) = −2(1 −ϵ), which concludes our proof We are now ready to prove the theorem Clearly,\n∥h∥2 = ∥hT0,1 + hT c\n0,1∥2 ≤∥hT0,1∥2 + ∥hT c\n0,1∥2 (23.5)\nTo prove the theorem we will show the following two claims:\nClaim 1: ∥hT c\n0,1∥2 ≤∥hT0∥2 + 2s−1/2∥x −xs∥1 Claim 2: ∥hT0,1∥2 ≤\n2ρ\n1−ρs−1/2∥x −xs∥1.",
      "word_count": 217,
      "source_page": 333,
      "start_position": 121221,
      "end_position": 121437,
      "sentences_count": 18
    },
    {
      "chunk_id": 538,
      "text": "334\nDimensionality Reduction\nCombining these two claims with Equation (23.5) we get that\n∥h∥2 ≤∥hT0,1∥2 + ∥hT c\n0,1∥2 ≤2∥hT0,1∥2 + 2s−1/2∥x −xs∥1\n≤2\n\u0010\n2ρ\n1−ρ + 1\n\u0011\ns−1/2∥x −xs∥1\n= 21 + ρ\n1 −ρ s−1/2∥x −xs∥1,\nand this will conclude our proof Proving Claim 1:\nTo prove this claim we do not use the RIP condition at all but only use the fact\nthat x⋆minimizes the ℓ1 norm Take j > 1 For each i ∈Tj and i′ ∈Tj−1 we\nhave that |hi| ≤|hi′| Therefore, ∥hTj∥∞≤∥hTj−1∥1/s Thus,\n∥hTj∥2 ≤s1/2∥hTj∥∞≤s−1/2∥hTj−1∥1 Summing this over j = 2, 3, and using the triangle inequality we obtain that\n∥hT c\n0,1∥2 ≤\nX\nj≥2\n∥hTj∥2 ≤s−1/2∥hT c\n0 ∥1\n(23.6)\nNext, we show that ∥hT c\n0 ∥1 cannot be large Indeed, from the deﬁnition of x⋆\nwe have that ∥x∥1 ≥∥x⋆∥1 = ∥x + h∥1 Thus, using the triangle inequality we\nobtain that\n∥x∥1 ≥∥x+h∥1 =\nX\ni∈T0\n|xi+hi|+\nX\ni∈T c\n0\n|xi+hi| ≥∥xT0∥1−∥hT0∥1+∥hT c\n0 ∥1−∥xT c\n0 ∥1\n(23.7)\nand since ∥xT c\n0 ∥1 = ∥x −xs∥1 = ∥x∥1 −∥xT0∥1 we get that\n∥hT c\n0 ∥1 ≤∥hT0∥1 + 2∥xT c\n0 ∥1 (23.8)\nCombining this with Equation (23.6) we get that\n∥hT c\n0,1∥2 ≤s−1/2 \u0000∥hT0∥1 + 2∥xT c\n0 ∥1\n\u0001\n≤∥hT0∥2 + 2s−1/2∥xT c\n0 ∥1,\nwhich concludes the proof of claim 1",
      "word_count": 231,
      "source_page": 334,
      "start_position": 121438,
      "end_position": 121668,
      "sentences_count": 11
    },
    {
      "chunk_id": 539,
      "text": "Thus, using the triangle inequality we\nobtain that\n∥x∥1 ≥∥x+h∥1 =\nX\ni∈T0\n|xi+hi|+\nX\ni∈T c\n0\n|xi+hi| ≥∥xT0∥1−∥hT0∥1+∥hT c\n0 ∥1−∥xT c\n0 ∥1\n(23.7)\nand since ∥xT c\n0 ∥1 = ∥x −xs∥1 = ∥x∥1 −∥xT0∥1 we get that\n∥hT c\n0 ∥1 ≤∥hT0∥1 + 2∥xT c\n0 ∥1 (23.8)\nCombining this with Equation (23.6) we get that\n∥hT c\n0,1∥2 ≤s−1/2 \u0000∥hT0∥1 + 2∥xT c\n0 ∥1\n\u0001\n≤∥hT0∥2 + 2s−1/2∥xT c\n0 ∥1,\nwhich concludes the proof of claim 1 Proving Claim 2:\nFor the second claim we use the RIP condition to get that\n(1 −ϵ)∥hT0,1∥2\n2 ≤∥WhT0,1∥2\n2 (23.9)\nSince WhT0,1 = Wh −P\nj≥2 WhTj = −P\nj≥2 WhTj we have that\n∥WhT0,1∥2\n2 = −\nX\nj≥2\n⟨WhT0,1, WhTj⟩= −\nX\nj≥2\n⟨WhT0 + WhT1, WhTj⟩ From the RIP condition on inner products we obtain that for all i ∈{1, 2} and\nj ≥2 we have\n|⟨WhTi, WhTj⟩| ≤ϵ∥hTi∥2∥hTj∥2.",
      "word_count": 158,
      "source_page": 334,
      "start_position": 121584,
      "end_position": 121741,
      "sentences_count": 5
    },
    {
      "chunk_id": 540,
      "text": "23.3 Compressed Sensing\n335\nSince ∥hT0∥2 + ∥hT1∥2 ≤\n√\n2∥hT0,1∥2 we therefore get that\n∥WhT0,1∥2\n2 ≤\n√\n2ϵ∥hT0,1∥2\nX\nj≥2\n∥hTj∥2 Combining this with Equation (23.6) and Equation (23.9) we obtain\n(1 −ϵ)∥hT0,1∥2\n2 ≤\n√\n2ϵ∥hT0,1∥2s−1/2∥hT c\n0 ∥1 Rearranging the inequality gives\n∥hT0,1∥2 ≤\n√\n2ϵ\n1 −ϵs−1/2∥hT c\n0 ∥1 Finally, using Equation (23.8) we get that\n∥hT0,1∥2 ≤ρs−1/2 (∥hT0∥1 + 2∥xT c\n0 ∥1) ≤ρ∥hT0∥2 + 2ρs−1/2∥xT c\n0 ∥1,\nbut since ∥hT0∥2 ≤∥hT0,1∥2 this implies\n∥hT0,1∥2 ≤\n2ρ\n1 −ρ s−1/2∥xT c\n0 ∥1,\nwhich concludes the proof of the second claim Proof of Theorem 23.9\nTo prove the theorem we follow an approach due to (Baraniuk, Davenport, De-\nVore & Wakin 2008) The idea is to combine the Johnson-Lindenstrauss (JL)\nlemma with a simple covering argument We start with a covering property of the unit ball lemma 23.11\nLet ϵ ∈(0, 1) There exists a ﬁnite set Q ⊂Rd of size |Q| ≤\n\u0000 3\nϵ\n\u0001d\nsuch that\nsup\nx:∥x∥≤1\nmin\nv∈Q ∥x −v∥≤ϵ Proof\nLet k be an integer and let\nQ′ = {x ∈Rd : ∀j ∈[d], ∃i ∈{−k, −k + 1, , k} s.t xj = i\nk} Clearly, |Q′| = (2k + 1)d We shall set Q = Q′ ∩B2(1), where B2(1) is the unit\nℓ2 ball of Rd",
      "word_count": 222,
      "source_page": 335,
      "start_position": 121742,
      "end_position": 121963,
      "sentences_count": 14
    },
    {
      "chunk_id": 541,
      "text": "336\nDimensionality Reduction\nwhere in the last inequality we used Stirling’s approximation Overall we obtained\nthat\n|Q| ≤(2k + 1)d (π/e)d/2 (d/2)−d/2 2−d (23.10)\nNow lets specify k For each x ∈B2(1) let v ∈Q be the vector whose ith element\nis sign(xi) ⌊|xi| k⌋/k Then, for each element we have that |xi −vi| ≤1/k and\nthus\n∥x −v∥≤\n√\nd\nk To ensure that the right-hand side will be at most ϵ we shall set k = ⌈\n√\nd/ϵ⌉ Plugging this value into Equation (23.10) we conclude that\n|Q| ≤(3\n√\nd/(2ϵ))d (π/e)d/2 (d/2)−d/2 =\n\u0010\n3\nϵ\nq\nπ\n2e\n\u0011d\n≤\n\u0000 3\nϵ\n\u0001d Let x be a vector that can be written as x = Uα with U being some orthonor-\nmal matrix and ∥α∥0 ≤s Combining the earlier covering property and the JL\nlemma (Lemma 23.4) enables us to show that a random W will not distort any\nsuch x lemma 23.12\nLet U be an orthonormal d × d matrix and let I ⊂[d] be a set\nof indices of size |I| = s Let S be the span of {Ui : i ∈I}, where Ui is the ith\ncolumn of U Let δ ∈(0, 1), ϵ ∈(0, 1), and n ∈N such that\nn ≥24 log(2/δ) + s log(12/ϵ)\nϵ2",
      "word_count": 219,
      "source_page": 336,
      "start_position": 122034,
      "end_position": 122252,
      "sentences_count": 12
    },
    {
      "chunk_id": 542,
      "text": "Let S be the span of {Ui : i ∈I}, where Ui is the ith\ncolumn of U Let δ ∈(0, 1), ϵ ∈(0, 1), and n ∈N such that\nn ≥24 log(2/δ) + s log(12/ϵ)\nϵ2 Then, with probability of at least 1−δ over a choice of a random matrix W ∈Rn,d\nsuch that each element of W is independently distributed according to N(0, 1/n),\nwe have\nsup\nx∈S\n\f\f\f\f\n∥Wx∥\n∥x∥\n−1\n\f\f\f\f < ϵ Proof\nIt suﬃces to prove the lemma for all x ∈S with ∥x∥= 1 We can write\nx = UIα where α ∈Rs, ∥α∥2 = 1, and UI is the matrix whose columns are\n{Ui : i ∈I} Using Lemma 23.11 we know that there exists a set Q of size\n|Q| ≤(12/ϵ)s such that\nsup\nα:∥α∥=1\nmin\nv∈Q ∥α −v∥≤(ϵ/4) But since U is orthogonal we also have that\nsup\nα:∥α∥=1\nmin\nv∈Q ∥UIα −UIv∥≤(ϵ/4) Applying Lemma 23.4 on the set {UIv : v ∈Q} we obtain that for n satisfying",
      "word_count": 167,
      "source_page": 336,
      "start_position": 122216,
      "end_position": 122382,
      "sentences_count": 8
    },
    {
      "chunk_id": 543,
      "text": "23.3 Compressed Sensing\n337\nthe condition given in the lemma, the following holds with probability of at least\n1 −δ:\nsup\nv∈Q\n\f\f\f\f\n∥WUIv∥2\n∥UIv∥2\n−1\n\f\f\f\f ≤ϵ/2,\nThis also implies that\nsup\nv∈Q\n\f\f\f\f\n∥WUIv∥\n∥UIv∥\n−1\n\f\f\f\f ≤ϵ/2 Let a be the smallest number such that\n∀x ∈S,\n∥Wx∥\n∥x∥\n≤1 + a Clearly a < ∞ Our goal is to show that a ≤ϵ This follows from the fact that\nfor any x ∈S of unit norm there exists v ∈Q such that ∥x −UIv∥≤ϵ/4 and\ntherefore\n∥Wx∥≤∥WUIv∥+ ∥W(x −UIv)∥≤1 + ϵ/2 + (1 + a)ϵ/4 Thus,\n∀x ∈S,\n∥Wx∥\n∥x∥\n≤1 + (ϵ/2 + (1 + a)ϵ/4) But the deﬁnition of a implies that\na ≤ϵ/2 + (1 + a)ϵ/4\n⇒\na ≤ϵ/2 + ϵ/4\n1 −ϵ/4\n≤ϵ This proves that for all x ∈S we have ∥W x∥\n∥x∥−1 ≤ϵ The other side follows from\nthis as well since\n∥Wx∥≥∥WUIv∥−∥W(x −UIv)∥≥1 −ϵ/2 −(1 + ϵ)ϵ/4 ≥1 −ϵ The preceding lemma tells us that for x ∈S of unit norm we have\n(1 −ϵ) ≤∥Wx∥≤(1 + ϵ),\nwhich implies that\n(1 −2 ϵ) ≤∥Wx∥2 ≤(1 + 3 ϵ) The proof of Theorem 23.9 follows from this by a union bound over all choices\nof I.",
      "word_count": 205,
      "source_page": 337,
      "start_position": 122383,
      "end_position": 122587,
      "sentences_count": 11
    },
    {
      "chunk_id": 544,
      "text": "338\nDimensionality Reduction\n23.4\nPCA or Compressed Sensing Suppose we would like to apply a dimensionality reduction technique to a given\nset of examples Which method should we use, PCA or compressed sensing In\nthis section we tackle this question, by underscoring the underlying assumptions\nbehind the two methods It is helpful ﬁrst to understand when each of the methods can guarantee per-\nfect recovery PCA guarantees perfect recovery whenever the set of examples is\ncontained in an n dimensional subspace of Rd Compressed sensing guarantees\nperfect recovery whenever the set of examples is sparse (in some basis) On the\nbasis of these observations, we can describe cases in which PCA will be better\nthan compressed sensing and vice versa As a ﬁrst example, suppose that the examples are the vectors of the standard\nbasis of Rd, namely, e1, , ed, where each ei is the all zeros vector except 1 in the\nith coordinate In this case, the examples are 1-sparse Hence, compressed sensing\nwill yield a perfect recovery whenever n ≥Ω(log(d)) On the other hand, PCA\nwill lead to poor performance, since the data is far from being in an n dimensional\nsubspace, as long as n < d Indeed, it is easy ro verify that in such a case, the\naveraged recovery error of PCA (i.e., the objective of Equation (23.1) divided by\nm) will be (d −n)/d, which is larger than 1/2 whenever n ≤d/2",
      "word_count": 239,
      "source_page": 338,
      "start_position": 122588,
      "end_position": 122826,
      "sentences_count": 14
    },
    {
      "chunk_id": 545,
      "text": "On the other hand, PCA\nwill lead to poor performance, since the data is far from being in an n dimensional\nsubspace, as long as n < d Indeed, it is easy ro verify that in such a case, the\naveraged recovery error of PCA (i.e., the objective of Equation (23.1) divided by\nm) will be (d −n)/d, which is larger than 1/2 whenever n ≤d/2 We next show a case where PCA is better than compressed sensing Consider\nm examples that are exactly on an n dimensional subspace Clearly, in such a\ncase, PCA will lead to perfect recovery As to compressed sensing, note that\nthe examples are n-sparse in any orthonormal basis whose ﬁrst n vectors span\nthe subspace Therefore, compressed sensing would also work if we will reduce\nthe dimension to Ω(n log(d)) However, with exactly n dimensions, compressed\nsensing might fail PCA has also better resilience to certain types of noise See\n(Chang, Weiss & Freeman 2009) for a discussion 23.5\nSummary\nWe introduced two methods for dimensionality reduction using linear transfor-\nmations: PCA and random projections We have shown that PCA is optimal in\nthe sense of averaged squared reconstruction error, if we restrict the reconstruc-\ntion procedure to be linear as well However, if we allow nonlinear reconstruction,\nPCA is not necessarily the optimal procedure In particular, for sparse data, ran-\ndom projections can signiﬁcantly outperform PCA This fact is at the heart of\nthe compressed sensing method.",
      "word_count": 244,
      "source_page": 338,
      "start_position": 122761,
      "end_position": 123004,
      "sentences_count": 15
    },
    {
      "chunk_id": 546,
      "text": "23.6 Bibliographic Remarks\n339\n23.6\nBibliographic Remarks\nPCA is equivalent to best subspace approximation using singular value decom-\nposition (SVD) The SVD method is described in Appendix C SVD dates back\nto Eugenio Beltrami (1873) and Camille Jordan (1874) It has been rediscovered\nmany times In the statistical literature, it was introduced by Pearson (1901) Be-\nsides PCA and SVD, there are additional names that refer to the same idea and\nare being used in diﬀerent scientiﬁc communities A few examples are the Eckart-\nYoung theorem (after Carl Eckart and Gale Young who analyzed the method in\n1936), the Schmidt-Mirsky theorem, factor analysis, and the Hotelling transform Compressed sensing was introduced in Donoho (2006) and in (Candes & Tao\n2005) See also Candes (2006) 23.7\nExercises\n1 In this exercise we show that in the general case, exact recovery of a linear\ncompression scheme is impossible 1 let A ∈Rn,d be an arbitrary compression matrix where n ≤d −1 Show\nthat there exists u, v ∈Rn, u ̸= v such that Au = Av 2 Conclude that exact recovery of a linear compression scheme is impossible 2 Let α ∈Rd such that α1 ≥α2 ≥· · · ≥αd ≥0 Show that\nmax\nβ∈[0,1]d:∥β∥1≤n\nd\nX\nj=1\nαjβj\n=\nn\nX\nj=1\nαj Hint: Take every vector β ∈[0, 1]d such that ∥β∥1 ≤n Let i be the minimal\nindex for which βi < 1 If i = n+1 we are done",
      "word_count": 241,
      "source_page": 339,
      "start_position": 123005,
      "end_position": 123245,
      "sentences_count": 22
    },
    {
      "chunk_id": 547,
      "text": "Let i be the minimal\nindex for which βi < 1 If i = n+1 we are done Otherwise, show that we can\nincrease βi, while possibly decreasing βj for some j > i, and obtain a better\nsolution This will imply that the optimal solution is to set βi = 1 for i ≤n\nand βi = 0 for i > n 3 Kernel PCA: In this exercise we show how PCA can be used for construct-\ning nonlinear dimensionality reduction on the basis of the kernel trick (see\nChapter 16) Let X be some instance space and let S = {x1, , xm} be a set of points\nin X Consider a feature mapping ψ : X →V , where V is some Hilbert space\n(possibly of inﬁnite dimension) Let K : X × X be a kernel function, that is,\nk(x, x′) = ⟨ψ(x), ψ(x′)⟩ Kernel PCA is the process of mapping the elements\nin S into V using ψ, and then applying PCA over {ψ(x1), , ψ(xm)} into\nRn The output of this process is the set of reduced elements Show how this process can be done in polynomial time in terms of m\nand n, assuming that each evaluation of K(·, ·) can be calculated in a con-\nstant time In particular, if your implementation requires multiplication of\ntwo matrices A and B, verify that their product can be computed Similarly,",
      "word_count": 236,
      "source_page": 339,
      "start_position": 123228,
      "end_position": 123463,
      "sentences_count": 16
    },
    {
      "chunk_id": 548,
      "text": "340\nDimensionality Reduction\nif an eigenvalue decomposition of some matrix C is required, verify that this\ndecomposition can be computed 4 An Interpretation of PCA as Variance Maximization:\nLet x1, , xm be m vectors in Rd, and let x be a random vector distributed\naccording to the uniform distribution over x1, , xm Assume that E[x] = 0 1 Consider the problem of ﬁnding a unit vector, w ∈Rd, such that the\nrandom variable ⟨w, x⟩has maximal variance That is, we would like to\nsolve the problem\nargmax\nw:∥w∥=1\nVar[⟨w, x⟩] = argmax\nw:∥w∥=1\n1\nm\nm\nX\ni=1\n(⟨w, xi⟩)2 Show that the solution of the problem is to set w to be the ﬁrst principle\nvector of x1, , xm 2 Let w1 be the ﬁrst principal component as in the previous question Now,\nsuppose we would like to ﬁnd a second unit vector, w2 ∈Rd, that maxi-\nmizes the variance of ⟨w2, x⟩, but is also uncorrelated to ⟨w1, x⟩ That is,\nwe would like to solve:\nargmax\nw:∥w∥=1, E[(⟨w1,x⟩)(⟨w,x⟩)]=0\nVar[⟨w, x⟩] Show that the solution to this problem is to set w to be the second principal\ncomponent of x1, , xm Hint: Note that\nE[(⟨w1, x⟩)(⟨w, x⟩)] = w⊤\n1 E[xx⊤]w = mw⊤\n1 Aw,\nwhere A = P\ni xix⊤\ni Since w is an eigenvector of A we have that the\nconstraint E[(⟨w1, x⟩)(⟨w, x⟩)] = 0 is equivalent to the constraint\n⟨w1, w⟩= 0 5",
      "word_count": 244,
      "source_page": 340,
      "start_position": 123464,
      "end_position": 123707,
      "sentences_count": 20
    },
    {
      "chunk_id": 549,
      "text": "Since w is an eigenvector of A we have that the\nconstraint E[(⟨w1, x⟩)(⟨w, x⟩)] = 0 is equivalent to the constraint\n⟨w1, w⟩= 0 5 The Relation between SVD and PCA:\nUse the SVD theorem (Corol-\nlary C.6) for providing an alternative proof of Theorem 23.2 6 Random Projections Preserve Inner Products: The Johnson-Lindenstrauss\nlemma tells us that a random projection preserves distances between a ﬁnite\nset of vectors In this exercise you need to prove that if the set of vectors are\nwithin the unit ball, then not only are the distances between any two vectors\npreserved, but the inner product is also preserved Let Q be a ﬁnite set of vectors in Rd and assume that for every x ∈Q we\nhave ∥x∥≤1 1 Let δ ∈(0, 1) and n be an integer such that\nϵ =\nr\n6 log(|Q|2/δ)\nn\n≤3 Prove that with probability of at least 1 −δ over a choice of a random",
      "word_count": 160,
      "source_page": 340,
      "start_position": 123682,
      "end_position": 123841,
      "sentences_count": 10
    },
    {
      "chunk_id": 550,
      "text": "24\nGenerative Models\nWe started this book with a distribution free learning framework; namely, we\ndid not impose any assumptions on the underlying distribution over the data Furthermore, we followed a discriminative approach in which our goal is not to\nlearn the underlying distribution but rather to learn an accurate predictor In\nthis chapter we describe a generative approach, in which it is assumed that the\nunderlying distribution over the data has a speciﬁc parametric form and our goal\nis to estimate the parameters of the model This task is called parametric density\nestimation The discriminative approach has the advantage of directly optimizing the\nquantity of interest (the prediction accuracy) instead of learning the underly-\ning distribution This was phrased as follows by Vladimir Vapnik in his principle\nfor solving problems using a restricted amount of information:\nWhen solving a given problem, try to avoid a more general problem as an intermediate\nstep Of course, if we succeed in learning the underlying distribution accurately,\nwe are considered to be “experts” in the sense that we can predict by using\nthe Bayes optimal classiﬁer The problem is that it is usually more diﬃcult to\nlearn the underlying distribution than to learn an accurate predictor However,\nin some situations, it is reasonable to adopt the generative learning approach For example, sometimes it is easier (computationally) to estimate the parameters\nof the model than to learn a discriminative predictor",
      "word_count": 236,
      "source_page": 342,
      "start_position": 123960,
      "end_position": 124195,
      "sentences_count": 10
    },
    {
      "chunk_id": 551,
      "text": "However,\nin some situations, it is reasonable to adopt the generative learning approach For example, sometimes it is easier (computationally) to estimate the parameters\nof the model than to learn a discriminative predictor Additionally, in some cases\nwe do not have a speciﬁc task at hand but rather would like to model the data\neither for making predictions at a later time without having to retrain a predictor\nor for the sake of interpretability of the data We start with a popular statistical method for estimating the parameters of\nthe data, which is called the maximum likelihood principle Next, we describe two\ngenerative assumptions which greatly simplify the learning process We also de-\nscribe the EM algorithm for calculating the maximum likelihood in the presence\nof latent variables We conclude with a brief description of Bayesian reasoning Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 166,
      "source_page": 342,
      "start_position": 124163,
      "end_position": 124328,
      "sentences_count": 12
    },
    {
      "chunk_id": 552,
      "text": "24.1 Maximum Likelihood Estimator\n343\n24.1\nMaximum Likelihood Estimator\nLet us start with a simple example A drug company developed a new drug to\ntreat some deadly disease We would like to estimate the probability of survival\nwhen using the drug To do so, the drug company sampled a training set of m\npeople and gave them the drug Let S = (x1, , xm) denote the training set,\nwhere for each i, xi = 1 if the ith person survived and xi = 0 otherwise We can\nmodel the underlying distribution using a single parameter, θ ∈[0, 1], indicating\nthe probability of survival We now would like to estimate the parameter θ on the basis of the training\nset S A natural idea is to use the average number of 1’s in S as an estimator That is,\nˆθ = 1\nm\nm\nX\ni=1\nxi (24.1)\nClearly, ES[ˆθ] = θ That is, ˆθ is an unbiased estimator of θ Furthermore, since ˆθ is\nthe average of m i.i.d binary random variables we can use Hoeﬀding’s inequality\nto get that with probability of at least 1 −δ over the choice of S we have that\n|ˆθ −θ| ≤\nr\nlog(2/δ)\n2 m (24.2)\nAnother interpretation of ˆθ is as the Maximum Likelihood Estimator, as we\nformally explain now We ﬁrst write the probability of generating the sample S:\nP[S = (x1, , xm)] =\nm\nY\ni=1\nθxi (1 −θ)1−xi = θ\nP\ni xi (1 −θ)\nP\ni(1−xi)",
      "word_count": 250,
      "source_page": 343,
      "start_position": 124329,
      "end_position": 124578,
      "sentences_count": 17
    },
    {
      "chunk_id": 553,
      "text": "344\nGenerative Models\n24.1.1\nMaximum Likelihood Estimation for Continuous Random Variables\nLet X be a continuous random variable Then, for most x ∈R we have P[X =\nx] = 0 and therefore the deﬁnition of likelihood as given before is trivialized To\novercome this technical problem we deﬁne the likelihood as log of the density of\nthe probability of X at x That is, given an i.i.d training set S = (x1, , xm)\nsampled according to a density distribution Pθ we deﬁne the likelihood of S given\nθ as\nL(S; θ) = log\n m\nY\ni=1\nPθ(xi) =\nm\nX\ni=1\nlog(Pθ(xi)) As before, the maximum likelihood estimator is a maximizer of L(S; θ) with\nrespect to θ As an example, consider a Gaussian random variable, for which the density\nfunction of X is parameterized by θ = (µ, σ) and is deﬁned as follows:\nPθ(x) =\n1\nσ\n√\n2π exp\n\u0012\n−(x −µ)2\n2σ2\n\u0013 We can rewrite the likelihood as\nL(S; θ) = −1\n2σ2\nm\nX\ni=1\n(xi −µ)2 −m log(σ\n√\n2 π) To ﬁnd a parameter θ = (µ, σ) that optimizes this we take the derivative of the\nlikelihood w.r.t µ and w.r.t σ and compare it to 0",
      "word_count": 207,
      "source_page": 344,
      "start_position": 124695,
      "end_position": 124901,
      "sentences_count": 13
    },
    {
      "chunk_id": 554,
      "text": "24.1 Maximum Likelihood Estimator\n345\n24.1.2\nMaximum Likelihood and Empirical Risk Minimization\nThe maximum likelihood estimator shares some similarity with the Empirical\nRisk Minimization (ERM) principle, which we studied extensively in previous\nchapters Recall that in the ERM principle we have a hypothesis class H and\nwe use the training set for choosing a hypothesis h ∈H that minimizes the\nempirical risk We now show that the maximum likelihood estimator is an ERM\nfor a particular loss function Given a parameter θ and an observation x, we deﬁne the loss of θ on x as\nℓ(θ, x) = −log(Pθ[x]) (24.4)\nThat is, ℓ(θ, x) is the negation of the log-likelihood of the observation x, assuming\nthe data is distributed according to Pθ This loss function is often referred to as\nthe log-loss On the basis of this deﬁnition it is immediate that the maximum\nlikelihood principle is equivalent to minimizing the empirical risk with respect\nto the loss function given in Equation (24.4) That is,\nargmin\nθ\nm\nX\ni=1\n(−log(Pθ[xi])) = argmax\nθ\nm\nX\ni=1\nlog(Pθ[xi])",
      "word_count": 178,
      "source_page": 345,
      "start_position": 125041,
      "end_position": 125218,
      "sentences_count": 8
    },
    {
      "chunk_id": 555,
      "text": "On the basis of this deﬁnition it is immediate that the maximum\nlikelihood principle is equivalent to minimizing the empirical risk with respect\nto the loss function given in Equation (24.4) That is,\nargmin\nθ\nm\nX\ni=1\n(−log(Pθ[xi])) = argmax\nθ\nm\nX\ni=1\nlog(Pθ[xi]) Assuming that the data is distributed according to a distribution P (not neces-\nsarily of the parametric form we employ), the true risk of a parameter θ becomes\nE\nx[ℓ(θ, x)] = −\nX\nx\nP[x] log(Pθ[x])\n=\nX\nx\nP[x] log\n\u0012 P[x]\nPθ[x]\n\u0013\n|\n{z\n}\nDRE[P||Pθ]\n+\nX\nx\nP[x] log\n\u0012\n1\nP[x]\n\u0013\n|\n{z\n}\nH(P)\n,\n(24.5)\nwhere DRE is called the relative entropy, and H is called the entropy func-\ntion The relative entropy is a divergence measure between two probabilities For discrete variables, it is always nonnegative and is equal to 0 only if the two\ndistributions are the same It follows that the true risk is minimal when Pθ = P The expression given in Equation (24.5) underscores how our generative as-\nsumption aﬀects our density estimation, even in the limit of inﬁnite data It\nshows that if the underlying distribution is indeed of a parametric form, then by\nchoosing the correct parameter we can make the risk be the entropy of the distri-\nbution",
      "word_count": 222,
      "source_page": 345,
      "start_position": 125173,
      "end_position": 125394,
      "sentences_count": 8
    },
    {
      "chunk_id": 556,
      "text": "346\nGenerative Models\nTo answer this question we need to deﬁne how we assess the quality of an approxi-\nmated solution of the density estimation problem Unlike discriminative learning,\nwhere there is a clear notion of “loss,” in generative learning there are various\nways to deﬁne the loss of a model On the basis of the previous subsection, one\nnatural candidate is the expected log-loss as given in Equation (24.5) In some situations, it is easy to prove that the maximum likelihood principle\nguarantees low true risk as well For example, consider the problem of estimating\nthe mean of a Gaussian variable of unit variance We saw previously that the\nmaximum likelihood estimator is the average: ˆµ = 1\nm\nP\ni xi Let µ⋆be the optimal\nparameter Then,\nE\nx∼N(µ⋆,1)[ℓ(ˆµ, x) −ℓ(µ⋆, x)] =\nE\nx∼N(µ⋆,1) log\n\u0012Pµ⋆[x]\nPˆµ[x]\n\u0013\n=\nE\nx∼N(µ⋆,1)\n\u0012\n−1\n2(x −µ⋆)2 + 1\n2(x −ˆµ)2\n\u0013\n= ˆµ2\n2 −(µ⋆)2\n2\n+ (µ⋆−ˆµ)\nE\nx∼N(µ⋆,1)[x]\n= ˆµ2\n2 −(µ⋆)2\n2\n+ (µ⋆−ˆµ) µ⋆\n= 1\n2(ˆµ −µ⋆)2 (24.6)\nNext, we note that ˆµ is the average of m Gaussian variables and therefore it is\nalso distributed normally with mean µ⋆and variance σ⋆/m From this fact we\ncan derive bounds of the form: with probability of at least 1 −δ we have that\n|ˆµ −µ⋆| ≤ϵ where ϵ depends on σ⋆/m and on δ In some situations, the maximum likelihood estimator clearly overﬁts",
      "word_count": 239,
      "source_page": 346,
      "start_position": 125443,
      "end_position": 125681,
      "sentences_count": 11
    },
    {
      "chunk_id": 557,
      "text": "From this fact we\ncan derive bounds of the form: with probability of at least 1 −δ we have that\n|ˆµ −µ⋆| ≤ϵ where ϵ depends on σ⋆/m and on δ In some situations, the maximum likelihood estimator clearly overﬁts For\nexample, consider a Bernoulli random variable X and let P[X = 1] = θ⋆ As\nwe saw previously, using Hoeﬀding’s inequality we can easily derive a guarantee\non |θ⋆−ˆθ| that holds with high probability (see Equation (24.2)) However, if\nour goal is to obtain a small value of the expected log-loss function as deﬁned in\nEquation (24.5) we might fail For example, assume that θ⋆is nonzero but very\nsmall Then, the probability that no element of a sample of size m will be 1 is\n(1 −θ⋆)m, which is greater than e−2θ⋆m It follows that whenever m ≤log(2)\n2θ⋆,\nthe probability that the sample is all zeros is at least 50%, and in that case, the\nmaximum likelihood rule will set ˆθ = 0 But the true risk of the estimate ˆθ = 0\nis\nE\nx∼θ⋆[ℓ(ˆθ, x)] = θ⋆ℓ(ˆθ, 1) + (1 −θ⋆)ℓ(ˆθ, 0)\n= θ⋆log(1/ˆθ) + (1 −θ⋆) log(1/(1 −ˆθ))\n= θ⋆log(1/0) = ∞ This simple example shows that we should be careful in applying the maximum\nlikelihood principle To overcome overﬁtting, we can use the variety of tools we encountered pre-",
      "word_count": 225,
      "source_page": 346,
      "start_position": 125642,
      "end_position": 125866,
      "sentences_count": 11
    },
    {
      "chunk_id": 558,
      "text": "24.2 Naive Bayes\n347\nviously in the book A simple regularization technique is outlined in Exercise\n2 24.2\nNaive Bayes\nThe Naive Bayes classiﬁer is a classical demonstration of how generative as-\nsumptions and parameter estimations simplify the learning process Consider\nthe problem of predicting a label y ∈{0, 1} on the basis of a vector of features\nx = (x1, , xd), where we assume that each xi is in {0, 1} Recall that the Bayes\noptimal classiﬁer is\nhBayes(x) = argmax\ny∈{0,1}\nP[Y = y|X = x] To describe the probability function P[Y = y|X = x] we need 2d parameters,\neach of which corresponds to P[Y = 1|X = x] for a certain value of x ∈{0, 1}d This implies that the number of examples we need grows exponentially with the\nnumber of features In the Naive Bayes approach we make the (rather naive) generative assumption\nthat given the label, the features are independent of each other That is,\nP[X = x|Y = y] =\nd\nY\ni=1\nP[Xi = xi|Y = y] With this assumption and using Bayes’ rule, the Bayes optimal classiﬁer can be\nfurther simpliﬁed:\nhBayes(x) = argmax\ny∈{0,1}\nP[Y = y|X = x]\n= argmax\ny∈{0,1}\nP[Y = y]P[X = x|Y = y]\n= argmax\ny∈{0,1}\nP[Y = y]\nd\nY\ni=1\nP[Xi = xi|Y = y] (24.7)\nThat is, now the number of parameters we need to estimate is only 2d + 1",
      "word_count": 241,
      "source_page": 347,
      "start_position": 125867,
      "end_position": 126107,
      "sentences_count": 12
    },
    {
      "chunk_id": 559,
      "text": "348\nGenerative Models\nvector of features x = (x1, , xd) But now the generative assumption is as\nfollows First, we assume that P[Y = 1] = P[Y = 0] = 1/2 Second, we assume\nthat the conditional probability of X given Y is a Gaussian distribution Finally,\nthe covariance matrix of the Gaussian distribution is the same for both values\nof the label Formally, let µ0, µ1 ∈Rd and let Σ be a covariance matrix Then,\nthe density distribution is given by\nP[X = x|Y = y] =\n1\n(2π)d/2|Σ|1/2 exp\n\u0012\n−1\n2(x −µy)T Σ−1(x −µy)\n\u0013 As we have shown in the previous section, using Bayes’ rule we can write\nhBayes(x) = argmax\ny∈{0,1}\nP[Y = y]P[X = x|Y = y] This means that we will predict hBayes(x) = 1 iﬀ\nlog\n\u0012P[Y = 1]P[X = x|Y = 1]\nP[Y = 0]P[X = x|Y = 0]\n\u0013\n> 0 This ratio is often called the log-likelihood ratio In our case, the log-likelihood ratio becomes\n1\n2(x −µ0)T Σ−1(x −µ0) −1\n2(x −µ1)T Σ−1(x −µ1)\nWe can rewrite this as ⟨w, x⟩+ b where\nw = (µ1 −µ0)T Σ−1\nand\nb = 1\n2\n\u0000µT\n0 Σ−1µ0 −µT\n1 Σ−1µ1\n\u0001 (24.8)\nAs a result of the preceding derivation we obtain that under the aforemen-\ntioned generative assumptions, the Bayes optimal classiﬁer is a linear classiﬁer",
      "word_count": 228,
      "source_page": 348,
      "start_position": 126187,
      "end_position": 126414,
      "sentences_count": 13
    },
    {
      "chunk_id": 560,
      "text": "In our case, the log-likelihood ratio becomes\n1\n2(x −µ0)T Σ−1(x −µ0) −1\n2(x −µ1)T Σ−1(x −µ1)\nWe can rewrite this as ⟨w, x⟩+ b where\nw = (µ1 −µ0)T Σ−1\nand\nb = 1\n2\n\u0000µT\n0 Σ−1µ0 −µT\n1 Σ−1µ1\n\u0001 (24.8)\nAs a result of the preceding derivation we obtain that under the aforemen-\ntioned generative assumptions, the Bayes optimal classiﬁer is a linear classiﬁer Additionally, one may train the classiﬁer by estimating the parameter µ0, µ1\nand Σ from the data, using, for example, the maximum likelihood estimator With those estimators at hand, the values of w and b can be calculated as in\nEquation (24.8) 24.4\nLatent Variables and the EM Algorithm\nIn generative models we assume that the data is generated by sampling from\na speciﬁc parametric distribution over our instance space X Sometimes, it is\nconvenient to express this distribution using latent random variables A natural\nexample is a mixture of k Gaussian distributions That is, X = Rd and we\nassume that each x is generated as follows First, we choose a random number in\n{1, , k} Let Y be a random variable corresponding to this choice, and denote\nP[Y = y] = cy Second, we choose x on the basis of the value of Y according to\na Gaussian distribution\nP[X = x|Y = y] =\n1\n(2π)d/2|Σy|1/2 exp\n\u0012\n−1\n2(x −µy)T Σ−1\ny (x −µy)\n\u0013 (24.9)",
      "word_count": 239,
      "source_page": 348,
      "start_position": 126347,
      "end_position": 126585,
      "sentences_count": 13
    },
    {
      "chunk_id": 561,
      "text": "24.4 Latent Variables and the EM Algorithm\n349\nTherefore, the density of X can be written as:\nP[X = x] =\nk\nX\ny=1\nP[Y = y]P[X = x|Y = y]\n=\nk\nX\ny=1\ncy\n1\n(2π)d/2|Σy|1/2 exp\n\u0012\n−1\n2(x −µy)T Σ−1\ny (x −µy)\n\u0013 Note that Y is a hidden variable that we do not observe in our data Neverthe-\nless, we introduce Y since it helps us describe a simple parametric form of the\nprobability of X More generally, let θ be the parameters of the joint distribution of X and Y\n(e.g., in the preceding example, θ consists of cy, µy, and Σy, for all y = 1, , k) Then, the log-likelihood of an observation x can be written as\nlog (Pθ[X = x]) = log\n k\nX\ny=1\nPθ[X = x, Y = y] Given an i.i.d sample, S = (x1, , xm), we would like to ﬁnd θ that maxi-\nmizes the log-likelihood of S,\nL(θ) = log\nm\nY\ni=1\nPθ[X = xi]\n=\nm\nX\ni=1\nlog Pθ[X = xi]\n=\nm\nX\ni=1\nlog\n k\nX\ny=1\nPθ[X = xi, Y = y] The maximum-likelihood estimator is therefore the solution of the maximization\nproblem\nargmax\nθ\nL(θ) = argmax\nθ\nm\nX\ni=1\nlog\n k\nX\ny=1\nPθ[X = xi, Y = y] In many situations, the summation inside the log makes the preceding opti-\nmization problem computationally hard",
      "word_count": 241,
      "source_page": 349,
      "start_position": 126586,
      "end_position": 126826,
      "sentences_count": 11
    },
    {
      "chunk_id": 562,
      "text": "350\nGenerative Models\nIf each row of Q deﬁnes a probability over the ith latent variable given X = xi,\nthen we can interpret F(Q, θ) as the expected log-likelihood of a training set\n(x1, y1), , (xm, ym), where the expectation is with respect to the choice of\neach yi on the basis of the ith row of Q In the deﬁnition of F, the summation is\noutside the log, and we assume that this makes the optimization problem with\nrespect to θ tractable:\nassumption 24.1\nFor any matrix Q ∈[0, 1]m,k, such that each row of Q sums\nto 1, the optimization problem\nargmax\nθ\nF(Q, θ)\nis tractable The intuitive idea of EM is that we have a “chicken and egg” problem On one\nhand, had we known Q, then by our assumption, the optimization problem of\nﬁnding the best θ is tractable On the other hand, had we known the parameters\nθ we could have set Qi,y to be the probability of Y = y given that X = xi The EM algorithm therefore alternates between ﬁnding θ given Q and ﬁnding Q\ngiven θ Formally, EM ﬁnds a sequence of solutions (Q(1), θ(1)), (Q(2), θ(2)), where at iteration t, we construct (Q(t+1), θ(t+1)) by performing two steps • Expectation Step: Set\nQ(t+1)\ni,y\n= Pθ(t)[Y = y|X = xi]",
      "word_count": 224,
      "source_page": 350,
      "start_position": 126929,
      "end_position": 127152,
      "sentences_count": 10
    },
    {
      "chunk_id": 563,
      "text": "where at iteration t, we construct (Q(t+1), θ(t+1)) by performing two steps • Expectation Step: Set\nQ(t+1)\ni,y\n= Pθ(t)[Y = y|X = xi] (24.10)\nThis step is called the Expectation step, because it yields a new probabil-\nity over the latent variables, which deﬁnes a new expected log-likelihood\nfunction over θ • Maximization Step: Set θ(t+1) to be the maximizer of the expected log-\nlikelihood, where the expectation is according to Q(t+1):\nθ(t+1) = argmax\nθ\nF(Q(t+1), θ) (24.11)\nBy our assumption, it is possible to solve this optimization problem eﬃ-\nciently The initial values of θ(1) and Q(1) are usually chosen at random and the\nprocedure terminates after the improvement in the likelihood value stops being\nsigniﬁcant 24.4.1\nEM as an Alternate Maximization Algorithm\nTo analyze the EM algorithm, we ﬁrst view it as an alternate maximization\nalgorithm Deﬁne the following objective function\nG(Q, θ) = F(Q, θ) −\nm\nX\ni=1\nk\nX\ny=1\nQi,y log(Qi,y).",
      "word_count": 159,
      "source_page": 350,
      "start_position": 127129,
      "end_position": 127287,
      "sentences_count": 8
    },
    {
      "chunk_id": 564,
      "text": "24.4 Latent Variables and the EM Algorithm\n351\nThe second term is the sum of the entropies of the rows of Q Let\nQ =\n(\nQ ∈[0, 1]m,k : ∀i,\nk\nX\ny=1\nQi,y = 1\n)\nbe the set of matrices whose rows deﬁne probabilities over [k] The following\nlemma shows that EM performs alternate maximization iterations for maximiz-\ning G lemma 24.2\nThe EM procedure can be rewritten as:\nQ(t+1) =\nargmax\nQ∈Q\nG(Q, θ(t))\nθ(t+1) =\nargmax\nθ\nG(Q(t+1), θ) Furthermore, G(Q(t+1), θ(t)) = L(θ(t)) Proof\nGiven Q(t+1) we clearly have that\nargmax\nθ\nG(Q(t+1), θ) = argmax\nθ\nF(Q(t+1), θ) Therefore, we only need to show that for any θ, the solution of argmaxQ∈Q G(Q, θ)\nis to set Qi,y = Pθ[Y = y|X = xi] Indeed, by Jensen’s inequality, for any Q ∈Q\nwe have that\nG(Q, θ) =\nm\nX\ni=1\n k\nX\ny=1\nQi,y log\n\u0012Pθ[X = xi, Y = y]\nQi,y\n\u0013 ≤\nm\nX\ni=1\n \nlog\n k\nX\ny=1\nQi,y\nPθ[X = xi, Y = y]\nQi,y =\nm\nX\ni=1\nlog\n k\nX\ny=1\nPθ[X = xi, Y = y] =\nm\nX\ni=1\nlog (Pθ[X = xi]) = L(θ),",
      "word_count": 201,
      "source_page": 351,
      "start_position": 127288,
      "end_position": 127488,
      "sentences_count": 11
    },
    {
      "chunk_id": 565,
      "text": "352\nGenerative Models\nwhile for Qi,y = Pθ[Y = y|X = xi] we have\nG(Q, θ) =\nm\nX\ni=1\n k\nX\ny=1\nPθ[Y = y|X = xi] log\n\u0012Pθ[X = xi, Y = y]\nPθ[Y = y|X = xi]\n\u0013 =\nm\nX\ni=1\nk\nX\ny=1\nPθ[Y = y|X = xi] log (Pθ[X = xi])\n=\nm\nX\ni=1\nlog (Pθ[X = xi])\nk\nX\ny=1\nPθ[Y = y|X = xi]\n=\nm\nX\ni=1\nlog (Pθ[X = xi]) = L(θ) This shows that setting Qi,y = Pθ[Y = y|X = xi] maximizes G(Q, θ) over Q ∈Q\nand shows that G(Q(t+1), θ(t)) = L(θ(t)) The preceding lemma immediately implies:\ntheorem 24.3\nThe EM procedure never decreases the log-likelihood; namely,\nfor all t,\nL(θ(t+1)) ≥L(θ(t)) Proof\nBy the lemma we have\nL(θ(t+1)) = G(Q(t+2), θ(t+1)) ≥G(Q(t+1), θ(t)) = L(θ(t)) 24.4.2\nEM for Mixture of Gaussians (Soft k-Means)\nConsider the case of a mixture of k Gaussians in which θ is a triplet (c, {µ1, , µk}, {Σ1, , Σk})\nwhere Pθ[Y = y] = cy and Pθ[X = x|Y = y] is as given in Equation (24.9) For\nsimplicity, we assume that Σ1 = Σ2 = · · · = Σk = I, where I is the identity\nmatrix",
      "word_count": 211,
      "source_page": 352,
      "start_position": 127489,
      "end_position": 127699,
      "sentences_count": 9
    },
    {
      "chunk_id": 566,
      "text": "24.5 Bayesian Reasoning\n353\nwhich in our case amounts to maximizing the following expression w.r.t c\nand µ:\nm\nX\ni=1\nk\nX\ny=1\nPθ(t)[Y = y|X = xi]\n\u0012\nlog(cy) −1\n2∥xi −µy∥2\n\u0013 (24.13)\nComparing the derivative of Equation (24.13) w.r.t µy to zero and rear-\nranging terms we obtain:\nµy =\nPm\ni=1 Pθ(t)[Y = y|X = xi] xi\nPm\ni=1 Pθ(t)[Y = y|X = xi] That is, µy is a weighted average of the xi where the weights are according\nto the probabilities calculated in the E step To ﬁnd the optimal c we need\nto be more careful since we must ensure that c is a probability vector In\nExercise 3 we show that the solution is:\ncy =\nPm\ni=1 Pθ(t)[Y = y|X = xi]\nPk\ny′=1\nPm\ni=1 Pθ(t)[Y = y′|X = xi] (24.14)\nIt is interesting to compare the preceding algorithm to the k-means algorithm\ndescribed in Chapter 22 In the k-means algorithm, we ﬁrst assign each example\nto a cluster according to the distance ∥xi −µy∥ Then, we update each center\nµy according to the average of the examples assigned to this cluster In the EM\napproach, however, we determine the probability that each example belongs to\neach cluster Then, we update the centers on the basis of a weighted sum over\nthe entire sample For this reason, the EM approach for k-means is sometimes\ncalled “soft k-means.”\n24.5\nBayesian Reasoning\nThe maximum likelihood estimator follows a frequentist approach",
      "word_count": 248,
      "source_page": 353,
      "start_position": 127789,
      "end_position": 128036,
      "sentences_count": 13
    },
    {
      "chunk_id": 567,
      "text": "Then, we update the centers on the basis of a weighted sum over\nthe entire sample For this reason, the EM approach for k-means is sometimes\ncalled “soft k-means.”\n24.5\nBayesian Reasoning\nThe maximum likelihood estimator follows a frequentist approach This means\nthat we refer to the parameter θ as a ﬁxed parameter and the only problem is\nthat we do not know its value A diﬀerent approach to parameter estimation\nis called Bayesian reasoning In the Bayesian approach, our uncertainty about\nθ is also modeled using probability theory That is, we think of θ as a random\nvariable as well and refer to the distribution P[θ] as a prior distribution As its\nname indicates, the prior distribution should be deﬁned by the learner prior to\nobserving the data As an example, let us consider again the drug company which developed a\nnew drug On the basis of past experience, the statisticians at the drug company\nbelieve that whenever a drug has reached the level of clinic experiments on\npeople, it is likely to be eﬀective They model this prior belief by deﬁning a\ndensity distribution on θ such that\nP[θ] =\n(\n0.8\nif θ > 0.5\n0.2\nif θ ≤0.5\n(24.15)",
      "word_count": 203,
      "source_page": 353,
      "start_position": 127997,
      "end_position": 128199,
      "sentences_count": 10
    },
    {
      "chunk_id": 568,
      "text": "354\nGenerative Models\nAs before, given a speciﬁc value of θ, it is assumed that the conditional proba-\nbility, P[X = x|θ], is known In the drug company example, X takes values in\n{0, 1} and P[X = x|θ] = θx(1 −θ)1−x Once the prior distribution over θ and the conditional distribution over X\ngiven θ are deﬁned, we again have complete knowledge of the distribution over\nX This is because we can write the probability over X as a marginal probability\nP[X = x] =\nX\nθ\nP[X = x, θ] =\nX\nθ\nP[θ]P[X = x|θ],\nwhere the last equality follows from the deﬁnition of conditional probability If\nθ is continuous we replace P[θ] with the density function and the sum becomes\nan integral:\nP[X = x] =\nZ\nθ\nP[θ]P[X = x|θ] dθ Seemingly, once we know P[X = x], a training set S = (x1, , xm) tells us\nnothing as we are already experts who know the distribution over a new point\nX However, the Bayesian view introduces dependency between S and X This is\nbecause we now refer to θ as a random variable A new point X and the previous\npoints in S are independent only conditioned on θ This is diﬀerent from the\nfrequentist philosophy in which θ is a parameter that we might not know, but\nsince it is just a parameter of the distribution, a new point X and previous points\nS are always independent",
      "word_count": 245,
      "source_page": 354,
      "start_position": 128200,
      "end_position": 128444,
      "sentences_count": 11
    },
    {
      "chunk_id": 569,
      "text": "A new point X and the previous\npoints in S are independent only conditioned on θ This is diﬀerent from the\nfrequentist philosophy in which θ is a parameter that we might not know, but\nsince it is just a parameter of the distribution, a new point X and previous points\nS are always independent In the Bayesian framework, since X and S are not independent anymore, what\nwe would like to calculate is the probability of X given S, which by the chain\nrule can be written as follows:\nP[X = x|S] =\nX\nθ\nP[X = x|θ, S] P[θ|S] =\nX\nθ\nP[X = x|θ] P[θ|S] The second inequality follows from the assumption that X and S are independent\nwhen we condition on θ Using Bayes’ rule we have\nP[θ|S] = P[S|θ] P[θ]\nP[S]\n,\nand together with the assumption that points are independent conditioned on θ,\nwe can write\nP[θ|S] = P[S|θ] P[θ]\nP[S]\n=\n1\nP[S]\nm\nY\ni=1\nP[X = xi|θ] P[θ] We therefore obtain the following expression for Bayesian prediction:\nP[X = x|S] =\n1\nP[S]\nX\nθ\nP[X = x|θ]\nm\nY\ni=1\nP[X = xi|θ] P[θ] (24.16)\nGetting back to our drug company example, we can rewrite P[X = x|S] as\nP[X = x|S] =\n1\nP[S]\nZ\nθx+P\ni xi(1 −θ)1−x+P\ni(1−xi) P[θ] dθ.",
      "word_count": 223,
      "source_page": 354,
      "start_position": 128390,
      "end_position": 128612,
      "sentences_count": 7
    },
    {
      "chunk_id": 570,
      "text": "24.6 Summary\n355\nIt is interesting to note that when P[θ] is uniform we obtain that\nP[X = x|S] ∝\nZ\nθx+P\ni xi(1 −θ)1−x+P\ni(1−xi) dθ Solving the preceding integral (using integration by parts) we obtain\nP[X = 1|S] = (P\ni xi) + 1\nm + 2 Recall that the prediction according to the maximum likelihood principle in this\ncase is P[X = 1|ˆθ] =\nP\ni xi\nm The Bayesian prediction with uniform prior is rather\nsimilar to the maximum likelihood prediction, except it adds “pseudoexamples”\nto the training set, thus biasing the prediction toward the uniform prior Maximum A Posteriori\nIn many situations, it is diﬃcult to ﬁnd a closed form solution to the integral\ngiven in Equation (24.16) Several numerical methods can be used to approxi-\nmate this integral Another popular solution is to ﬁnd a single θ which maximizes\nP[θ|S] The value of θ which maximizes P[θ|S] is called the Maximum A Poste-\nriori estimator Once this value is found, we can calculate the probability that\nX = x given the maximum a posteriori estimator and independently on S 24.6\nSummary\nIn the generative approach to machine learning we aim at modeling the distri-\nbution over the data In particular, in parametric density estimation we further\nassume that the underlying distribution over the data has a speciﬁc paramet-\nric form and our goal is to estimate the parameters of the model",
      "word_count": 237,
      "source_page": 355,
      "start_position": 128613,
      "end_position": 128849,
      "sentences_count": 11
    },
    {
      "chunk_id": 571,
      "text": "24.6\nSummary\nIn the generative approach to machine learning we aim at modeling the distri-\nbution over the data In particular, in parametric density estimation we further\nassume that the underlying distribution over the data has a speciﬁc paramet-\nric form and our goal is to estimate the parameters of the model We have\ndescribed several principles for parameter estimation, including maximum like-\nlihood, Bayesian estimation, and maximum a posteriori We have also described\nseveral speciﬁc algorithms for implementing the maximum likelihood under dif-\nferent assumptions on the underlying data distribution, in particular, Naive\nBayes, LDA, and EM 24.7\nBibliographic Remarks\nThe maximum likelihood principle was studied by Ronald Fisher in the beginning\nof the 20th century Bayesian statistics follow Bayes’ rule, which is named after\nthe 18th century English mathematician Thomas Bayes There are many excellent books on the generative and Bayesian approaches\nto machine learning See, for example, (Bishop 2006, Koller & Friedman 2009,\nMacKay 2003, Murphy 2012, Barber 2012).",
      "word_count": 162,
      "source_page": 355,
      "start_position": 128798,
      "end_position": 128959,
      "sentences_count": 8
    },
    {
      "chunk_id": 572,
      "text": "356\nGenerative Models\n24.8\nExercises\n1 Prove that the maximum likelihood estimator of the variance of a Gaussian\nvariable is biased 2 Regularization for Maximum Likelihood: Consider the following regularized\nloss minimization:\n1\nm\nm\nX\ni=1\nlog(1/Pθ[xi]) + 1\nm (log(1/θ) + log(1/(1 −θ))) • Show that the preceding objective is equivalent to the usual empirical error\nhad we added two pseudoexamples to the training set Conclude that\nthe regularized maximum likelihood estimator would be\nˆθ =\n1\nm + 2\n \n1 +\nm\nX\ni=1\nxi • Derive a high probability bound on |ˆθ−θ⋆| Hint: Rewrite this as |ˆθ−E[ˆθ]+\nE[ˆθ] −θ⋆| and then use the triangle inequality and Hoeﬀding inequality • Use this to bound the true risk Hint: Use the fact that now ˆθ ≥\n1\nm+2 to\nrelate |ˆθ −θ⋆| to the relative entropy 3 • Consider a general optimization problem of the form:\nmax\nc\nk\nX\ny=1\nνy log(cy)\ns.t cy > 0,\nX\ny\ncy = 1 ,\nwhere ν ∈Rk\n+ is a vector of nonnegative weights Verify that the M step\nof soft k-means involves solving such an optimization problem • Let c⋆=\n1\nP\ny νy ν Show that c⋆is a probability vector • Show that the optimization problem is equivalent to the problem:\nmin\nc DRE(c⋆||c)\ns.t cy > 0,\nX\ny\ncy = 1 • Using properties of the relative entropy, conclude that c⋆is the solution to\nthe optimization problem.",
      "word_count": 242,
      "source_page": 356,
      "start_position": 128960,
      "end_position": 129201,
      "sentences_count": 19
    },
    {
      "chunk_id": 573,
      "text": "25\nFeature Selection and Generation\nIn the beginning of the book, we discussed the abstract model of learning, in\nwhich the prior knowledge utilized by the learner is fully encoded by the choice\nof the hypothesis class However, there is another modeling choice, which we\nhave so far ignored: How do we represent the instance space X For example, in\nthe papayas learning problem, we proposed the hypothesis class of rectangles in\nthe softness-color two dimensional plane That is, our ﬁrst modeling choice was\nto represent a papaya as a two dimensional point corresponding to its softness\nand color Only after that did we choose the hypothesis class of rectangles as a\nclass of mappings from the plane into the label set The transformation from the\nreal world object “papaya” into the scalar representing its softness or its color\nis called a feature function or a feature for short; namely, any measurement of\nthe real world object can be regarded as a feature If X is a subset of a vector\nspace, each x ∈X is sometimes referred to as a feature vector It is important to\nunderstand that the way we encode real world objects as an instance space X is\nby itself prior knowledge about the problem Furthermore, even when we already have an instance space X which is rep-\nresented as a subset of a vector space, we might still want to change it into a\ndiﬀerent representation and apply a hypothesis class on top of it",
      "word_count": 250,
      "source_page": 357,
      "start_position": 129202,
      "end_position": 129451,
      "sentences_count": 9
    },
    {
      "chunk_id": 574,
      "text": "It is important to\nunderstand that the way we encode real world objects as an instance space X is\nby itself prior knowledge about the problem Furthermore, even when we already have an instance space X which is rep-\nresented as a subset of a vector space, we might still want to change it into a\ndiﬀerent representation and apply a hypothesis class on top of it That is, we\nmay deﬁne a hypothesis class on X by composing some class H on top of a\nfeature function which maps X into some other vector space X ′ We have al-\nready encountered examples of such compositions – in Chapter 15 we saw that\nkernel-based SVM learns a composition of the class of halfspaces over a feature\nmapping ψ that maps each original instance in X into some Hilbert space And,\nindeed, the choice of ψ is another form of prior knowledge we impose on the\nproblem In this chapter we study several methods for constructing a good feature set We start with the problem of feature selection, in which we have a large pool\nof features and our goal is to select a small number of features that will be\nused by our predictor Next, we discuss feature manipulations and normalization These include simple transformations that we apply on our original features Such\ntransformations may decrease the sample complexity of our learning algorithm,\nits bias, or its computational complexity Last, we discuss several approaches for\nfeature learning",
      "word_count": 248,
      "source_page": 357,
      "start_position": 129385,
      "end_position": 129632,
      "sentences_count": 11
    },
    {
      "chunk_id": 575,
      "text": "358\nFeature Selection and Generation\nWe emphasize that while there are some common techniques for feature learn-\ning one may want to try, the No-Free-Lunch theorem implies that there is no ulti-\nmate feature learner Any feature learning algorithm might fail on some problem In other words, the success of each feature learner relies (sometimes implicitly)\non some form of prior assumption on the data distribution Furthermore, the\nrelative quality of features highly depends on the learning algorithm we are later\ngoing to apply using these features This is illustrated in the following example Example 25.1\nConsider a regression problem in which X = R2, Y = R, and\nthe loss function is the squared loss Suppose that the underlying distribution\nis such that an example (x, y) is generated as follows: First, we sample x1 from\nthe uniform distribution over [−1, 1] Then, we deterministically set y = x12 Finally, the second feature is set to be x2 = y + z, where z is sampled from the\nuniform distribution over [−0.01, 0.01] Suppose we would like to choose a single\nfeature Intuitively, the ﬁrst feature should be preferred over the second feature\nas the target can be perfectly predicted based on the ﬁrst feature alone, while it\ncannot be perfectly predicted based on the second feature Indeed, choosing the\nﬁrst feature would be the right choice if we are later going to apply polynomial\nregression of degree at least 2",
      "word_count": 242,
      "source_page": 358,
      "start_position": 129674,
      "end_position": 129915,
      "sentences_count": 12
    },
    {
      "chunk_id": 576,
      "text": "Intuitively, the ﬁrst feature should be preferred over the second feature\nas the target can be perfectly predicted based on the ﬁrst feature alone, while it\ncannot be perfectly predicted based on the second feature Indeed, choosing the\nﬁrst feature would be the right choice if we are later going to apply polynomial\nregression of degree at least 2 However, if the learner is going to be a linear\nregressor, then we should prefer the second feature over the ﬁrst one, since the\noptimal linear predictor based on the ﬁrst feature will have a larger risk than\nthe optimal linear predictor based on the second feature 25.1\nFeature Selection\nThroughout this section we assume that X = Rd That is, each instance is repre-\nsented as a vector of d features Our goal is to learn a predictor that only relies\non k ≪d features Predictors that use only a small subset of features require a\nsmaller memory footprint and can be applied faster Furthermore, in applications\nsuch as medical diagnostics, obtaining each possible “feature” (e.g., test result)\ncan be costly; therefore, a predictor that uses only a small number of features\nis desirable even at the cost of a small degradation in performance, relative to\na predictor that uses more features Finally, constraining the hypothesis class to\nuse a small subset of features can reduce its estimation error and thus prevent\noverﬁtting",
      "word_count": 233,
      "source_page": 358,
      "start_position": 129857,
      "end_position": 130089,
      "sentences_count": 9
    },
    {
      "chunk_id": 577,
      "text": "Furthermore, in applications\nsuch as medical diagnostics, obtaining each possible “feature” (e.g., test result)\ncan be costly; therefore, a predictor that uses only a small number of features\nis desirable even at the cost of a small degradation in performance, relative to\na predictor that uses more features Finally, constraining the hypothesis class to\nuse a small subset of features can reduce its estimation error and thus prevent\noverﬁtting Ideally, we could have tried all subsets of k out of d features and choose the\nsubset which leads to the best performing predictor However, such an exhaustive\nsearch is usually computationally intractable In the following we describe three\ncomputationally feasible approaches for feature selection While these methods\ncannot guarantee ﬁnding the optimal subset, they often work reasonably well in\npractice Some of the methods come with formal guarantees on the quality of the\nselected subsets under certain assumptions We do not discuss these guarantees\nhere.",
      "word_count": 155,
      "source_page": 358,
      "start_position": 130021,
      "end_position": 130175,
      "sentences_count": 8
    },
    {
      "chunk_id": 578,
      "text": "25.1 Feature Selection\n359\n25.1.1\nFilters\nMaybe the simplest approach for feature selection is the ﬁlter method, in which\nwe assess individual features, independently of other features, according to some\nquality measure We can then select the k features that achieve the highest score\n(alternatively, decide also on the number of features to select according to the\nvalue of their scores) Many quality measures for features have been proposed in the literature Maybe the most straightforward approach is to set the score of a feature ac-\ncording to the error rate of a predictor that is trained solely by that feature To illustrate this, consider a linear regression problem with the squared loss Let v = (x1,j, , xm,j) ∈Rm be a vector designating the values of the jth\nfeature on a training set of m examples and let y = (y1, , ym) ∈Rm be the\nvalues of the target on the same m examples The empirical squared loss of an\nERM linear predictor that uses only the jth feature would be\nmin\na,b∈R\n1\nm∥av + b −y∥2,\nwhere the meaning of adding a scalar b to a vector v is adding b to all coordinates\nof v To solve this problem, let ¯v =\n1\nm\nPm\ni=1 vi be the averaged value of the\nfeature and let ¯y =\n1\nm\nPm\ni=1 yi be the averaged value of the target",
      "word_count": 235,
      "source_page": 359,
      "start_position": 130176,
      "end_position": 130410,
      "sentences_count": 10
    },
    {
      "chunk_id": 579,
      "text": "The empirical squared loss of an\nERM linear predictor that uses only the jth feature would be\nmin\na,b∈R\n1\nm∥av + b −y∥2,\nwhere the meaning of adding a scalar b to a vector v is adding b to all coordinates\nof v To solve this problem, let ¯v =\n1\nm\nPm\ni=1 vi be the averaged value of the\nfeature and let ¯y =\n1\nm\nPm\ni=1 yi be the averaged value of the target Clearly (see\nExercise 1),\nmin\na,b∈R\n1\nm∥av + b −y∥2 = min\na,b∈R\n1\nm∥a(v −¯v) + b −(y −¯y)∥2 (25.1)\nTaking the derivative of the right-hand side objective with respect to b and\ncomparing it to zero we obtain that b = 0 Similarly, solving for a (once we know\nthat b = 0) yields a = ⟨v −¯v, y −¯y⟩/∥v −¯v∥2 Plugging this value back into the\nobjective we obtain the value\n∥y −¯y∥2 −(⟨v −¯v, y −¯y⟩)2\n∥v −¯v∥2 Ranking the features according to the minimal loss they achieve is equivalent\nto ranking them according to the absolute value of the following score (where\nnow a higher score yields a better feature):\n⟨v −¯v, y −¯y⟩\n∥v −¯v∥∥y −¯y∥=\n1\nm ⟨v −¯v, y −¯y⟩\nq\n1\nm∥v −¯v∥2\nq\n1\nm∥y −¯y∥2 (25.2)\nThe preceding expression is known as Pearson’s correlation coeﬃcient",
      "word_count": 226,
      "source_page": 359,
      "start_position": 130332,
      "end_position": 130557,
      "sentences_count": 8
    },
    {
      "chunk_id": 580,
      "text": "360\nFeature Selection and Generation\nIf Pearson’s coeﬃcient equals zero it means that the optimal linear function\nfrom v to y is the all-zeros function, which means that v alone is useless for\npredicting y However, this does not mean that v is a bad feature, as it might\nbe the case that together with other features v can perfectly predict y Indeed,\nconsider a simple example in which the target is generated by the function y =\nx1 + 2x2 Assume also that x1 is generated from the uniform distribution over\n{±1}, and x2 = −1\n2x1 + 1\n2z, where z is also generated i.i.d from the uniform\ndistribution over {±1} Then, E[x1] = E[x2] = E[y] = 0, and we also have\nE[yx1] = E[x2\n1] + 2 E[x2x1] = E[x2\n1] −E[x2\n1] + E[zx1] = 0 Therefore, for a large enough training set, the ﬁrst feature is likely to have a\nPearson’s correlation coeﬃcient that is close to zero, and hence it will most\nprobably not be selected However, no function can predict the target value well\nwithout knowing the ﬁrst feature There are many other score functions that can be used by a ﬁlter method Notable examples are estimators of the mutual information or the area under\nthe receiver operating characteristic (ROC) curve All of these score functions\nsuﬀer from similar problems to the one illustrated previously We refer the reader\nto Guyon & Elisseeﬀ(2003)",
      "word_count": 241,
      "source_page": 360,
      "start_position": 130638,
      "end_position": 130878,
      "sentences_count": 12
    },
    {
      "chunk_id": 581,
      "text": "All of these score functions\nsuﬀer from similar problems to the one illustrated previously We refer the reader\nto Guyon & Elisseeﬀ(2003) 25.1.2\nGreedy Selection Approaches\nGreedy selection is another popular approach for feature selection Unlike ﬁlter\nmethods, greedy selection approaches are coupled with the underlying learning\nalgorithm The simplest instance of greedy selection is forward greedy selection We start with an empty set of features, and then we gradually add one feature\nat a time to the set of selected features Given that our current set of selected\nfeatures is I, we go over all i /∈I, and apply the learning algorithm on the set\nof features I ∪{i} Each such application yields a diﬀerent predictor, and we\nchoose to add the feature that yields the predictor with the smallest risk (on\nthe training set or on a validation set) This process continues until we either\nselect k features, where k is a predeﬁned budget of allowed features, or achieve\nan accurate enough predictor Example 25.2 (Orthogonal Matching Pursuit)\nTo illustrate the forward\ngreedy selection approach, we specify it to the problem of linear regression with\nthe squared loss Let X ∈Rm,d be a matrix whose rows are the m training\ninstances Let y ∈Rm be the vector of the m labels For every i ∈[d], let Xi\nbe the ith column of X Given a set I ⊂[d] we denote by XI the matrix whose\ncolumns are {Xi : i ∈I}",
      "word_count": 243,
      "source_page": 360,
      "start_position": 130857,
      "end_position": 131099,
      "sentences_count": 14
    },
    {
      "chunk_id": 582,
      "text": "25.1 Feature Selection\n361\nThen, we update It = It−1 ∪{jt} We now describe a more eﬃcient implementation of the forward greedy selec-\ntion approach for linear regression which is called Orthogonal Matching Pursuit\n(OMP) The idea is to keep an orthogonal basis of the features aggregated so\nfar Let Vt be a matrix whose columns form an orthonormal basis of the columns\nof XIt Clearly,\nmin\nw ∥XItw −y∥2 = min\nθ∈Rt ∥Vtθ −y∥2 We will maintain a vector θt which minimizes the right-hand side of the equation Initially, we set I0 = ∅, V0 = ∅, and θ1 to be the empty vector At round t, for\nevery j, we decompose Xj = vj + uj where vj = Vt−1V ⊤\nt−1Xj is the projection\nof Xj onto the subspace spanned by Vt−1 and uj is the part of Xj orthogonal to\nVt−1 (see Appendix C) Then,\nmin\nθ,α ∥Vt−1θ + αuj −y∥2\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, Vt−1θ −y⟩\n\u0003\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, −y⟩\n\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2 It follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2",
      "word_count": 227,
      "source_page": 361,
      "start_position": 131129,
      "end_position": 131355,
      "sentences_count": 10
    },
    {
      "chunk_id": 583,
      "text": "Then,\nmin\nθ,α ∥Vt−1θ + αuj −y∥2\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, Vt−1θ −y⟩\n\u0003\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, −y⟩\n\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2 It follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2 The rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015 The OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained\nby a procedure similar to Gram-Schmidt orthonormalization In practice, the\nGram-Schmidt procedure is often numerically unstable In the pseudocode that\nfollows we use SVD (see Section C.4) at the end of each round to obtain an\northonormal basis in a numerically stable manner.",
      "word_count": 168,
      "source_page": 361,
      "start_position": 131277,
      "end_position": 131444,
      "sentences_count": 6
    },
    {
      "chunk_id": 584,
      "text": "362\nFeature Selection and Generation\nOrthogonal Matching Pursuit (OMP)\ninput:\ndata matrix X ∈Rm,d, labels vector y ∈Rm,\nbudget of features T\ninitialize: I1 = ∅\nfor t = 1, , T\nuse SVD to ﬁnd an orthonormal basis V ∈Rm,t−1 of XIt\n(for t = 1 set V to be the all zeros matrix)\nforeach j ∈[d] \\ It let uj = Xj −V V ⊤Xj\nlet jt = argmaxj /∈It:∥uj∥>0\n(⟨uj,y⟩)2\n∥uj∥2\nupdate It+1 = It ∪{jt}\noutput IT +1\nMore Eﬃcient Greedy Selection Criteria\nLet R(w) be the empirical risk of a vector w At each round of the forward\ngreedy selection method, and for every possible j, we should minimize R(w)\nover the vectors w whose support is It−1 ∪{j} This might be time consuming A simpler approach is to choose jt that minimizes\nargmin\nj\nmin\nη∈R R(wt−1 + ηej),\nwhere ej is the all zeros vector except 1 in the jth element That is, we keep\nthe weights of the previously chosen coordinates intact and only optimize over\nthe new variable Therefore, for each j we need to solve an optimization problem\nover a single variable, which is a much easier task than optimizing over t An even simpler approach is to upper bound R(w) using a “simple” function\nand then choose the feature which leads to the largest decrease in this upper\nbound",
      "word_count": 230,
      "source_page": 362,
      "start_position": 131445,
      "end_position": 131674,
      "sentences_count": 8
    },
    {
      "chunk_id": 585,
      "text": "Therefore, for each j we need to solve an optimization problem\nover a single variable, which is a much easier task than optimizing over t An even simpler approach is to upper bound R(w) using a “simple” function\nand then choose the feature which leads to the largest decrease in this upper\nbound For example, if R is a β-smooth function (see Equation (12.5) in Chap-\nter 12), then\nR(w + ηej) ≤R(w) + η ∂R(w)\n∂wj\n+ βη2/2 Minimizing the right-hand side over η yields η = −∂R(w)\n∂wj\n· 1\nβ and plugging this\nvalue into the above yields\nR(w + ηej) ≤R(w) −1\n2β\n\u0012∂R(w)\n∂wj\n\u00132 This value is minimized if the partial derivative of R(w) with respect to wj is\nmaximal We can therefore choose jt to be the index of the largest coordinate of\nthe gradient of R(w) at w Remark 25.3 (AdaBoost as a Forward Greedy Selection Procedure)\nIt is pos-\nsible to interpret the AdaBoost algorithm from Chapter 10 as a forward greedy",
      "word_count": 171,
      "source_page": 362,
      "start_position": 131622,
      "end_position": 131792,
      "sentences_count": 7
    },
    {
      "chunk_id": 586,
      "text": "25.1 Feature Selection\n363\nselection procedure with respect to the function\nR(w) = log\n\n\nm\nX\ni=1\nexp\n\n−yi\nd\nX\nj=1\nwjhj(xi)\n\n\n\n (25.3)\nSee Exercise 3 Backward Elimination\nAnother popular greedy selection approach is backward elimination Here, we\nstart with the full set of features, and then we gradually remove one feature at a\ntime from the set of features Given that our current set of selected features is I,\nwe go over all i ∈I, and apply the learning algorithm on the set of features I\\{i} Each such application yields a diﬀerent predictor, and we choose to remove the\nfeature i for which the predictor obtained from I \\ {i} has the smallest risk (on\nthe training set or on a validation set) Naturally, there are many possible variants of the backward elimination idea It is also possible to combine forward and backward greedy steps 25.1.3\nSparsity-Inducing Norms\nThe problem of minimizing the empirical risk subject to a budget of k features\ncan be written as\nmin\nw LS(w)\ns.t ∥w∥0 ≤k,\nwhere1\n∥w∥0 = |{i : wi ̸= 0}| In other words, we want w to be sparse, which implies that we only need to\nmeasure the features corresponding to nonzero elements of w Solving this optimization problem is computationally hard (Natarajan 1995,\nDavis, Mallat & Avellaneda 1997)",
      "word_count": 228,
      "source_page": 363,
      "start_position": 131793,
      "end_position": 132020,
      "sentences_count": 12
    },
    {
      "chunk_id": 587,
      "text": "In other words, we want w to be sparse, which implies that we only need to\nmeasure the features corresponding to nonzero elements of w Solving this optimization problem is computationally hard (Natarajan 1995,\nDavis, Mallat & Avellaneda 1997) A possible relaxation is to replace the non-\nconvex function ∥w∥0 with the ℓ1 norm, ∥w∥1 = Pd\ni=1 |wi|, and to solve the\nproblem\nmin\nw LS(w)\ns.t ∥w∥1 ≤k1,\n(25.4)\nwhere k1 is a parameter Since the ℓ1 norm is a convex function, this problem\ncan be solved eﬃciently as long as the loss function is convex A related problem\nis minimizing the sum of LS(w) plus an ℓ1 norm regularization term,\nmin\nw (LS(w) + λ∥w∥1) ,\n(25.5)\nwhere λ is a regularization parameter Since for any k1 there exists a λ such that\n1 The function ∥· ∥0 is often referred to as the ℓ0 norm Despite the use of the “norm”\nnotation, ∥· ∥0 is not really a norm; for example, it does not satisfy the positive\nhomogeneity property of norms, ∥aw∥0 ̸= |a| ∥w∥0.",
      "word_count": 179,
      "source_page": 363,
      "start_position": 131982,
      "end_position": 132160,
      "sentences_count": 8
    },
    {
      "chunk_id": 588,
      "text": "364\nFeature Selection and Generation\nEquation (25.4) and Equation (25.5) lead to the same solution, the two problems\nare in some sense equivalent The ℓ1 regularization often induces sparse solutions To illustrate this, let us\nstart with the simple optimization problem\nmin\nw∈R\n\u00121\n2w2 −xw + λ|w|\n\u0013 (25.6)\nIt is easy to verify (see Exercise 2) that the solution to this problem is the “soft\nthresholding” operator\nw = sign(x) [|x| −λ]+ ,\n(25.7)\nwhere [a]+\ndef\n= max{a, 0} That is, as long as the absolute value of x is smaller\nthan λ, the optimal solution will be zero Next, consider a one dimensional regression problem with respect to the squared\nloss:\nargmin\nw∈Rm\n \n1\n2m\nm\nX\ni=1\n(xiw −yi)2 + λ|w| We can rewrite the problem as\nargmin\nw∈Rm\n \n1\n2\n \n1\nm\nX\ni\nx2\ni w2 −\n \n1\nm\nm\nX\ni=1\nxiyi w + λ|w| For simplicity let us assume that\n1\nm\nP\ni x2\ni = 1, and denote ⟨x, y⟩= Pm\ni=1 xiyi;\nthen the optimal solution is\nw = sign(⟨x, y⟩) [|⟨x, y⟩|/m −λ]+ That is, the solution will be zero unless the correlation between the feature x\nand the labels vector y is larger than λ Remark 25.4\nUnlike the ℓ1 norm, the ℓ2 norm does not induce sparse solutions Indeed, consider the problem above with an ℓ2 regularization, namely,\nargmin\nw∈Rm\n \n1\n2m\nm\nX\ni=1\n(xiw −yi)2 + λw2",
      "word_count": 244,
      "source_page": 364,
      "start_position": 132161,
      "end_position": 132404,
      "sentences_count": 13
    },
    {
      "chunk_id": 589,
      "text": "25.2 Feature Manipulation and Normalization\n365\nAdding ℓ1 regularization to a linear regression problem with the squared loss\nyields the LASSO algorithm, deﬁned as\nargmin\nw\n\u0012 1\n2m∥Xw −y∥2 + λ∥w∥1\n\u0013 (25.8)\nUnder some assumptions on the distribution and the regularization parameter\nλ, the LASSO will ﬁnd sparse solutions (see, for example, (Zhao & Yu 2006)\nand the references therein) Another advantage of the ℓ1 norm is that a vector\nwith low ℓ1 norm can be “sparsiﬁed” (see, for example, (Shalev-Shwartz, Zhang\n& Srebro 2010) and the references therein) 25.2\nFeature Manipulation and Normalization\nFeature manipulations or normalization include simple transformations that we\napply on each of our original features Such transformations may decrease the\napproximation or estimation errors of our hypothesis class or can yield a faster\nalgorithm Similarly to the problem of feature selection, here again there are no\nabsolute “good” and “bad” transformations, but rather each transformation that\nwe apply should be related to the learning algorithm we are going to apply on\nthe resulting feature vector as well as to our prior assumptions on the problem To motivate normalization, consider a linear regression problem with the\nsquared loss Let X ∈Rm,d be a matrix whose rows are the instance vectors\nand let y ∈Rm be a vector of target values Recall that ridge regression returns\nthe vector\nargmin\nw\n\u0014 1\nm∥Xw −y∥2 + λ∥w∥2\n\u0015\n= (2λmI + X⊤X)−1X⊤y Suppose that d = 2 and the underlying data distribution is as follows",
      "word_count": 249,
      "source_page": 365,
      "start_position": 132463,
      "end_position": 132711,
      "sentences_count": 10
    },
    {
      "chunk_id": 590,
      "text": "Recall that ridge regression returns\nthe vector\nargmin\nw\n\u0014 1\nm∥Xw −y∥2 + λ∥w∥2\n\u0015\n= (2λmI + X⊤X)−1X⊤y Suppose that d = 2 and the underlying data distribution is as follows First we\nsample y uniformly at random from {±1} Then, we set x1 to be y +0.5α, where\nα is sampled uniformly at random from {±1}, and we set x2 to be 0.0001y Note\nthat the optimal weight vector is w⋆= [0; 10000], and LD(w⋆) = 0 However,\nthe objective of ridge regression at w⋆is λ108 In contrast, the objective of ridge\nregression at w = [1; 0] is likely to be close to 0.25 + λ It follows that whenever\nλ >\n0.25\n108−1 ≈0.25 × 10−8, the objective of ridge regression is smaller at the\nsuboptimal solution w = [1; 0] Since λ typically should be at least 1/m (see\nthe analysis in Chapter 13), it follows that in the aforementioned example, if the\nnumber of examples is smaller than 108 then we are likely to output a suboptimal\nsolution The crux of the preceding example is that the two features have completely\ndiﬀerent scales Feature normalization can overcome this problem There are\nmany ways to perform feature normalization, and one of the simplest approaches\nis simply to make sure that each feature receives values between −1 and 1 In\nthe preceding example, if we divide each feature by the maximal value it attains",
      "word_count": 239,
      "source_page": 365,
      "start_position": 132679,
      "end_position": 132917,
      "sentences_count": 13
    },
    {
      "chunk_id": 591,
      "text": "366\nFeature Selection and Generation\nwe will obtain that x1 = y+0.5α\n1.5\nand x2 = y Then, for λ ≤10−3 the solution of\nridge regression is quite close to w⋆ Moreover, the generalization bounds we have derived in Chapter 13 for reg-\nularized loss minimization depend on the norm of the optimal vector w⋆and\non the maximal norm of the instance vectors.2 Therefore, in the aforementioned\nexample, before we normalize the features we have that ∥w⋆∥2 = 108, while af-\nter we normalize the features we have that ∥w⋆∥2 = 1 The maximal norm of\nthe instance vector remains roughly the same; hence the normalization greatly\nimproves the estimation error Feature normalization can also improve the runtime of the learning algorithm For example, in Section 14.5.3 we have shown how to use the Stochastic Gradient\nDescent (SGD) optimization algorithm for solving the regularized loss minimiza-\ntion problem The number of iterations required by SGD to converge also depends\non the norm of w⋆and on the maximal norm of ∥x∥ Therefore, as before, using\nnormalization can greatly decrease the runtime of SGD Next, we demonstrate in the following how a simple transformation on features,\nsuch as clipping, can sometime decrease the approximation error of our hypoth-\nesis class Consider again linear regression with the squared loss",
      "word_count": 216,
      "source_page": 366,
      "start_position": 132918,
      "end_position": 133133,
      "sentences_count": 10
    },
    {
      "chunk_id": 592,
      "text": "Next, we demonstrate in the following how a simple transformation on features,\nsuch as clipping, can sometime decrease the approximation error of our hypoth-\nesis class Consider again linear regression with the squared loss Let a > 1 be\na large number, suppose that the target y is chosen uniformly at random from\n{±1}, and then the single feature x is set to be y with probability (1 −1/a)\nand set to be ay with probability 1/a That is, most of the time our feature is\nbounded but with a very small probability it gets a very high value Then, for\nany w, the expected squared loss of w is\nLD(w) = E 1\n2(wx −y)2\n=\n\u0012\n1 −1\na\n\u0013 1\n2(wy −y)2 + 1\na\n1\n2(awy −y)2 Solving for w we obtain that w⋆=\n2a−1\na2+a−1, which goes to zero as a goes to inﬁn-\nity Therefore, the objective at w⋆goes to 0.5 as a goes to inﬁnity For example,\nfor a = 100 we will obtain LD(w⋆) ≥0.48 Next, suppose we apply a “clipping”\ntransformation; that is, we use the transformation x 7→sign(x) min{1, |x|} Then,\nfollowing this transformation, w⋆becomes 1 and LD(w⋆) = 0 This simple ex-\nample shows that a simple transformation can have a signiﬁcant inﬂuence on the\napproximation error Of course, it is not hard to think of examples in which the same feature trans-\nformation actually hurts performance and increases the approximation error",
      "word_count": 243,
      "source_page": 366,
      "start_position": 133100,
      "end_position": 133342,
      "sentences_count": 12
    },
    {
      "chunk_id": 593,
      "text": "This simple ex-\nample shows that a simple transformation can have a signiﬁcant inﬂuence on the\napproximation error Of course, it is not hard to think of examples in which the same feature trans-\nformation actually hurts performance and increases the approximation error This is not surprising, as we have already argued that feature transformations\n2\nMore precisely, the bounds we derived in Chapter 13 for regularized loss minimization\ndepend on ∥w⋆∥2 and on either the Lipschitzness or the smoothness of the loss function For linear predictors and loss functions of the form ℓ(w, (x, y)) = φ(⟨w, x⟩, y), where φ is\nconvex and either 1-Lipschitz or 1-smooth with respect to its ﬁrst argument, we have that\nℓis either ∥x∥-Lipschitz or ∥x∥2-smooth For example, for the squared loss,\nφ(a, y) = 1\n2 (a −y)2, and ℓ(w, (x, y)) = 1\n2 (⟨w, x⟩−y)2 is ∥x∥2-smooth with respect to its\nﬁrst argument.",
      "word_count": 153,
      "source_page": 366,
      "start_position": 133300,
      "end_position": 133452,
      "sentences_count": 5
    },
    {
      "chunk_id": 594,
      "text": "25.2 Feature Manipulation and Normalization\n367\nshould rely on our prior assumptions on the problem In the aforementioned ex-\nample, a prior assumption that may lead us to use the “clipping” transformation\nis that features that get values larger than a predeﬁned threshold value give us no\nadditional useful information, and therefore we can clip them to the predeﬁned\nthreshold 25.2.1\nExamples of Feature Transformations\nWe now list several common techniques for feature transformations Usually, it\nis helpful to combine some of these transformations (e.g., centering + scaling) In the following, we denote by f = (f1, , fm) ∈Rm the value of the feature f\nover the m training examples Also, we denote by ¯f =\n1\nm\nPm\ni=1 fi the empirical\nmean of the feature over all examples Centering:\nThis transformation makes the feature have zero mean, by setting fi ←fi −¯f Unit Range:\nThis transformation makes the range of each feature be [0, 1] Formally, let\nfmax = maxi fi and fmin = mini fi Then, we set fi ←\nfi−fmin\nfmax−fmin Similarly,\nwe can make the range of each feature be [−1, 1] by the transformation fi ←\n2\nfi−fmin\nfmax−fmin −1 Of course, it is easy to make the range [0, b] or [−b, b], where b is\na user-speciﬁed parameter Standardization:\nThis transformation makes all features have a zero mean and unit variance Formally, let ν =\n1\nm\nPm\ni=1(fi −¯f)2 be the empirical variance of the feature",
      "word_count": 246,
      "source_page": 367,
      "start_position": 133453,
      "end_position": 133698,
      "sentences_count": 15
    },
    {
      "chunk_id": 595,
      "text": "368\nFeature Selection and Generation\nLogarithmic Transformation:\nThe transformation is fi ←log(b+fi), where b is a user-speciﬁed parameter This\nis widely used when the feature is a “counting” feature For example, suppose\nthat the feature represents the number of appearances of a certain word in a\ntext document Then, the diﬀerence between zero occurrences of the word and\na single occurrence is much more important than the diﬀerence between 1000\noccurrences and 1001 occurrences Remark 25.5\nIn the aforementioned transformations, each feature is trans-\nformed on the basis of the values it obtains on the training set, independently\nof other features’ values In some situations we would like to set the parameter\nof the transformation on the basis of other features as well A notable example\nis a transformation in which one applies a scaling to the features so that the\nempirical average of some norm of the instances becomes 1 25.3\nFeature Learning\nSo far we have discussed feature selection and manipulations In these cases, we\nstart with a predeﬁned vector space Rd, representing our features Then, we select\na subset of features (feature selection) or transform individual features (feature\ntransformation) In this section we describe feature learning, in which we start\nwith some instance space, X, and would like to learn a function, ψ : X →Rd,\nwhich maps instances in X into a representation as d-dimensional feature vectors",
      "word_count": 231,
      "source_page": 368,
      "start_position": 133791,
      "end_position": 134021,
      "sentences_count": 11
    },
    {
      "chunk_id": 596,
      "text": "Then, we select\na subset of features (feature selection) or transform individual features (feature\ntransformation) In this section we describe feature learning, in which we start\nwith some instance space, X, and would like to learn a function, ψ : X →Rd,\nwhich maps instances in X into a representation as d-dimensional feature vectors The idea of feature learning is to automate the process of ﬁnding a good rep-\nresentation of the input space As mentioned before, the No-Free-Lunch theorem\ntells us that we must incorporate some prior knowledge on the data distribution\nin order to build a good feature representation In this section we present a few\nfeature learning approaches and demonstrate conditions on the underlying data\ndistribution in which these methods can be useful Throughout the book we have already seen several useful feature construc-\ntions For example, in the context of polynomial regression, we have mapped the\noriginal instances into the vector space of all their monomials (see Section 9.2.2\nin Chapter 9) After performing this mapping, we trained a linear predictor on\ntop of the constructed features Automation of this process would be to learn\na transformation ψ : X →Rd, such that the composition of the class of linear\npredictors on top of ψ yields a good hypothesis class for the task at hand In the following we describe a technique of feature construction called dictio-\nnary learning",
      "word_count": 233,
      "source_page": 368,
      "start_position": 133968,
      "end_position": 134200,
      "sentences_count": 10
    },
    {
      "chunk_id": 597,
      "text": "25.3 Feature Learning\n369\nand given a document, (p1, , pd), where each pi is a word in the document,\nwe represent the document as a vector x ∈{0, 1}k, where xi is 1 if wi = pj for\nsome j ∈[d], and xi = 0 otherwise It was empirically observed in many text\nprocessing tasks that linear predictors are quite powerful when applied on this\nrepresentation Intuitively, we can think of each word as a feature that measures\nsome aspect of the document Given labeled examples (e.g., topics of the doc-\numents), a learning algorithm searches for a linear predictor that weights these\nfeatures so that a right combination of appearances of words is indicative of the\nlabel While in text processing there is a natural meaning to words and to the dic-\ntionary, in other applications we do not have such an intuitive representation\nof an instance For example, consider the computer vision application of object\nrecognition Here, the instance is an image and the goal is to recognize which\nobject appears in the image Applying a linear predictor on the pixel-based rep-\nresentation of the image does not yield a good classiﬁer What we would like\nto have is a mapping ψ that would take the pixel-based representation of the\nimage and would output a bag of “visual words,” representing the content of the\nimage",
      "word_count": 228,
      "source_page": 369,
      "start_position": 134245,
      "end_position": 134472,
      "sentences_count": 10
    },
    {
      "chunk_id": 598,
      "text": "Applying a linear predictor on the pixel-based rep-\nresentation of the image does not yield a good classiﬁer What we would like\nto have is a mapping ψ that would take the pixel-based representation of the\nimage and would output a bag of “visual words,” representing the content of the\nimage For example, a “visual word” can be “there is an eye in the image.” If\nwe had such representation, we could have applied a linear predictor on top of\nthis representation to train a classiﬁer for, say, face recognition Our question is,\ntherefore, how can we learn a dictionary of “visual words” such that a bag-of-\nwords representation of an image would be helpful for predicting which object\nappears in the image A ﬁrst naive approach for dictionary learning relies on a clustering algorithm\n(see Chapter 22) Suppose that we learn a function c : X →{1, , k}, where\nc(x) is the cluster to which x belongs Then, we can think of the clusters as\n“words,” and of instances as “documents,” where a document x is mapped to\nthe vector ψ(x) ∈{0, 1}k, where ψ(x)i is 1 if and only if x belongs to the ith\ncluster Now, it is straightforward to see that applying a linear predictor on ψ(x)\nis equivalent to assigning the same target value to all instances that belong to\nthe same cluster",
      "word_count": 229,
      "source_page": 369,
      "start_position": 134422,
      "end_position": 134650,
      "sentences_count": 9
    },
    {
      "chunk_id": 599,
      "text": "Then, we can think of the clusters as\n“words,” and of instances as “documents,” where a document x is mapped to\nthe vector ψ(x) ∈{0, 1}k, where ψ(x)i is 1 if and only if x belongs to the ith\ncluster Now, it is straightforward to see that applying a linear predictor on ψ(x)\nis equivalent to assigning the same target value to all instances that belong to\nthe same cluster Furthermore, if the clustering is based on distances from a\nclass center (e.g., k-means), then a linear predictor on ψ(x) yields a piece-wise\nconstant predictor on x Both the k-means and PCA approaches can be regarded as special cases of a\nmore general approach for dictionary learning which is called auto-encoders In an\nauto-encoder we learn a pair of functions: an “encoder” function, ψ : Rd →Rk,\nand a “decoder” function, φ : Rk →Rd The goal of the learning process is to\nﬁnd a pair of functions such that the reconstruction error, P\ni ∥xi −φ(ψ(xi))∥2,\nis small Of course, we can trivially set k = d and both ψ, φ to be the identity\nmapping, which yields a perfect reconstruction We therefore must restrict ψ and\nφ in some way In PCA, we constrain k < d and further restrict ψ and φ to be\nlinear functions In k-means, k is not restricted to be smaller than d, but now\nψ and φ rely on k centroids, µ1, , µk, and ψ(x) returns an indicator vector",
      "word_count": 248,
      "source_page": 369,
      "start_position": 134581,
      "end_position": 134828,
      "sentences_count": 11
    },
    {
      "chunk_id": 600,
      "text": "370\nFeature Selection and Generation\nin {0, 1}k that indicates the closest centroid to x, while φ takes as input an\nindicator vector and returns the centroid representing this vector An important property of the k-means construction, which is key in allowing\nk to be larger than d, is that ψ maps instances into sparse vectors In fact, in\nk-means only a single coordinate of ψ(x) is nonzero An immediate extension of\nthe k-means construction is therefore to restrict the range of ψ to be vectors with\nat most s nonzero elements, where s is a small integer In particular, let ψ and φ\nbe functions that depend on µ1, , µk The function ψ maps an instance vector\nx to a vector ψ(x) ∈Rk, where ψ(x) should have at most s nonzero elements The function φ(v) is deﬁned to be Pk\ni=1 viµi As before, our goal is to have a\nsmall reconstruction error, and therefore we can deﬁne\nψ(x) = argmin\nv\n∥x −φ(v)∥2 s.t ∥v∥0 ≤s,\nwhere ∥v∥0 = |{j : vj ̸= 0}| Note that when s = 1 and we further restrict ∥v∥1 =\n1 then we obtain the k-means encoding function; that is, ψ(x) is the indicator\nvector of the centroid closest to x For larger values of s, the optimization problem\nin the preceding deﬁnition of ψ becomes computationally diﬃcult",
      "word_count": 227,
      "source_page": 370,
      "start_position": 134829,
      "end_position": 135055,
      "sentences_count": 12
    },
    {
      "chunk_id": 601,
      "text": "Note that when s = 1 and we further restrict ∥v∥1 =\n1 then we obtain the k-means encoding function; that is, ψ(x) is the indicator\nvector of the centroid closest to x For larger values of s, the optimization problem\nin the preceding deﬁnition of ψ becomes computationally diﬃcult Therefore, in\npractice, we sometime use ℓ1 regularization instead of the sparsity constraint and\ndeﬁne ψ to be\nψ(x) = argmin\nv\n\u0002\n∥x −φ(v)∥2 + λ∥v∥1\n\u0003\n,\nwhere λ > 0 is a regularization parameter Anyway, the dictionary learning\nproblem is now to ﬁnd the vectors µ1, , µk such that the reconstruction er-\nror, Pm\ni=1 ∥xi −φ(ψ(x))∥2, is as small as possible Even if ψ is deﬁned using\nthe ℓ1 regularization, this is still a computationally hard problem (similar to\nthe k-means problem) However, several heuristic search algorithms may give\nreasonably good solutions These algorithms are beyond the scope of this book 25.4\nSummary\nMany machine learning algorithms take the feature representation of instances\nfor granted Yet the choice of representation requires careful attention We dis-\ncussed approaches for feature selection, introducing ﬁlters, greedy selection al-\ngorithms, and sparsity-inducing norms Next we presented several examples for\nfeature transformations and demonstrated their usefulness Last, we discussed\nfeature learning, and in particular dictionary learning We have shown that fea-\nture selection, manipulation, and learning all depend on some prior knowledge\non the data.",
      "word_count": 235,
      "source_page": 370,
      "start_position": 135006,
      "end_position": 135240,
      "sentences_count": 14
    },
    {
      "chunk_id": 602,
      "text": "25.5 Bibliographic Remarks\n371\n25.5\nBibliographic Remarks\nGuyon & Elisseeﬀ(2003) surveyed several feature selection procedures, including\nmany types of ﬁlters Forward greedy selection procedures for minimizing a convex objective sub-\nject to a polyhedron constraint date back to the Frank-Wolfe algorithm (Frank\n& Wolfe 1956) The relation to boosting has been studied by several authors,\nincluding, (Warmuth, Liao & Ratsch 2006, Warmuth, Glocer & Vishwanathan\n2008, Shalev-Shwartz & Singer 2008) Matching pursuit has been studied in the\nsignal processing community (Mallat & Zhang 1993) Several papers analyzed\ngreedy selection methods under various conditions See, for example, Shalev-\nShwartz, Zhang & Srebro (2010) and the references therein The use of the ℓ1-norm as a surrogate for sparsity has a long history (e.g Tib-\nshirani (1996) and the references therein), and much work has been done on un-\nderstanding the relationship between the ℓ1-norm and sparsity It is also closely\nrelated to compressed sensing (see Chapter 23) The ability to sparsify low ℓ1\nnorm predictors dates back to Maurey (Pisier 1980-1981) In Section 26.4 we\nalso show that low ℓ1 norm can be used to bound the estimation error of our\npredictor Feature learning and dictionary learning have been extensively studied recently\nin the context of deep neural networks See, for example, (Lecun & Bengio 1995,\nHinton et al 2006, Ranzato et al 2007, Collobert & Weston 2008, Lee et al 2009, Le et al 2012, Bengio 2009) and the references therein 25.6\nExercises\n1 Prove the equality given in Equation (25.1)",
      "word_count": 250,
      "source_page": 371,
      "start_position": 135241,
      "end_position": 135490,
      "sentences_count": 19
    },
    {
      "chunk_id": 603,
      "text": "25.6\nExercises\n1 Prove the equality given in Equation (25.1) Hint: Let a∗, b∗be minimizers of\nthe left-hand side Find a, b such that the objective value of the right-hand\nside is smaller than that of the left-hand side Do the same for the other\ndirection 2 Show that Equation (25.7) is the solution of Equation (25.6) 3 AdaBoost as a Forward Greedy Selection Algorithm: Recall the Ad-\naBoost algorithm from Chapter 10 In this section we give another interpre-\ntation of AdaBoost as a forward greedy selection algorithm • Given a set of m instances x1, , xm, and a hypothesis class H of ﬁnite\nVC dimension, show that there exist d and h1, , hd such that for every\nh ∈H there exists i ∈[d] with hi(xj) = h(xj) for every j ∈[m] • Let R(w) be as deﬁned in Equation (25.3) Given some w, deﬁne fw to be\nthe function\nfw(·) =\nd\nX\ni=1\nwihi(·).",
      "word_count": 159,
      "source_page": 371,
      "start_position": 135481,
      "end_position": 135639,
      "sentences_count": 15
    },
    {
      "chunk_id": 604,
      "text": "26\nRademacher Complexities\nIn Chapter 4 we have shown that uniform convergence is a suﬃcient condition\nfor learnability In this chapter we study the Rademacher complexity, which\nmeasures the rate of uniform convergence We will provide generalization bounds\nbased on this measure 26.1\nThe Rademacher Complexity\nRecall the deﬁnition of an ϵ-representative sample from Chapter 4, repeated here\nfor convenience definition 26.1 (ϵ-Representative Sample)\nA training set S is called ϵ-representative\n(w.r.t domain Z, hypothesis class H, loss function ℓ, and distribution D) if\nsup\nh∈H\n|LD(h) −LS(h)| ≤ϵ We have shown that if S is an ϵ/2 representative sample then the ERM rule\nis ϵ-consistent, namely, LD(ERMH(S)) ≤minh∈H LD(h) + ϵ To simplify our notation, let us denote\nF\ndef\n= ℓ◦H\ndef\n= {z 7→ℓ(h, z) : h ∈H},\nand given f ∈F, we deﬁne\nLD(f) = E\nz∼D[f(z)]\n,\nLS(f) = 1\nm\nm\nX\ni=1\nf(zi) We deﬁne the representativeness of S with respect to F as the largest gap be-\ntween the true error of a function f and its empirical error, namely,\nRepD(F, S)\ndef\n=\nsup\nf∈F\n\u0000LD(f) −LS(f)\n\u0001 (26.1)\nNow, suppose we would like to estimate the representativeness of S using the\nsample S only One simple idea is to split S into two disjoint sets, S = S1 ∪S2;\nrefer to S1 as a validation set and to S2 as a training set We can then estimate\nthe representativeness of S by\nsup\nf∈F\n\u0000LS1(f) −LS2(f)\n\u0001",
      "word_count": 247,
      "source_page": 375,
      "start_position": 135729,
      "end_position": 135975,
      "sentences_count": 12
    },
    {
      "chunk_id": 605,
      "text": "376\nRademacher Complexities\nThis can be written more compactly by deﬁning σ = (σ1, , σm) ∈{±1}m to\nbe a vector such that S1 = {zi : σi = 1} and S2 = {zi : σi = −1} Then, if we\nfurther assume that |S1| = |S2| then Equation (26.2) can be rewritten as\n2\nm sup\nf∈F\nm\nX\ni=1\nσif(zi) (26.3)\nThe Rademacher complexity measure captures this idea by considering the ex-\npectation of the above with respect to a random choice of σ Formally, let F ◦S\nbe the set of all possible evaluations a function f ∈F can achieve on a sample\nS, namely,\nF ◦S = {(f(z1), , f(zm)) : f ∈F} Let the variables in σ be distributed i.i.d according to P[σi = 1] = P[σi = −1] =\n1\n2 Then, the Rademacher complexity of F with respect to S is deﬁned as follows:\nR(F ◦S)\ndef\n=\n1\nm\nE\nσ∼{±1}m\n\"\nsup\nf∈F\nm\nX\ni=1\nσif(zi)\n# (26.4)\nMore generally, given a set of vectors, A ⊂Rm, we deﬁne\nR(A)\ndef\n=\n1\nm E\nσ\n\"\nsup\na∈A\nm\nX\ni=1\nσiai\n# (26.5)\nThe following lemma bounds the expected value of the representativeness of\nS by twice the expected Rademacher complexity lemma 26.2\nE\nS∼Dm [ RepD(F, S)]\n≤2\nE\nS∼Dm R(F ◦S) Proof\nLet S′ = {z′\n1, , z′\nm} be another i.i.d sample Clearly, for all f ∈F,\nLD(f) = ES′[LS′(f)]",
      "word_count": 246,
      "source_page": 376,
      "start_position": 136006,
      "end_position": 136251,
      "sentences_count": 16
    },
    {
      "chunk_id": 606,
      "text": "26.1 The Rademacher Complexity\n377\nNext, we note that for each j, zj and z′\nj are i.i.d variables Therefore, we can\nreplace them without aﬀecting the expectation:\nE\nS,S′\n\nsup\nf∈F\n\n(f(z′\nj) −f(zj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n=\nE\nS,S′\n\nsup\nf∈F\n\n(f(zj) −f(z′\nj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n (26.7)\nLet σj be a random variable such that P[σj = 1] = P[σj = −1] = 1/2 From\nEquation (26.7) we obtain that\nE\nS,S′,σj\n\nsup\nf∈F\n\nσj(f(z′\nj) −f(zj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n\n= 1\n2(l.h.s of Equation (26.7)) + 1\n2(r.h.s of Equation (26.7))\n=\nE\nS,S′\n\nsup\nf∈F\n\n(f(z′\nj) −f(zj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n (26.8)\nRepeating this for all j we obtain that\nE\nS,S′\n\"\nsup\nf∈F\nm\nX\ni=1\n(f(z′\ni) −f(zi))\n#\n=\nE\nS,S′,σ\n\"\nsup\nf∈F\nm\nX\ni=1\nσi(f(z′\ni) −f(zi))\n#",
      "word_count": 175,
      "source_page": 377,
      "start_position": 136360,
      "end_position": 136534,
      "sentences_count": 8
    },
    {
      "chunk_id": 607,
      "text": "of Equation (26.7))\n=\nE\nS,S′\n\nsup\nf∈F\n\n(f(z′\nj) −f(zj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n (26.8)\nRepeating this for all j we obtain that\nE\nS,S′\n\"\nsup\nf∈F\nm\nX\ni=1\n(f(z′\ni) −f(zi))\n#\n=\nE\nS,S′,σ\n\"\nsup\nf∈F\nm\nX\ni=1\nσi(f(z′\ni) −f(zi))\n# (26.9)\nFinally,\nsup\nf∈F\nX\ni\nσi(f(z′\ni) −f(zi)) ≤sup\nf∈F\nX\ni\nσif(z′\ni) + sup\nf∈F\nX\ni\n−σif(zi)\nand since the probability of σ is the same as the probability of −σ, the right-hand\nside of Equation (26.9) can be bounded by\nE\nS,S′,σ\n\"\nsup\nf∈F\nX\ni\nσif(z′\ni) + sup\nf∈F\nX\ni\nσif(zi)\n#\n= m E\nS′[R(F ◦S′)] + m E\nS[R(F ◦S)] = 2m E\nS[R(F ◦S)] The lemma immediately yields that, in expectation, the ERM rule ﬁnds a\nhypothesis which is close to the optimal hypothesis in H theorem 26.3\nWe have\nE\nS∼Dm [LD(ERMH(S)) −LS(ERMH(S))] ≤2\nE\nS∼Dm R(ℓ◦H ◦S) Furthermore, for any h⋆∈H\nE\nS∼Dm [LD(ERMH(S)) −LD(h⋆)]\n≤2\nE\nS∼Dm R(ℓ◦H ◦S).",
      "word_count": 181,
      "source_page": 377,
      "start_position": 136478,
      "end_position": 136658,
      "sentences_count": 6
    },
    {
      "chunk_id": 608,
      "text": "378\nRademacher Complexities\nFurthermore, if h⋆= argminh LD(h) then for each δ ∈(0, 1) with probability of\nat least 1 −δ over the choice of S we have\nLD(ERMH(S)) −LD(h⋆) ≤2 ES′∼Dm R(ℓ◦H ◦S′)\nδ Proof\nThe ﬁrst inequality follows directly from Lemma 26.2 The second in-\nequality follows because for any ﬁxed h⋆,\nLD(h⋆) = E\nS[LS(h⋆)] ≥E\nS[LS(ERMH(S))] The third inequality follows from the previous inequality by relying on Markov’s\ninequality (note that the random variable LD(ERMH(S)) −LD(h⋆) is nonnega-\ntive) Next, we derive bounds similar to the bounds in Theorem 26.3 with a better\ndependence on the conﬁdence parameter δ To do so, we ﬁrst introduce the\nfollowing bounded diﬀerences concentration inequality lemma 26.4 (McDiarmid’s Inequality)\nLet V be some set and let f : V m →R\nbe a function of m variables such that for some c > 0, for all i ∈[m] and for all\nx1, , xm, x′\ni ∈V we have\n|f(x1, , xm) −f(x1, , xi−1, x′\ni, xi+1, , xm)| ≤c Let X1, , Xm be m independent random variables taking values in V Then,\nwith probability of at least 1 −δ we have\n|f(X1, , Xm) −E[f(X1, , Xm)]| ≤c\nq\nln\n\u0000 2\nδ\n\u0001\nm/2 On the basis of the McDiarmid inequality we can derive generalization bounds\nwith a better dependence on the conﬁdence parameter theorem 26.5\nAssume that for all z and h ∈H we have that |ℓ(h, z)| ≤c Then,\n1",
      "word_count": 246,
      "source_page": 378,
      "start_position": 136659,
      "end_position": 136904,
      "sentences_count": 19
    },
    {
      "chunk_id": 609,
      "text": "26.1 The Rademacher Complexity\n379\nProof\nFirst note that the random variable RepD(F, S) = suph∈H (LD(h) −LS(h))\nsatisﬁes the bounded diﬀerences condition of Lemma 26.4 with a constant 2c/m Combining the bounds in Lemma 26.4 with Lemma 26.2 we obtain that with\nprobability of at least 1 −δ,\nRepD(F, S) ≤E RepD(F, S) + c\nr\n2 ln(2/δ)\nm\n≤2 E\nS′ R(ℓ◦H ◦S′) + c\nr\n2 ln(2/δ)\nm The ﬁrst inequality of the theorem follows from the deﬁnition of RepD(F, S) For the second inequality we note that the random variable R(ℓ◦H ◦S) also\nsatisﬁes the bounded diﬀerences condition of Lemma 26.4 with a constant 2c/m Therefore, the second inequality follows from the ﬁrst inequality, Lemma 26.4,\nand the union bound Finally, for the last inequality, denote hS = ERMH(S)\nand note that\nLD(hS) −LD(h⋆)\n= LD(hS) −LS(hS) + LS(hS) −LS(h⋆) + LS(h⋆) −LD(h⋆)\n≤(LD(hS) −LS(hS)) + (LS(h⋆) −LD(h⋆)) (26.10)\nThe ﬁrst summand on the right-hand side is bounded by the second inequality of\nthe theorem For the second summand, we use the fact that h⋆does not depend\non S; hence by using Hoeﬀding’s inequality we obtain that with probaility of at\nleast 1 −δ/2,\nLS(h⋆) −LD(h⋆) ≤c\nr\nln(4/δ)\n2m (26.11)\nCombining this with the union bound we conclude our proof The preceding theorem tells us that if the quantity R(ℓ◦H◦S) is small then it\nis possible to learn the class H using the ERM rule",
      "word_count": 240,
      "source_page": 379,
      "start_position": 136994,
      "end_position": 137233,
      "sentences_count": 10
    },
    {
      "chunk_id": 610,
      "text": "(26.11)\nCombining this with the union bound we conclude our proof The preceding theorem tells us that if the quantity R(ℓ◦H◦S) is small then it\nis possible to learn the class H using the ERM rule It is important to emphasize\nthat the last two bounds given in the theorem depend on the speciﬁc training\nset S That is, we use S both for learning a hypothesis from H as well as for\nestimating the quality of it This type of bound is called a data-dependent bound 26.1.1\nRademacher Calculus\nLet us now discuss some properties of the Rademacher complexity measure These properties will help us in deriving some simple bounds on R(ℓ◦H ◦S) for\nspeciﬁc cases of interest The following lemma is immediate from the deﬁnition lemma 26.6\nFor any A ⊂Rm, scalar c ∈R, and vector a0 ∈Rm, we have\nR({c a + a0 : a ∈A}) ≤|c| R(A) The following lemma tells us that the convex hull of A has the same complexity\nas A.",
      "word_count": 168,
      "source_page": 379,
      "start_position": 137198,
      "end_position": 137365,
      "sentences_count": 10
    },
    {
      "chunk_id": 611,
      "text": "380\nRademacher Complexities\nlemma 26.7\nLet A be a subset of Rm and let A′ = {PN\nj=1 αja(j) : N ∈\nN, ∀j, a(j) ∈A, αj ≥0, ∥α∥1 = 1} Then, R(A′) = R(A) Proof\nThe main idea follows from the fact that for any vector v we have\nsup\nα≥0:∥α∥1=1\nN\nX\nj=1\nαjvj = max\nj\nvj Therefore,\nm R(A′) = E\nσ\nsup\nα≥0:∥α∥1=1\nsup\na(1),...,a(N)\nm\nX\ni=1\nσi\nN\nX\nj=1\nαja(j)\ni\n= E\nσ\nsup\nα≥0:∥α∥1=1\nN\nX\nj=1\nαj sup\na(j)\nm\nX\ni=1\nσia(j)\ni\n= E\nσ sup\na∈A\nm\nX\ni=1\nσiai\n= m R(A),\nand we conclude our proof The next lemma, due to Massart, states that the Rademacher complexity of\na ﬁnite set grows logarithmically with the size of the set lemma 26.8 (Massart lemma)\nLet A = {a1, , aN} be a ﬁnite set of vectors\nin Rm Deﬁne ¯a = 1\nN\nPN\ni=1 ai Then,\nR(A) ≤\nmax\na∈A ∥a −¯a∥\np\n2 log(N)\nm Proof\nBased on Lemma 26.6, we can assume without loss of generality that\n¯a = 0 Let λ > 0 and let A′ = {λa1, , λaN} We upper bound the Rademacher\ncomplexity as follows:\nmR(A′) = E\nσ\n\u0014\nmax\na∈A′⟨σ, a⟩\n\u0015\n= E\nσ\n\u0014\nlog\n\u0012\nmax\na∈A′ e⟨σ,a⟩\n\u0013\u0015\n≤E\nσ\n\"\nlog\n X\na∈A′\ne⟨σ,a⟩\n!#\n≤log\n \nE\nσ\n\" X\na∈A′\ne⟨σ,a⟩\n#",
      "word_count": 242,
      "source_page": 380,
      "start_position": 137366,
      "end_position": 137607,
      "sentences_count": 13
    },
    {
      "chunk_id": 612,
      "text": "26.1 The Rademacher Complexity\n381\nand therefore\nmR(A′) ≤log\n X\na∈A′\nm\nY\ni=1\nexp\n\u0012a2\ni\n2\n\u0013 = log\n X\na∈A′\nexp\n\u0000∥a∥2/2\n\u0001 ≤log\n\u0012\n|A′| max\na∈A′ exp\n\u0000∥a∥2/2\n\u0001\u0013\n= log(|A′|) + max\na∈A′(∥a∥2/2) Since R(A) = 1\nλR(A′) we obtain from the equation that\nR(A) ≤log(|A|) + λ2 maxa∈A(∥a∥2/2)\nλm Setting λ =\np\n2 log(|A|)/ maxa∈A ∥a∥2 and rearranging terms we conclude our\nproof The following lemma shows that composing A with a Lipschitz function does\nnot blow up the Rademacher complexity The proof is due to Kakade and Tewari lemma 26.9 (Contraction lemma)\nFor each i ∈[m], let φi : R →R be a ρ-\nLipschitz function, namely for all α, β ∈R we have |φi(α) −φi(β)| ≤ρ |α −β| For a ∈Rm let φ(a) denote the vector (φ1(a1), , φm(ym)) Let φ◦A = {φ(a) :\na ∈A} Then,\nR(φ ◦A) ≤ρ R(A) Proof\nFor simplicity, we prove the lemma for the case ρ = 1 The case ρ ̸=\n1 will follow by deﬁning φ′ =\n1\nρφ and then using Lemma 26.6 Let Ai =\n{(a1, , ai−1, φi(ai), ai+1, , am) : a ∈A} Clearly, it suﬃces to prove that\nfor any set A and all i we have R(Ai) ≤R(A) Without loss of generality we will\nprove the latter claim for i = 1 and to simplify notation we omit the subscript\nfrom φ1",
      "word_count": 236,
      "source_page": 381,
      "start_position": 137656,
      "end_position": 137891,
      "sentences_count": 19
    },
    {
      "chunk_id": 613,
      "text": "Clearly, it suﬃces to prove that\nfor any set A and all i we have R(Ai) ≤R(A) Without loss of generality we will\nprove the latter claim for i = 1 and to simplify notation we omit the subscript\nfrom φ1 We have\nmR(A1) = E\nσ\n\"\nsup\na∈A1\nm\nX\ni=1\nσiai\n#\n= E\nσ\n\"\nsup\na∈A\nσ1φ(a1) +\nm\nX\ni=2\nσiai\n#\n= 1\n2\nE\nσ2,...,σm\n\"\nsup\na∈A\n \nφ(a1) +\nm\nX\ni=2\nσiai + sup\na∈A\n \n−φ(a1) +\nm\nX\ni=2\nσiai\n!#\n= 1\n2\nE\nσ2,...,σm\n\"\nsup\na,a′∈A\n \nφ(a1) −φ(a′\n1) +\nm\nX\ni=2\nσiai +\nm\nX\ni=2\nσia′\ni\n!#\n≤1\n2\nE\nσ2,...,σm\n\"\nsup\na,a′∈A\n \n|a1 −a′\n1| +\nm\nX\ni=2\nσiai +\nm\nX\ni=2\nσia′\ni\n!#\n,\n(26.12)\nwhere in the last inequality we used the assumption that φ is Lipschitz Next,\nwe note that the absolute value on |a1 −a′\n1| in the preceding expression can",
      "word_count": 168,
      "source_page": 381,
      "start_position": 137851,
      "end_position": 138018,
      "sentences_count": 5
    },
    {
      "chunk_id": 614,
      "text": "382\nRademacher Complexities\nbe omitted since both a and a′ are from the same set A and the rest of the\nexpression in the supremum is not aﬀected by replacing a and a′ Therefore,\nmR(A1) ≤1\n2\nE\nσ2,...,σm\n\"\nsup\na,a′∈A\n \na1 −a′\n1 +\nm\nX\ni=2\nσiai +\nm\nX\ni=2\nσia′\ni\n!# (26.13)\nBut, using the same equalities as in Equation (26.12), it is easy to see that the\nright-hand side of Equation (26.13) exactly equals m R(A), which concludes our\nproof 26.2\nRademacher Complexity of Linear Classes\nIn this section we analyze the Rademacher complexity of linear classes To sim-\nplify the derivation we ﬁrst deﬁne the following two classes:\nH1 = {x 7→⟨w, x⟩: ∥w∥1 ≤1}\n,\nH2 = {x 7→⟨w, x⟩: ∥w∥2 ≤1} (26.14)\nThe following lemma bounds the Rademacher complexity of H2 We allow\nthe xi to be vectors in any Hilbert space (even inﬁnite dimensional), and the\nbound does not depend on the dimensionality of the Hilbert space This property\nbecomes useful when analyzing kernel methods lemma 26.10\nLet S = (x1, , xm) be vectors in a Hilbert space Deﬁne: H2 ◦\nS = {(⟨w, x1⟩, , ⟨w, xm⟩) : ∥w∥2 ≤1} Then,\nR(H2 ◦S) ≤maxi ∥xi∥2\n√m Proof\nUsing Cauchy-Schwartz inequality we know that for any vectors w, v we\nhave ⟨w, v⟩≤∥w∥∥v∥",
      "word_count": 225,
      "source_page": 382,
      "start_position": 138019,
      "end_position": 138243,
      "sentences_count": 14
    },
    {
      "chunk_id": 615,
      "text": "26.3 Generalization Bounds for SVM\n383\nFinally, since the variables σ1, , σm are independent we have\nE\nσ\n\"\n∥\nm\nX\ni=1\nσixi∥2\n2\n#\n= E\nσ\n\nX\ni,j\nσiσj⟨xi, xj⟩\n\n\n=\nX\ni̸=j\n⟨xi, xj⟩E\nσ [σiσj] +\nm\nX\ni=1\n⟨xi, xi⟩E\nσ\n\u0002\nσ2\ni\n\u0003\n=\nm\nX\ni=1\n∥xi∥2\n2 ≤m max\ni\n∥xi∥2\n2 Combining this with Equation (26.15) and Equation (26.16) we conclude our\nproof Next we bound the Rademacher complexity of H1 ◦S lemma 26.11\nLet S = (x1, , xm) be vectors in Rn Then,\nR(H1 ◦S) ≤\nmax\ni\n∥xi∥∞\nr\n2 log(2n)\nm Proof\nUsing Holder’s inequality we know that for any vectors w, v we have\n⟨w, v⟩≤∥w∥1 ∥v∥∞ Therefore,\nmR(H1 ◦S) = E\nσ\n\"\nsup\na∈H1◦S\nm\nX\ni=1\nσiai\n#\n= E\nσ\n\"\nsup\nw:∥w∥1≤1\nm\nX\ni=1\nσi⟨w, xi⟩\n#\n= E\nσ\n\"\nsup\nw:∥w∥1≤1\n⟨w,\nm\nX\ni=1\nσixi⟩\n#\n≤E\nσ\n\"\n∥\nm\nX\ni=1\nσixi∥∞\n# (26.17)\nFor each j ∈[n], let vj = (x1,j, , xm,j) ∈Rm Note that ∥vj∥2 ≤√m maxi ∥xi∥∞ Let V = {v1, , vn, −v1, , −vn} The right-hand side of Equation (26.17) is\nm R(V ) Using Massart lemma (Lemma 26.8) we have that\nR(V ) ≤max\ni\n∥xi∥∞\np\n2 log(2n)/m,\nwhich concludes our proof",
      "word_count": 231,
      "source_page": 383,
      "start_position": 138342,
      "end_position": 138572,
      "sentences_count": 17
    },
    {
      "chunk_id": 616,
      "text": "384\nRademacher Complexities\nWe shall consider the following general constraint-based formulation Let H =\n{w : ∥w∥2 ≤B} be our hypothesis class, and let Z = X × Y be the examples\ndomain Assume that the loss function ℓ: H × Z →R is of the form\nℓ(w, (x, y)) = φ(⟨w, x⟩, y),\n(26.18)\nwhere φ : R × Y →R is such that for all y ∈Y, the scalar function a 7→φ(a, y)\nis ρ-Lipschitz For example, the hinge-loss function, ℓ(w, (x, y)) = max{0, 1 −\ny⟨w, x⟩}, can be written as in Equation (26.18) using φ(a, y) = max{0, 1 −\nya}, and note that φ is 1-Lipschitz for all y ∈{±1} Another example is the\nabsolute loss function, ℓ(w, (x, y)) = |⟨w, x⟩−y|, which can be written as in\nEquation (26.18) using φ(a, y) = |a −y|, which is also 1-Lipschitz for all y ∈R The following theorem bounds the generalization error of all predictors in H\nusing their empirical error theorem 26.12\nSuppose that D is a distribution over X × Y such that with\nprobability 1 we have that ∥x∥2 ≤R Let H = {w : ∥w∥2 ≤B} and let\nℓ: H × Z →R be a loss function of the form given in Equation (26.18)\nsuch that for all y ∈Y, a 7→φ(a, y) is a ρ-Lipschitz function and such that\nmaxa∈[−BR,BR] |φ(a, y)| ≤c",
      "word_count": 234,
      "source_page": 384,
      "start_position": 138610,
      "end_position": 138843,
      "sentences_count": 8
    },
    {
      "chunk_id": 617,
      "text": "theorem 26.12\nSuppose that D is a distribution over X × Y such that with\nprobability 1 we have that ∥x∥2 ≤R Let H = {w : ∥w∥2 ≤B} and let\nℓ: H × Z →R be a loss function of the form given in Equation (26.18)\nsuch that for all y ∈Y, a 7→φ(a, y) is a ρ-Lipschitz function and such that\nmaxa∈[−BR,BR] |φ(a, y)| ≤c Then, for any δ ∈(0, 1), with probability of at least\n1 −δ over the choice of an i.i.d sample of size m,\n∀w ∈H,\nLD(w) ≤LS(w) + 2ρBR\n√m + c\nr\n2 ln(2/δ)\nm Proof\nLet F = {(x, y) 7→φ(⟨w, x⟩, y) : w ∈H} We will show that with\nprobability 1, R(F ◦S) ≤ρBR/√m and then the theorem will follow from\nTheorem 26.5 Indeed, the set F ◦S can be written as\nF ◦S = {(φ(⟨w, x1⟩, y1), , φ(⟨w, xm⟩, ym)) : w ∈H},\nand the bound on R(F◦S) follows directly by combining Lemma 26.9, Lemma 26.10,\nand the assumption that ∥x∥2 ≤R with probability 1 We next derive a generalization bound for hard-SVM based on the previous\ntheorem For simplicity, we do not allow a bias term and consider the hard-SVM\nproblem:\nargmin\nw\n∥w∥2\ns.t ∀i, yi⟨w, xi⟩≥1\n(26.19)\ntheorem 26.13\nConsider a distribution D over X ×{±1} such that there exists\nsome vector w⋆with P(x,y)∼D[y⟨w⋆, x⟩≥1] = 1 and such that ∥x∥2 ≤R with\nprobability 1 Let wS be the output of Equation (26.19)",
      "word_count": 249,
      "source_page": 384,
      "start_position": 138777,
      "end_position": 139025,
      "sentences_count": 12
    },
    {
      "chunk_id": 618,
      "text": "26.3 Generalization Bounds for SVM\n385\nProof\nThroughout the proof, let the loss function be the ramp loss (see Sec-\ntion 15.2.3) Note that the range of the ramp loss is [0, 1] and that it is a\n1-Lipschitz function Since the ramp loss upper bounds the zero-one loss, we\nhave that\nP\n(x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤LD(wS) Let B = ∥w⋆∥2 and consider the set H = {w : ∥w∥2 ≤B} By the deﬁnition of\nhard-SVM and our assumption on the distribution, we have that wS ∈H with\nprobability 1 and that LS(wS) = 0 Therefore, using Theorem 26.12 we have\nthat\nLD(wS) ≤LS(wS) + 2BR\n√m +\nr\n2 ln(2/δ)\nm Remark 26.1\nTheorem 26.13 implies that the sample complexity of hard-SVM\ngrows like R2 ∥w⋆∥2\nϵ2 Using a more delicate analysis and the separability assump-\ntion, it is possible to improve the bound to an order of R2 ∥w⋆∥2\nϵ The bound in the preceding theorem depends on ∥w⋆∥, which is unknown In the following we derive a bound that depends on the norm of the output of\nSVM; hence it can be calculated from the training set itself The proof is similar\nto the derivation of bounds for structure risk minimization (SRM) theorem 26.14\nAssume that the conditions of Theorem 26.13 hold Then,\nwith probability of at least 1 −δ over the choice of S ∼Dm, we have that\nP\n(x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤4R∥wS∥\n√m\n+\ns\nln( 4 log2(∥wS∥)\nδ\n)\nm",
      "word_count": 248,
      "source_page": 385,
      "start_position": 139062,
      "end_position": 139309,
      "sentences_count": 13
    },
    {
      "chunk_id": 619,
      "text": "theorem 26.14\nAssume that the conditions of Theorem 26.13 hold Then,\nwith probability of at least 1 −δ over the choice of S ∼Dm, we have that\nP\n(x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤4R∥wS∥\n√m\n+\ns\nln( 4 log2(∥wS∥)\nδ\n)\nm Proof\nFor any integer i, let Bi = 2i, Hi = {w : ∥w∥≤Bi}, and let δi =\nδ\n2i2 Fix i, then using Theorem 26.12 we have that with probability of at least 1 −δi\n∀w ∈Hi,\nLD(w) ≤LS(w) + 2BiR\n√m +\nr\n2 ln(2/δi)\nm\nApplying the union bound and using P∞\ni=1 δi ≤δ we obtain that with probability\nof at least 1−δ this holds for all i Therefore, for all w, if we let i = ⌈log2(∥w∥)⌉\nthen w ∈Hi, Bi ≤2∥w∥, and\n2\nδi = (2i)2\nδ\n≤(4 log2(∥w∥))2\nδ Therefore,\nLD(w) ≤LS(w) + 2BiR\n√m +\nr\n2 ln(2/δi)\nm\n≤LS(w) + 4∥w∥R\n√m\n+\nr\n4(ln(4 log2(∥w∥)) + ln(1/δ))\nm In particular, it holds for wS, which concludes our proof.",
      "word_count": 170,
      "source_page": 385,
      "start_position": 139268,
      "end_position": 139437,
      "sentences_count": 7
    },
    {
      "chunk_id": 620,
      "text": "386\nRademacher Complexities\nRemark 26.2\nNote that all the bounds we have derived do not depend on the\ndimension of w This property is utilized when learning SVM with kernels, where\nthe dimension of w can be extremely large 26.4\nGeneralization Bounds for Predictors with Low ℓ1 Norm\nIn the previous section we derived generalization bounds for linear predictors\nwith an ℓ2-norm constraint In this section we consider the following general ℓ1-\nnorm constraint formulation Let H = {w : ∥w∥1 ≤B} be our hypothesis class,\nand let Z = X × Y be the examples domain Assume that the loss function,\nℓ: H × Z →R, is of the same form as in Equation (26.18), with φ : R × Y →R\nbeing ρ-Lipschitz w.r.t its ﬁrst argument The following theorem bounds the\ngeneralization error of all predictors in H using their empirical error theorem 26.15\nSuppose that D is a distribution over X × Y such that with\nprobability 1 we have that ∥x∥∞≤R Let H = {w ∈Rd : ∥w∥1 ≤B} and\nlet ℓ: H × Z →R be a loss function of the form given in Equation (26.18)\nsuch that for all y ∈Y, a 7→φ(a, y) is an ρ-Lipschitz function and such that\nmaxa∈[−BR,BR] |φ(a, y)| ≤c Then, for any δ ∈(0, 1), with probability of at least\n1 −δ over the choice of an i.i.d",
      "word_count": 231,
      "source_page": 386,
      "start_position": 139438,
      "end_position": 139668,
      "sentences_count": 11
    },
    {
      "chunk_id": 621,
      "text": "Let H = {w ∈Rd : ∥w∥1 ≤B} and\nlet ℓ: H × Z →R be a loss function of the form given in Equation (26.18)\nsuch that for all y ∈Y, a 7→φ(a, y) is an ρ-Lipschitz function and such that\nmaxa∈[−BR,BR] |φ(a, y)| ≤c Then, for any δ ∈(0, 1), with probability of at least\n1 −δ over the choice of an i.i.d sample of size m,\n∀w ∈H,\nLD(w) ≤LS(w) + 2ρBR\nr\n2 log(2d)\nm\n+ c\nr\n2 ln(2/δ)\nm Proof\nThe proof is identical to the proof of Theorem 26.12, while relying on\nLemma 26.11 instead of relying on Lemma 26.10 It is interesting to compare the two bounds given in Theorem 26.12 and The-\norem 26.15 Apart from the extra log(d) factor that appears in Theorem 26.15,\nboth bounds look similar However, the parameters B, R have diﬀerent meanings\nin the two bounds In Theorem 26.12, the parameter B imposes an ℓ2 constraint\non w and the parameter R captures a low ℓ2-norm assumption on the instances In contrast, in Theorem 26.15 the parameter B imposes an ℓ1 constraint on w\n(which is stronger than an ℓ2 constraint) while the parameter R captures a low\nℓ∞-norm assumption on the instance (which is weaker than a low ℓ2-norm as-\nsumption) Therefore, the choice of the constraint should depend on our prior\nknowledge of the set of instances and on prior assumptions on good predictors",
      "word_count": 240,
      "source_page": 386,
      "start_position": 139604,
      "end_position": 139843,
      "sentences_count": 10
    },
    {
      "chunk_id": 622,
      "text": "27\nCovering Numbers\nIn this chapter we describe another way to measure the complexity of sets, which\nis called covering numbers 27.1\nCovering\ndefinition 27.1 (Covering)\nLet A ⊂Rm be a set of vectors We say that A\nis r-covered by a set A′, with respect to the Euclidean metric, if for all a ∈A\nthere exists a′ ∈A′ with ∥a −a′∥≤r We deﬁne by N(r, A) the cardinality of\nthe smallest A′ that r-covers A Example 27.1 (Subspace)\nSuppose that A ⊂Rm, let c = maxa∈A ∥a∥, and as-\nsume that A lies in a d-dimensional subspace of Rm Then, N(r, A) ≤(2c\n√\nd/r)d To see this, let v1, , vd be an orthonormal basis of the subspace Then, any\na ∈A can be written as a = Pd\ni=1 αivi with ∥α∥∞≤∥α∥2 = ∥a∥2 ≤c Let\nϵ ∈R and consider the set\nA′ =\n( d\nX\ni=1\nα′\nivi : ∀i, α′\ni ∈{−c, −c + ϵ, −c + 2ϵ, , c}\n) Given a ∈A s.t a = Pd\ni=1 αivi with ∥α∥∞≤c, there exists a′ ∈A′ such that\n∥a −a′∥2 = ∥\nX\ni\n(α′\ni −αi)vi∥2 ≤ϵ2 X\ni\n∥vi∥2 ≤ϵ2 d Choose ϵ = r/\n√\nd; then ∥a −a′∥≤r and therefore A′ is an r-cover of A Hence,\nN(r, A) ≤|A′| =\n\u00122c\nϵ\n\u0013d\n=\n \n2c\n√\nd\nr\n!d 27.1.1\nProperties\nThe following lemma is immediate from the deﬁnition",
      "word_count": 240,
      "source_page": 388,
      "start_position": 139935,
      "end_position": 140174,
      "sentences_count": 16
    },
    {
      "chunk_id": 623,
      "text": "27.2 From Covering to Rademacher Complexity via Chaining\n389\nNext, we derive a contraction principle lemma 27.3\nFor each i ∈[m], let φi : R →R be a ρ-Lipschitz function;\nnamely, for all α, β ∈R we have |φi(α) −φi(β)| ≤ρ |α −β| For a ∈Rm let\nφ(a) denote the vector (φ1(a1), , φm(am)) Let φ ◦A = {φ(a) : a ∈A} Then,\nN(ρ r, φ ◦A) ≤N(r, A) Proof\nDeﬁne B = φ ◦A Let A′ be an r-cover of A and deﬁne B′ = φ ◦A′ Then, for all a ∈A there exists a′ ∈A′ with ∥a −a′∥≤r So,\n∥φ(a) −φ(a′)∥2 =\nX\ni\n(φi(ai) −φi(a′\ni))2 ≤ρ2 X\ni\n(ai −a′\ni)2 ≤(ρr)2 Hence, B′ is an (ρ r)-cover of B 27.2\nFrom Covering to Rademacher Complexity via Chaining\nThe following lemma bounds the Rademacher complexity of A based on the\ncovering numbers N(r, A) This technique is called Chaining and is attributed\nto Dudley lemma 27.4\nLet c = min¯a maxa∈A ∥a −¯a∥ Then, for any integer M > 0,\nR(A) ≤c 2−M\n√m + 6 c\nm\nM\nX\nk=1\n2−k\nq\nlog(N(c 2−k, A)) Proof\nLet ¯a be a minimizer of the objective function given in the deﬁnition\nof c On the basis of Lemma 26.6, we can analyze the Rademacher complexity\nassuming that ¯a = 0 Consider the set B0 = {0} and note that it is a c-cover of A Let B1,",
      "word_count": 240,
      "source_page": 389,
      "start_position": 140233,
      "end_position": 140472,
      "sentences_count": 19
    },
    {
      "chunk_id": 624,
      "text": "390\nCovering Numbers\nWe can now write\nR(A) = 1\nm E⟨σ, a∗⟩\n= 1\nm E\n\"\n⟨σ, a∗−b(M)⟩+\nM\nX\nk=1\n⟨σ, b(k) −b(k−1)⟩\n#\n≤1\nm E\nh\n∥σ∥∥a∗−b(M)∥\ni\n+\nM\nX\nk=1\n1\nm E\n\"\nsup\na∈ˆ\nBk\n⟨σ, a⟩\n# Since ∥σ∥= √m and ∥a∗−b(M)∥≤c 2−M, the ﬁrst summand is at most\nc\n√m 2−M Additionally, by Massart lemma,\n1\nm E sup\na∈ˆ\nBk\n⟨σ, a⟩≤3 c 2−k\np\n2 log(N(c 2−k, A)2)\nm\n= 6 c 2−k\np\nlog(N(c 2−k, A))\nm Therefore,\nR(A) ≤c 2−M\n√m + 6c\nm\nM\nX\nk=1\n2−k\nq\nlog(N(c2−k, A)) As a corollary we obtain the following:\nlemma 27.5\nAssume that there are α, β > 0 such that for any k ≥1 we have\nq\nlog(N(c2−k, A)) ≤α + βk Then,\nR(A) ≤6c\nm (α + 2β) Proof\nThe bound follows from Lemma 27.4 by taking M →∞and noting that\nP∞\nk=1 2−k = 1 and P∞\nk=1 k2−k = 2 Example 27.2\nConsider a set A which lies in a d dimensional subspace of Rm\nand such that c = maxa∈A ∥a∥ We have shown that N(r, A) ≤\n\u0010\n2c\n√\nd\nr\n\u0011d There-\nfore, for any k,\nq\nlog(N(c2−k, A)) ≤\nr\nd log\n\u0010\n2k+1√\nd\n\u0011\n≤\nq\nd log(2\n√\nd) +\n√\nk d\n≤\nq\nd log(2\n√\nd) +\n√\nd k",
      "word_count": 239,
      "source_page": 390,
      "start_position": 140584,
      "end_position": 140822,
      "sentences_count": 10
    },
    {
      "chunk_id": 625,
      "text": "28\nProof of the Fundamental Theorem\nof Learning Theory\nIn this chapter we prove Theorem 6.8 from Chapter 6 We remind the reader\nthe conditions of the theorem, which will hold throughout this chapter: H is a\nhypothesis class of functions from a domain X to {0, 1}, the loss function is the\n0 −1 loss, and VCdim(H) = d < ∞ We shall prove the upper bound for both the realizable and agnostic cases\nand shall prove the lower bound for the agnostic case The lower bound for the\nrealizable case is left as an exercise 28.1\nThe Upper Bound for the Agnostic Case\nFor the upper bound we need to prove that there exists C such that H is agnostic\nPAC learnable with sample complexity\nmH(ϵ, δ) ≤C d + ln(1/δ)\nϵ2 We will prove the slightly looser bound:\nmH(ϵ, δ) ≤C d log(d/ϵ) + ln(1/δ)\nϵ2 (28.1)\nThe tighter bound in the theorem statement requires a more involved proof, in\nwhich a more careful analysis of the Rademacher complexity using a technique\ncalled “chaining” should be used This is beyond the scope of this book To prove Equation (28.1), it suﬃces to show that applying the ERM with a\nsample size\nm ≥432d\nϵ2 · log\n\u001264d\nϵ2\n\u0013\n+ 8\nϵ2 · (8d log(e/d) + 2 log(4/δ))\nyields an ϵ, δ-learner for H We prove this result on the basis of Theorem 26.5 Let (x1, y1), , (xm, ym) be a classiﬁcation training set",
      "word_count": 248,
      "source_page": 392,
      "start_position": 140897,
      "end_position": 141144,
      "sentences_count": 12
    },
    {
      "chunk_id": 626,
      "text": "28.2 The Lower Bound for the Agnostic Case\n393\nCombining this with Lemma 26.8 we obtain the following bound on the Rademacher\ncomplexity:\nR(A) ≤\nr\n2d log(em/d)\nm Using Theorem 26.5 we obtain that with probability of at least 1 −δ, for every\nh ∈H we have that\nLD(h) −LS(h) ≤\nr\n8d log(em/d)\nm\n+\nr\n2 log(2/δ)\nm Repeating the previous argument for minus the zero-one loss and applying the\nunion bound we obtain that with probability of at least 1 −δ, for every h ∈H\nit holds that\n|LD(h) −LS(h)| ≤\nr\n8d log(em/d)\nm\n+\nr\n2 log(4/δ)\nm\n≤2\nr\n8d log(em/d) + 2 log(4/δ)\nm To ensure that this is smaller than ϵ we need\nm ≥4\nϵ2 · (8d log(m) + 8d log(e/d) + 2 log(4/δ)) Using Lemma A.2, a suﬃcient condition for the inequality to hold is that\nm ≥432d\nϵ2 · log\n\u001264d\nϵ2\n\u0013\n+ 8\nϵ2 · (8d log(e/d) + 2 log(4/δ)) 28.2\nThe Lower Bound for the Agnostic Case\nHere, we prove that there exists C such that H is agnostic PAC learnable with\nsample complexity\nmH(ϵ, δ) ≥C d + ln(1/δ)\nϵ2 We will prove the lower bound in two parts First, we will show that m(ϵ, δ) ≥\n0.5 log(1/(4δ))/ϵ2, and second we will show that for every δ ≤1/8 we have that\nm(ϵ, δ) ≥8d/ϵ2 These two bounds will conclude the proof",
      "word_count": 238,
      "source_page": 393,
      "start_position": 141218,
      "end_position": 141455,
      "sentences_count": 9
    },
    {
      "chunk_id": 627,
      "text": "394\nProof of the Fundamental Theorem of Learning Theory\nthat there are h+, h−∈H for which h+(c) = 1 and h−(c) = −1 Deﬁne two\ndistributions, D+ and D−, such that for b ∈{±1} we have\nDb({(x, y)}) =\n( 1+ybϵ\n2\nif x = c\n0\notherwise That is, all the distribution mass is concentrated on two examples (c, 1) and\n(c, −1), where the probability of (c, b) is 1+bϵ\n2\nand the probability of (c, −b) is\n1−bϵ\n2 Let A be an arbitrary algorithm Any training set sampled from Db has the\nform S = (c, y1), , (c, ym) Therefore, it is fully characterized by the vector\ny = (y1, , ym) ∈{±1}m Upon receiving a training set S, the algorithm A\nreturns a hypothesis h : X →{±1} Since the error of A w.r.t Db only depends\non h(c), we can think of A as a mapping from {±1}m into {±1} Therefore,\nwe denote by A(y) the value in {±1} corresponding to the prediction of h(c),\nwhere h is the hypothesis that A outputs upon receiving the training set S =\n(c, y1), , (c, ym) Note that for any hypothesis h we have\nLDb(h) = 1 −h(c)bϵ\n2 In particular, the Bayes optimal hypothesis is hb and\nLDb(A(y)) −LDb(hb) = 1 −A(y)bϵ\n2\n−1 −ϵ\n2\n=\n(\nϵ\nif A(y) ̸= b\n0\notherwise Fix A For b ∈{±1}, let Y b = {y ∈{0, 1}m : A(y) ̸= b}",
      "word_count": 248,
      "source_page": 394,
      "start_position": 141517,
      "end_position": 141764,
      "sentences_count": 17
    },
    {
      "chunk_id": 628,
      "text": "28.2 The Lower Bound for the Agnostic Case\n395\nTherefore,\nmax\nb∈{±1} P [LDb(A(y)) −LDb(hb) = ϵ]\n= max\nb∈{±1}\nX\ny\nPb[y]1[A(y)̸=b]\n≥1\n2\nX\ny\nP+[y]1[A(y)̸=+] + 1\n2\nX\ny\nP−[y]1[A(y)̸=−]\n= 1\n2\nX\ny∈N+\n(P+[y]1[A(y)̸=+] + P−[y]1[A(y)̸=−]) + 1\n2\nX\ny∈N−\n(P+[y]1[A(y)̸=+] + P−[y]1[A(y)̸=−])\n≥1\n2\nX\ny∈N+\n(P−[y]1[A(y)̸=+] + P−[y]1[A(y)̸=−]) + 1\n2\nX\ny∈N−\n(P+[y]1[A(y)̸=+] + P+[y]1[A(y)̸=−])\n= 1\n2\nX\ny∈N+\nP−[y] + 1\n2\nX\ny∈N−\nP+[y] Next note that P\ny∈N+ P−[y] = P\ny∈N−P+[y], and both values are the prob-\nability that a Binomial (m, (1 −ϵ)/2) random variable will have value greater\nthan m/2 Using Lemma B.11, this probability is lower bounded by\n1\n2\n\u0010\n1 −\np\n1 −exp(−mϵ2/(1 −ϵ2))\n\u0011\n≥1\n2\n\u0010\n1 −\np\n1 −exp(−2mϵ2)\n\u0011\n,\nwhere we used the assumption that ϵ2 ≤1/2 It follows that if m ≤0.5 log(1/(4δ))/ϵ2\nthen there exists b such that\nP [LDb(A(y)) −LDb(hb) = ϵ]\n≥1\n2\n\u0012\n1 −\nq\n1 −\n√\n4δ\n\u0013\n≥δ,\nwhere the last inequality follows by standard algebraic manipulations This con-\ncludes our proof 28.2.2\nShowing That m(ϵ, 1/8) ≥8d/ϵ2\nWe shall now prove that for every ϵ < 1/(8\n√\n2) we have that m(ϵ, δ) ≥8d\nϵ2 Let ρ = 8ϵ and note that ρ ∈(0, 1/\n√\n2) We will construct a family of distri-\nbutions as follows First, let C = {c1,",
      "word_count": 240,
      "source_page": 395,
      "start_position": 141826,
      "end_position": 142065,
      "sentences_count": 9
    },
    {
      "chunk_id": 629,
      "text": "396\nProof of the Fundamental Theorem of Learning Theory\nh ∈H such that h(ci) = bi for all i ∈[d], and its error is 1−ρ\n2 In addition, for\nany other function f : X →{±1}, it is easy to verify that\nLDb(f) = 1 + ρ\n2\n· |{i ∈[d] : f(ci) ̸= bi}|\nd\n+ 1 −ρ\n2\n· |{i ∈[d] : f(ci) = bi}|\nd Therefore,\nLDb(f) −min\nh∈H LDb(h) = ρ · |{i ∈[d] : f(ci) ̸= bi}|\nd (28.2)\nNext, ﬁx some learning algorithm A As in the proof of the No-Free-Lunch\ntheorem, we have that\nmax\nDb:b∈{±1}d\nE\nS∼Dm\nb\n\u0014\nLDb(A(S)) −min\nh∈H LDb(h)\n\u0015\n(28.3)\n≥\nE\nDb:b∼U({±1}d)\nE\nS∼Dm\nb\n\u0014\nLDb(A(S)) −min\nh∈H LDb(h)\n\u0015\n(28.4)\n=\nE\nDb:b∼U({±1}d)\nE\nS∼Dm\nb\n\u0014\nρ · |{i ∈[d] : A(S)(ci) ̸= bi|\nd\n\u0015\n(28.5)\n= ρ\nd\nd\nX\ni=1\nE\nDb:b∼U({±1}d)\nE\nS∼Dm\nb\n1[A(S)(ci)̸=bi],\n(28.6)\nwhere the ﬁrst equality follows from Equation (28.2) In addition, using the\ndeﬁnition of Db, to sample S ∼Db we can ﬁrst sample (j1, , jm) ∼U([d])m, set\nxr = cji, and ﬁnally sample yr such that P[yr = bji] = (1 + ρ)/2 Let us simplify\nthe notation and use y ∼b to denote sampling according to P[y = b] = (1+ρ)/2 Therefore, the right-hand side of Equation (28.6) equals\nρ\nd\nd\nX\ni=1\nE\nj∼U([d])m\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[A(S)(ci)̸=bi] (28.7)\nWe now proceed in two steps",
      "word_count": 247,
      "source_page": 396,
      "start_position": 142164,
      "end_position": 142410,
      "sentences_count": 10
    },
    {
      "chunk_id": 630,
      "text": "Therefore, the right-hand side of Equation (28.6) equals\nρ\nd\nd\nX\ni=1\nE\nj∼U([d])m\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[A(S)(ci)̸=bi] (28.7)\nWe now proceed in two steps First, we show that among all learning algorithms,\nA, the one which minimizes Equation (28.7) (and hence also Equation (28.4))\nis the Maximum-Likelihood learning rule, denoted AML Formally, for each i,\nAML(S)(ci) is the majority vote among the set {yr : r ∈[m], xr = ci} Second,\nwe lower bound Equation (28.7) for AML lemma 28.1\nAmong all algorithms, Equation (28.4) is minimized for A being\nthe Maximum-Likelihood algorithm, AML, deﬁned as\n∀i,\nAML(S)(ci) = sign\n X\nr:xr=ci\nyr Proof\nFix some j ∈[d]m Note that given j and y ∈{±1}m, the training set\nS is fully determined Therefore, we can write A(j, y) instead of A(S) Let us\nalso ﬁx i ∈[d] Denote b¬i the sequence (b1, , bi−1, bi+1, , bm) Also, for any",
      "word_count": 153,
      "source_page": 396,
      "start_position": 142384,
      "end_position": 142536,
      "sentences_count": 14
    },
    {
      "chunk_id": 631,
      "text": "28.2 The Lower Bound for the Agnostic Case\n397\ny ∈{±1}m, let yI denote the elements of y corresponding to indices for which\njr = i and let y¬I be the rest of the elements of y We have\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[A(S)(ci)̸=bi]\n= 1\n2\nX\nbi∈{±1}\nE\nb¬i∼U({±1}d−1)\nX\ny\nP[y|b¬i, bi]1[A(j,y)(ci)̸=bi]\n=\nE\nb¬i∼U({±1}d−1)\nX\ny¬I\nP[y¬I|b¬i]1\n2\nX\nyI\n\n\nX\nbi∈{±1}\nP[yI|bi]1[A(j,y)(ci)̸=bi]\n\n The sum within the parentheses is minimized when A(j, y)(ci) is the maximizer\nof P[yI|bi] over bi ∈{±1}, which is exactly the Maximum-Likelihood rule Re-\npeating the same argument for all i we conclude our proof Fix i For every j, let ni(j) = {|t : jt = i|} be the number of instances in which\nthe instance is ci For the Maximum-Likelihood rule, we have that the quantity\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[AML(S)(ci)̸=bi]\nis exactly the probability that a binomial (ni(j), (1 −ρ)/2) random variable will\nbe larger than ni(j)/2 Using Lemma B.11, and the assumption ρ2 ≤1/2, we\nhave that\nP[B ≥ni(j)/2] ≥1\n2\n\u0010\n1 −\np\n1 −e−2ni(j)ρ2\u0011 We have thus shown that\nρ\nd\nd\nX\ni=1\nE\nj∼U([d])m\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[A(S)(ci)̸=bi]\n≥ρ\n2d\nd\nX\ni=1\nE\nj∼U([d])m\n\u0010\n1 −\np\n1 −e−2ρ2ni(j)\n\u0011\n≥ρ\n2d\nd\nX\ni=1\nE\nj∼U([d])m\n\u0010\n1 −\np\n2ρ2ni(j)\n\u0011\n,\nwhere in the last inequality we used the inequality 1 −e−a ≤a",
      "word_count": 240,
      "source_page": 397,
      "start_position": 142537,
      "end_position": 142776,
      "sentences_count": 9
    },
    {
      "chunk_id": 632,
      "text": "398\nProof of the Fundamental Theorem of Learning Theory\nAs long as m <\nd\n8ρ2 , this term would be larger than ρ/4 In summary, we have shown that if m <\nd\n8ρ2 then for any algorithm there\nexists a distribution such that\nE\nS∼Dm\n\u0014\nLD(A(S)) −min\nh∈H LD(h)\n\u0015\n≥ρ/4 Finally, Let ∆= 1\nρ(LD(A(S)) −minh∈H LD(h)) and note that ∆∈[0, 1] (see\nEquation (28.5)) Therefore, using Lemma B.1, we get that\nP[LD(A(S)) −min\nh∈H LD(h) > ϵ] = P\n\u0014\n∆> ϵ\nρ\n\u0015\n≥E[∆] −ϵ\nρ\n≥1\n4 −ϵ\nρ Choosing ρ = 8ϵ we conclude that if m <\nd\n512 ϵ2 , then with probability of at least\n1/8 we will have LD(A(S)) −minh∈H LD(h) ≥ϵ 28.3\nThe Upper Bound for the Realizable Case\nHere we prove that there exists C such that H is PAC learnable with sample\ncomplexity\nmH(ϵ, δ) ≤C d ln(1/ϵ) + ln(1/δ)\nϵ We do so by showing that for m ≥C d ln(1/ϵ)+ln(1/δ)\nϵ\n, H is learnable using the\nERM rule We prove this claim based on the notion of ϵ-nets definition 28.2 (ϵ-net)\nLet X be a domain S ⊂X is an ϵ-net for H ⊂2X\nwith respect to a distribution D over X if\n∀h ∈H :\nD(h) ≥ϵ ⇒h ∩S ̸= ∅ theorem 28.3\nLet H ⊂2X with VCdim(H) = d",
      "word_count": 229,
      "source_page": 398,
      "start_position": 142832,
      "end_position": 143060,
      "sentences_count": 11
    },
    {
      "chunk_id": 633,
      "text": "28.3 The Upper Bound for the Realizable Case\n399\nClaim 1\nP[S ∈B] ≤2 P[(S, T) ∈B′] Proof of Claim 1: Since S and T are chosen independently we can write\nP[(S, T) ∈B′] =\nE\n(S,T )∼D2m\n\u0002\n1[(S,T )∈B′]\n\u0003\n=\nE\nS∼Dm\nh\nE\nT ∼Dm\n\u0002\n1[(S,T )∈B′]\n\u0003i Note that (S, T) ∈B′ implies S ∈B and therefore 1[(S,T )∈B′] = 1[(S,T )∈B′] 1[S∈B],\nwhich gives\nP[(S, T) ∈B′] =\nE\nS∼Dm\nE\nT ∼Dm 1[(S,T )∈B′] 1[S∈B]\n=\nE\nS∼Dm 1[S∈B]\nE\nT ∼Dm 1[(S,T )∈B′] Fix some S Then, either 1[S∈B] = 0 or S ∈B and then ∃hS such that D(hS) ≥ϵ\nand |hS ∩S| = 0 It follows that a suﬃcient condition for (S, T) ∈B′ is that\n|T ∩hS| > ϵm\n2 Therefore, whenever S ∈B we have\nE\nT ∼Dm 1[(S,T )∈B′] ≥\nP\nT ∼Dm[|T ∩hS| > ϵm\n2 ] But, since we now assume S ∈B we know that D(hS) = ρ ≥ϵ Therefore,\n|T ∩hS| is a binomial random variable with parameters ρ (probability of success\nfor a single try) and m (number of tries) Chernoﬀ’s inequality implies\nP[|T∩hS| ≤ρm\n2 ] ≤e\n−2\nmρ (mρ−mρ/2)2\n= e−mρ/2 ≤e−mϵ/2 ≤e−d log(1/δ)/2 = δd/2 ≤1/2 Thus,\nP[|T ∩hS| > ϵm\n2 ] = 1 −P[|T ∩hS| ≤ϵm\n2 ] ≥1 −P[|T ∩hS| ≤ρm\n2 ] ≥1/2 Combining all the preceding we conclude the proof of Claim 1 Claim 2 (Symmetrization):\nP[(S, T) ∈B′] ≤e−ϵm/4 τH(2m)",
      "word_count": 248,
      "source_page": 399,
      "start_position": 143166,
      "end_position": 143413,
      "sentences_count": 13
    },
    {
      "chunk_id": 634,
      "text": "Combining all the preceding we conclude the proof of Claim 1 Claim 2 (Symmetrization):\nP[(S, T) ∈B′] ≤e−ϵm/4 τH(2m) Proof of Claim 2: To simplify notation, let α = mϵ/2 and for a sequence A =\n(x1, , x2m) let A0 = (x1, , xm) Using the deﬁnition of B′ we get that\nP[A ∈B′] =\nE\nA∼D2m max\nh∈H 1[D(h)≥ϵ] 1[|h∩A0|=0] 1[|h∩A|≥α]\n≤\nE\nA∼D2m max\nh∈H 1[|h∩A0|=0] 1[|h∩A|≥α] Now, let us deﬁne by HA the eﬀective number of diﬀerent hypotheses on A,\nnamely, HA = {h ∩A : h ∈H } It follows that\nP[A ∈B′] ≤\nE\nA∼D2m max\nh∈HA 1[|h∩A0|=0] 1[|h∩A|≥α]\n≤\nE\nA∼D2m\nX\nh∈HA\n1[|h∩A0|=0] 1[|h∩A|≥α] Let J = {j ⊂[2m] : |j| = m} For any j ∈J and A = (x1, , x2m) deﬁne\nAj = (xj1, , xjm) Since the elements of A are chosen i.i.d., we have that\nfor any j ∈J and any function f(A, A0) it holds that EA∼D2m[f(A, A0)] =",
      "word_count": 163,
      "source_page": 399,
      "start_position": 143395,
      "end_position": 143557,
      "sentences_count": 13
    },
    {
      "chunk_id": 635,
      "text": "400\nProof of the Fundamental Theorem of Learning Theory\nEA∼D2m[f(A, Aj)] Since this holds for any j it also holds for the expectation of\nj chosen at random from J In particular, it holds for the function f(A, A0) =\nP\nh∈HA 1[|h∩A0|=0] 1[|h∩A|≥α] We therefore obtain that\nP[A ∈B′] ≤\nE\nA∼D2m E\nj∼J\nX\nh∈HA\n1[|h∩Aj|=0] 1[|h∩A|≥α]\n=\nE\nA∼D2m\nX\nh∈HA\n1[|h∩A|≥α] E\nj∼J 1[|h∩Aj|=0] Now, ﬁx some A s.t |h ∩A| ≥α Then, Ej 1[|h∩Aj|=0] is the probability that\nwhen choosing m balls from a bag with at least α red balls, we will never choose\na red ball This probability is at most\n(1 −α/(2m))m = (1 −ϵ/4)m ≤e−ϵm/4 We therefore get that\nP[A ∈B′] ≤\nE\nA∼D2m\nX\nh∈HA\ne−ϵm/4 ≤e−ϵm/4\nE\nA∼D2m |HA| Using the deﬁnition of the growth function we conclude the proof of Claim 2 Completing the Proof: By Sauer’s lemma we know that τH(2m) ≤(2em/d)d Combining this with the two claims we obtain that\nP[S ∈B] ≤2(2em/d)d e−ϵm/4 We would like the right-hand side of the inequality to be at most δ; that is,\n2(2em/d)d e−ϵm/4 ≤δ Rearranging, we obtain the requirement\nm ≥4\nϵ (d log(2em/d) + log(2/δ)) = 4d\nϵ log(m) + 4\nϵ (d log(2e/d) + log(2/δ) Using Lemma A.2, a suﬃcient condition for the preceding to hold is that\nm ≥16d\nϵ\nlog\n\u00128d\nϵ\n\u0013\n+ 8\nϵ (d log(2e/d) + log(2/δ)",
      "word_count": 237,
      "source_page": 400,
      "start_position": 143558,
      "end_position": 143794,
      "sentences_count": 15
    },
    {
      "chunk_id": 636,
      "text": "28.3 The Upper Bound for the Realizable Case\n401\n28.3.1\nFrom ϵ-Nets to PAC Learnability\ntheorem 28.4\nLet H be a hypothesis class over X with VCdim(H) = d Let\nD be a distribution over X and let c ∈H be a target hypothesis Fix ϵ, δ ∈(0, 1)\nand let m be as deﬁned in Theorem 28.3 Then, with probability of at least 1 −δ\nover a choice of m i.i.d instances from X with labels according to c we have that\nany ERM hypothesis has a true error of at most ϵ Proof\nDeﬁne the class Hc = {c a h : h ∈H}, where c a h = (h\\c)∪(c\\h) It is\neasy to verify that if some A ⊂X is shattered by H then it is also shattered by Hc\nand vice versa Hence, VCdim(H) = VCdim(Hc) Therefore, using Theorem 28.3\nwe know that with probability of at least 1 −δ, the sample S is an ϵ-net for Hc Note that LD(h) = D(h a c) Therefore, for any h ∈H with LD(h) ≥ϵ we have\nthat |(h a c)∩S| > 0, which implies that h cannot be an ERM hypothesis, which\nconcludes our proof.",
      "word_count": 198,
      "source_page": 401,
      "start_position": 143850,
      "end_position": 144047,
      "sentences_count": 11
    },
    {
      "chunk_id": 637,
      "text": "29\nMulticlass Learnability\nIn Chapter 17 we have introduced the problem of multiclass categorization, in\nwhich the goal is to learn a predictor h : X →[k] In this chapter we address PAC\nlearnability of multiclass predictors with respect to the 0-1 loss As in Chapter 6,\nthe main goal of this chapter is to:\n• Characterize which classes of multiclass hypotheses are learnable in the (mul-\nticlass) PAC model • Quantify the sample complexity of such hypothesis classes In view of the fundamental theorem of learning theory (Theorem 6.8), it is natu-\nral to seek a generalization of the VC dimension to multiclass hypothesis classes In Section 29.1 we show such a generalization, called the Natarajan dimension,\nand state a generalization of the fundamental theorem based on the Natarajan\ndimension Then, we demonstrate how to calculate the Natarajan dimension of\nseveral important hypothesis classes Recall that the main message of the fundamental theorem of learning theory\nis that a hypothesis class of binary classiﬁers is learnable (with respect to the\n0-1 loss) if and only if it has the uniform convergence property, and then it\nis learnable by any ERM learner In Chapter 13, Exercise 2, we have shown\nthat this equivalence breaks down for a certain convex learning problem The\nlast section of this chapter is devoted to showing that the equivalence between\nlearnability and uniform convergence breaks down even in multiclass problems\nwith the 0-1 loss, which are very similar to binary classiﬁcation",
      "word_count": 246,
      "source_page": 402,
      "start_position": 144048,
      "end_position": 144293,
      "sentences_count": 10
    },
    {
      "chunk_id": 638,
      "text": "In Chapter 13, Exercise 2, we have shown\nthat this equivalence breaks down for a certain convex learning problem The\nlast section of this chapter is devoted to showing that the equivalence between\nlearnability and uniform convergence breaks down even in multiclass problems\nwith the 0-1 loss, which are very similar to binary classiﬁcation Indeed, we\nconstruct a hypothesis class which is learnable by a speciﬁc ERM learner, but\nfor which other ERM learners might fail and the uniform convergence property\ndoes not hold 29.1\nThe Natarajan Dimension\nIn this section we deﬁne the Natarajan dimension, which is a generalization of\nthe VC dimension to classes of multiclass predictors Throughout this section,\nlet H be a hypothesis class of multiclass predictors; namely, each h ∈H is a\nfunction from X to [k] Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 161,
      "source_page": 402,
      "start_position": 144240,
      "end_position": 144400,
      "sentences_count": 10
    },
    {
      "chunk_id": 639,
      "text": "29.2 The Multiclass Fundamental Theorem\n403\nTo deﬁne the Natarajan dimension, we ﬁrst generalize the deﬁnition of shat-\ntering definition 29.1 (Shattering (Multiclass Version))\nWe say that a set C ⊂X\nis shattered by H if there exist two functions f0, f1 : C →[k] such that\n• For every x ∈C, f0(x) ̸= f1(x) • For every B ⊂C, there exists a function h ∈H such that\n∀x ∈B, h(x) = f0(x) and ∀x ∈C \\ B, h(x) = f1(x) definition 29.2 (Natarajan Dimension)\nThe Natarajan dimension of H, de-\nnoted Ndim(H), is the maximal size of a shattered set C ⊂X It is not hard to see that in the case that there are exactly two classes,\nNdim(H) = VCdim(H) Therefore, the Natarajan dimension generalizes the VC\ndimension We next show that the Natarajan dimension allows us to general-\nize the fundamental theorem of statistical learning from binary classiﬁcation to\nmulticlass classiﬁcation 29.2\nThe Multiclass Fundamental Theorem\ntheorem 29.3 (The Multiclass Fundamental Theorem)\nThere exist absolute\nconstants C1, C2 > 0 such that the following holds For every hypothesis class H\nof functions from X to [k], such that the Natarajan dimension of H is d, we have\n1 H has the uniform convergence property with sample complexity\nC1\nd + log(1/δ)\nϵ2\n≤mUC\nH (ϵ, δ) ≤C2\nd log (k) + log(1/δ)\nϵ2 2 H is agnostic PAC learnable with sample complexity\nC1\nd + log(1/δ)\nϵ2\n≤mH(ϵ, δ) ≤C2\nd log (k) + log(1/δ)\nϵ2 3",
      "word_count": 250,
      "source_page": 403,
      "start_position": 144401,
      "end_position": 144650,
      "sentences_count": 13
    },
    {
      "chunk_id": 640,
      "text": "404\nMulticlass Learnability\nlemma 29.4 (Natarajan)\n|H| ≤|X|Ndim(H) · k2Ndim(H) The proof of Natarajan’s lemma shares the same spirit of the proof of Sauer’s\nlemma and is left as an exercise (see Exercise 3) 29.3\nCalculating the Natarajan Dimension\nIn this section we show how to calculate (or estimate) the Natarajan dimen-\nsion of several popular classes, some of which were studied in Chapter 17 As\nthese calculations indicate, the Natarajan dimension is often proportional to the\nnumber of parameters required to deﬁne a hypothesis 29.3.1\nOne-versus-All Based Classes\nIn Chapter 17 we have seen two reductions of multiclass categorization to bi-\nnary classiﬁcation: One-versus-All and All-Pairs In this section we calculate the\nNatarajan dimension of the One-versus-All method Recall that in One-versus-All we train, for each label, a binary classiﬁer that\ndistinguishes between that label and the rest of the labels This naturally sug-\ngests considering multiclass hypothesis classes of the following form Let Hbin ⊂\n{0, 1}X be a binary hypothesis class For every ¯h = (h1, , hk) ∈(Hbin)k deﬁne\nT(¯h) : X →[k] by\nT(¯h)(x) = argmax\ni∈[k]\nhi(x) If there are two labels that maximize hi(x), we choose the smaller one Also, let\nHOvA,k\nbin\n= {T(¯h) : ¯h ∈(Hbin)k} What “should” be the Natarajan dimension of HOvA,k\nbin Intuitively, to specify a\nhypothesis in Hbin we need d = VCdim(Hbin) parameters To specify a hypothe-\nsis in HOvA,k\nbin\n, we need to specify k hypotheses in Hbin Therefore, kd parameters\nshould suﬃce",
      "word_count": 249,
      "source_page": 404,
      "start_position": 144768,
      "end_position": 145016,
      "sentences_count": 17
    },
    {
      "chunk_id": 641,
      "text": "29.3 Calculating the Natarajan Dimension\n405\nBy Sauer’s lemma, | (Hbin)C | ≤|C|d We conclude that\n2|C| ≤\n\f\f\f\n\u0010\nHOvA,k\nbin\n\u0011\nC\n\f\f\f ≤|C|dk The proof follows by taking the logarithm and applying Lemma A.1 How tight is Lemma 29.5 It is not hard to see that for some classes, Ndim(HOvA,k\nbin\n)\ncan be much smaller than dk (see Exercise 1) However there are several natural\nbinary classes, Hbin (e.g., halfspaces), for which Ndim(HOvA,k\nbin\n) = Ω(dk) (see\nExercise 6) 29.3.2\nGeneral Multiclass-to-Binary Reductions\nThe same reasoning used to establish Lemma 29.5 can be used to upper bound\nthe Natarajan dimension of more general multiclass-to-binary reductions These\nreductions train several binary classiﬁers on the data Then, given a new in-\nstance, they predict its label by using some rule that takes into account the\nlabels predicted by the binary classiﬁers These reductions include One-versus-\nAll and All-Pairs Suppose that such a method trains l binary classiﬁers from a binary class Hbin,\nand r : {0, 1}l →[k] is the rule that determines the (multiclass) label according\nto the predictions of the binary classiﬁers The hypothesis class corresponding\nto this method can be deﬁned as follows For every ¯h = (h1, , hl) ∈(Hbin)l\ndeﬁne R(¯h) : X →[k] by\nR(¯h)(x) = r(h1(x), , hl(x)) Finally, let\nHr\nbin = {R(¯h) : ¯h ∈(Hbin)l} Similarly to Lemma 29.5 it can be proven that:\nlemma 29.6\nIf d = VCdim(Hbin) then\nNdim(Hr\nbin) ≤3 l d log (l d)",
      "word_count": 249,
      "source_page": 405,
      "start_position": 145086,
      "end_position": 145334,
      "sentences_count": 17
    },
    {
      "chunk_id": 642,
      "text": "406\nMulticlass Learnability\ntheorem 29.7\nNdim(HΨ) ≤d Proof\nLet C ⊂X be a shattered set, and let f0, f1 : C →[k] be the two\nfunctions that witness the shattering We need to show that |C| ≤d For every\nx ∈C let ρ(x) = Ψ(x, f0(x)) −Ψ(x, f1(x)) We claim that the set ρ(C)\ndef\n=\n{ρ(x) : x ∈C} consists of |C| elements (i.e., ρ is one to one) and is shattered\nby the binary hypothesis class of homogeneous linear separators on Rd,\nH = {x 7→sign(⟨w, x⟩) : w ∈Rd} Since VCdim(H) = d, it will follow that |C| = |ρ(C)| ≤d, as required To establish our claim it is enough to show that |Hρ(C)| = 2|C| Indeed, given\na subset B ⊂C, by the deﬁnition of shattering, there exists hB ∈HΨ for which\n∀x ∈B, hB(x) = f0(x)\nand\n∀x ∈C \\ B, hB(x) = f1(x) Let wB ∈Rd be a vector that deﬁnes hB We have that, for every x ∈B,\n⟨w, Ψ(x, f0(x))⟩> ⟨w, Ψ(x, f1(x))⟩⇒⟨w, ρ(x)⟩> 0 Similarly, for every x ∈C \\ B,\n⟨w, ρ(x)⟩< 0 It follows that the hypothesis gB ∈H deﬁned by the same w ∈Rd label the\npoints in ρ(B) by 1 and the points in ρ(C \\ B) by 0 Since this holds for every\nB ⊆C we obtain that |C| = |ρ(C)| and |Hρ(C)| = 2|C|, which concludes our\nproof The theorem is tight in the sense that there are mappings Ψ for which Ndim(HΨ) =\nΩ(d)",
      "word_count": 250,
      "source_page": 406,
      "start_position": 145415,
      "end_position": 145664,
      "sentences_count": 14
    },
    {
      "chunk_id": 643,
      "text": "Since this holds for every\nB ⊆C we obtain that |C| = |ρ(C)| and |Hρ(C)| = 2|C|, which concludes our\nproof The theorem is tight in the sense that there are mappings Ψ for which Ndim(HΨ) =\nΩ(d) For example, this is true for the multivector construction (see Section 17.2\nand the Bibliographic Remarks at the end of this chapter) We therefore con-\nclude:\ncorollary 29.8\nLet X = Rn and let Ψ : X ×[k] →Rnk be the class sensitive\nfeature mapping for the multi-vector construction:\nΨ(x, y) = [ 0, , 0\n| {z }\n∈R(y−1)n\n, x1, , xn\n|\n{z\n}\n∈Rn\n, 0, , 0\n| {z }\n∈R(k−y)n\n] Let HΨ be as deﬁned in Equation (29.1) Then, the Natarajan dimension of HΨ\nsatisﬁes\n(k −1)(n −1) ≤Ndim(HΨ) ≤kn 29.4\nOn Good and Bad ERMs\nIn this section we present an example of a hypothesis class with the property\nthat not all ERMs for the class are equally successful Furthermore, if we allow\nan inﬁnite number of labels, we will also obtain an example of a class that is",
      "word_count": 185,
      "source_page": 406,
      "start_position": 145627,
      "end_position": 145811,
      "sentences_count": 11
    },
    {
      "chunk_id": 644,
      "text": "29.4 On Good and Bad ERMs\n407\nlearnable by some ERM, but other ERMs will fail to learn it Clearly, this also\nimplies that the class is learnable but it does not have the uniform convergence\nproperty For simplicity, we consider only the realizable case The class we consider is deﬁned as follows The instance space X will be any\nﬁnite or countable set Let Pf(X) be the collection of all ﬁnite and coﬁnite\nsubsets of X (that is, for each A ∈Pf(X), either A or X \\ A must be ﬁnite) Instead of [k], the label set is Y = Pf(X) ∪{∗}, where ∗is some special label For every A ∈Pf(X) deﬁne hA : X →Y by\nhA(x) =\n(\nA\nx ∈A\n∗\nx /∈A\nFinally, the hypothesis class we take is\nH = {hA : A ∈Pf(X)} Let A be some ERM algorithm for H Assume that A operates on a sample\nlabeled by hA ∈H Since hA is the only hypothesis in H that might return\nthe label A, if A observes the label A, it “knows” that the learned hypothesis\nis hA, and, as an ERM, must return it (note that in this case the error of the\nreturned hypothesis is 0) Therefore, to specify an ERM, we should only specify\nthe hypothesis it returns upon receiving a sample of the form\nS = {(x1, ∗), , (xm, ∗)}",
      "word_count": 234,
      "source_page": 407,
      "start_position": 145812,
      "end_position": 146045,
      "sentences_count": 13
    },
    {
      "chunk_id": 645,
      "text": "Therefore, to specify an ERM, we should only specify\nthe hypothesis it returns upon receiving a sample of the form\nS = {(x1, ∗), , (xm, ∗)} We consider two ERMs: The ﬁrst, Agood, is deﬁned by\nAgood(S) = h∅;\nthat is, it outputs the hypothesis which predicts ‘*’ for every x ∈X The second\nERM, Abad, is deﬁned by\nAbad(S) = h{x1,...xm}c The following claim shows that the sample complexity of Abad is about |X|-times\nlarger than the sample complexity of Agood This establishes a gap between\ndiﬀerent ERMs If X is inﬁnite, we even obtain a learnable class that is not\nlearnable by every ERM claim 29.9\n1 Let ϵ, δ > 0, D a distribution over X and hA ∈H Let S be an i.i.d sample\nconsisting of m ≥1\nϵ log\n\u0000 1\nδ\n\u0001\nexamples, sampled according to D and labeled by\nhA Then, with probability of at least 1 −δ, the hypothesis returned by Agood\nwill have an error of at most ϵ 2 There exists a constant a > 0 such that for every 0 < ϵ < a there exists a\ndistribution D over X and hA ∈H such that the following holds The hypoth-\nesis returned by Abad upon receiving a sample of size m ≤|X|−1\n6ϵ\n, sampled\naccording to D and labeled by hA, will have error ≥ϵ with probability ≥e−1\n6 .",
      "word_count": 234,
      "source_page": 407,
      "start_position": 146019,
      "end_position": 146252,
      "sentences_count": 15
    },
    {
      "chunk_id": 646,
      "text": "408\nMulticlass Learnability\nProof\nLet D be a distribution over X and suppose that the correct labeling\nis hA For any sample, Agood returns either h∅or hA If it returns hA then its\ntrue error is zero Thus, it returns a hypothesis with error ≥ϵ only if all the m\nexamples in the sample are from X \\ A while the error of h∅, LD(h∅) = PD[A],\nis ≥ϵ Assume m ≥1\nϵ log( 1\nδ ); then the probability of the latter event is no more\nthan (1 −ϵ)m ≤e−ϵm ≤δ This establishes item 1 Next we prove item 2 We restrict the proof to the case that |X| = d < ∞ The proof for inﬁnite X is similar Suppose that X = {x0, , xd−1} Let a > 0 be small enough such that 1 −2ϵ ≥e−4ϵ for every ϵ < a and ﬁx\nsome ϵ < a Deﬁne a distribution on X by setting P[x0] = 1 −2ϵ and for all\n1 ≤i ≤d −1, P[xi] =\n2ϵ\nd−1 Suppose that the correct hypothesis is h∅and let the\nsample size be m Clearly, the hypothesis returned by Abad will err on all the\nexamples from X which are not in the sample By Chernoﬀ’s bound, if m ≤d−1\n6ϵ ,\nthen with probability ≥e−1\n6 , the sample will include no more than d−1\n2\nexamples\nfrom X Thus the returned hypothesis will have error ≥ϵ",
      "word_count": 240,
      "source_page": 408,
      "start_position": 146253,
      "end_position": 146492,
      "sentences_count": 17
    },
    {
      "chunk_id": 647,
      "text": "By Chernoﬀ’s bound, if m ≤d−1\n6ϵ ,\nthen with probability ≥e−1\n6 , the sample will include no more than d−1\n2\nexamples\nfrom X Thus the returned hypothesis will have error ≥ϵ The conclusion of the example presented is that in multiclass classiﬁcation,\nthe sample complexity of diﬀerent ERMs may diﬀer Are there “good” ERMs\nfor every hypothesis class The following conjecture asserts that the answer is\nyes conjecture 29.10\nThe realizable sample complexity of every hypothesis class\nH ⊂[k]X is\nmH(ϵ, δ) = ˜O\n\u0012Ndim(H)\nϵ\n\u0013 We emphasize that the ˜O notation may hide only poly-log factors of ϵ, δ, and\nNdim(H), but no factor of k 29.5\nBibliographic Remarks\nThe Natarajan dimension is due to Natarajan (1989) That paper also established\nthe Natarajan lemma and the generalization of the fundamental theorem Gen-\neralizations and sharper versions of the Natarajan lemma are studied in Haussler\n& Long (1995) Ben-David, Cesa-Bianchi, Haussler & Long (1995) deﬁned a large\nfamily of notions of dimensions, all of which generalize the VC dimension and\nmay be used to estimate the sample complexity of multiclass classiﬁcation The calculation of the Natarajan dimension, presented here, together with\ncalculation of other classes, can be found in Daniely et al (2012) The example\nof good and bad ERMs, as well as conjecture 29.10, are from Daniely et al (2011).",
      "word_count": 225,
      "source_page": 408,
      "start_position": 146459,
      "end_position": 146683,
      "sentences_count": 15
    },
    {
      "chunk_id": 648,
      "text": "29.6 Exercises\n409\n29.6\nExercises\n1 Let d, k > 0 Show that there exists a binary hypothesis Hbin of VC dimension\nd such that Ndim(HOvA,k\nbin\n) = d 2 Prove Lemma 29.6 3 Prove Natarajan’s lemma Hint: Fix some x0 ∈X For i, j ∈[k], denote by Hij all the functions f :\nX \\ {x0} →[k] that can be extended to a function in H both by deﬁning\nf(x0) = i and by deﬁning f(x0) = j Show that |H| ≤|HX\\{x0}|+P\ni̸=j |Hij|\nand use induction 4 Adapt the proof of the binary fundamental theorem and Natarajan’s lemma\nto prove that, for some universal constant C > 0 and for every hypothesis\nclass of Natarajan dimension d, the agnostic sample complexity of H is\nmH(ϵ, δ) ≤C d log\n\u0000 kd\nϵ\n\u0001\n+ log(1/δ)\nϵ2 5 Prove that, for some universal constant C > 0 and for every hypothesis class\nof Natarajan dimension d, the agnostic sample complexity of H is\nmH(ϵ, δ) ≥C d + log(1/δ)\nϵ2 Hint: Deduce it from the binary fundamental theorem 6 Let H be the binary hypothesis class of (nonhomogenous) halfspaces in Rd The goal of this exercise is to prove that Ndim(HOvA,k) ≥(d −1) · (k −1) 1",
      "word_count": 209,
      "source_page": 409,
      "start_position": 146684,
      "end_position": 146892,
      "sentences_count": 19
    },
    {
      "chunk_id": 649,
      "text": "30\nCompression Bounds\nThroughout the book, we have tried to characterize the notion of learnability\nusing diﬀerent approaches At ﬁrst we have shown that the uniform conver-\ngence property of a hypothesis class guarantees successful learning Later on we\nintroduced the notion of stability and have shown that stable algorithms are\nguaranteed to be good learners Yet there are other properties which may be\nsuﬃcient for learning, and in this chapter and its sequel we will introduce two\napproaches to this issue: compression bounds and the PAC-Bayes approach In this chapter we study compression bounds Roughly speaking, we shall see\nthat if a learning algorithm can express the output hypothesis using a small sub-\nset of the training set, then the error of the hypothesis on the rest of the examples\nestimates its true error In other words, an algorithm that can “compress” its\noutput is a good learner 30.1\nCompression Bounds\nTo motivate the results, let us ﬁrst consider the following learning protocol First, we sample a sequence of k examples denoted T On the basis of these\nexamples, we construct a hypothesis denoted hT Now we would like to estimate\nthe performance of hT so we sample a fresh sequence of m−k examples, denoted\nV , and calculate the error of hT on V Since V and T are independent, we\nimmediately get the following from Bernstein’s inequality (see Lemma B.10) lemma 30.1\nAssume that the range of the loss function is [0, 1]",
      "word_count": 246,
      "source_page": 410,
      "start_position": 147015,
      "end_position": 147260,
      "sentences_count": 13
    },
    {
      "chunk_id": 650,
      "text": "Since V and T are independent, we\nimmediately get the following from Bernstein’s inequality (see Lemma B.10) lemma 30.1\nAssume that the range of the loss function is [0, 1] Then,\nP\n\"\nLD(hT ) −LV (hT ) ≥\ns\n2LV (hT ) log(1/δ)\n|V |\n+ 4 log(1/δ)\n|V |\n#\n≤δ To derive this bound, all we needed was independence between T and V Therefore, we can redeﬁne the protocol as follows First, we agree on a sequence\nof k indices I = (i1, , ik) ∈[m]k Then, we sample a sequence of m examples\nS = (z1, , zm) Now, deﬁne T = SI = (zi1, , zik) and deﬁne V to be the\nrest of the examples in S Note that this protocol is equivalent to the protocol\nwe deﬁned before – hence Lemma 30.1 still holds Applying a union bound over the choice of the sequence of indices we obtain\nthe following theorem Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 187,
      "source_page": 410,
      "start_position": 147231,
      "end_position": 147417,
      "sentences_count": 18
    },
    {
      "chunk_id": 651,
      "text": "30.1 Compression Bounds\n411\ntheorem 30.2\nLet k be an integer and let B : Zk →H be a mapping from\nsequences of k examples to the hypothesis class Let m ≥2k be a training set\nsize and let A : Zm →H be a learning rule that receives a training sequence S\nof size m and returns a hypothesis such that A(S) = B(zi1, , zik) for some\n(i1, , ik) ∈[m]k Let V = {zj : j /∈(i1, , ik)} be the set of examples which\nwere not selected for deﬁning A(S) Then, with probability of at least 1 −δ over\nthe choice of S we have\nLD(A(S)) ≤LV (A(S)) +\nr\nLV (A(S))4k log(m/δ)\nm\n+ 8k log(m/δ)\nm Proof\nFor any I ∈[m]k let hI = B(zi1, , zik) Let n = m −k Combining\nLemma 30.1 with the union bound we have\nP\n\"\n∃I ∈[m]k s.t LD(hI) −LV (hI) ≥\nr\n2LV (hI) log(1/δ)\nn\n+ 4 log(1/δ)\nn\n#\n≤\nX\nI∈[m]k\nP\n\"\nLD(hI) −LV (hI) ≥\nr\n2LV (hI) log(1/δ)\nn\n+ 4 log(1/δ)\nn\n#\n≤mkδ Denote δ′ = mkδ Using the assumption k ≤m/2, which implies that n =\nm −k ≥m/2, the above implies that with probability of at least 1 −δ′ we have\nthat\nLD(A(S)) ≤LV (A(S)) +\nr\nLV (A(S))4k log(m/δ′)\nm\n+ 8k log(m/δ′)\nm\n,\nwhich concludes our proof",
      "word_count": 235,
      "source_page": 411,
      "start_position": 147418,
      "end_position": 147652,
      "sentences_count": 14
    },
    {
      "chunk_id": 652,
      "text": "Denote δ′ = mkδ Using the assumption k ≤m/2, which implies that n =\nm −k ≥m/2, the above implies that with probability of at least 1 −δ′ we have\nthat\nLD(A(S)) ≤LV (A(S)) +\nr\nLV (A(S))4k log(m/δ′)\nm\n+ 8k log(m/δ′)\nm\n,\nwhich concludes our proof As a direct corollary we obtain:\ncorollary 30.3\nAssuming the conditions of Theorem 30.2, and further as-\nsuming that LV (A(S)) = 0, then, with probability of at least 1−δ over the choice\nof S we have\nLD(A(S)) ≤8k log(m/δ)\nm These results motivate the following deﬁnition:\ndefinition 30.4\n(Compression Scheme) Let H be a hypothesis class of\nfunctions from X to Y and let k be an integer We say that H has a compression\nscheme of size k if the following holds:\nFor all m there exists A : Zm →[m]k and B : Zk →H such that for all h ∈H,\nif we feed any training set of the form (x1, h(x1)), , (xm, h(xm)) into A and\nthen feed (xi1, h(xi1)), , (xik, h(xik)) into B, where (i1, , ik) is the output\nof A, then the output of B, denoted h′, satisﬁes LS(h′) = 0 It is possible to generalize the deﬁnition for unrealizable sequences as follows.",
      "word_count": 211,
      "source_page": 411,
      "start_position": 147604,
      "end_position": 147814,
      "sentences_count": 9
    },
    {
      "chunk_id": 653,
      "text": "412\nCompression Bounds\ndefinition 30.5\n(Compression Scheme for Unrealizable Sequences)\nLet H be a hypothesis class of functions from X to Y and let k be an integer We say that H has a compression scheme of size k if the following holds:\nFor all m there exists A : Zm →[m]k and B : Zk →H such that for all h ∈H,\nif we feed any training set of the form (x1, y1), , (xm, ym) into A and then\nfeed (xi1, yi1), , (xik, yik) into B, where (i1, , ik) is the output of A, then\nthe output of B, denoted h′, satisﬁes LS(h′) ≤LS(h) The following lemma shows that the existence of a compression scheme for\nthe realizable case also implies the existence of a compression scheme for the\nunrealizable case lemma 30.6\nLet H be a hypothesis class for binary classiﬁcation, and assume\nit has a compression scheme of size k in the realizable case Then, it has a\ncompression scheme of size k for the unrealizable case as well Proof\nConsider the following scheme: First, ﬁnd an ERM hypothesis and denote\nit by h Then, discard all the examples on which h errs Now, apply the realizable\ncompression scheme on the examples that have not been removed The output of\nthe realizable compression scheme, denoted h′, must be correct on the examples\nthat have not been removed",
      "word_count": 233,
      "source_page": 412,
      "start_position": 147815,
      "end_position": 148047,
      "sentences_count": 12
    },
    {
      "chunk_id": 654,
      "text": "Now, apply the realizable\ncompression scheme on the examples that have not been removed The output of\nthe realizable compression scheme, denoted h′, must be correct on the examples\nthat have not been removed Since h errs on the removed examples it follows\nthat the error of h′ cannot be larger than the error of h; hence h′ is also an ERM\nhypothesis 30.2\nExamples\nIn the examples that follows, we present compression schemes for several hy-\npothesis classes for binary classiﬁcation In light of Lemma 30.6 we focus on the\nrealizable case Therefore, to show that a certain hypothesis class has a com-\npression scheme, it is necessary to show that there exist A, B, and k for which\nLS(h′) = 0 30.2.1\nAxis Aligned Rectangles\nNote that this is an uncountable inﬁnite class We show that there is a simple\ncompression scheme Consider the algorithm A that works as follows: For each\ndimension, choose the two positive examples with extremal values at this dimen-\nsion Deﬁne B to be the function that returns the minimal enclosing rectangle Then, for k = 2d, we have that in the realizable case, LS(B(A(S))) = 0 30.2.2\nHalfspaces\nLet X = Rd and consider the class of homogenous halfspaces, {x 7→sign(⟨w, x⟩) :\nw ∈Rd}.",
      "word_count": 213,
      "source_page": 412,
      "start_position": 148014,
      "end_position": 148226,
      "sentences_count": 12
    },
    {
      "chunk_id": 655,
      "text": "30.2 Examples\n413\nA Compression Scheme:\nW.l.o.g assume all labels are positive (otherwise, replace xi by yixi) The com-\npression scheme we propose is as follows First, A ﬁnds the vector w which is\nin the convex hull of {x1, , xm} and has minimal norm Then, it represents it\nas a convex combination of d points in the sample (it will be shown later that\nthis is always possible) The output of A are these d points The algorithm B\nreceives these d points and set w to be the point in their convex hull of minimal\nnorm Next we prove that this indeed is a compression sceme Since the data is\nlinearly separable, the convex hull of {x1, , xm} does not contain the origin Consider the point w in this convex hull closest to the origin (This is a unique\npoint which is the Euclidean projection of the origin onto this convex hull.) We\nclaim that w separates the data.1 To see this, assume by contradiction that\n⟨w, xi⟩≤0 for some i Take w′ = (1 −α)w + αxi for α =\n∥w∥2\n∥xi∥2+∥w∥2 ∈(0, 1) Then w′ is also in the convex hull and\n∥w′∥2 = (1 −α)2∥w∥2 + α2∥xi∥2 + 2α(1 −α)⟨w, xi⟩\n≤(1 −α)2∥w∥2 + α2∥xi∥2\n= ∥xi∥4∥w∥2 + ∥xi∥2∥w∥4\n(∥w∥2 + ∥xi∥2)2\n=\n∥xi∥2∥w∥2\n∥w∥2 + ∥xi∥2\n= ∥w∥2 ·\n1\n∥w∥2/∥xi∥2 + 1\n< ∥w∥2,\nwhich leads to a contradiction We have thus shown that w is also an ERM",
      "word_count": 248,
      "source_page": 413,
      "start_position": 148227,
      "end_position": 148474,
      "sentences_count": 16
    },
    {
      "chunk_id": 656,
      "text": "Then w′ is also in the convex hull and\n∥w′∥2 = (1 −α)2∥w∥2 + α2∥xi∥2 + 2α(1 −α)⟨w, xi⟩\n≤(1 −α)2∥w∥2 + α2∥xi∥2\n= ∥xi∥4∥w∥2 + ∥xi∥2∥w∥4\n(∥w∥2 + ∥xi∥2)2\n=\n∥xi∥2∥w∥2\n∥w∥2 + ∥xi∥2\n= ∥w∥2 ·\n1\n∥w∥2/∥xi∥2 + 1\n< ∥w∥2,\nwhich leads to a contradiction We have thus shown that w is also an ERM Finally, since w is in the convex\nhull of the examples, we can apply Caratheodory’s theorem to obtain that w is\nalso in the convex hull of a subset of d + 1 points of the polygon Furthermore,\nthe minimality of w implies that w must be on a face of the polygon and this\nimplies it can be represented as a convex combination of d points It remains to show that w is also the projection onto the polygon deﬁned by the\nd points But this must be true: On one hand, the smaller polygon is a subset of\nthe larger one; hence the projection onto the smaller cannot be smaller in norm On the other hand, w itself is a valid solution The uniqueness of projection\nconcludes our proof 30.2.3\nSeparating Polynomials\nLet X = Rd and consider the class x 7→sign(p(x)) where p is a degree r polyno-\nmial 1 It can be shown that w is the direction of the max-margin solution.",
      "word_count": 225,
      "source_page": 413,
      "start_position": 148416,
      "end_position": 148640,
      "sentences_count": 10
    },
    {
      "chunk_id": 657,
      "text": "414\nCompression Bounds\nNote that p(x) can be rewritten as ⟨w, ψ(x)⟩where the elements of ψ(x) are all\nthe monomials of x up to degree r Therefore, the problem of constructing a com-\npression scheme for p(x) reduces to the problem of constructing a compression\nscheme for halfspaces in Rd′ where d′ = O(dr) 30.2.4\nSeparation with Margin\nSuppose that a training set is separated with margin γ The Perceptron algorithm\nguarantees to make at most 1/γ2 updates before converging to a solution that\nmakes no mistakes on the entire training set Hence, we have a compression\nscheme of size k ≤1/γ2 30.3\nBibliographic Remarks\nCompression schemes and their relation to learning were introduced by Little-\nstone & Warmuth (1986) As we have shown, if a class has a compression scheme\nthen it is learnable For binary classiﬁcation problems, it follows from the funda-\nmental theorem of learning that the class has a ﬁnite VC dimension The other\ndirection, namely, whether every hypothesis class of ﬁnite VC dimension has a\ncompression scheme of ﬁnite size, is an open problem posed by Manfred War-\nmuth and is still open (see also (Floyd 1989, Floyd & Warmuth 1995, Ben-David\n& Litman 1998, Livni & Simon 2013).",
      "word_count": 204,
      "source_page": 414,
      "start_position": 148641,
      "end_position": 148844,
      "sentences_count": 9
    },
    {
      "chunk_id": 658,
      "text": "31\nPAC-Bayes\nThe Minimum Description Length (MDL) and Occam’s razor principles allow a\npotentially very large hypothesis class but deﬁne a hierarchy over hypotheses and\nprefer to choose hypotheses that appear higher in the hierarchy In this chapter\nwe describe the PAC-Bayesian approach that further generalizes this idea In\nthe PAC-Bayesian approach, one expresses the prior knowledge by deﬁning prior\ndistribution over the hypothesis class 31.1\nPAC-Bayes Bounds\nAs in the MDL paradigm, we deﬁne a hierarchy over hypotheses in our class H Now, the hierarchy takes the form of a prior distribution over H That is, we\nassign a probability (or density if H is continuous) P(h) ≥0 for each h ∈H\nand refer to P(h) as the prior score of h Following the Bayesian reasoning\napproach, the output of the learning algorithm is not necessarily a single hy-\npothesis Instead, the learning process deﬁnes a posterior probability over H,\nwhich we denote by Q In the context of a supervised learning problem, where\nH contains functions from X to Y, one can think of Q as deﬁning a randomized\nprediction rule as follows Whenever we get a new instance x, we randomly pick\na hypothesis h ∈H according to Q and predict h(x) We deﬁne the loss of Q on\nan example z to be\nℓ(Q, z)\ndef\n=\nE\nh∼Q[ℓ(h, z)]",
      "word_count": 224,
      "source_page": 415,
      "start_position": 148845,
      "end_position": 149068,
      "sentences_count": 11
    },
    {
      "chunk_id": 659,
      "text": "Whenever we get a new instance x, we randomly pick\na hypothesis h ∈H according to Q and predict h(x) We deﬁne the loss of Q on\nan example z to be\nℓ(Q, z)\ndef\n=\nE\nh∼Q[ℓ(h, z)] By the linearity of expectation, the generalization loss and training loss of Q can\nbe written as\nLD(Q)\ndef\n=\nE\nh∼Q[LD(h)]\nand\nLS(Q)\ndef\n=\nE\nh∼Q[LS(h)] The following theorem tells us that the diﬀerence between the generalization\nloss and the empirical loss of a posterior Q is bounded by an expression that\ndepends on the Kullback-Leibler divergence between Q and the prior distribu-\ntion P The Kullback-Leibler is a natural measure of the distance between two\ndistributions The theorem suggests that if we would like to minimize the gen-\neralization loss of Q, we should jointly minimize both the empirical loss of Q\nand the Kullback-Leibler distance between Q and the prior distribution We will\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 185,
      "source_page": 415,
      "start_position": 149030,
      "end_position": 149214,
      "sentences_count": 11
    },
    {
      "chunk_id": 660,
      "text": "416\nPAC-Bayes\nlater show how in some cases this idea leads to the regularized risk minimization\nprinciple theorem 31.1\nLet D be an arbitrary distribution over an example domain Z Let H be a hypothesis class and let ℓ: H×Z →[0, 1] be a loss function Let P be\na prior distribution over H and let δ ∈(0, 1) Then, with probability of at least\n1−δ over the choice of an i.i.d training set S = {z1, , zm} sampled according\nto D, for all distributions Q over H (even such that depend on S), we have\nLD(Q) ≤LS(Q) +\ns\nD(Q||P) + ln m/δ\n2(m −1)\n,\nwhere\nD(Q||P)\ndef\n=\nE\nh∼Q[ln(Q(h)/P(h))]\nis the Kullback-Leibler divergence Proof\nFor any function f(S), using Markov’s inequality:\nP\nS[f(S) ≥ϵ] = P\nS[ef(S) ≥eϵ] ≤ES[ef(S)]\neϵ (31.1)\nLet ∆(h) = LD(h) −LS(h) We will apply Equation (31.1) with the function\nf(S) = sup\nQ\n\u0012\n2(m −1) E\nh∼Q(∆(h))2 −D(Q||P)\n\u0013 We now turn to bound ES[ef(S)] The main trick is to upper bound f(S) by\nusing an expression that does not depend on Q but rather depends on the prior\nprobability P To do so, ﬁx some S and note that from the deﬁnition of D(Q||P)\nwe get that for all Q,\n2(m −1) E\nh∼Q(∆(h))2 −D(Q||P) =\nE\nh∼Q[ln(e2(m−1)∆(h)2P(h)/Q(h))]\n≤ln E\nh∼Q[e2(m−1)∆(h)2P(h)/Q(h)]\n= ln E\nh∼P[e2(m−1)∆(h)2],\n(31.2)\nwhere the inequality follows from Jensen’s inequality and the concavity of the\nlog function Therefore,\nE\nS[ef(S)] ≤E\nS E\nh∼P[e2(m−1)∆(h)2]",
      "word_count": 249,
      "source_page": 416,
      "start_position": 149215,
      "end_position": 149463,
      "sentences_count": 14
    },
    {
      "chunk_id": 661,
      "text": "31.2 Bibliographic Remarks\n417\nNext, we claim that for all h we have ES[e2(m−1)∆(h)2] ≤m To do so, recall that\nHoeﬀding’s inequality tells us that\nP\nS[∆(h) ≥ϵ] ≤e−2mϵ2 This implies that ES[e2(m−1)∆(h)2] ≤m (see Exercise 1) Combining this with\nEquation (31.4) and plugging into Equation (31.1) we get\nP\nS[f(S) ≥ϵ] ≤m\neϵ (31.5)\nDenote the right-hand side of the above δ, thus ϵ = ln(m/δ), and we therefore\nobtain that with probability of at least 1 −δ we have that for all Q\n2(m −1) E\nh∼Q(∆(h))2 −D(Q||P) ≤ϵ = ln(m/δ) Rearranging the inequality and using Jensen’s inequality again (the function x2\nis convex) we conclude that\n\u0012\nE\nh∼Q ∆(h)\n\u00132\n≤\nE\nh∼Q(∆(h))2 ≤ln(m/δ) + D(Q||P)\n2(m −1) (31.6)\nRemark 31.1 (Regularization)\nThe PAC-Bayes bound leads to the following\nlearning rule:\nGiven a prior P, return a posterior Q that minimizes the function\nLS(Q) +\ns\nD(Q||P) + ln m/δ\n2(m −1) (31.7)\nThis rule is similar to the regularized risk minimization principle That is, we\njointly minimize the empirical loss of Q on the sample and the Kullback-Leibler\n“distance” between Q and P 31.2\nBibliographic Remarks\nPAC-Bayes bounds were ﬁrst introduced by McAllester (1998) See also (McAllester\n1999, McAllester 2003, Seeger 2003, Langford & Shawe-Taylor 2003, Langford\n2006) 31.3\nExercises\n1 Let X be a random variable that satisﬁes P[X ≥ϵ] ≤e−2mϵ2 Prove that\nE[e2(m−1)X2] ≤m.",
      "word_count": 231,
      "source_page": 417,
      "start_position": 149506,
      "end_position": 149736,
      "sentences_count": 14
    },
    {
      "chunk_id": 662,
      "text": "Appendix A Technical Lemmas\nlemma A.1\nLet a > 0 Then: x ≥2a log(a) ⇒x ≥a log(x) It follows that a\nnecessary condition for the inequality x < a log(x) to hold is that x < 2a log(a) Proof\nFirst note that for a ∈(0, √e ] the inequality x ≥a log(x) holds uncon-\nditionally and therefore the claim is trivial From now on, assume that a > √e Consider the function f(x) = x −a log(x) The derivative is f ′(x) = 1 −a/x Thus, for x > a the derivative is positive and the function increases In addition,\nf(2a log(a)) = 2a log(a) −a log(2a log(a))\n= 2a log(a) −a log(a) −a log(2 log(a))\n= a log(a) −a log(2 log(a)) Since a −2 log(a) > 0 for all a > 0, the proof follows lemma A.2\nLet a ≥1 and b > 0 Then: x ≥4a log(2a)+2b ⇒x ≥a log(x)+b Proof\nIt suﬃces to prove that x ≥4a log(2a) + 2b implies that both x ≥\n2a log(x) and x ≥2b Since we assume a ≥1 we clearly have that x ≥2b In addition, since b > 0 we have that x ≥4a log(2a) which using Lemma A.1\nimplies that x ≥2a log(x) This concludes our proof lemma A.3\nLet X be a random variable and x′ ∈R be a scalar and assume\nthat there exists a > 0 such that for all t ≥0 we have P[|X −x′| > t] ≤2e−t2/a2 Then, E[|X −x′|] ≤4 a",
      "word_count": 249,
      "source_page": 419,
      "start_position": 149815,
      "end_position": 150063,
      "sentences_count": 18
    },
    {
      "chunk_id": 663,
      "text": "lemma A.3\nLet X be a random variable and x′ ∈R be a scalar and assume\nthat there exists a > 0 such that for all t ≥0 we have P[|X −x′| > t] ≤2e−t2/a2 Then, E[|X −x′|] ≤4 a Proof\nFor all i = 0, 1, 2, denote ti = a i Since ti is monotonically increasing\nwe have that E[|X −x′|] is at most P∞\ni=1 ti P[|X −x′| > ti−1] Combining this\nwith the assumption in the lemma we get that E[|X −x′|] ≤2 a P∞\ni=1 ie−(i−1)2 The proof now follows from the inequalities\n∞\nX\ni=1\nie−(i−1)2 ≤\n5\nX\ni=1\nie−(i−1)2 +\nZ ∞\n5\nxe−(x−1)2dx < 1.8 + 10−7 < 2 lemma A.4\nLet X be a random variable and x′ ∈R be a scalar and assume\nthat there exists a > 0 and b ≥e such that for all t ≥0 we have P[|X −x′| >\nt] ≤2b e−t2/a2 Then, E[|X −x′|] ≤a(2 +\np\nlog(b)) Understanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution Do not post Please link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning",
      "word_count": 193,
      "source_page": 419,
      "start_position": 150024,
      "end_position": 150216,
      "sentences_count": 14
    },
    {
      "chunk_id": 664,
      "text": "420\nTechnical Lemmas\nProof\nFor all i = 0, 1, 2, denote ti = a (i+\np\nlog(b)) Since ti is monotonically\nincreasing we have that\nE[|X −x′|] ≤a\np\nlog(b) +\n∞\nX\ni=1\nti P[|X −x′| > ti−1] Using the assumption in the lemma we have\n∞\nX\ni=1\nti P[|X −x′| > ti−1] ≤2 a b\n∞\nX\ni=1\n(i +\np\nlog(b))e−(i−1+√\nlog(b))2\n≤2 a b\nZ ∞\n1+√\nlog(b)\nxe−(x−1)2dx\n= 2 a b\nZ ∞\n√\nlog(b)\n(y + 1)e−y2dy\n≤4 a b\nZ ∞\n√\nlog(b)\nye−y2dy\n= 2 a b\nh\n−e−y2i∞\n√\nlog(b)\n= 2 a b/b = 2 a Combining the preceding inequalities we conclude our proof lemma A.5\nLet m, d be two positive integers such that d ≤m −2 Then,\nd\nX\nk=0\n\u0012m\nk\n\u0013\n≤\n\u0010e m\nd\n\u0011d Proof\nWe prove the claim by induction For d = 1 the left-hand side equals\n1 + m while the right-hand side equals em; hence the claim is true Assume that\nthe claim holds for d and let us prove it for d + 1 By the induction assumption\nwe have\nd+1\nX\nk=0\n\u0012m\nk\n\u0013\n≤\n\u0010e m\nd\n\u0011d\n+\n\u0012 m\nd + 1\n\u0013\n=\n\u0010e m\nd\n\u0011d\n \n1 +\n\u0012 d\ne m\n\u0013d m(m −1)(m −2) · · · (m −d)\n(d + 1)d ≤\n\u0010em\nd\n\u0011d\n \n1 +\n\u0012d\ne\n\u0013d (m −d)\n(d + 1)d .",
      "word_count": 250,
      "source_page": 420,
      "start_position": 150217,
      "end_position": 150466,
      "sentences_count": 13
    },
    {
      "chunk_id": 665,
      "text": "Technical Lemmas\n421\nUsing Stirling’s approximation we further have that\n≤\n\u0010e m\nd\n\u0011d\n \n1 +\n\u0012d\ne\n\u0013d\n(m −d)\n(d + 1)\n√\n2πd(d/e)d =\n\u0010e m\nd\n\u0011d \u0012\n1 +\nm −d\n√\n2πd(d + 1)\n\u0013\n=\n\u0010e m\nd\n\u0011d\n· d + 1 + (m −d)/\n√\n2πd\nd + 1\n≤\n\u0010e m\nd\n\u0011d\n· d + 1 + (m −d)/2\nd + 1\n=\n\u0010e m\nd\n\u0011d\n· d/2 + 1 + m/2\nd + 1\n≤\n\u0010e m\nd\n\u0011d\n·\nm\nd + 1,\nwhere in the last inequality we used the assumption that d ≤m −2 On the\nother hand,\n\u0012 e m\nd + 1\n\u0013d+1\n=\n\u0010e m\nd\n\u0011d\n· em\nd + 1 ·\n\u0012\nd\nd + 1\n\u0013d\n=\n\u0010e m\nd\n\u0011d\n· em\nd + 1 ·\n1\n(1 + 1/d)d\n≥\n\u0010e m\nd\n\u0011d\n· em\nd + 1 · 1\ne\n=\n\u0010e m\nd\n\u0011d\n·\nm\nd + 1,\nwhich proves our inductive argument lemma A.6\nFor all a ∈R we have\nea + e−a\n2\n≤ea2/2 Proof\nObserve that\nea =\n∞\nX\nn=0\nan\nn Therefore,\nea + e−a\n2\n=\n∞\nX\nn=0\na2n\n(2n)!,\nand\nea2/2 =\n∞\nX\nn=0\na2n\n2n n Observing that (2n) ≥2n n for every n ≥0 we conclude our proof.",
      "word_count": 238,
      "source_page": 421,
      "start_position": 150467,
      "end_position": 150704,
      "sentences_count": 9
    },
    {
      "chunk_id": 666,
      "text": "Appendix B Measure Concentration\nLet Z1, , Zm be an i.i.d sequence of random variables and let µ be their mean The strong law of large numbers states that when m tends to inﬁnity, the em-\npirical average,\n1\nm\nPm\ni=1 Zi, converges to the expected value µ, with probability\n1 Measure concentration inequalities quantify the deviation of the empirical\naverage from the expectation when m is ﬁnite B.1\nMarkov’s Inequality\nWe start with an inequality which is called Markov’s inequality Let Z be a\nnonnegative random variable The expectation of Z can be written as follows:\nE[Z] =\nZ ∞\nx=0\nP[Z ≥x]dx (B.1)\nSince P[Z ≥x] is monotonically nonincreasing we obtain\n∀a ≥0,\nE[Z] ≥\nZ a\nx=0\nP[Z ≥x]dx ≥\nZ a\nx=0\nP[Z ≥a]dx = a P[Z ≥a] (B.2)\nRearranging the inequality yields Markov’s inequality:\n∀a ≥0,\nP[Z ≥a] ≤E[Z]\na (B.3)\nFor random variables that take value in [0, 1], we can derive from Markov’s\ninequality the following lemma B.1\nLet Z be a random variable that takes values in [0, 1] Assume that\nE[Z] = µ Then, for any a ∈(0, 1),\nP[Z > 1 −a] ≥µ −(1 −a)\na This also implies that for every a ∈(0, 1),\nP[Z > a] ≥µ −a\n1 −a ≥µ −a Proof\nLet Y = 1 −Z Then Y is a nonnegative random variable with E[Y ] =\n1 −E[Z] = 1 −µ",
      "word_count": 237,
      "source_page": 422,
      "start_position": 150705,
      "end_position": 150941,
      "sentences_count": 17
    },
    {
      "chunk_id": 667,
      "text": "B.2 Chebyshev’s Inequality\n423\nTherefore,\nP[Z > 1 −a] ≥1 −1 −µ\na\n= a + µ −1\na B.2\nChebyshev’s Inequality\nApplying Markov’s inequality on the random variable (Z −E[Z])2 we obtain\nChebyshev’s inequality:\n∀a > 0,\nP[|Z −E[Z]| ≥a] = P[(Z −E[Z])2 ≥a2] ≤Var[Z]\na2\n,\n(B.4)\nwhere Var[Z] = E[(Z −E[Z])2] is the variance of Z Consider the random variable\n1\nm\nPm\ni=1 Zi Since Z1, , Zm are i.i.d it is easy\nto verify that\nVar\n\"\n1\nm\nm\nX\ni=1\nZi\n#\n= Var[Z1]\nm Applying Chebyshev’s inequality, we obtain the following:\nlemma B.2\nLet Z1, , Zm be a sequence of i.i.d random variables and assume\nthat E[Z1] = µ and Var[Z1] ≤1 Then, for any δ ∈(0, 1), with probability of at\nleast 1 −δ we have\n\f\f\f\f\f\n1\nm\nm\nX\ni=1\nZi −µ\n\f\f\f\f\f ≤\nr\n1\nδ m Proof\nApplying Chebyshev’s inequality we obtain that for all a > 0\nP\n\"\f\f\f\f\f\n1\nm\nm\nX\ni=1\nZi −µ\n\f\f\f\f\f > a\n#\n≤Var[Z1]\nm a2\n≤\n1\nm a2 The proof follows by denoting the right-hand side δ and solving for a The deviation between the empirical average and the mean given previously\ndecreases polynomially with m It is possible to obtain a signiﬁcantly faster\ndecrease In the sections that follow we derive bounds that decrease exponentially\nfast B.3\nChernoﬀ’s Bounds\nLet Z1,",
      "word_count": 233,
      "source_page": 423,
      "start_position": 150995,
      "end_position": 151227,
      "sentences_count": 16
    },
    {
      "chunk_id": 668,
      "text": "424\nMeasure Concentration\nmonotonicity of the exponent function and Markov’s inequality, we have that for\nevery t > 0\nP[Z > (1 + δ)p] = P[etZ > et(1+δ)p] ≤E[etZ]\ne(1+δ)tp (B.5)\nNext,\nE[etZ] = E[et P\ni Zi] = E[\nY\ni\netZi]\n=\nY\ni\nE[etZi]\nby independence\n=\nY\ni\n\u0000piet + (1 −pi)e0\u0001\n=\nY\ni\n\u00001 + pi(et −1)\n\u0001\n≤\nY\ni\nepi(et−1)\nusing 1 + x ≤ex\n= e\nP\ni pi(et−1)\n= e(et−1)p Combining the above with Equation (B.5) and choosing t = log(1+δ) we obtain\nlemma B.3\nLet Z1, , Zm be independent Bernoulli variables where for every\ni, P[Zi = 1] = pi and P[Zi = 0] = 1 −pi Let p = Pm\ni=1 pi and let Z = Pm\ni=1 Zi Then, for any δ > 0,\nP[Z > (1 + δ)p] ≤e−h(δ) p,\nwhere\nh(δ) = (1 + δ) log(1 + δ) −δ Using the inequality h(a) ≥a2/(2 + 2a/3) we obtain\nlemma B.4\nUsing the notation of Lemma B.3 we also have\nP[Z > (1 + δ)p] ≤e−p\nδ2\n2+2δ/3 For the other direction, we apply similar calculations:\nP[Z < (1−δ)p] = P[−Z > −(1−δ)p] = P[e−tZ > e−t(1−δ)p] ≤E[e−tZ]\ne−(1−δ)tp , (B.6)",
      "word_count": 206,
      "source_page": 424,
      "start_position": 151265,
      "end_position": 151470,
      "sentences_count": 8
    },
    {
      "chunk_id": 669,
      "text": "B.4 Hoeﬀding’s Inequality\n425\nand,\nE[e−tZ] = E[e−t P\ni Zi] = E[\nY\ni\ne−tZi]\n=\nY\ni\nE[e−tZi]\nby independence\n=\nY\ni\n\u00001 + pi(e−t −1)\n\u0001\n≤\nY\ni\nepi(e−t−1)\nusing 1 + x ≤ex\n= e(e−t−1)p Setting t = −log(1 −δ) yields\nP[Z < (1 −δ)p] ≤\ne−δp\ne(1−δ) log(1−δ) p = e−ph(−δ) It is easy to verify that h(−δ) ≥h(δ) and hence\nlemma B.5\nUsing the notation of Lemma B.3 we also have\nP[Z < (1 −δ)p] ≤e−ph(−δ) ≤e−ph(δ) ≤e−p\nδ2\n2+2δ/3 B.4\nHoeﬀding’s Inequality\nlemma B.6 (Hoeﬀding’s inequality)\nLet Z1, , Zm be a sequence of i.i.d random variables and let ¯Z =\n1\nm\nPm\ni=1 Zi Assume that E[ ¯Z] = µ and P[a ≤\nZi ≤b] = 1 for every i Then, for any ϵ > 0\nP\n\"\f\f\f\f\f\n1\nm\nm\nX\ni=1\nZi −µ\n\f\f\f\f\f > ϵ\n#\n≤2 exp\n\u0000−2 m ϵ2/(b −a)2\u0001 Proof\nDenote Xi = Zi −E[Zi] and ¯X =\n1\nm\nP\ni Xi Using the monotonicity of\nthe exponent function and Markov’s inequality, we have that for every λ > 0\nand ϵ > 0,\nP[ ¯X ≥ϵ] = P[eλ ¯\nX ≥eλϵ] ≤e−λϵ E[eλ ¯\nX] Using the independence assumption we also have\nE[eλ ¯\nX] = E\n\"Y\ni\neλXi/m\n#\n=\nY\ni\nE[eλXi/m] By Hoeﬀding’s lemma (Lemma B.7 later), for every i we have\nE[eλXi/m] ≤e\nλ2(b−a)2\n8m2\n.",
      "word_count": 239,
      "source_page": 425,
      "start_position": 151471,
      "end_position": 151709,
      "sentences_count": 12
    },
    {
      "chunk_id": 670,
      "text": "426\nMeasure Concentration\nTherefore,\nP[ ¯X ≥ϵ] ≤e−λϵ Y\ni\ne\nλ2(b−a)2\n8m2\n= e−λϵ+ λ2(b−a)2\n8m Setting λ = 4mϵ/(b −a)2 we obtain\nP[ ¯X ≥ϵ] ≤e\n−2mϵ2\n(b−a)2 Applying the same arguments on the variable −¯X we obtain that P[ ¯X ≤−ϵ] ≤\ne\n−2mϵ2\n(b−a)2 The theorem follows by applying the union bound on the two cases lemma B.7 (Hoeﬀding’s lemma)\nLet X be a random variable that takes values\nin the interval [a, b] and such that E[X] = 0 Then, for every λ > 0,\nE[eλX] ≤e\nλ2(b−a)2\n8 Proof\nSince f(x) = eλx is a convex function, we have that for every α ∈(0, 1),\nand x ∈[a, b],\nf(x) ≤αf(a) + (1 −α)f(b) Setting α = b−x\nb−a ∈[0, 1] yields\neλx ≤b −x\nb −aeλa + x −a\nb −a eλb Taking the expectation, we obtain that\nE[eλX] ≤b −E[X]\nb −a\neλa + E[x] −a\nb −a\neλb =\nb\nb −aeλa −\na\nb −aeλb,\nwhere we used the fact that E[X] = 0 Denote h = λ(b −a), p =\n−a\nb−a, and\nL(h) = −hp + log(1 −p + peh) Then, the expression on the right-hand side of\nthe above can be rewritten as eL(h) Therefore, to conclude our proof it suﬃces\nto show that L(h) ≤\nh2\n8 This follows from Taylor’s theorem using the facts:\nL(0) = L′(0) = 0 and L′′(h) ≤1/4 for all h",
      "word_count": 240,
      "source_page": 426,
      "start_position": 151710,
      "end_position": 151949,
      "sentences_count": 13
    },
    {
      "chunk_id": 671,
      "text": "B.5 Bennet’s and Bernstein’s Inequalities\n427\nThen for all ϵ > 0,\nP\n\" m\nX\ni=1\nZi > ϵ\n#\n≤e−mσ2h(\nϵ\nmσ2 ) where\nh(a) = (1 + a) log(1 + a) −a By using the inequality h(a) ≥a2/(2 + 2a/3) it is possible to derive the\nfollowing:\nlemma B.9 (Bernstein’s inequality)\nLet Z1, , Zm be i.i.d random variables\nwith a zero mean If for all i, P(|Zi| < M) = 1, then for all t > 0 :\nP\n\" m\nX\ni=1\nZi > t\n#\n≤exp\n \n−\nt2/2\nP E Z2\nj + Mt/3 B.5.1\nApplication\nBernstein’s inequality can be used to interpolate between the rate 1/ϵ we derived\nfor PAC learning in the realizable case (in Chapter 2) and the rate 1/ϵ2 we derived\nfor the unrealizable case (in Chapter 4) lemma B.10\nLet ℓ: H × Z →[0, 1] be a loss function Let D be an arbitrary\ndistribution over Z Fix some h Then, for any δ ∈(0, 1) we have\n1 P\nS∼Dm\n\"\nLS(h) ≥LD(h) +\nr\n2LD(h) log(1/δ)\n3 m\n+ 2 log(1/δ)\nm\n#\n≤δ\n2 P\nS∼Dm\n\"\nLD(h) ≥LS(h) +\nr\n2LS(h) log(1/δ)\nm\n+ 4 log(1/δ)\nm\n#\n≤δ\nProof\nDeﬁne random variables α1, , αm s.t αi = ℓ(h, zi) −LD(h)",
      "word_count": 218,
      "source_page": 427,
      "start_position": 152025,
      "end_position": 152242,
      "sentences_count": 15
    },
    {
      "chunk_id": 672,
      "text": "428\nMeasure Concentration\nSolving for t yields\nt2/2\nm LD(h) + t/3 = log(1/δ)\n⇒\nt2/2 −log(1/δ)\n3\nt −log(1/δ) m LD(h) = 0\n⇒\nt = log(1/δ)\n3\n+\ns\nlog2(1/δ)\n32\n+ 2 log(1/δ) m LD(h)\n≤2 log(1/δ)\n3\n+\np\n2 log(1/δ) m LD(h)\nSince 1\nm\nP\ni αi = LS(h)−LD(h), it follows that with probability of at least 1−δ,\nLS(h) −LD(h) ≤2 log(1/δ)\n3m\n+\nr\n2 log(1/δ) LD(h)\nm\n,\nwhich proves the ﬁrst inequality The second part of the lemma follows in a\nsimilar way B.6\nSlud’s Inequality\nLet X be a (m, p) binomial variable That is, X = Pm\ni=1 Zi, where each Zi is 1\nwith probability p and 0 with probability 1−p Assume that p = (1−ϵ)/2 Slud’s\ninequality (Slud 1977) tells us that P[X ≥m/2] is lower bounded by the proba-\nbility that a normal variable will be greater than or equal to\np\nmϵ2/(1 −ϵ2) The\nfollowing lemma follows by standard tail bounds for the normal distribution lemma B.11\nLet X be a (m, p) binomial variable and assume that p = (1−ϵ)/2 Then,\nP[X ≥m/2] ≥1\n2\n\u0010\n1 −\np\n1 −exp(−mϵ2/(1 −ϵ2))\n\u0011 B.7\nConcentration of χ2 Variables\nLet X1, , Xk be k independent normally distributed random variables That\nis, for all i, Xi ∼N(0, 1)",
      "word_count": 223,
      "source_page": 428,
      "start_position": 152328,
      "end_position": 152550,
      "sentences_count": 12
    },
    {
      "chunk_id": 673,
      "text": "B.7 Concentration of χ2 Variables\n429\nFinally, for all ϵ ∈(0, 3),\nP [(1 −ϵ)k ≤Z ≤(1 + ϵ)k] ≥1 −2e−ϵ2k/6 Proof\nLet us write Z = Pk\ni=1 X2\ni where Xi ∼N(0, 1) To prove both bounds\nwe use Chernoﬀ’s bounding method For the ﬁrst inequality, we ﬁrst bound\nE[e−λX2\n1 ], where λ > 0 will be speciﬁed later Since e−a ≤1−a+ a2\n2 for all a ≥0\nwe have that\nE[e−λX2\n1 ] ≤1 −λ E[X2\n1] + λ2\n2 E[X4\n1] Using the well known equalities, E[X2\n1] = 1 and E[X4\n1] = 3, and the fact that\n1 −a ≤e−a we obtain that\nE[e−λX2\n1 ] ≤1 −λ + 3\n2λ2 ≤e−λ+ 3\n2 λ2 Now, applying Chernoﬀ’s bounding method we get that\nP[−Z ≥−(1 −ϵ)k] = P\nh\ne−λZ ≥e−(1−ϵ)kλi\n≤e(1−ϵ)kλ E\n\u0002\ne−λZ\u0003\n= e(1−ϵ)kλ \u0010\nE\nh\ne−λX2\n1\ni\u0011k\n≤e(1−ϵ)kλ e−λk+ 3\n2 λ2k\n= e−ϵkλ+ 3\n2 kλ2 Choose λ = ϵ/3 we obtain the ﬁrst inequality stated in the lemma For the second inequality, we use a known closed form expression for the\nmoment generating function of a χ2\nk distributed random variable:\n∀λ < 1\n2,\nE\nh\neλZ2i\n= (1 −2λ)−k/2 (B.7)\nOn the basis of the equation and using Chernoﬀ’s bounding method we have\nP[Z ≥(1 + ϵ)k)] = P\nh\neλZ ≥e(1+ϵ)kλi\n≤e−(1+ϵ)kλ E\n\u0002\neλZ\u0003\n= e−(1+ϵ)kλ (1 −2λ)−k/2\n≤e−(1+ϵ)kλ ekλ = e−ϵkλ,\nwhere the last inequality occurs because (1 −a) ≤e−a",
      "word_count": 250,
      "source_page": 429,
      "start_position": 152646,
      "end_position": 152895,
      "sentences_count": 10
    },
    {
      "chunk_id": 674,
      "text": "Appendix C Linear Algebra\nC.1\nBasic Deﬁnitions\nIn this chapter we only deal with linear algebra over ﬁnite dimensional Euclidean\nspaces We refer to vectors as column vectors Given two d dimensional vectors u, v ∈Rd, their inner product is\n⟨u, v⟩=\nd\nX\ni=1\nuivi The Euclidean norm (a.k.a the ℓ2 norm) is ∥u∥=\np\n⟨u, u⟩ We also use the ℓ1\nnorm, ∥u∥1 = Pd\ni=1 |ui| and the ℓ∞norm ∥u∥∞= maxi |ui| A subspace of Rd is a subset of Rd which is closed under addition and scalar\nmultiplication The span of a set of vectors u1, , uk is the subspace containing\nall vectors of the form\nk\nX\ni=1\nαiui\nwhere for all i, αi ∈R A set of vectors U = {u1, , uk} is independent if for every i, ui is not in the\nspan of u1, , ui−1, ui+1, , uk We say that U spans a subspace V if V is the\nspan of the vectors in U We say that U is a basis of V if it is both independent\nand spans V The dimension of V is the size of a basis of V (and it can be veriﬁed\nthat all bases of V have the same size) We say that U is an orthogonal set if for\nall i ̸= j, ⟨ui, uj⟩= 0 We say that U is an orthonormal set if it is orthogonal\nand if for every i, ∥ui∥= 1",
      "word_count": 246,
      "source_page": 430,
      "start_position": 152931,
      "end_position": 153176,
      "sentences_count": 18
    },
    {
      "chunk_id": 675,
      "text": "C.2 Eigenvalues and Eigenvectors\n431\nC.2\nEigenvalues and Eigenvectors\nLet A ∈Rd,d be a matrix A non-zero vector u is an eigenvector of A with a\ncorresponding eigenvalue λ if\nAu = λu theorem C.1 (Spectral Decomposition)\nIf A ∈Rd,d is a symmetric matrix of\nrank k, then there exists an orthonormal basis of Rd, u1, , ud, such that each\nui is an eigenvector of A Furthermore, A can be written as A = Pd\ni=1 λiuiu⊤\ni ,\nwhere each λi is the eigenvalue corresponding to the eigenvector ui This can\nbe written equivalently as A = UDU ⊤, where the columns of U are the vectors\nu1, , ud, and D is a diagonal matrix with Di,i = λi and for i ̸= j, Di,j =\n0 Finally, the number of λi which are nonzero is the rank of the matrix, the\neigenvectors which correspond to the nonzero eigenvalues span the range of A,\nand the eigenvectors which correspond to zero eigenvalues span the null space of\nA C.3\nPositive deﬁnite matrices\nA symmetric matrix A ∈Rd,d is positive deﬁnite if all its eigenvalues are positive A is positive semideﬁnite if all its eigenvalues are nonnegative theorem C.2\nLet A ∈Rd,d be a symmetric matrix Then, the following are\nequivalent deﬁnitions of positive semideﬁniteness of A:\n• All the eigenvalues of A are nonnegative • For every vector u, ⟨u, Au⟩≥0 • There exists a matrix B such that A = BB⊤",
      "word_count": 245,
      "source_page": 431,
      "start_position": 153280,
      "end_position": 153524,
      "sentences_count": 14
    },
    {
      "chunk_id": 676,
      "text": "432\nLinear Algebra\nlemma C.3\nLet A ∈Rm,n be a matrix of rank r Assume that v1, , vr is an\northonormal set of right singular vectors of A, u1, , ur is an orthonormal set\nof corresponding left singular vectors of A, and σ1, , σr are the corresponding\nsingular values Then,\nA =\nr\nX\ni=1\nσiuiv⊤\ni It follows that if U is a matrix whose columns are the ui’s, V is a matrix whose\ncolumns are the vi’s, and D is a diagonal matrix with Di,i = σi, then\nA = UDV ⊤ Proof\nAny right singular vector of A must be in the range of A⊤(otherwise,\nthe singular value will have to be zero) Therefore, v1, , vr is an orthonormal\nbasis of the range of A Let us complete it to an orthonormal basis of Rn by\nadding the vectors vr+1, , vn Deﬁne B = Pr\ni=1 σiuiv⊤\ni It suﬃces to prove\nthat for all i, Avi = Bvi Clearly, if i > r then Avi = 0 and Bvi = 0 as well For i ≤r we have\nBvi =\nr\nX\nj=1\nσjujv⊤\nj vi = σiui = Avi,\nwhere the last equality follows from the deﬁnition The next lemma relates the singular values of A to the eigenvalues of A⊤A\nand AA⊤",
      "word_count": 223,
      "source_page": 432,
      "start_position": 153655,
      "end_position": 153877,
      "sentences_count": 17
    },
    {
      "chunk_id": 677,
      "text": "For i ≤r we have\nBvi =\nr\nX\nj=1\nσjujv⊤\nj vi = σiui = Avi,\nwhere the last equality follows from the deﬁnition The next lemma relates the singular values of A to the eigenvalues of A⊤A\nand AA⊤ lemma C.4\nv, u are right and left singular vectors of A with singular value σ\niﬀv is an eigenvector of A⊤A with corresponding eigenvalue σ2 and u = σ−1Av\nis an eigenvector of AA⊤with corresponding eigenvalue σ2 Proof\nSuppose that σ is a singular value of A with v ∈Rn being the corre-\nsponding right singular vector Then,\nA⊤Av = σA⊤u = σ2v Similarly,\nAA⊤u = σAv = σ2u For the other direction, if λ ̸= 0 is an eigenvalue of A⊤A, with v being the\ncorresponding eigenvector, then λ > 0 because A⊤A is positive semideﬁnite Let\nσ =\n√\nλ, u = σ−1Av Then,\nσu =\n√\nλAv\n√\nλ\n= Av,\nand\nA⊤u = 1\nσ A⊤Av = λ\nσ v = σv.",
      "word_count": 168,
      "source_page": 432,
      "start_position": 153837,
      "end_position": 154004,
      "sentences_count": 9
    },
    {
      "chunk_id": 678,
      "text": "C.4 Singular Value Decomposition (SVD)\n433\nFinally, we show that if A has rank r then it has r orthonormal singular\nvectors lemma C.5\nLet A ∈Rm,n with rank r Deﬁne the following vectors:\nv1 =\nargmax\nv∈Rn:∥v∥=1\n∥Av∥\nv2 =\nargmax\nv∈Rn:∥v∥=1\n⟨v,v1⟩=0\n∥Av∥ vr =\nargmax\nv∈Rn:∥v∥=1\n∀i<r, ⟨v,vi⟩=0\n∥Av∥\nThen, v1, , vr is an orthonormal set of right singular vectors of A Proof\nFirst note that since the rank of A is r, the range of A is a subspace of\ndimension r, and therefore it is easy to verify that for all i = 1, , r, ∥Avi∥> 0 Let W ∈Rn,n be an orthonormal matrix obtained by the eigenvalue decompo-\nsition of A⊤A, namely, A⊤A = WDW ⊤, with D being a diagonal matrix with\nD1,1 ≥D2,2 ≥· · · ≥0 We will show that v1, , vr are eigenvectors of A⊤A\nthat correspond to nonzero eigenvalues, and, hence, using Lemma C.4 it follows\nthat these are also right singular vectors of A The proof is by induction For the\nbasis of the induction, note that any unit vector v can be written as v = Wx,\nfor x = W ⊤v, and note that ∥x∥= 1 Therefore,\n∥Av∥2 = ∥AWx∥2 = ∥WDW ⊤Wx∥2 = ∥WDx∥2 = ∥Dx∥2 =\nn\nX\ni=1\nD2\ni,ixi\n2 Therefore,\nmax\nv:∥v∥=1 ∥Av∥2 =\nmax\nx:∥x∥=1\nn\nX\ni=1\nD2\ni,ixi\n2 The solution of the right-hand side is to set x = (1, 0,",
      "word_count": 247,
      "source_page": 433,
      "start_position": 154005,
      "end_position": 154251,
      "sentences_count": 15
    },
    {
      "chunk_id": 679,
      "text": "Therefore,\nmax\nv:∥v∥=1 ∥Av∥2 =\nmax\nx:∥x∥=1\nn\nX\ni=1\nD2\ni,ixi\n2 The solution of the right-hand side is to set x = (1, 0, , 0), which implies that\nv1 is the ﬁrst eigenvector of A⊤A Since ∥Av1∥> 0 it follows that D1,1 > 0 as\nrequired For the induction step, assume that the claim holds for some 1 ≤t ≤\nr −1 Then, any v which is orthogonal to v1, , vt can be written as v = Wx\nwith all the ﬁrst t elements of x being zero It follows that\nmax\nv:∥v∥=1,∀i≤t,v⊤vi=0 ∥Av∥2 =\nmax\nx:∥x∥=1\nn\nX\ni=t+1\nD2\ni,ixi\n2 The solution of the right-hand side is the all zeros vector except xt+1 = 1 This\nimplies that vt+1 is the (t + 1)th column of W Finally, since ∥Avt+1∥> 0 it\nfollows that Dt+1,t+1 > 0 as required This concludes our proof.",
      "word_count": 150,
      "source_page": 433,
      "start_position": 154226,
      "end_position": 154375,
      "sentences_count": 12
    },
    {
      "chunk_id": 680,
      "text": "References\nAbernethy, J., Bartlett, P L., Rakhlin, A & Tewari, A (2008), Optimal strategies and\nminimax lower bounds for online convex games, in ‘Proceedings of the Nineteenth\nAnnual Conference on Computational Learning Theory’ Ackerman, M & Ben-David, S (2008), Measures of clustering quality: A working set\nof axioms for clustering, in ‘Proceedings of Neural Information Processing Systems\n(NIPS)’, pp 121–128 Agarwal, S & Roth, D (2005), Learnability of bipartite ranking functions, in ‘Pro-\nceedings of the 18th Annual Conference on Learning Theory’, pp 16–31 Agmon, S (1954), ‘The relaxation method for linear inequalities’, Canadian Journal\nof Mathematics 6(3), 382–392 Aizerman, M A., Braverman, E M & Rozonoer, L I (1964), ‘Theoretical foundations\nof the potential function method in pattern recognition learning’, Automation and\nRemote Control 25, 821–837 Allwein, E L., Schapire, R & Singer, Y (2000), ‘Reducing multiclass to binary: A uni-\nfying approach for margin classiﬁers’, Journal of Machine Learning Research 1, 113–\n141 Alon, N., Ben-David, S., Cesa-Bianchi, N & Haussler, D (1997), ‘Scale-sensitive dimen-\nsions, uniform convergence, and learnability’, Journal of the ACM 44(4), 615–631 Anthony, M & Bartlet, P (1999), Neural Network Learning: Theoretical Foundations,\nCambridge University Press Baraniuk, R., Davenport, M., DeVore, R & Wakin, M (2008), ‘A simple proof of\nthe restricted isometry property for random matrices’, Constructive Approximation\n28(3), 253–263 Barber, D (2012), Bayesian reasoning and machine learning, Cambridge University\nPress Bartlett, P., Bousquet, O & Mendelson, S (2005), ‘Local rademacher complexities’,\nAnnals of Statistics 33(4), 1497–1537 Bartlett, P L & Ben-David, S",
      "word_count": 250,
      "source_page": 437,
      "start_position": 154489,
      "end_position": 154738,
      "sentences_count": 41
    },
    {
      "chunk_id": 681,
      "text": "438\nReferences\nBartlett, P L & Mendelson, S (2002), ‘Rademacher and Gaussian complexities: Risk\nbounds and structural results’, Journal of Machine Learning Research 3, 463–482 Ben-David, S., Cesa-Bianchi, N., Haussler, D & Long, P (1995), ‘Characterizations\nof learnability for classes of {0, , n}-valued functions’, Journal of Computer and\nSystem Sciences 50, 74–86 Ben-David, S., Eiron, N & Long, P (2003), ‘On the diﬃculty of approximately maxi-\nmizing agreements’, Journal of Computer and System Sciences 66(3), 496–514 Ben-David, S & Litman, A (1998), ‘Combinatorial variability of vapnik-chervonenkis\nclasses with applications to sample compression schemes’, Discrete Applied Mathe-\nmatics 86(1), 3–25 Ben-David, S., Pal, D., & Shalev-Shwartz, S (2009), Agnostic online learning, in ‘Con-\nference on Learning Theory (COLT)’ Ben-David, S & Simon, H (2001), ‘Eﬃcient learning of linear perceptrons’, Advances\nin Neural Information Processing Systems pp 189–195 Bengio, Y (2009), ‘Learning deep architectures for AI’, Foundations and Trends in\nMachine Learning 2(1), 1–127 Bengio, Y & LeCun, Y (2007), ‘Scaling learning algorithms towards ai’, Large-Scale\nKernel Machines 34 Bertsekas, D (1999), Nonlinear Programming, Athena Scientiﬁc Beygelzimer, A., Langford, J & Ravikumar, P (2007), ‘Multiclass classiﬁcation with\nﬁlter trees’, Preprint, June Birkhoﬀ, G (1946), ‘Three observations on linear algebra’, Revi Univ Nac Tucuman,\nser A 5, 147–151 Bishop, C M (2006), Pattern recognition and machine learning, Vol 1, springer New\nYork Blum, L., Shub, M & Smale, S (1989), ‘On a theory of computation and complexity\nover the real numbers: Np-completeness, recursive functions and universal machines’,\nAm Math Soc 21(1), 1–46",
      "word_count": 250,
      "source_page": 438,
      "start_position": 154848,
      "end_position": 155097,
      "sentences_count": 44
    },
    {
      "chunk_id": 682,
      "text": "Math Soc 21(1), 1–46 Blumer, A., Ehrenfeucht, A., Haussler, D & Warmuth, M K (1987), ‘Occam’s razor’,\nInformation Processing Letters 24(6), 377–380 Blumer, A., Ehrenfeucht, A., Haussler, D & Warmuth, M K (1989), ‘Learnability\nand the Vapnik-Chervonenkis dimension’, Journal of the Association for Computing\nMachinery 36(4), 929–965 Borwein, J & Lewis, A (2006), Convex Analysis and Nonlinear Optimization, Springer Boser, B E., Guyon, I M & Vapnik, V N (1992), A training algorithm for optimal\nmargin classiﬁers, in ‘Conference on Learning Theory (COLT)’, pp 144–152 Bottou, L & Bousquet, O (2008), The tradeoﬀs of large scale learning, in ‘NIPS’,\npp 161–168 Boucheron, S., Bousquet, O & Lugosi, G (2005), ‘Theory of classiﬁcation: a survey of\nrecent advances’, ESAIM: Probability and Statistics 9, 323–375 Bousquet, O (2002), Concentration Inequalities and Empirical Processes Theory Ap-\nplied to the Analysis of Learning Algorithms, PhD thesis, Ecole Polytechnique Bousquet, O & Elisseeﬀ, A (2002), ‘Stability and generalization’, Journal of Machine\nLearning Research 2, 499–526 Boyd, S & Vandenberghe, L (2004), Convex Optimization, Cambridge University\nPress.",
      "word_count": 171,
      "source_page": 438,
      "start_position": 155094,
      "end_position": 155264,
      "sentences_count": 35
    },
    {
      "chunk_id": 683,
      "text": "References\n439\nBreiman, L (1996), Bias, variance, and arcing classiﬁers, Technical Report 460, Statis-\ntics Department, University of California at Berkeley Breiman, L (2001), ‘Random forests’, Machine learning 45(1), 5–32 Breiman, L., Friedman, J H., Olshen, R A & Stone, C J (1984), Classiﬁcation and\nRegression Trees, Wadsworth & Brooks Cand`es, E (2008), ‘The restricted isometry property and its implications for com-\npressed sensing’, Comptes Rendus Mathematique 346(9), 589–592 Candes, E J (2006), Compressive sampling, in ‘Proc of the Int Congress of Math.,\nMadrid, Spain’ Candes, E & Tao, T (2005), ‘Decoding by linear programming’, IEEE Trans on\nInformation Theory 51, 4203–4215 Cesa-Bianchi, N & Lugosi, G (2006), Prediction, learning, and games, Cambridge\nUniversity Press Chang, H S., Weiss, Y & Freeman, W T (2009), ‘Informative sensing’, arXiv preprint\narXiv:0901.4275 Chapelle, O., Le, Q & Smola, A (2007), Large margin optimization of ranking mea-\nsures, in ‘NIPS Workshop: Machine Learning for Web Search’ Collins, M (2000), Discriminative reranking for natural language parsing, in ‘Machine\nLearning’ Collins, M (2002), Discriminative training methods for hidden Markov models: Theory\nand experiments with perceptron algorithms, in ‘Conference on Empirical Methods\nin Natural Language Processing’ Collobert, R & Weston, J (2008), A uniﬁed architecture for natural language process-\ning: deep neural networks with multitask learning, in ‘International Conference on\nMachine Learning (ICML)’ Cortes, C & Vapnik, V (1995), ‘Support-vector networks’, Machine Learning\n20(3), 273–297 Cover, T (1965), ‘Behavior of sequential predictors of binary sequences’, Trans 4th\nPrague Conf",
      "word_count": 243,
      "source_page": 439,
      "start_position": 155265,
      "end_position": 155507,
      "sentences_count": 45
    },
    {
      "chunk_id": 684,
      "text": "(1965), ‘Behavior of sequential predictors of binary sequences’, Trans 4th\nPrague Conf Information Theory Statistical Decision Functions, Random Processes\npp 263–272 Cover, T & Hart, P (1967), ‘Nearest neighbor pattern classiﬁcation’, Information\nTheory, IEEE Transactions on 13(1), 21–27 Crammer, K & Singer, Y (2001), ‘On the algorithmic implementation of multiclass\nkernel-based vector machines’, Journal of Machine Learning Research 2, 265–292 Cristianini, N & Shawe-Taylor, J (2000), An Introduction to Support Vector Machines,\nCambridge University Press Daniely, A., Sabato, S., Ben-David, S & Shalev-Shwartz, S (2011), Multiclass learn-\nability and the erm principle, in ‘Conference on Learning Theory (COLT)’ Daniely, A., Sabato, S & Shwartz, S S (2012), Multiclass learning approaches: A\ntheoretical comparison with implications, in ‘NIPS’ Davis, G., Mallat, S & Avellaneda, M (1997), ‘Greedy adaptive approximation’, Jour-\nnal of Constructive Approximation 13, 57–98 Devroye, L & Gy¨orﬁ, L (1985), Nonparametric Density Estimation: The L B1 S View,\nWiley Devroye, L., Gy¨orﬁ, L & Lugosi, G (1996), A Probabilistic Theory of Pattern Recog-\nnition, Springer.",
      "word_count": 166,
      "source_page": 439,
      "start_position": 155496,
      "end_position": 155661,
      "sentences_count": 29
    },
    {
      "chunk_id": 685,
      "text": "440\nReferences\nDietterich, T G & Bakiri, G (1995), ‘Solving multiclass learning problems via error-\ncorrecting output codes’, Journal of Artiﬁcial Intelligence Research 2, 263–286 Donoho, D L (2006), ‘Compressed sensing’, Information Theory, IEEE Transactions\non 52(4), 1289–1306 Dudley, R., Gine, E & Zinn, J (1991), ‘Uniform and universal glivenko-cantelli classes’,\nJournal of Theoretical Probability 4(3), 485–510 Dudley, R M (1987), ‘Universal Donsker classes and metric entropy’, Annals of Prob-\nability 15(4), 1306–1326 Fisher, R A (1922), ‘On the mathematical foundations of theoretical statistics’, Philo-\nsophical Transactions of the Royal Society of London Series A, Containing Papers\nof a Mathematical or Physical Character 222, 309–368 Floyd, S (1989), Space-bounded learning and the Vapnik-Chervonenkis dimension, in\n‘Conference on Learning Theory (COLT)’, pp 349–364 Floyd, S & Warmuth, M (1995), ‘Sample compression, learnability, and the Vapnik-\nChervonenkis dimension’, Machine Learning 21(3), 269–304 Frank, M & Wolfe, P (1956), ‘An algorithm for quadratic programming’, Naval Res Logist Quart 3, 95–110 Freund, Y & Schapire, R (1995), A decision-theoretic generalization of on-line learning\nand an application to boosting, in ‘European Conference on Computational Learning\nTheory (EuroCOLT)’, Springer-Verlag, pp 23–37 Freund, Y & Schapire, R E (1999), ‘Large margin classiﬁcation using the perceptron\nalgorithm’, Machine Learning 37(3), 277–296 Garcia, J & Koelling, R (1996), ‘Relation of cue to consequence in avoidance learning’,\nFoundations of animal behavior: classic papers with commentaries 4, 374 Gentile, C (2003), ‘The robustness of the p-norm algorithms’, Machine Learning\n53(3), 265–299 Georghiades, A., Belhumeur, P & Kriegman, D",
      "word_count": 247,
      "source_page": 440,
      "start_position": 155662,
      "end_position": 155908,
      "sentences_count": 44
    },
    {
      "chunk_id": 686,
      "text": "Georghiades, A., Belhumeur, P & Kriegman, D (2001), ‘From few to many: Illumina-\ntion cone models for face recognition under variable lighting and pose’, IEEE Trans Pattern Anal Mach Intelligence 23(6), 643–660 Gordon, G (1999), Regret bounds for prediction problems, in ‘Conference on Learning\nTheory (COLT)’ Gottlieb, L.-A., Kontorovich, L & Krauthgamer, R (2010), Eﬃcient classiﬁcation for\nmetric data, in ‘23rd Conference on Learning Theory’, pp 433–440 Guyon, I & Elisseeﬀ, A (2003), ‘An introduction to variable and feature selection’,\nJournal of Machine Learning Research, Special Issue on Variable and Feature Selec-\ntion 3, 1157–1182 Hadamard, J (1902), ‘Sur les probl`emes aux d´eriv´ees partielles et leur signiﬁcation\nphysique’, Princeton University Bulletin 13, 49–52 Hastie, T., Tibshirani, R & Friedman, J (2001), The Elements of Statistical Learning,\nSpringer Haussler, D (1992), ‘Decision theoretic generalizations of the PAC model for neural\nnet and other learning applications’, Information and Computation 100(1), 78–150 Haussler, D & Long, P M (1995), ‘A generalization of sauer’s lemma’, Journal of\nCombinatorial Theory, Series A 71(2), 219–240 Hazan, E., Agarwal, A & Kale, S (2007), ‘Logarithmic regret algorithms for online\nconvex optimization’, Machine Learning 69(2–3), 169–192.",
      "word_count": 188,
      "source_page": 440,
      "start_position": 155902,
      "end_position": 156089,
      "sentences_count": 29
    },
    {
      "chunk_id": 687,
      "text": "References\n441\nHinton, G E., Osindero, S & Teh, Y.-W (2006), ‘A fast learning algorithm for deep\nbelief nets’, Neural Computation 18(7), 1527–1554 Hiriart-Urruty, J.-B & Lemar´echal, C (1996), Convex Analysis and Minimization Al-\ngorithms: Part 1: Fundamentals, Vol 1, Springer Hsu, C.-W., Chang, C.-C & Lin, C.-J (2003), ‘A practical guide to support vector\nclassiﬁcation’ Hyaﬁl, L & Rivest, R L (1976), ‘Constructing optimal binary decision trees is NP-\ncomplete’, Information Processing Letters 5(1), 15–17 Joachims, T (2005), A support vector method for multivariate performance measures,\nin ‘Proceedings of the International Conference on Machine Learning (ICML)’ Kakade, S., Sridharan, K & Tewari, A (2008), On the complexity of linear prediction:\nRisk bounds, margin bounds, and regularization, in ‘NIPS’ Karp, R M (1972), Reducibility among combinatorial problems, Springer Kearns, M J., Schapire, R E & Sellie, L M (1994), ‘Toward eﬃcient agnostic learn-\ning’, Machine Learning 17, 115–141 Kearns, M & Mansour, Y (1996), On the boosting ability of top-down decision tree\nlearning algorithms, in ‘ACM Symposium on the Theory of Computing (STOC)’ Kearns, M & Ron, D (1999), ‘Algorithmic stability and sanity-check bounds for leave-\none-out cross-validation’, Neural Computation 11(6), 1427–1453 Kearns, M & Valiant, L G (1988), Learning Boolean formulae or ﬁnite automata is\nas hard as factoring, Technical Report TR-14-88, Harvard University Aiken Compu-\ntation Laboratory Kearns, M & Vazirani, U (1994), An Introduction to Computational Learning Theory,\nMIT Press Kleinberg, J (2003), ‘An impossibility theorem for clustering’, Advances in Neural\nInformation Processing Systems pp 463–470",
      "word_count": 249,
      "source_page": 441,
      "start_position": 156090,
      "end_position": 156338,
      "sentences_count": 45
    },
    {
      "chunk_id": 688,
      "text": "(2003), ‘An impossibility theorem for clustering’, Advances in Neural\nInformation Processing Systems pp 463–470 Klivans, A R & Sherstov, A A (2006), Cryptographic hardness for learning intersec-\ntions of halfspaces, in ‘FOCS’ Koller, D & Friedman, N (2009), Probabilistic Graphical Models: Principles and Tech-\nniques, MIT Press Koltchinskii, V & Panchenko, D (2000), Rademacher processes and bounding the risk\nof function learning, in ‘High Dimensional Probability II’, Springer, pp 443–457 Kuhn, H W (1955), ‘The hungarian method for the assignment problem’, Naval re-\nsearch logistics quarterly 2(1-2), 83–97 Kutin, S & Niyogi, P (2002), Almost-everywhere algorithmic stability and general-\nization error, in ‘Proceedings of the 18th Conference in Uncertainty in Artiﬁcial\nIntelligence’, pp 275–282 Laﬀerty, J., McCallum, A & Pereira, F (2001), Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data, in ‘International Conference on\nMachine Learning’, pp 282–289 Langford, J (2006), ‘Tutorial on practical prediction theory for classiﬁcation’, Journal\nof machine learning research 6(1), 273 Langford, J & Shawe-Taylor, J (2003), PAC-Bayes & margins, in ‘NIPS’, pp 423–430 Le Cun, L (2004), Large scale online learning., in ‘Advances in Neural Information\nProcessing Systems 16: Proceedings of the 2003 Conference’, Vol 16, MIT Press,\np 217.",
      "word_count": 198,
      "source_page": 441,
      "start_position": 156325,
      "end_position": 156522,
      "sentences_count": 35
    },
    {
      "chunk_id": 689,
      "text": "442\nReferences\nLe, Q V., Ranzato, M.-A., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J &\nNg, A Y (2012), Building high-level features using large scale unsupervised learning,\nin ‘International Conference on Machine Learning (ICML)’ Lecun, Y & Bengio, Y (1995), Convolutional Networks for Images, Speech and Time\nSeries, The MIT Press, pp 255–258 Lee, H., Grosse, R., Ranganath, R & Ng, A (2009), Convolutional deep belief networks\nfor scalable unsupervised learning of hierarchical representations, in ‘International\nConference on Machine Learning (ICML)’ Littlestone, N (1988), ‘Learning quickly when irrelevant attributes abound: A new\nlinear-threshold algorithm’, Machine Learning 2, 285–318 Littlestone, N & Warmuth, M (1986), Relating data compression and learnability Unpublished manuscript Littlestone, N & Warmuth, M K (1994), ‘The weighted majority algorithm’, Infor-\nmation and Computation 108, 212–261 Livni, R., Shalev-Shwartz, S & Shamir, O (2013), ‘A provably eﬃcient algorithm for\ntraining deep networks’, arXiv preprint arXiv:1304.7045 Livni, R & Simon, P (2013), Honest compressions and their application to compression\nschemes, in ‘Conference on Learning Theory (COLT)’ MacKay, D J (2003), Information theory, inference and learning algorithms,\nCambridge university press Mallat, S & Zhang, Z (1993), ‘Matching pursuits with time-frequency dictionaries’,\nIEEE Transactions on Signal Processing 41, 3397–3415 McAllester, D A (1998), Some PAC-Bayesian theorems, in ‘Conference on Learning\nTheory (COLT)’ McAllester, D A (1999), PAC-Bayesian model averaging, in ‘Conference on Learning\nTheory (COLT)’, pp 164–170 McAllester, D A (2003), Simpliﬁed PAC-Bayesian margin bounds., in ‘Conference on\nLearning Theory (COLT)’, pp 203–215 Minsky, M & Papert, S",
      "word_count": 250,
      "source_page": 442,
      "start_position": 156523,
      "end_position": 156772,
      "sentences_count": 47
    },
    {
      "chunk_id": 690,
      "text": "Minsky, M & Papert, S (1969), Perceptrons: An Introduction to Computational Ge-\nometry, The MIT Press Mukherjee, S., Niyogi, P., Poggio, T & Rifkin, R (2006), ‘Learning theory: stability is\nsuﬃcient for generalization and necessary and suﬃcient for consistency of empirical\nrisk minimization’, Advances in Computational Mathematics 25(1-3), 161–193 Murata, N (1998), ‘A statistical study of on-line learning’, Online Learning and Neural\nNetworks Cambridge University Press, Cambridge, UK Murphy, K P (2012), Machine learning: a probabilistic perspective, The MIT Press Natarajan, B (1995), ‘Sparse approximate solutions to linear systems’, SIAM J Com-\nputing 25(2), 227–234 Natarajan, B K (1989), ‘On learning sets and functions’, Mach Learn 4, 67–97 Nemirovski, A., Juditsky, A., Lan, G & Shapiro, A (2009), ‘Robust stochastic ap-\nproximation approach to stochastic programming’, SIAM Journal on Optimization\n19(4), 1574–1609 Nemirovski, A & Yudin, D (1978), Problem complexity and method eﬃciency in opti-\nmization, Nauka Publishers, Moscow Nesterov, Y (2005), Primal-dual subgradient methods for convex problems, Technical\nreport, Center for Operations Research and Econometrics (CORE), Catholic Univer-\nsity of Louvain (UCL).",
      "word_count": 173,
      "source_page": 442,
      "start_position": 156768,
      "end_position": 156940,
      "sentences_count": 28
    },
    {
      "chunk_id": 691,
      "text": "References\n443\nNesterov, Y & Nesterov, I (2004), Introductory lectures on convex optimization: A\nbasic course, Vol 87, Springer Netherlands Novikoﬀ, A B J (1962), On convergence proofs on perceptrons, in ‘Proceedings of the\nSymposium on the Mathematical Theory of Automata’, Vol XII, pp 615–622 Parberry, I (1994), Circuit complexity and neural networks, The MIT press Pearson, K (1901), ‘On lines and planes of closest ﬁt to systems of points in space’,\nThe London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science\n2(11), 559–572 Phillips, D L (1962), ‘A technique for the numerical solution of certain integral equa-\ntions of the ﬁrst kind’, Journal of the ACM 9(1), 84–97 Pisier, G (1980-1981), ‘Remarques sur un r´esultat non publi´e de B maurey’ Pitt, L & Valiant, L (1988), ‘Computational limitations on learning from examples’,\nJournal of the Association for Computing Machinery 35(4), 965–984 Poon, H & Domingos, P (2011), Sum-product networks: A new deep architecture, in\n‘Conference on Uncertainty in Artiﬁcial Intelligence (UAI)’ Quinlan, J R (1986), ‘Induction of decision trees’, Machine Learning 1, 81–106 Quinlan, J R (1993), C4.5: Programs for Machine Learning, Morgan Kaufmann Rabiner, L & Juang, B (1986), ‘An introduction to hidden markov models’, IEEE\nASSP Magazine 3(1), 4–16 Rakhlin, A., Shamir, O & Sridharan, K (2012), Making gradient descent optimal for\nstrongly convex stochastic optimization, in ‘International Conference on Machine\nLearning (ICML)’ Rakhlin, A., Sridharan, K & Tewari, A (2010), Online learning: Random averages,\ncombinatorial parameters, and learnability, in ‘NIPS’ Rakhlin, S., Mukherjee, S",
      "word_count": 249,
      "source_page": 443,
      "start_position": 156941,
      "end_position": 157189,
      "sentences_count": 42
    },
    {
      "chunk_id": 692,
      "text": "(2010), Online learning: Random averages,\ncombinatorial parameters, and learnability, in ‘NIPS’ Rakhlin, S., Mukherjee, S & Poggio, T (2005), ‘Stability results in learning theory’,\nAnalysis and Applications 3(4), 397–419 Ranzato, M., Huang, F., Boureau, Y & Lecun, Y (2007), Unsupervised learning of\ninvariant feature hierarchies with applications to object recognition, in ‘Computer\nVision and Pattern Recognition, 2007 CVPR’07 IEEE Conference on’, IEEE, pp 1–\n8 Rissanen, J (1978), ‘Modeling by shortest data description’, Automatica 14, 465–471 Rissanen, J (1983), ‘A universal prior for integers and estimation by minimum descrip-\ntion length’, The Annals of Statistics 11(2), 416–431 Robbins, H & Monro, S (1951), ‘A stochastic approximation method’, The Annals of\nMathematical Statistics pp 400–407 Rogers, W & Wagner, T (1978), ‘A ﬁnite sample distribution-free performance bound\nfor local discrimination rules’, The Annals of Statistics 6(3), 506–514 Rokach, L (2007), Data mining with decision trees: theory and applications, Vol 69,\nWorld scientiﬁc Rosenblatt, F (1958), ‘The perceptron: A probabilistic model for information storage\nand organization in the brain’, Psychological Review 65, 386–407 (Reprinted in\nNeurocomputing (MIT Press, 1988).) Rumelhart, D E., Hinton, G E & Williams, R J (1986), Learning internal represen-\ntations by error propagation, in D E Rumelhart & J L McClelland, eds, ‘Paral-\nlel Distributed Processing – Explorations in the Microstructure of Cognition’, MIT\nPress, chapter 8, pp 318–362.",
      "word_count": 221,
      "source_page": 443,
      "start_position": 157175,
      "end_position": 157395,
      "sentences_count": 38
    },
    {
      "chunk_id": 693,
      "text": "444\nReferences\nSankaran, J K (1993), ‘A note on resolving infeasibility in linear programs by con-\nstraint relaxation’, Operations Research Letters 13(1), 19–20 Sauer, N (1972), ‘On the density of families of sets’, Journal of Combinatorial Theory\nSeries A 13, 145–147 Schapire, R (1990), ‘The strength of weak learnability’, Machine Learning 5(2), 197–\n227 Schapire, R E & Freund, Y (2012), Boosting: Foundations and Algorithms, MIT press Sch¨olkopf, B., Herbrich, R & Smola, A (2001), A generalized representer theorem, in\n‘Computational learning theory’, pp 416–426 Sch¨olkopf, B., Herbrich, R., Smola, A & Williamson, R (2000), A generalized repre-\nsenter theorem, in ‘NeuroCOLT’ Sch¨olkopf, B & Smola, A J (2002), Learning with Kernels: Support Vector Machines,\nRegularization, Optimization and Beyond, MIT Press Sch¨olkopf, B., Smola, A & M¨uller, K.-R (1998), ‘Nonlinear component analysis as a\nkernel eigenvalue problem’, Neural computation 10(5), 1299–1319 Seeger, M (2003), ‘Pac-bayesian generalisation error bounds for gaussian process clas-\nsiﬁcation’, The Journal of Machine Learning Research 3, 233–269 Shakhnarovich, G., Darrell, T & Indyk, P (2006), Nearest-neighbor methods in learning\nand vision: theory and practice, MIT Press Shalev-Shwartz, S (2007), Online Learning: Theory, Algorithms, and Applications,\nPhD thesis, The Hebrew University Shalev-Shwartz, S (2011), ‘Online learning and online convex optimization’, Founda-\ntions and Trends R\n⃝in Machine Learning 4(2), 107–194 Shalev-Shwartz, S., Shamir, O., Srebro, N & Sridharan, K (2010), ‘Learnability,\nstability and uniform convergence’, The Journal of Machine Learning Research\n9999, 2635–2670 Shalev-Shwartz, S., Shamir, O & Sridharan, K",
      "word_count": 243,
      "source_page": 444,
      "start_position": 157396,
      "end_position": 157638,
      "sentences_count": 39
    },
    {
      "chunk_id": 694,
      "text": "Shalev-Shwartz, S., Shamir, O & Sridharan, K (2010), Learning kernel-based halfs-\npaces with the zero-one loss, in ‘Conference on Learning Theory (COLT)’ Shalev-Shwartz, S., Shamir, O., Sridharan, K & Srebro, N (2009), Stochastic convex\noptimization, in ‘Conference on Learning Theory (COLT)’ Shalev-Shwartz, S & Singer, Y (2008), On the equivalence of weak learnability and\nlinear separability: New relaxations and eﬃcient boosting algorithms, in ‘Proceedings\nof the Nineteenth Annual Conference on Computational Learning Theory’ Shalev-Shwartz, S., Singer, Y & Srebro, N (2007), Pegasos: Primal Estimated sub-\nGrAdient SOlver for SVM, in ‘International Conference on Machine Learning’,\npp 807–814 Shalev-Shwartz, S & Srebro, N (2008), SVM optimization: Inverse dependence on\ntraining set size, in ‘International Conference on Machine Learning’, pp 928–935 Shalev-Shwartz, S., Zhang, T & Srebro, N (2010), ‘Trading accuracy for sparsity\nin optimization problems with sparsity constraints’, Siam Journal on Optimization\n20, 2807–2832 Shamir, O & Zhang, T (2013), Stochastic gradient descent for non-smooth optimiza-\ntion: Convergence results and optimal averaging schemes, in ‘International Confer-\nence on Machine Learning (ICML)’ Shapiro, A., Dentcheva, D & Ruszczy´nski, A (2009), Lectures on stochastic program-\nming: modeling and theory, Vol 9, Society for Industrial and Applied Mathematics.",
      "word_count": 194,
      "source_page": 444,
      "start_position": 157632,
      "end_position": 157825,
      "sentences_count": 27
    },
    {
      "chunk_id": 695,
      "text": "References\n445\nShelah, S (1972), ‘A combinatorial problem; stability and order for models and theories\nin inﬁnitary languages’, Pac J Math 4, 247–261 Sipser, M (2006), Introduction to the Theory of Computation, Thomson Course Tech-\nnology Slud, E V (1977), ‘Distribution inequalities for the binomial law’, The Annals of\nProbability 5(3), 404–412 Steinwart, I & Christmann, A (2008), Support vector machines, Springerverlag New\nYork Stone, C (1977), ‘Consistent nonparametric regression’, The annals of statistics\n5(4), 595–620 Taskar, B., Guestrin, C & Koller, D (2003), Max-margin markov networks, in ‘NIPS’ Tibshirani, R (1996), ‘Regression shrinkage and selection via the lasso’, J Royal Statist Soc B 58(1), 267–288 Tikhonov, A N (1943), ‘On the stability of inverse problems’, Dolk Akad Nauk SSSR\n39(5), 195–198 Tishby, N., Pereira, F & Bialek, W (1999), The information bottleneck method, in\n‘The 37’th Allerton Conference on Communication, Control, and Computing’ Tsochantaridis, I., Hofmann, T., Joachims, T & Altun, Y (2004), Support vector\nmachine learning for interdependent and structured output spaces, in ‘Proceedings\nof the Twenty-First International Conference on Machine Learning’ Valiant, L G (1984), ‘A theory of the learnable’, Communications of the ACM\n27(11), 1134–1142 Vapnik, V (1992), Principles of risk minimization for learning theory, in J E Moody,\nS J Hanson & R P Lippmann, eds, ‘Advances in Neural Information Processing\nSystems 4’, Morgan Kaufmann, pp 831–838 Vapnik, V (1995), The Nature of Statistical Learning Theory, Springer Vapnik, V N (1982), Estimation of Dependences Based on Empirical Data, Springer-\nVerlag Vapnik, V N",
      "word_count": 248,
      "source_page": 445,
      "start_position": 157826,
      "end_position": 158073,
      "sentences_count": 53
    },
    {
      "chunk_id": 696,
      "text": "Vapnik, V N (1998), Statistical Learning Theory, Wiley Vapnik, V N & Chervonenkis, A Y (1971), ‘On the uniform convergence of relative\nfrequencies of events to their probabilities’, Theory of Probability and its applications\nXVI(2), 264–280 Vapnik, V N & Chervonenkis, A Y (1974), Theory of pattern recognition, Nauka,\nMoscow (In Russian) Von Luxburg, U (2007), ‘A tutorial on spectral clustering’, Statistics and computing\n17(4), 395–416 von Neumann, J (1928), ‘Zur theorie der gesellschaftsspiele (on the theory of parlor\ngames)’, Math Ann 100, 295—320 Von Neumann, J (1953), ‘A certain zero-sum two-person game equivalent to the opti-\nmal assignment problem’, Contributions to the Theory of Games 2, 5–12 Vovk, V G (1990), Aggregating strategies, in ‘Conference on Learning Theory\n(COLT)’, pp 371–383 Warmuth, M., Glocer, K & Vishwanathan, S (2008), Entropy regularized lpboost, in\n‘Algorithmic Learning Theory (ALT)’ Warmuth, M., Liao, J & Ratsch, G (2006), Totally corrective boosting algorithms\nthat maximize the margin, in ‘Proceedings of the 23rd international conference on\nMachine learning’.",
      "word_count": 164,
      "source_page": 445,
      "start_position": 158071,
      "end_position": 158234,
      "sentences_count": 32
    },
    {
      "chunk_id": 697,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbackpropagation, 278\nbackward elimination, 363\nbag-of-words, 209\nbase hypothesis, 137\nBayes optimal, 46, 52, 260\nBayes rule, 354\nBayesian reasoning, 353\nBennet’s inequality, 426\nBernstein’s inequality, 426\nbias, 37, 61, 64\nbias-complexity tradeoﬀ, 65\nboolean conjunctions, 51, 79, 106\nboosting, 130\nboosting the conﬁdence, 142\nboundedness, 165\nC4.5, 254\nCART, 254\nchaining, 389\nChebyshev’s inequality, 423\nChernoﬀbounds, 423\nclass-sensitive feature mapping, 230\nclassiﬁer, 34\nclustering, 307\nspectral, 315\ncompressed sensing, 330\ncompression bounds, 410\ncompression scheme, 411\ncomputational complexity, 100\nconﬁdence, 38, 43\nconsistency, 92\nConsistent, 289\ncontraction lemma, 381\nconvex, 156\nfunction, 157\nset, 156\nstrongly convex, 174, 195\nconvex-Lipschitz-bounded learning, 166\nconvex-smooth-bounded learning, 166\ncovering numbers, 388\ncurse of dimensionality, 263\ndecision stumps, 132, 133\ndecision trees, 250\ndendrogram, 309, 310\ndictionary learning, 368\ndiﬀerential set, 188\ndimensionality reduction, 323\ndiscretization trick, 57\ndiscriminative, 342\ndistribution free, 342\ndomain, 33\ndomain of examples, 48\ndoubly stochastic matrix, 242\nduality, 211\nstrong duality, 211\nweak duality, 211\nDudley classes, 81\neﬃcient computable, 100\nEM, 348\nempirical error, 35\nempirical risk, 35, 48\nEmpirical Risk Minimization, see ERM\nentropy, 345\nrelative entropy, 345\nepigraph, 157\nERM, 35\nerror decomposition, 64, 168\nestimation error, 61, 64\nExpectation-Maximization, see EM\nface recognition, see Viola-Jones\nfeasible, 100\nfeature, 33\nfeature learning, 368\nfeature normalization, 365\nfeature selection, 357, 358\nfeature space, 215\nfeature transformations, 367\nﬁlters, 359\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press",
      "word_count": 271,
      "source_page": 447,
      "start_position": 158370,
      "end_position": 158640,
      "sentences_count": 1
    },
    {
      "chunk_id": 698,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbackpropagation, 278\nbackward elimination, 363\nbag-of-words, 209\nbase hypothesis, 137\nBayes optimal, 46, 52, 260\nBayes rule, 354\nBayesian reasoning, 353\nBennet’s inequality, 426\nBernstein’s inequality, 426\nbias, 37, 61, 64\nbias-complexity tradeoﬀ, 65\nboolean conjunctions, 51, 79, 106\nboosting, 130\nboosting the conﬁdence, 142\nboundedness, 165\nC4.5, 254\nCART, 254\nchaining, 389\nChebyshev’s inequality, 423\nChernoﬀbounds, 423\nclass-sensitive feature mapping, 230\nclassiﬁer, 34\nclustering, 307\nspectral, 315\ncompressed sensing, 330\ncompression bounds, 410\ncompression scheme, 411\ncomputational complexity, 100\nconﬁdence, 38, 43\nconsistency, 92\nConsistent, 289\ncontraction lemma, 381\nconvex, 156\nfunction, 157\nset, 156\nstrongly convex, 174, 195\nconvex-Lipschitz-bounded learning, 166\nconvex-smooth-bounded learning, 166\ncovering numbers, 388\ncurse of dimensionality, 263\ndecision stumps, 132, 133\ndecision trees, 250\ndendrogram, 309, 310\ndictionary learning, 368\ndiﬀerential set, 188\ndimensionality reduction, 323\ndiscretization trick, 57\ndiscriminative, 342\ndistribution free, 342\ndomain, 33\ndomain of examples, 48\ndoubly stochastic matrix, 242\nduality, 211\nstrong duality, 211\nweak duality, 211\nDudley classes, 81\neﬃcient computable, 100\nEM, 348\nempirical error, 35\nempirical risk, 35, 48\nEmpirical Risk Minimization, see ERM\nentropy, 345\nrelative entropy, 345\nepigraph, 157\nERM, 35\nerror decomposition, 64, 168\nestimation error, 61, 64\nExpectation-Maximization, see EM\nface recognition, see Viola-Jones\nfeasible, 100\nfeature, 33\nfeature learning, 368\nfeature normalization, 365\nfeature selection, 357, 358\nfeature space, 215\nfeature transformations, 367\nﬁlters, 359\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only",
      "word_count": 274,
      "source_page": 447,
      "start_position": 158370,
      "end_position": 158643,
      "sentences_count": 2
    },
    {
      "chunk_id": 699,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbackpropagation, 278\nbackward elimination, 363\nbag-of-words, 209\nbase hypothesis, 137\nBayes optimal, 46, 52, 260\nBayes rule, 354\nBayesian reasoning, 353\nBennet’s inequality, 426\nBernstein’s inequality, 426\nbias, 37, 61, 64\nbias-complexity tradeoﬀ, 65\nboolean conjunctions, 51, 79, 106\nboosting, 130\nboosting the conﬁdence, 142\nboundedness, 165\nC4.5, 254\nCART, 254\nchaining, 389\nChebyshev’s inequality, 423\nChernoﬀbounds, 423\nclass-sensitive feature mapping, 230\nclassiﬁer, 34\nclustering, 307\nspectral, 315\ncompressed sensing, 330\ncompression bounds, 410\ncompression scheme, 411\ncomputational complexity, 100\nconﬁdence, 38, 43\nconsistency, 92\nConsistent, 289\ncontraction lemma, 381\nconvex, 156\nfunction, 157\nset, 156\nstrongly convex, 174, 195\nconvex-Lipschitz-bounded learning, 166\nconvex-smooth-bounded learning, 166\ncovering numbers, 388\ncurse of dimensionality, 263\ndecision stumps, 132, 133\ndecision trees, 250\ndendrogram, 309, 310\ndictionary learning, 368\ndiﬀerential set, 188\ndimensionality reduction, 323\ndiscretization trick, 57\ndiscriminative, 342\ndistribution free, 342\ndomain, 33\ndomain of examples, 48\ndoubly stochastic matrix, 242\nduality, 211\nstrong duality, 211\nweak duality, 211\nDudley classes, 81\neﬃcient computable, 100\nEM, 348\nempirical error, 35\nempirical risk, 35, 48\nEmpirical Risk Minimization, see ERM\nentropy, 345\nrelative entropy, 345\nepigraph, 157\nERM, 35\nerror decomposition, 64, 168\nestimation error, 61, 64\nExpectation-Maximization, see EM\nface recognition, see Viola-Jones\nfeasible, 100\nfeature, 33\nfeature learning, 368\nfeature normalization, 365\nfeature selection, 357, 358\nfeature space, 215\nfeature transformations, 367\nﬁlters, 359\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press Personal use only Not for distribution",
      "word_count": 277,
      "source_page": 447,
      "start_position": 158370,
      "end_position": 158646,
      "sentences_count": 3
    },
    {
      "chunk_id": 700,
      "text": "448\nIndex\nforward greedy selection, 360\nfrequentist, 353\ngain, 253\nGD, see gradient descent\ngeneralization error, 35\ngenerative models, 342\nGini index, 254\nGlivenko-Cantelli, 58\ngradient, 158\ngradient descent, 185\nGram matrix, 219\ngrowth function, 73\nhalfspace, 118\nhomogenous, 118, 205\nnon-separable, 119\nseparable, 118\nHalving, 289\nhidden layers, 270\nHilbert space, 217\nHoeﬀding’s inequality, 56, 425\nhold out, 146\nhypothesis, 34\nhypothesis class, 36\ni.i.d., 38\nID3, 252\nimproper, see representation independent\ninductive bias, see bias\ninformation bottleneck, 317\ninformation gain, 254\ninstance, 33\ninstance space, 33\nintegral image, 143\nJohnson-Lindenstrauss lemma, 329\nk-means, 311, 313\nsoft k-means, 352\nk-median, 312\nk-medoids, 312\nKendall tau, 239\nkernel PCA, 326\nkernels, 215\nGaussian kernel, 220\nkernel trick, 217\npolynomial kernel, 220\nRBF kernel, 220\nlabel, 33\nLasso, 365, 386\ngeneralization bounds, 386\nlatent variables, 348\nLDA, 347\nLdim, 290, 291\nlearning curves, 153\nleast squares, 124\nlikelihood ratio, 348\nlinear discriminant analysis, see LDA\nlinear predictor, 117\nhomogenous, 118\nlinear programming, 119\nlinear regression, 122\nlinkage, 310\nLipschitzness, 160, 176, 191\nsub-gradient, 190\nLittlestone dimension, see Ldim\nlocal minimum, 158\nlogistic regression, 126\nloss, 35\nloss function, 48\n0-1 loss, 48, 167\nabsolute value loss, 124, 128, 166\nconvex loss, 163\ngeneralized hinge-loss, 233\nhinge loss, 167\nLipschitz loss, 166\nlog-loss, 345\nlogistic loss, 127\nramp loss, 209\nsmooth loss, 166\nsquare loss, 48\nsurrogate loss, 167, 302\nmargin, 203\nMarkov’s inequality, 422\nMassart lemma, 380\nmax linkage, 310\nmaximum a-posteriori, 355\nmaximum likelihood, 343\nMcDiarmid’s inequality, 378\nMDL, 89, 90, 251\nmeasure concentration, 55, 422\nMinimum Description Length, see MDL\nmistake bound, 288\nmixture of Gaussians, 348\nmodel selection, 144, 147\nmulticlass, 47, 227, 402\ncost-sensitive, 232\nlinear predictors, 230, 405\nmulti-vector, 231, 406\nPerceptron, 248\nreductions, 227, 405\nSGD, 235\nSVM, 234\nmultivariate performance measures, 243\nNaive Bayes, 347\nNatarajan dimension, 402\nNDCG, 239\nNearest Neighbor, 258\nk-NN, 258\nneural networks, 268\nfeedforward networks, 269\nlayered networks, 269\nSGD, 277\nno-free-lunch, 61\nnon-uniform learning, 84",
      "word_count": 327,
      "source_page": 448,
      "start_position": 158654,
      "end_position": 158980,
      "sentences_count": 1
    },
    {
      "chunk_id": 701,
      "text": "Index\n449\nNormalized Discounted Cumulative Gain,\nsee NDCG\nOccam’s razor, 91\nOMP, 360\none-vs-all, 227\none-vs-rest, see one-vs-all\none-vs.-all, 404\nonline convex optimization, 300\nonline gradient descent, 300\nonline learning, 287\noptimization error, 168\noracle inequality, 179\northogonal matching pursuit, see OMP\noverﬁtting, 35, 65, 152\nPAC, 43\nagnostic PAC, 45, 46\nagnostic PAC for general loss, 49\nPAC-Bayes, 415\nparametric density estimation, 342\nPCA, 324\nPearson’s correlation coeﬃcient, 359\nPerceptron, 120\nkernelized Perceptron, 225\nmulticlass, 248\nonline, 301\npermutation matrix, 242\npolynomial regression, 125\nprecision, 244\npredictor, 34\npreﬁx free language, 89\nPrincipal Component Analysis, see PCA\nprior knowledge, 63\nProbably Approximately Correct, see PAC\nprojection, 193\nprojection lemma, 193\nproper, 49\npruning, 254\nRademacher complexity, 375\nrandom forests, 255\nrandom projections, 329\nranking, 238\nbipartite, 243\nrealizability, 37\nrecall, 244\nregression, 47, 122, 172\nregularization, 171\nTikhonov, 172, 174\nregularized loss minimization, see RLM\nrepresentation independent, 49, 107\nrepresentative sample, 54, 375\nrepresenter theorem, 218\nridge regression, 172\nkernel ridge regression, 225\nRIP, 331\nrisk, 35, 45, 48\nRLM, 171, 199\nsample complexity, 44\nSauer’s lemma, 73\nself-boundedness, 162\nsensitivity, 244\nSGD, 190\nshattering, 69, 403\nsingle linkage, 310\nSingular Value Decomposition, see SVD\nSlud’s inequality, 428\nsmoothness, 162, 177, 198\nSOA, 292\nsparsity-inducing norms, 363\nspeciﬁcity, 244\nspectral clustering, 315\nSRM, 85, 145\nstability, 173\nStochastic Gradient Descent, see SGD\nstrong learning, 132\nStructural Risk Minimization, see SRM\nstructured output prediction, 236\nsub-gradient, 188\nSupport Vector Machines, see SVM\nSVD, 431\nSVM, 202, 383\nduality, 211\ngeneralization bounds, 208, 383\nhard-SVM, 203, 204\nhomogenous, 205\nkernel trick, 217\nsoft-SVM, 206\nsupport vectors, 210\ntarget set, 47\nterm-frequency, 231\nTF-IDF, 231\ntraining error, 35\ntraining set, 33\ntrue error, 35, 45\nunderﬁtting, 65, 152\nuniform convergence, 54, 55\nunion bound, 39\nunsupervised learning, 308\nvalidation, 144, 146\ncross validation, 149\ntrain-validation-test split, 150\nVapnik-Chervonenkis dimension, see VC\ndimension\nVC dimension, 67, 70\nversion space, 289\nViola-Jones, 139\nweak learning, 130, 131\nWeighted-Majority, 295",
      "word_count": 325,
      "source_page": 449,
      "start_position": 158981,
      "end_position": 159305,
      "sentences_count": 1
    }
  ]
}
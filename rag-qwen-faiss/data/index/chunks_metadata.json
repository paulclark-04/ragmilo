{
  "model_info": {
    "model_name": "all-mpnet-base-v2",
    "embedding_dimension": 768,
    "total_chunks": 701,
    "index_type": "IndexFlatIP",
    "normalization": true,
    "similarity_metric": "cosine"
  },
  "chunks": [
    {
      "chunk_id": 1,
      "text": "Understanding Machine Learning\nMachine learning is one of the fastest growing areas of computer science,\nwith far-reaching applications The aim of this textbook is to introduce\nmachine learning, and t...",
      "word_count": 204,
      "source_page": 3,
      "vector_index": 0,
      "start_position": 68,
      "end_position": 271
    },
    {
      "chunk_id": 2,
      "text": "32 Avenue of the Americas, New York, NY 10013-2473, USA\nCambridge University Press is part of the University of Cambridge It furthers the University’s mission by disseminating knowledge in the pursuit...",
      "word_count": 164,
      "source_page": 5,
      "vector_index": 1,
      "start_position": 272,
      "end_position": 435
    },
    {
      "chunk_id": 3,
      "text": "vii\nPreface\nThe term machine learning refers to the automated detection of meaningful\npatterns in data In the past couple of decades it has become a common tool in\nalmost any task that requires inform...",
      "word_count": 224,
      "source_page": 7,
      "vector_index": 2,
      "start_position": 436,
      "end_position": 659
    },
    {
      "chunk_id": 4,
      "text": "Taking example from\nintelligent beings, many of our skills are acquired or reﬁned through learning from\nour experience (rather than following explicit instructions given to us) Machine\nlearning tools ...",
      "word_count": 238,
      "source_page": 7,
      "vector_index": 3,
      "start_position": 619,
      "end_position": 856
    },
    {
      "chunk_id": 5,
      "text": "viii\na “no-free-lunch” theorem We also discuss how much computation time is re-\nquired for learning In the second part of the book we describe various learning\nalgorithms For some of the algorithms, w...",
      "word_count": 224,
      "source_page": 8,
      "vector_index": 4,
      "start_position": 931,
      "end_position": 1154
    },
    {
      "chunk_id": 6,
      "text": "Acknowledgements\nThe book is based on Introduction to Machine Learning courses taught by Shai\nShalev-Shwartz at the Hebrew University and by Shai Ben-David at the Univer-\nsity of Waterloo The ﬁrst dra...",
      "word_count": 239,
      "source_page": 8,
      "vector_index": 5,
      "start_position": 1099,
      "end_position": 1337
    },
    {
      "chunk_id": 7,
      "text": "Contents\nPreface\npage vii\n1\nIntroduction\n19\n1.1\nWhat Is Learning 19\n1.2\nWhen Do We Need Machine Learning 21\n1.3\nTypes of Learning\n22\n1.4\nRelations to Other Fields\n24\n1.5\nHow to Read This Book\n25\n1.5.1...",
      "word_count": 194,
      "source_page": 9,
      "vector_index": 6,
      "start_position": 1370,
      "end_position": 1563
    },
    {
      "chunk_id": 8,
      "text": "x\nContents\n4.3\nSummary\n58\n4.4\nBibliographic Remarks\n58\n4.5\nExercises\n58\n5\nThe Bias-Complexity Tradeoﬀ\n60\n5.1\nThe No-Free-Lunch Theorem\n61\n5.1.1\nNo-Free-Lunch and Prior Knowledge\n63\n5.2\nError Decomposi...",
      "word_count": 200,
      "source_page": 10,
      "vector_index": 7,
      "start_position": 1564,
      "end_position": 1763
    },
    {
      "chunk_id": 9,
      "text": "Contents\nxi\n8.1.1\nFormal Deﬁnition*\n102\n8.2\nImplementing the ERM Rule\n103\n8.2.1\nFinite Classes\n104\n8.2.2\nAxis Aligned Rectangles\n105\n8.2.3\nBoolean Conjunctions\n106\n8.2.4\nLearning 3-Term DNF\n107\n8.3\nEﬃ...",
      "word_count": 200,
      "source_page": 11,
      "vector_index": 8,
      "start_position": 1764,
      "end_position": 1963
    },
    {
      "chunk_id": 10,
      "text": "xii\nContents\n11.2.4\nk-Fold Cross Validation\n149\n11.2.5\nTrain-Validation-Test Split\n150\n11.3\nWhat to Do If Learning Fails\n151\n11.4\nSummary\n154\n11.5\nExercises\n154\n12\nConvex Learning Problems\n156\n12.1\nCo...",
      "word_count": 198,
      "source_page": 12,
      "vector_index": 9,
      "start_position": 1964,
      "end_position": 2161
    },
    {
      "chunk_id": 11,
      "text": "Contents\nxiii\n14.4.4\nStrongly Convex Functions*\n195\n14.5\nLearning with SGD\n196\n14.5.1\nSGD for Risk Minimization\n196\n14.5.2\nAnalyzing SGD for Convex-Smooth Learning Problems\n198\n14.5.3\nSGD for Regulari...",
      "word_count": 213,
      "source_page": 13,
      "vector_index": 10,
      "start_position": 2162,
      "end_position": 2374
    },
    {
      "chunk_id": 12,
      "text": "xiv\nContents\n17.4.1\nLinear Predictors for Ranking\n240\n17.5\nBipartite Ranking and Multivariate Performance Measures\n243\n17.5.1\nLinear Predictors for Bipartite Ranking\n245\n17.6\nSummary\n247\n17.7\nBibliogr...",
      "word_count": 194,
      "source_page": 14,
      "vector_index": 11,
      "start_position": 2375,
      "end_position": 2568
    },
    {
      "chunk_id": 13,
      "text": "Contents\nxv\n21.1.1\nOnline Learnability\n290\n21.2\nOnline Classiﬁcation in the Unrealizable Case\n294\n21.2.1\nWeighted-Majority\n295\n21.3\nOnline Convex Optimization\n300\n21.4\nThe Online Perceptron Algorithm\n...",
      "word_count": 203,
      "source_page": 15,
      "vector_index": 12,
      "start_position": 2569,
      "end_position": 2771
    },
    {
      "chunk_id": 14,
      "text": "xvi\nContents\n24.4.1\nEM as an Alternate Maximization Algorithm\n350\n24.4.2\nEM for Mixture of Gaussians (Soft k-Means)\n352\n24.5\nBayesian Reasoning\n353\n24.6\nSummary\n355\n24.7\nBibliographic Remarks\n355\n24.8...",
      "word_count": 214,
      "source_page": 16,
      "vector_index": 13,
      "start_position": 2772,
      "end_position": 2985
    },
    {
      "chunk_id": 15,
      "text": "1\nIntroduction\nThe subject of this book is automated learning, or, as we will more often call\nit, Machine Learning (ML) That is, we wish to program computers so that\nthey can “learn” from input availa...",
      "word_count": 246,
      "source_page": 19,
      "vector_index": 14,
      "start_position": 3113,
      "end_position": 3358
    },
    {
      "chunk_id": 16,
      "text": "Bait Shyness – Rats Learning to Avoid Poisonous Baits: When rats encounter\nfood items with novel look or smell, they will ﬁrst eat very small amounts, and\nsubsequent feeding will depend on the ﬂavor o...",
      "word_count": 229,
      "source_page": 19,
      "vector_index": 15,
      "start_position": 3293,
      "end_position": 3521
    },
    {
      "chunk_id": 17,
      "text": "20\nIntroduction\nof previous spam e-mails If it matches one of them, it will be trashed Otherwise,\nit will be moved to the user’s inbox folder While the preceding “learning by memorization” approach is...",
      "word_count": 229,
      "source_page": 20,
      "vector_index": 16,
      "start_position": 3522,
      "end_position": 3750
    },
    {
      "chunk_id": 18,
      "text": "F Skinner,\nhe placed a bunch of hungry pigeons in a cage An automatic mechanism had\nbeen attached to the cage, delivering food to the pigeons at regular intervals\nwith no reference whatsoever to the b...",
      "word_count": 241,
      "source_page": 20,
      "vector_index": 17,
      "start_position": 3739,
      "end_position": 3979
    },
    {
      "chunk_id": 19,
      "text": "1.2 When Do We Need Machine Learning 21\nrats turns out to be more complex than what one may expect In experiments\ncarried out by Garcia (Garcia & Koelling 1996), it was demonstrated that if the\nunplea...",
      "word_count": 234,
      "source_page": 21,
      "vector_index": 18,
      "start_position": 4028,
      "end_position": 4261
    },
    {
      "chunk_id": 20,
      "text": "The pigeons in\nthe experiment are willing to adopt any explanation for the occurrence of food However, the rats “know” that food cannot cause an electric shock and that the\nco-occurrence of noise with...",
      "word_count": 223,
      "source_page": 21,
      "vector_index": 19,
      "start_position": 4215,
      "end_position": 4437
    },
    {
      "chunk_id": 21,
      "text": "22\nIntroduction\ndeﬁned program Examples of such tasks include driving, speech\nrecognition, and image understanding In all of these tasks, state\nof the art machine learning programs, programs that “lea...",
      "word_count": 238,
      "source_page": 22,
      "vector_index": 20,
      "start_position": 4508,
      "end_position": 4745
    },
    {
      "chunk_id": 22,
      "text": "However, many tasks change over time or from one user to another Machine learning tools – programs whose behavior adapts to their input\ndata – oﬀer a solution to such issues; they are, by nature, adap...",
      "word_count": 202,
      "source_page": 22,
      "vector_index": 21,
      "start_position": 4702,
      "end_position": 4903
    },
    {
      "chunk_id": 23,
      "text": "1.3 Types of Learning\n23\nillustrative example, consider the task of learning to detect spam e-mail\nversus the task of anomaly detection For the spam detection task, we\nconsider a setting in which the ...",
      "word_count": 238,
      "source_page": 23,
      "vector_index": 22,
      "start_position": 4904,
      "end_position": 5141
    },
    {
      "chunk_id": 24,
      "text": "The learner processes input data with the goal of coming\nup with some summary, or compressed version of that data Clustering\na data set into subsets of similar objets is a typical example of such a\nta...",
      "word_count": 222,
      "source_page": 23,
      "vector_index": 23,
      "start_position": 5105,
      "end_position": 5326
    },
    {
      "chunk_id": 25,
      "text": "24\nIntroduction\nful for achieving the learning goal In contrast, when a scientist learns\nabout nature, the environment, playing the role of the teacher, can be\nbest thought of as passive – apples drop...",
      "word_count": 242,
      "source_page": 24,
      "vector_index": 24,
      "start_position": 5397,
      "end_position": 5638
    },
    {
      "chunk_id": 26,
      "text": "For example, a stockbroker has to make daily\ndecisions, based on the experience collected so far He may become an\nexpert over time, but might have made costly mistakes in the process In\ncontrast, in m...",
      "word_count": 224,
      "source_page": 24,
      "vector_index": 25,
      "start_position": 5607,
      "end_position": 5830
    },
    {
      "chunk_id": 27,
      "text": "1.5 How to Read This Book\n25\nspecial abilities of computers to complement human intelligence, often perform-\ning tasks that fall way beyond human capabilities For example, the ability to\nscan and proc...",
      "word_count": 230,
      "source_page": 25,
      "vector_index": 26,
      "start_position": 5860,
      "end_position": 6089
    },
    {
      "chunk_id": 28,
      "text": "In contrast, machine learning aims to use the data gathered from\nsamples of patients to come up with a description of the causes of heart disease The hope is that automated techniques may be able to ﬁ...",
      "word_count": 187,
      "source_page": 25,
      "vector_index": 27,
      "start_position": 6039,
      "end_position": 6225
    },
    {
      "chunk_id": 29,
      "text": "Namely, given the\nsize of available samples, machine learning theory aims to ﬁgure out the degree\nof accuracy that a learner can expect on the basis of such samples There are further diﬀerences betwee...",
      "word_count": 180,
      "source_page": 25,
      "vector_index": 28,
      "start_position": 6180,
      "end_position": 6359
    },
    {
      "chunk_id": 30,
      "text": "26\nIntroduction\nof the book is built This part could serve as a basis for a minicourse on the\ntheoretical foundations of ML The second part of the book introduces the most commonly used algorithmic\nap...",
      "word_count": 243,
      "source_page": 26,
      "vector_index": 29,
      "start_position": 6360,
      "end_position": 6602
    },
    {
      "chunk_id": 31,
      "text": "1.6 Notation\n27\n6 Chapter 30 7 Chapters 12, 13 8 Chapter 14 9 Chapter 8 10 Chapter 17 11 Chapter 29 12 Chapter 19 13 Chapter 20 14 Chapter 21 1.6\nNotation\nMost of the notation we use throughout the bo...",
      "word_count": 245,
      "source_page": 27,
      "vector_index": 30,
      "start_position": 6661,
      "end_position": 6905
    },
    {
      "chunk_id": 32,
      "text": "The ith element of xt is denoted by xt,i Throughout the book, we make use of basic notions from probability We\ndenote by D a distribution over some set,2 for example, Z We use the notation\nz ∼D to den...",
      "word_count": 239,
      "source_page": 27,
      "vector_index": 31,
      "start_position": 6886,
      "end_position": 7124
    },
    {
      "chunk_id": 33,
      "text": "28\nIntroduction\nTable 1.1 Summary of notation\nsymbol\nmeaning\nR\nthe set of real numbers\nRd\nthe set of d-dimensional vectors over R\nR+\nthe set of non-negative real numbers\nN\nthe set of natural numbers\nO...",
      "word_count": 187,
      "source_page": 28,
      "vector_index": 32,
      "start_position": 7125,
      "end_position": 7311
    },
    {
      "chunk_id": 34,
      "text": "Ai,j = xixj (where x ∈Rd)\nx1, , xm\na sequence of m vectors\nxi,j\nthe jth element of the ith vector in the sequence\nw(1), , w(T )\nthe values of a vector w during an iterative algorithm\nw(t)\ni\nthe ith el...",
      "word_count": 211,
      "source_page": 28,
      "vector_index": 33,
      "start_position": 7286,
      "end_position": 7496
    },
    {
      "chunk_id": 35,
      "text": ", zm i.i.d according to D\nP, E\nprobability and expectation of a random variable\nPz∼D[f(z)]\n= D({z : f(z) = true}) for f : Z →{true, false}\nEz∼D[f(z)]\nexpectation of the random variable f : Z →R\nN(µ, C...",
      "word_count": 168,
      "source_page": 28,
      "vector_index": 34,
      "start_position": 7407,
      "end_position": 7574
    },
    {
      "chunk_id": 36,
      "text": "1.6 Notation\n29\nx0 such that for all x > x0 we have f(x) ≤αg(x) We write f = Ω(g) if there\nexist x0, α ∈R+ such that for all x > x0 we have f(x) ≥αg(x) The notation\nf = ω(g) is deﬁned analogously The ...",
      "word_count": 244,
      "source_page": 29,
      "vector_index": 35,
      "start_position": 7575,
      "end_position": 7818
    },
    {
      "chunk_id": 37,
      "text": "2\nA Gentle Start\nLet us begin our mathematical analysis by showing how successful learning can be\nachieved in a relatively simpliﬁed setting Imagine you have just arrived in some\nsmall Paciﬁc island Y...",
      "word_count": 237,
      "source_page": 33,
      "vector_index": 36,
      "start_position": 7819,
      "end_position": 8055
    },
    {
      "chunk_id": 38,
      "text": "2.1\nA Formal Model – The Statistical Learning Framework\n• The learner’s input: In the basic statistical learning setting, the learner has\naccess to the following:\n– Domain set: An arbitrary set, X Thi...",
      "word_count": 222,
      "source_page": 33,
      "vector_index": 37,
      "start_position": 8011,
      "end_position": 8232
    },
    {
      "chunk_id": 39,
      "text": "34\nA Gentle Start\ntasted and their color, softness, and tastiness) Such labeled examples\nare often called training examples We sometimes also refer to S as a\ntraining set.1\n• The learner’s output: The...",
      "word_count": 249,
      "source_page": 34,
      "vector_index": 38,
      "start_position": 8233,
      "end_position": 8481
    },
    {
      "chunk_id": 40,
      "text": "This assumption will be relaxed in\nthe next chapter The labeling function is unknown to the learner In fact,\nthis is just what the learner is trying to ﬁgure out In summary, each pair\nin the training ...",
      "word_count": 236,
      "source_page": 34,
      "vector_index": 39,
      "start_position": 8465,
      "end_position": 8700
    },
    {
      "chunk_id": 41,
      "text": "2.2 Empirical Risk Minimization\n35\ncorrect labeling function f We omit this subscript when it is clear from\nthe context L(D,f)(h) has several synonymous names such as the general-\nization error, the r...",
      "word_count": 243,
      "source_page": 35,
      "vector_index": 40,
      "start_position": 8790,
      "end_position": 9032
    },
    {
      "chunk_id": 42,
      "text": "2.2\nEmpirical Risk Minimization\nAs mentioned earlier, a learning algorithm receives as input a training set S,\nsampled from an unknown distribution D and labeled by some target function\nf, and should ...",
      "word_count": 242,
      "source_page": 35,
      "vector_index": 41,
      "start_position": 8960,
      "end_position": 9201
    },
    {
      "chunk_id": 43,
      "text": "36\nA Gentle Start\npredict the taste of a papaya on the basis of its softness and color Consider a\nsample as depicted in the following:\nAssume that the probability distribution D is such that instances...",
      "word_count": 216,
      "source_page": 36,
      "vector_index": 42,
      "start_position": 9202,
      "end_position": 9417
    },
    {
      "chunk_id": 44,
      "text": "We have found a predictor whose performance on the training set is excellent,\nyet its performance on the true “world” is very poor This phenomenon is called\noverﬁtting Intuitively, overﬁtting occurs w...",
      "word_count": 220,
      "source_page": 36,
      "vector_index": 43,
      "start_position": 9390,
      "end_position": 9609
    },
    {
      "chunk_id": 45,
      "text": "2.3 Empirical Risk Minimization with Inductive Bias\n37\nwith the lowest possible error over S Formally,\nERMH(S) ∈argmin\nh∈H\nLS(h),\nwhere argmin stands for the set of hypotheses in H that achieve the mi...",
      "word_count": 248,
      "source_page": 37,
      "vector_index": 44,
      "start_position": 9610,
      "end_position": 9857
    },
    {
      "chunk_id": 46,
      "text": "We will study this question later\nin the book Intuitively, choosing a more restricted hypothesis class better protects us\nagainst overﬁtting but at the same time might cause us a stronger inductive\nbi...",
      "word_count": 241,
      "source_page": 37,
      "vector_index": 45,
      "start_position": 9825,
      "end_position": 10065
    },
    {
      "chunk_id": 47,
      "text": "38\nA Gentle Start\ndefinition 2.1 (The Realizability Assumption)\nThere exists h⋆∈H s.t L(D,f)(h⋆) = 0 Note that this assumption implies that with probability 1 over\nrandom samples, S, where the instanc...",
      "word_count": 249,
      "source_page": 38,
      "vector_index": 46,
      "start_position": 10084,
      "end_position": 10332
    },
    {
      "chunk_id": 48,
      "text": "We denote this assumption by S ∼Dm where\nm is the size of S, and Dm denotes the probability over m-tuples induced\nby applying D to pick each element of the tuple independently of the other\nmembers of ...",
      "word_count": 223,
      "source_page": 38,
      "vector_index": 47,
      "start_position": 10266,
      "end_position": 10488
    },
    {
      "chunk_id": 49,
      "text": "It is not realistic to expect that with full certainty S will suﬃce to\ndirect the learner toward a good classiﬁer (from the point of view of D), as\nthere is always some probability that the sampled tr...",
      "word_count": 205,
      "source_page": 38,
      "vector_index": 48,
      "start_position": 10398,
      "end_position": 10602
    },
    {
      "chunk_id": 50,
      "text": "2.3 Empirical Risk Minimization with Inductive Bias\n39\ncommonly denoted by ϵ We interpret the event L(D,f)(hS) > ϵ as a failure of the\nlearner, while if L(D,f)(hS) ≤ϵ we view the output of the algorit...",
      "word_count": 247,
      "source_page": 39,
      "vector_index": 49,
      "start_position": 10603,
      "end_position": 10849
    },
    {
      "chunk_id": 51,
      "text": "40\nA Gentle Start\nto the event ∀i, h(xi) = f(xi) Since the examples in the training set are sampled\ni.i.d we get that\nDm({S|x : LS(h) = 0}) = Dm({S|x : ∀i, h(xi) = f(xi)})\n=\nm\nY\ni=1\nD({xi : h(xi) = f(...",
      "word_count": 236,
      "source_page": 40,
      "vector_index": 50,
      "start_position": 10961,
      "end_position": 11196
    },
    {
      "chunk_id": 52,
      "text": "2.4 Exercises\n41\nand let m be an integer that satisﬁes\nm ≥log(|H|/δ)\nϵ Then, for any labeling function, f, and for any distribution, D, for which the\nrealizability assumption holds (that is, for some ...",
      "word_count": 236,
      "source_page": 41,
      "vector_index": 51,
      "start_position": 11283,
      "end_position": 11518
    },
    {
      "chunk_id": 53,
      "text": "2 Let H be a class of binary classiﬁers over a domain X Let D be an unknown\ndistribution over X, and let f be the target hypothesis in H Fix some h ∈H Show that the expected value of LS(h) over the ch...",
      "word_count": 156,
      "source_page": 41,
      "vector_index": 52,
      "start_position": 11506,
      "end_position": 11661
    },
    {
      "chunk_id": 54,
      "text": "42\nA Gentle Start\n1 Let A be the algorithm that returns the smallest rectangle enclosing all\npositive examples in the training set Show that A is an ERM 2 Show that if A receives a training set of siz...",
      "word_count": 238,
      "source_page": 42,
      "vector_index": 53,
      "start_position": 11662,
      "end_position": 11899
    },
    {
      "chunk_id": 55,
      "text": "3\nA Formal Learning Model\nIn this chapter we deﬁne our main formal learning model – the PAC learning\nmodel and its extensions We will consider other notions of learnability in Chap-\nter 7 3.1\nPAC Lear...",
      "word_count": 213,
      "source_page": 43,
      "vector_index": 54,
      "start_position": 11958,
      "end_position": 12170
    },
    {
      "chunk_id": 56,
      "text": "examples generated by D and labeled by f, the algorithm returns\na hypothesis h such that, with probability of at least 1 −δ (over the choice of\nthe examples), L(D,f)(h) ≤ϵ The deﬁnition of Probably Ap...",
      "word_count": 212,
      "source_page": 43,
      "vector_index": 55,
      "start_position": 12129,
      "end_position": 12340
    },
    {
      "chunk_id": 57,
      "text": "44\nA Formal Learning Model\nto reﬂect Our accuracy parameter, ϵ, allows “forgiving” the learner’s classiﬁer\nfor making minor errors Sample Complexity\nThe function mH : (0, 1)2 →N determines the sample ...",
      "word_count": 245,
      "source_page": 44,
      "vector_index": 56,
      "start_position": 12341,
      "end_position": 12585
    },
    {
      "chunk_id": 58,
      "text": "There are inﬁnite classes that are learnable as well (see, for example, Exer-\ncise 3) Later on we will show that what determines the PAC learnability of\na class is not its ﬁniteness but rather a combi...",
      "word_count": 157,
      "source_page": 44,
      "vector_index": 57,
      "start_position": 12544,
      "end_position": 12700
    },
    {
      "chunk_id": 59,
      "text": "3.2 A More General Learning Model\n45\nLearning Problems beyond Binary Classiﬁcation\nThe learning task that we have been discussing so far has to do with predicting a\nbinary label to a given example (li...",
      "word_count": 229,
      "source_page": 45,
      "vector_index": 58,
      "start_position": 12701,
      "end_position": 12929
    },
    {
      "chunk_id": 60,
      "text": "Furthermore, it is maybe more realistic not to assume that the labels\nare fully determined by the features we measure on input elements (in the case of\nthe papayas, it is plausible that two papayas of...",
      "word_count": 250,
      "source_page": 45,
      "vector_index": 59,
      "start_position": 12862,
      "end_position": 13111
    },
    {
      "chunk_id": 61,
      "text": "46\nA Formal Learning Model\nremains the same as before, namely,\nLS(h)\ndef\n=\n|{i ∈[m] : h(xi) ̸= yi}|\nm Given S, a learner can compute LS(h) for any function h : X →{0, 1} Note\nthat LS(h) = LD(uniform o...",
      "word_count": 230,
      "source_page": 46,
      "vector_index": 60,
      "start_position": 13185,
      "end_position": 13414
    },
    {
      "chunk_id": 62,
      "text": "We can now\npresent the formal deﬁnition of agnostic PAC learnability, which is a natural\nextension of the deﬁnition of PAC learnability to the more realistic, nonrealizable,\nlearning setup we have jus...",
      "word_count": 229,
      "source_page": 46,
      "vector_index": 61,
      "start_position": 13355,
      "end_position": 13583
    },
    {
      "chunk_id": 63,
      "text": "3.2 A More General Learning Model\n47\nClearly, if the realizability assumption holds, agnostic PAC learning provides\nthe same guarantee as PAC learning In that sense, agnostic PAC learning gener-\nalize...",
      "word_count": 246,
      "source_page": 47,
      "vector_index": 62,
      "start_position": 13584,
      "end_position": 13829
    },
    {
      "chunk_id": 64,
      "text": "A learning algorithm for such a task\nwill have access to examples of correctly classiﬁed documents and, on the\nbasis of these examples, should output a program that can take as input a\nnew document an...",
      "word_count": 221,
      "source_page": 47,
      "vector_index": 63,
      "start_position": 13776,
      "end_position": 13996
    },
    {
      "chunk_id": 65,
      "text": "Once we determine our domain and label sets, the other components\nof our framework look exactly the same as in the papaya tasting example;\nOur training sample will be a ﬁnite sequence of (feature vect...",
      "word_count": 214,
      "source_page": 47,
      "vector_index": 64,
      "start_position": 13891,
      "end_position": 14104
    },
    {
      "chunk_id": 66,
      "text": "48\nA Formal Learning Model\ndiﬀerent We may evaluate the quality of a hypothesis function, h : X →Y,\nby the expected square diﬀerence between the true labels and their predicted\nvalues, namely,\nLD(h)\nd...",
      "word_count": 237,
      "source_page": 48,
      "vector_index": 65,
      "start_position": 14105,
      "end_position": 14341
    },
    {
      "chunk_id": 67,
      "text": "(3.3)\nThat is, we consider the expectation of the loss of h over objects z picked ran-\ndomly according to D Similarly, we deﬁne the empirical risk to be the expected\nloss over a given sample S = (z1, ...",
      "word_count": 185,
      "source_page": 48,
      "vector_index": 66,
      "start_position": 14303,
      "end_position": 14487
    },
    {
      "chunk_id": 68,
      "text": "3.3 Summary\n49\nThis loss function is used in regression problems We will later see more examples of useful instantiations of loss functions To summarize, we formally deﬁne agnostic PAC learnability fo...",
      "word_count": 230,
      "source_page": 49,
      "vector_index": 67,
      "start_position": 14488,
      "end_position": 14717
    },
    {
      "chunk_id": 69,
      "text": "For that, we need\nto require that the function ℓ(h, ·) is measurable Formally, we assume that there\nis a σ-algebra of subsets of Z, over which the probability D is deﬁned, and that\nthe preimage of eve...",
      "word_count": 240,
      "source_page": 49,
      "vector_index": 68,
      "start_position": 14672,
      "end_position": 14911
    },
    {
      "chunk_id": 70,
      "text": "50\nA Formal Learning Model\nnot impose any restrictions on the underlying distribution over the examples We\nalso generalized the PAC model to arbitrary loss functions We will sometimes\nrefer to the mos...",
      "word_count": 234,
      "source_page": 50,
      "vector_index": 69,
      "start_position": 14912,
      "end_position": 15145
    },
    {
      "chunk_id": 71,
      "text": "As we will see in Chapter 6, if a problem is at all PAC\nlearnable then the sample complexity depends polynomially on 1/ϵ and log(1/δ) Valiant’s deﬁnition also requires that the runtime of the learning...",
      "word_count": 243,
      "source_page": 50,
      "vector_index": 70,
      "start_position": 15104,
      "end_position": 15346
    },
    {
      "chunk_id": 72,
      "text": "3.5 Exercises\n51\n1 Describe an algorithm that implements the ERM rule for learning HSingleton\nin the realizable setup 2 Show that HSingleton is PAC learnable Provide an upper bound on the\nsample compl...",
      "word_count": 228,
      "source_page": 51,
      "vector_index": 71,
      "start_position": 15378,
      "end_position": 15605
    },
    {
      "chunk_id": 73,
      "text": "We consider the hypothesis class of all conjunctions of literals over the d\nvariables The empty conjunction is interpreted as the all-positive hypothesis\n(namely, the function that returns h(x) = 1 fo...",
      "word_count": 222,
      "source_page": 51,
      "vector_index": 72,
      "start_position": 15572,
      "end_position": 15793
    },
    {
      "chunk_id": 74,
      "text": "52\nA Formal Learning Model\nHint: Use the geometric-arithmetic mean inequality 6 Let H be a hypothesis class of binary classiﬁers Show that if H is agnostic\nPAC learnable, then H is PAC learnable as we...",
      "word_count": 231,
      "source_page": 52,
      "vector_index": 73,
      "start_position": 15871,
      "end_position": 16101
    },
    {
      "chunk_id": 75,
      "text": "That is, given such an h and an input, x, the label for\nx is predicted by tossing a coin with bias h(x) toward Heads and predicting\n1 iﬀthe coin comes up Heads Formally, we deﬁne a probabilistic label...",
      "word_count": 245,
      "source_page": 52,
      "vector_index": 74,
      "start_position": 16054,
      "end_position": 16298
    },
    {
      "chunk_id": 76,
      "text": "4\nLearning via Uniform Convergence\nThe ﬁrst formal learning model that we have discussed was the PAC model In Chapter 2 we have shown that under the realizability assumption, any ﬁnite\nhypothesis clas...",
      "word_count": 242,
      "source_page": 54,
      "vector_index": 75,
      "start_position": 16491,
      "end_position": 16732
    },
    {
      "chunk_id": 77,
      "text": "Put another way, we need that uniformly over all hypotheses in\nthe hypothesis class, the empirical risk will be close to the true risk, as formalized\nin the following definition 4.1 (ϵ-representative ...",
      "word_count": 150,
      "source_page": 54,
      "vector_index": 76,
      "start_position": 16692,
      "end_position": 16841
    },
    {
      "chunk_id": 78,
      "text": "4.2 Finite Classes Are Agnostic PAC Learnable\n55\nProof\nFor every h ∈H,\nLD(hS) ≤LS(hS) + ϵ\n2 ≤LS(h) + ϵ\n2 ≤LD(h) + ϵ\n2 + ϵ\n2 = LD(h) + ϵ,\nwhere the ﬁrst and third inequalities are due to the assumption...",
      "word_count": 240,
      "source_page": 55,
      "vector_index": 77,
      "start_position": 16842,
      "end_position": 17081
    },
    {
      "chunk_id": 79,
      "text": "according to D, then, with probability of at least 1 −δ, S\nis ϵ-representative Similar to the deﬁnition of sample complexity for PAC learning, the function\nmUC\nH measures the (minimal) sample complexi...",
      "word_count": 235,
      "source_page": 55,
      "vector_index": 78,
      "start_position": 17021,
      "end_position": 17255
    },
    {
      "chunk_id": 80,
      "text": "56\nLearning via Uniform Convergence\ni.i.d from D we have that for all h ∈H, |LS(h) −LD(h)| ≤ϵ That is,\nDm({S : ∀h ∈H, |LS(h) −LD(h)| ≤ϵ}) ≥1 −δ Equivalently, we need to show that\nDm({S : ∃h ∈H, |LS(h)...",
      "word_count": 234,
      "source_page": 56,
      "vector_index": 79,
      "start_position": 17287,
      "end_position": 17520
    },
    {
      "chunk_id": 81,
      "text": "Hence, the quantity |LD(h)−LS(h)| is the deviation\nof the random variable LS(h) from its expectation We therefore need to show\nthat the measure of LS(h) is concentrated around its expected value A bas...",
      "word_count": 238,
      "source_page": 56,
      "vector_index": 80,
      "start_position": 17490,
      "end_position": 17727
    },
    {
      "chunk_id": 82,
      "text": "4.2 Finite Classes Are Agnostic PAC Learnable\n57\nfurther assume that the range of ℓis [0, 1] and therefore θi ∈[0, 1] We therefore\nobtain that\nDm({S : |LS(h) −LD(h)| > ϵ}) = P\n\"\f\f\f\f\f\n1\nm\nm\nX\ni=1\nθi −µ...",
      "word_count": 240,
      "source_page": 57,
      "vector_index": 81,
      "start_position": 17728,
      "end_position": 17967
    },
    {
      "chunk_id": 83,
      "text": "Consider a hypothesis class that is parameterized by d parameters For\nexample, let X = R, Y = {±1}, and the hypothesis class, H, be all functions\nof the form hθ(x) = sign(x −θ) That is, each hypothesi...",
      "word_count": 181,
      "source_page": 57,
      "vector_index": 82,
      "start_position": 17934,
      "end_position": 18114
    },
    {
      "chunk_id": 84,
      "text": "58\nLearning via Uniform Convergence\nclasses is bounded by 128d+2 log(2/δ)\nϵ2 This upper bound on the sample complex-\nity has the deﬁciency of being dependent on the speciﬁc representation of real\nnumb...",
      "word_count": 239,
      "source_page": 58,
      "vector_index": 83,
      "start_position": 18115,
      "end_position": 18353
    },
    {
      "chunk_id": 85,
      "text": "The relation between uniform con-\nvergence and learnability was thoroughly studied by Vapnik – see (Vapnik 1992,\nVapnik 1995, Vapnik 1998) In fact, as we will see later in Chapter 6, the funda-\nmental...",
      "word_count": 173,
      "source_page": 58,
      "vector_index": 84,
      "start_position": 18294,
      "end_position": 18466
    },
    {
      "chunk_id": 86,
      "text": "5\nThe Bias-Complexity Tradeoﬀ\nIn Chapter 2 we saw that unless one is careful, the training data can mislead the\nlearner, and result in overﬁtting To overcome this problem, we restricted the\nsearch spa...",
      "word_count": 250,
      "source_page": 60,
      "vector_index": 85,
      "start_position": 18531,
      "end_position": 18780
    },
    {
      "chunk_id": 87,
      "text": "examples from D, there is a high chance it\noutputs a predictor h that has a low risk The ﬁrst part of this chapter addresses this question formally The No-Free-\nLunch theorem states that no such unive...",
      "word_count": 231,
      "source_page": 60,
      "vector_index": 86,
      "start_position": 18753,
      "end_position": 18983
    },
    {
      "chunk_id": 88,
      "text": "5.1 The No-Free-Lunch Theorem\n61\nagnostic PAC model, in which we require that the risk of the output hypothesis\nwill not be much larger than minh∈H LD(h) In the second part of this chapter we study th...",
      "word_count": 249,
      "source_page": 61,
      "vector_index": 87,
      "start_position": 19027,
      "end_position": 19275
    },
    {
      "chunk_id": 89,
      "text": "We do this by showing\nthat no learner can succeed on all learning tasks, as formalized in the following\ntheorem:\ntheorem 5.1 (No-Free-Lunch)\nLet A be any learning algorithm for the task\nof binary clas...",
      "word_count": 249,
      "source_page": 61,
      "vector_index": 88,
      "start_position": 19217,
      "end_position": 19465
    },
    {
      "chunk_id": 90,
      "text": "62\nThe Bias-Complexity Tradeoﬀ\nC × {0, 1} deﬁned by\nDi({(x, y)}) =\n(\n1/|C|\nif y = fi(x)\n0\notherwise That is, the probability to choose a pair (x, y) is 1/|C| if the label y is indeed\nthe true label ac...",
      "word_count": 250,
      "source_page": 62,
      "vector_index": 89,
      "start_position": 19497,
      "end_position": 19746
    },
    {
      "chunk_id": 91,
      "text": "5.1 The No-Free-Lunch Theorem\n63\nfunction h : C →{0, 1} and every i we have\nLDi(h) =\n1\n2m\nX\nx∈C\n1[h(x)̸=fi(x)]\n≥\n1\n2m\np\nX\nr=1\n1[h(vr)̸=fi(vr)]\n≥1\n2p\np\nX\nr=1\n1[h(vr)̸=fi(vr)] (5.5)\nHence,\n1\nT\nT\nX\ni=1\nL...",
      "word_count": 243,
      "source_page": 63,
      "vector_index": 90,
      "start_position": 19874,
      "end_position": 20116
    },
    {
      "chunk_id": 92,
      "text": "64\nThe Bias-Complexity Tradeoﬀ\nProof\nAssume, by way of contradiction, that the class is learnable Choose\nsome ϵ < 1/8 and δ < 1/7 By the deﬁnition of PAC learnability, there must\nbe some learning algo...",
      "word_count": 240,
      "source_page": 64,
      "vector_index": 91,
      "start_position": 20186,
      "end_position": 20425
    },
    {
      "chunk_id": 93,
      "text": "But how should we choose a good hypothesis class On the one hand, we want\nto believe that this class includes the hypothesis that has no error at all (in the\nPAC setting), or at least that the smalles...",
      "word_count": 208,
      "source_page": 64,
      "vector_index": 92,
      "start_position": 20371,
      "end_position": 20578
    },
    {
      "chunk_id": 94,
      "text": "5.3 Summary\n65\n• The Estimation Error – the diﬀerence between the approximation error\nand the error achieved by the ERM predictor The estimation error results\nbecause the empirical risk (i.e., trainin...",
      "word_count": 227,
      "source_page": 65,
      "vector_index": 93,
      "start_position": 20652,
      "end_position": 20878
    },
    {
      "chunk_id": 95,
      "text": "On the other hand, choosing H to be a\nvery small set reduces the estimation error but might increase the approximation\nerror or, in other words, might lead to underﬁtting Of course, a great choice for...",
      "word_count": 241,
      "source_page": 65,
      "vector_index": 94,
      "start_position": 20829,
      "end_position": 21069
    },
    {
      "chunk_id": 96,
      "text": "66\nThe Bias-Complexity Tradeoﬀ\nbe small In the next chapter we will study in more detail the behavior of the\nestimation error In Chapter 7 we will discuss alternative ways to express prior\nknowledge 5...",
      "word_count": 242,
      "source_page": 66,
      "vector_index": 95,
      "start_position": 21152,
      "end_position": 21393
    },
    {
      "chunk_id": 97,
      "text": "6\nThe VC-Dimension\nIn the previous chapter, we decomposed the error of the ERMH rule into ap-\nproximation error and estimation error The approximation error depends on\nthe ﬁt of our prior knowledge (a...",
      "word_count": 247,
      "source_page": 67,
      "vector_index": 96,
      "start_position": 21487,
      "end_position": 21733
    },
    {
      "chunk_id": 98,
      "text": "This characterization was ﬁrst discovered by Vladimir Vapnik and Alexey\nChervonenkis in 1970 and relies on a combinatorial notion called the Vapnik-\nChervonenkis dimension (VC-dimension) We formally d...",
      "word_count": 206,
      "source_page": 67,
      "vector_index": 97,
      "start_position": 21678,
      "end_position": 21883
    },
    {
      "chunk_id": 99,
      "text": "68\nThe VC-Dimension\nsize Nevertheless, the following lemma shows that H is learnable in the PAC\nmodel using the ERM algorithm Lemma 6.1\nLet H be the class of thresholds as deﬁned earlier Then, H is\nPA...",
      "word_count": 232,
      "source_page": 68,
      "vector_index": 98,
      "start_position": 21884,
      "end_position": 22115
    },
    {
      "chunk_id": 100,
      "text": "Therefore, a suﬃcient condition for LD(hS) ≤ϵ is that both\nb0 ≥a0 and b1 ≤a1 In other words,\nP\nS∼Dm[LD(hS) > ϵ] ≤\nP\nS∼Dm[b0 < a0 ∨b1 > a1],\nand using the union bound we can bound the preceding by\nP\nS∼...",
      "word_count": 210,
      "source_page": 68,
      "vector_index": 99,
      "start_position": 22061,
      "end_position": 22270
    },
    {
      "chunk_id": 101,
      "text": "6.2 The VC-Dimension\n69\nrestricting the hypothesis class, for any learning algorithm, an adversary can\nconstruct a distribution for which the learning algorithm will perform poorly,\nwhile there is ano...",
      "word_count": 232,
      "source_page": 69,
      "vector_index": 100,
      "start_position": 22271,
      "end_position": 22502
    },
    {
      "chunk_id": 102,
      "text": "That is,\nHC = {(h(c1), , h(cm)) : h ∈H},\nwhere we represent each function from C to {0, 1} as a vector in {0, 1}|C| If the restriction of H to C is the set of all functions from C to {0, 1}, then\nwe s...",
      "word_count": 214,
      "source_page": 69,
      "vector_index": 101,
      "start_position": 22477,
      "end_position": 22690
    },
    {
      "chunk_id": 103,
      "text": "No h ∈H can account for the labeling (0, 1), because any threshold that assigns\nthe label 0 to c1 must assign the label 0 to c2 as well Therefore not all functions\nfrom C to {0, 1} are included in HC;...",
      "word_count": 195,
      "source_page": 69,
      "vector_index": 102,
      "start_position": 22642,
      "end_position": 22836
    },
    {
      "chunk_id": 104,
      "text": "70\nThe VC-Dimension\nCorollary 6.4 tells us that if H shatters some set C of size 2m then we cannot\nlearn H using m examples Intuitively, if a set C is shattered by H, and we\nreceive a sample containin...",
      "word_count": 249,
      "source_page": 70,
      "vector_index": 103,
      "start_position": 22837,
      "end_position": 23085
    },
    {
      "chunk_id": 105,
      "text": "6.3 Examples\n71\n6.3.2\nIntervals\nLet H be the class of intervals over R, namely, H = {ha,b : a, b ∈R, a < b},\nwhere ha,b : R →{0, 1} is a function such that ha,b(x) = 1[x∈(a,b)] Take the set\nC = {1, 2}...",
      "word_count": 247,
      "source_page": 71,
      "vector_index": 104,
      "start_position": 23187,
      "end_position": 23433
    },
    {
      "chunk_id": 106,
      "text": "Now, consider any set C ⊂R2 of 5 points In C, take a\nleftmost point (whose ﬁrst coordinate is the smallest in C), a rightmost point\n(ﬁrst coordinate is the largest), a lowest point (second coordinate ...",
      "word_count": 167,
      "source_page": 71,
      "vector_index": 105,
      "start_position": 23386,
      "end_position": 23552
    },
    {
      "chunk_id": 107,
      "text": "72\nThe VC-Dimension\n6.3.4\nFinite Classes\nLet H be a ﬁnite class Then, clearly, for any set C we have |HC| ≤|H| and thus C\ncannot be shattered if |H| < 2|C| This implies that VCdim(H) ≤log2(|H|) This\ns...",
      "word_count": 244,
      "source_page": 72,
      "vector_index": 106,
      "start_position": 23553,
      "end_position": 23796
    },
    {
      "chunk_id": 108,
      "text": "It\nis possible to prove that VCdim(H) = ∞, namely, for every d, one can ﬁnd d\npoints that are shattered by H (see Exercise 8) 6.4\nThe Fundamental Theorem of PAC learning\nWe have already shown that a c...",
      "word_count": 225,
      "source_page": 72,
      "vector_index": 107,
      "start_position": 23751,
      "end_position": 23975
    },
    {
      "chunk_id": 109,
      "text": "6.5 Proof of Theorem 6.7\n73\n1 H has the uniform convergence property with sample complexity\nC1\nd + log(1/δ)\nϵ2\n≤m\nUC\nH (ϵ, δ) ≤C2\nd + log(1/δ)\nϵ2\n2 H is agnostic PAC learnable with sample complexity\nC...",
      "word_count": 215,
      "source_page": 73,
      "vector_index": 108,
      "start_position": 23976,
      "end_position": 24190
    },
    {
      "chunk_id": 110,
      "text": "The implications 4 →6 and 5 →6 follow from\nthe No-Free-Lunch theorem The diﬃcult part is to show that 6 →1 The proof\nis based on two main claims:\n• If VCdim(H) = d, then even though H might be inﬁnite...",
      "word_count": 197,
      "source_page": 73,
      "vector_index": 109,
      "start_position": 24170,
      "end_position": 24366
    },
    {
      "chunk_id": 111,
      "text": "74\nThe VC-Dimension\ndefinition 6.9 (Growth Function)\nLet H be a hypothesis class Then the\ngrowth function of H, denoted τH : N →N, is deﬁned as\nτH(m) =\nmax\nC⊂X:|C|=m\n\f\fHC In words, τH(m) is the number...",
      "word_count": 235,
      "source_page": 74,
      "vector_index": 110,
      "start_position": 24367,
      "end_position": 24601
    },
    {
      "chunk_id": 112,
      "text": ", cm} we have\n∀H,\n|HC| ≤|{B ⊆C : H shatters B}| (6.3)\nThe reason why Equation (6.3) is suﬃcient to prove the lemma is that if VCdim(H) ≤\nd then no set whose size is larger than d is shattered by H and...",
      "word_count": 216,
      "source_page": 74,
      "vector_index": 111,
      "start_position": 24544,
      "end_position": 24759
    },
    {
      "chunk_id": 113,
      "text": "6.5 Proof of Theorem 6.7\n75\nNext, deﬁne H′ ⊆H to be\nH′ = {h ∈H : ∃h′ ∈H s.t (1 −h′(c1), h′(c2), , h′(cm))\n= (h(c1), h(c2), , h(cm)},\nnamely, H′ contains pairs of hypotheses that agree on C′ and diﬀer ...",
      "word_count": 242,
      "source_page": 75,
      "vector_index": 112,
      "start_position": 24795,
      "end_position": 25036
    },
    {
      "chunk_id": 114,
      "text": "Formally,\ntheorem 6.11\nLet H be a class and let τH be its growth function Then, for\nevery D and every δ ∈(0, 1), with probability of at least 1 −δ over the choice of\nS ∼Dm we have\n|LD(h) −LS(h)| ≤4 +\n...",
      "word_count": 158,
      "source_page": 75,
      "vector_index": 113,
      "start_position": 24989,
      "end_position": 25146
    },
    {
      "chunk_id": 115,
      "text": "76\nThe VC-Dimension\nTo ensure that the preceding is at most ϵ we need that\nm ≥2d log(m)\n(δϵ)2\n+ 2 d log(2e/d)\n(δϵ)2 Standard algebraic manipulations (see Lemma A.2 in Appendix A) show that a\nsuﬃcient ...",
      "word_count": 245,
      "source_page": 76,
      "vector_index": 114,
      "start_position": 25147,
      "end_position": 25391
    },
    {
      "chunk_id": 116,
      "text": "6.5 Proof of Theorem 6.7\n77\nThe expectation on the right-hand side is over a choice of two i.i.d samples\nS = z1, , zm and S′ = z′\n1, , z′\nm Since all of these 2m vectors are chosen\ni.i.d., nothing wil...",
      "word_count": 227,
      "source_page": 77,
      "vector_index": 115,
      "start_position": 25441,
      "end_position": 25667
    },
    {
      "chunk_id": 117,
      "text": "Next, ﬁx S and S′, and let C be the instances appearing in S and S′ Then, we\ncan take the supremum only over h ∈HC Therefore,\nE\nσ∼U m\n±\n\"\nsup\nh∈H\n1\nm\n\f\f\f\f\f\nm\nX\ni=1\nσi(ℓ(h, z′\ni) −ℓ(h, zi))\n\f\f\f\f\f\n#\n=\nE...",
      "word_count": 198,
      "source_page": 77,
      "vector_index": 116,
      "start_position": 25642,
      "end_position": 25839
    },
    {
      "chunk_id": 118,
      "text": "78\nThe VC-Dimension\n6.6\nSummary\nThe fundamental theorem of learning theory characterizes PAC learnability of\nclasses of binary classiﬁers using VC-dimension The VC-dimension of a class\nis a combinator...",
      "word_count": 235,
      "source_page": 78,
      "vector_index": 117,
      "start_position": 25840,
      "end_position": 26074
    },
    {
      "chunk_id": 119,
      "text": "See (Shalev-Shwartz, Shamir, Srebro &\nSridharan 2010, Daniely, Sabato, Ben-David & Shalev-Shwartz 2011) Sauer’s lemma has been proved by Sauer in response to a problem of Erdos\n(Sauer 1972) Shelah (wi...",
      "word_count": 179,
      "source_page": 78,
      "vector_index": 118,
      "start_position": 26046,
      "end_position": 26224
    },
    {
      "chunk_id": 120,
      "text": "6.8 Exercises\n79\n2 Hat−most−k = {h ∈{0, 1}X : |{x : h(x) = 1}| ≤k or |{x : h(x) = 0}| ≤k} 3 Let X be the Boolean hypercube {0, 1}n For a set I ⊆{1, 2, , n} we deﬁne\na parity function hI as follows On ...",
      "word_count": 246,
      "source_page": 79,
      "vector_index": 119,
      "start_position": 26225,
      "end_position": 26470
    },
    {
      "chunk_id": 121,
      "text": ", xd (d ≥2) We already know that this\nclass is ﬁnite and thus (agnostic) PAC learnable In this question we calculate\nVCdim(Hd\ncon) 1 Show that |Hd\ncon| ≤3d + 1 2 Conclude that VCdim(H) ≤d log 3 3 Show...",
      "word_count": 207,
      "source_page": 79,
      "vector_index": 120,
      "start_position": 26454,
      "end_position": 26660
    },
    {
      "chunk_id": 122,
      "text": "80\nThe VC-Dimension\nAs in Hd\ncon, the empty conjunction is interpreted as the all-positive hy-\npothesis We augment Hd\nmcon with the all-negative hypothesis h− Show\nthat VCdim(Hd\nmcon) = d 7 We have sh...",
      "word_count": 243,
      "source_page": 80,
      "vector_index": 121,
      "start_position": 26661,
      "end_position": 26903
    },
    {
      "chunk_id": 123,
      "text": "Consider the domain X = R, and the hypothesis class\nH = {x 7→⌈sin(θx)⌉: θ ∈R}\n(here, we take ⌈−1⌉= 0) Prove that VCdim(H) = ∞ Hint: There is more than one way to prove the required result One option\ni...",
      "word_count": 218,
      "source_page": 80,
      "vector_index": 122,
      "start_position": 26878,
      "end_position": 27095
    },
    {
      "chunk_id": 124,
      "text": "6.8 Exercises\n81\n1 Prove that\nVCdim (∪r\ni=1Hi) ≤4d log(2d) + 2 log(r) Hint: Take a set of k examples and assume that they are shattered by\nthe union class Therefore, the union class can produce all 2k...",
      "word_count": 247,
      "source_page": 81,
      "vector_index": 123,
      "start_position": 27096,
      "end_position": 27342
    },
    {
      "chunk_id": 125,
      "text": "Note that if a family of functions\nis linearly closed then we can view it as a vector space over the reals For a\nfunction g : Rn →R and a family of functions F, let F +g\ndef\n= {f +g : f ∈F} Hypothesis...",
      "word_count": 248,
      "source_page": 81,
      "vector_index": 124,
      "start_position": 27298,
      "end_position": 27545
    },
    {
      "chunk_id": 126,
      "text": "7\nNonuniform Learnability\nThe notions of PAC learnability discussed so far in the book allow the sample\nsizes to depend on the accuracy and conﬁdence parameters, but they are uniform\nwith respect to t...",
      "word_count": 244,
      "source_page": 83,
      "vector_index": 125,
      "start_position": 27691,
      "end_position": 27934
    },
    {
      "chunk_id": 127,
      "text": "84\nNonuniform Learnability\nwith a low risk compared to the minimal risk achieved by hypotheses in our class\n(in the agnostic case) Therefore, the sample size depends only on the accuracy\nand conﬁdence...",
      "word_count": 229,
      "source_page": 84,
      "vector_index": 126,
      "start_position": 28053,
      "end_position": 28281
    },
    {
      "chunk_id": 128,
      "text": "At this point it might be useful to recall the deﬁnition of agnostic PAC learn-\nability (Deﬁnition 3.3):\nA hypothesis class H is agnostically PAC learnable if there exist a learning algo-\nrithm, A, an...",
      "word_count": 243,
      "source_page": 84,
      "vector_index": 127,
      "start_position": 28187,
      "end_position": 28429
    },
    {
      "chunk_id": 129,
      "text": "7.2 Structural Risk Minimization\n85\ntheorem 7.3\nLet H be a hypothesis class that can be written as a countable\nunion of hypothesis classes, H = S\nn∈N Hn, where each Hn enjoys the uniform\nconvergence p...",
      "word_count": 238,
      "source_page": 85,
      "vector_index": 128,
      "start_position": 28471,
      "end_position": 28708
    },
    {
      "chunk_id": 130,
      "text": "In addition, using the deﬁnition of mNUL\nH\nwe know that\nfor any distribution D that satisﬁes the realizability assumption with respect to\nHn, with probability of at least 6/7 over S ∼Dn we have that L...",
      "word_count": 247,
      "source_page": 85,
      "vector_index": 129,
      "start_position": 28646,
      "end_position": 28892
    },
    {
      "chunk_id": 131,
      "text": "86\nNonuniform Learnability\nConcretely, let H be a hypothesis class that can be written as H = S\nn∈N Hn For example, H may be the class of all polynomial classiﬁers where each Hn is\nthe class of polyno...",
      "word_count": 230,
      "source_page": 86,
      "vector_index": 130,
      "start_position": 28978,
      "end_position": 29207
    },
    {
      "chunk_id": 132,
      "text": "We refer to w as\na weight function over the hypothesis classes H1, H2, Such a weight function\ncan reﬂect the importance that the learner attributes to each hypothesis class,\nor some measure of the com...",
      "word_count": 226,
      "source_page": 86,
      "vector_index": 131,
      "start_position": 29168,
      "end_position": 29393
    },
    {
      "chunk_id": 133,
      "text": "The SRM paradigm searches for h that minimizes this bound, as formalized\nin the following pseudocode:\nStructural Risk Minimization (SRM)\nprior knowledge:\nH = S\nn Hn where Hn has uniform convergence wi...",
      "word_count": 220,
      "source_page": 87,
      "vector_index": 132,
      "start_position": 29626,
      "end_position": 29845
    },
    {
      "chunk_id": 134,
      "text": "88\nNonuniform Learnability\nProof\nLet A be the SRM algorithm with respect to the weighting function w For every h ∈H, ϵ, and δ, let m ≥mUC\nHn(h)(ϵ, w(n(h))δ) Using the fact that\nP\nn w(n) = 1, we can ap...",
      "word_count": 230,
      "source_page": 88,
      "vector_index": 133,
      "start_position": 29846,
      "end_position": 30075
    },
    {
      "chunk_id": 135,
      "text": "It turns out that, for any inﬁnite domain set, X, the class of all binary valued\nfunctions over X is not a countable union of classes of ﬁnite VC-dimension We\nleave the proof of this claim as a (nontr...",
      "word_count": 215,
      "source_page": 88,
      "vector_index": 134,
      "start_position": 30033,
      "end_position": 30247
    },
    {
      "chunk_id": 136,
      "text": "7.3 Minimum Description Length and Occam’s Razor\n89\nthe index of the ﬁrst class in which h resides That cost increases with the index\nof the class, which can be interpreted as reﬂecting the value of k...",
      "word_count": 236,
      "source_page": 89,
      "vector_index": 135,
      "start_position": 30314,
      "end_position": 30549
    },
    {
      "chunk_id": 137,
      "text": "We assign higher weights to hypotheses that we\nbelieve are more likely to be the correct one, and in the learning algorithm we\nprefer hypotheses that have higher weights In this section we discuss a p...",
      "word_count": 234,
      "source_page": 89,
      "vector_index": 136,
      "start_position": 30493,
      "end_position": 30726
    },
    {
      "chunk_id": 138,
      "text": "90\nNonuniform Learnability\nlemma 7.6 (Kraft Inequality)\nIf S ⊆{0, 1}∗is a preﬁx-free set of strings, then\nX\nσ∈S\n1\n2|σ| ≤1 Proof\nDeﬁne a probability distribution over the members of S as follows: Re-\np...",
      "word_count": 245,
      "source_page": 90,
      "vector_index": 137,
      "start_position": 30778,
      "end_position": 31022
    },
    {
      "chunk_id": 139,
      "text": "This observation immediately yields the following:\ntheorem 7.7\nLet H be a hypothesis class and let d : H →{0, 1}∗be a preﬁx-\nfree description language for H Then, for every sample size, m, every conﬁd...",
      "word_count": 238,
      "source_page": 90,
      "vector_index": 138,
      "start_position": 30945,
      "end_position": 31182
    },
    {
      "chunk_id": 140,
      "text": "7.3 Minimum Description Length and Occam’s Razor\n91\nbinary string obtained by running the gzip command on the program (this yields\na preﬁx-free description language over the alphabet {0, 1}) Then, |h|...",
      "word_count": 243,
      "source_page": 91,
      "vector_index": 139,
      "start_position": 31183,
      "end_position": 31425
    },
    {
      "chunk_id": 141,
      "text": "At a second glance, our Occam razor claim might seem somewhat problematic In the context in which the Occam razor principle is usually invoked in science,\nthe language according to which complexity is...",
      "word_count": 234,
      "source_page": 91,
      "vector_index": 140,
      "start_position": 31378,
      "end_position": 31611
    },
    {
      "chunk_id": 142,
      "text": "92\nNonuniform Learnability\n7.4\nOther Notions of Learnability – Consistency\nThe notion of learnability can be further relaxed by allowing the needed sample\nsizes to depend not only on ϵ, δ, and h but a...",
      "word_count": 240,
      "source_page": 92,
      "vector_index": 141,
      "start_position": 31685,
      "end_position": 31924
    },
    {
      "chunk_id": 143,
      "text": "Clearly if an algorithm nonuniformly learns a class\nH it is also universally consistent for that class The relaxation is strict in the\nsense that there are consistent learning rules that are not succe...",
      "word_count": 217,
      "source_page": 92,
      "vector_index": 142,
      "start_position": 31889,
      "end_position": 32105
    },
    {
      "chunk_id": 144,
      "text": "7.5 Discussing the Diﬀerent Notions of Learnability\n93\nwhich led to overﬁtting, is in fact the Memorize algorithm In the next section\nwe discuss the signiﬁcance of the diﬀerent notions of learnability...",
      "word_count": 242,
      "source_page": 93,
      "vector_index": 143,
      "start_position": 32183,
      "end_position": 32424
    },
    {
      "chunk_id": 145,
      "text": "When approaching a learning problem, a natural question is how many exam-\nples we need to collect in order to learn it Here, PAC learning gives a crisp\nanswer However, for both nonuniform learning and...",
      "word_count": 249,
      "source_page": 93,
      "vector_index": 144,
      "start_position": 32396,
      "end_position": 32644
    },
    {
      "chunk_id": 146,
      "text": "94\nNonuniform Learnability\nerror term, we do not know how many more examples are needed to make the\nestimation error small How to Learn How to Express Prior Knowledge Maybe the most useful aspect of t...",
      "word_count": 201,
      "source_page": 94,
      "vector_index": 145,
      "start_position": 32680,
      "end_position": 32880
    },
    {
      "chunk_id": 147,
      "text": "We elaborate on model selection in Chapter 11 and here we give a brief example Consider the problem of ﬁtting a one dimensional polynomial to data; namely,\nour goal is to learn a function, h : R →R, a...",
      "word_count": 207,
      "source_page": 94,
      "vector_index": 146,
      "start_position": 32832,
      "end_position": 33038
    },
    {
      "chunk_id": 148,
      "text": "Therefore, if we choose H to be the class of all polynomials up to degree 10 then\nthe ERM rule with respect to this class would output a 10 degree polynomial\nand would overﬁt On the other hand, if we ...",
      "word_count": 162,
      "source_page": 94,
      "vector_index": 147,
      "start_position": 32975,
      "end_position": 33136
    },
    {
      "chunk_id": 149,
      "text": "7.5 Discussing the Diﬀerent Notions of Learnability\n95\nadvance how many examples are needed to compete with the best hypothesis in\nH Unlike the notions of PAC learnability and nonuniform learnability,...",
      "word_count": 243,
      "source_page": 95,
      "vector_index": 148,
      "start_position": 33137,
      "end_position": 33379
    },
    {
      "chunk_id": 150,
      "text": "However, this argument is problematic for\ntwo reasons First, maybe it is the case that for most “natural” distributions we\nwill observe in practice that the sample complexity of the consistent algorit...",
      "word_count": 246,
      "source_page": 95,
      "vector_index": 149,
      "start_position": 33328,
      "end_position": 33573
    },
    {
      "chunk_id": 151,
      "text": "96\nNonuniform Learnability\nwill get a sample of m i.i.d training examples, labeled by h⋆, then A is likely to\nreturn a classiﬁer with a larger error The consistency of Memorize implies the following: ...",
      "word_count": 222,
      "source_page": 96,
      "vector_index": 150,
      "start_position": 33666,
      "end_position": 33887
    },
    {
      "chunk_id": 152,
      "text": "We discussed the usefulness of the diﬀerent deﬁnitions of learnability For hypothesis classes that are countable, we can apply the Minimum Descrip-\ntion Length scheme, where hypotheses with shorter de...",
      "word_count": 244,
      "source_page": 96,
      "vector_index": 151,
      "start_position": 33850,
      "end_position": 34093
    },
    {
      "chunk_id": 153,
      "text": "7.7 Bibliographic Remarks\n97\n7.7\nBibliographic Remarks\nOur deﬁnition of nonuniform learnability is related to the deﬁnition of an Occam-\nalgorithm in Blumer, Ehrenfeucht, Haussler & Warmuth (1987) The...",
      "word_count": 248,
      "source_page": 97,
      "vector_index": 152,
      "start_position": 34117,
      "end_position": 34364
    },
    {
      "chunk_id": 154,
      "text": "That is, if i < j, then\nw(hi) ≤w(hj) 3 • Consider a hypothesis class H = S∞\nn=1 Hn, where for every n ∈N, Hn is\nﬁnite Find a weighting function w : H →[0, 1] such that P\nh∈H w(h) ≤\n1 and so that for a...",
      "word_count": 160,
      "source_page": 97,
      "vector_index": 153,
      "start_position": 34355,
      "end_position": 34514
    },
    {
      "chunk_id": 155,
      "text": "98\nNonuniform Learnability\nProve a bound on LD(hS)−LD(h∗\nB) in terms of B, the conﬁdence parameter\nδ, and the size of the training set m • Note: Such bounds are known as oracle inequalities in the lit...",
      "word_count": 249,
      "source_page": 98,
      "vector_index": 154,
      "start_position": 34515,
      "end_position": 34763
    },
    {
      "chunk_id": 156,
      "text": "3 Let H be a class that shatters an inﬁnite set Then, for every sequence\nof classes (Hn : n ∈N) such that H = S\nn∈N Hn, there exists some n for\nwhich VCdim(Hn) = ∞ Hint: Given a class H that shatters ...",
      "word_count": 242,
      "source_page": 98,
      "vector_index": 155,
      "start_position": 34753,
      "end_position": 34994
    },
    {
      "chunk_id": 157,
      "text": "8\nThe Runtime of Learning\nSo far in the book we have studied the statistical perspective of learning, namely,\nhow many samples are needed for learning In other words, we focused on the\namount of infor...",
      "word_count": 245,
      "source_page": 100,
      "vector_index": 156,
      "start_position": 35137,
      "end_position": 35381
    },
    {
      "chunk_id": 158,
      "text": "The actual runtime (in seconds) of an algorithm depends on the speciﬁc ma-\nchine the algorithm is being implemented on (e.g., what the clock rate of the\nmachine’s CPU is) To avoid dependence on the sp...",
      "word_count": 203,
      "source_page": 100,
      "vector_index": 157,
      "start_position": 35332,
      "end_position": 35534
    },
    {
      "chunk_id": 159,
      "text": "8.1 Computational Complexity of Learning\n101\nnumber of bits in its representation) For machine learning tasks, the notion of\nan input size is not so clear An algorithm aims to detect some pattern in a...",
      "word_count": 244,
      "source_page": 101,
      "vector_index": 158,
      "start_position": 35610,
      "end_position": 35853
    },
    {
      "chunk_id": 160,
      "text": "Given parameters\nϵ, δ, the algorithm should output a hypothesis h such that with probability of\nat least 1 −δ,\nLD(h) ≤min\nh′∈H LD(h′) + ϵ As mentioned before, the actual runtime of an algorithm in sec...",
      "word_count": 218,
      "source_page": 101,
      "vector_index": 159,
      "start_position": 35812,
      "end_position": 36029
    },
    {
      "chunk_id": 161,
      "text": "102\nThe Runtime of Learning\ndomain set, or some measures of the complexity of the hypothesis class with\nwhich the algorithm’s output is compared To illustrate this, consider a learning algorithm for t...",
      "word_count": 227,
      "source_page": 102,
      "vector_index": 160,
      "start_position": 36098,
      "end_position": 36324
    },
    {
      "chunk_id": 162,
      "text": "On the basis of the preceding, a learning algorithm can “cheat,” by\ntransferring the computational burden to the output hypothesis For example,\nthe algorithm can simply deﬁne the output hypothesis to ...",
      "word_count": 203,
      "source_page": 102,
      "vector_index": 161,
      "start_position": 36261,
      "end_position": 36463
    },
    {
      "chunk_id": 163,
      "text": "In the next subsection the advanced\nreader may ﬁnd a formal deﬁnition of the computational complexity of learning 8.1.1\nFormal Deﬁnition*\nThe deﬁnition that follows relies on a notion of an underlying...",
      "word_count": 212,
      "source_page": 102,
      "vector_index": 162,
      "start_position": 36416,
      "end_position": 36627
    },
    {
      "chunk_id": 164,
      "text": "8.2 Implementing the ERM Rule\n103\nover Z, and input ϵ, δ ∈(0, 1), when A has access to samples generated i.i.d by D,\n• A terminates after performing at most cf(ϵ, δ) operations\n• The output of A, deno...",
      "word_count": 245,
      "source_page": 103,
      "vector_index": 163,
      "start_position": 36628,
      "end_position": 36872
    },
    {
      "chunk_id": 165,
      "text": "We say that A is an eﬃcient algorithm with respect to a sequence (Zn, Hn, ℓn)\nif its runtime is O(p(n, 1/ϵ, 1/δ)) for some polynomial p From this deﬁnition we see that the question whether a general l...",
      "word_count": 237,
      "source_page": 103,
      "vector_index": 164,
      "start_position": 36814,
      "end_position": 37050
    },
    {
      "chunk_id": 166,
      "text": "104\nThe Runtime of Learning\nOn a ﬁnite input sample S ∈Zm output some h ∈H that minimizes the empirical loss,\nLS(h) =\n1\n|S|\nP\nz∈S ℓ(h, z) This section studies the runtime of implementing the ERM rule ...",
      "word_count": 235,
      "source_page": 104,
      "vector_index": 165,
      "start_position": 37113,
      "end_position": 37347
    },
    {
      "chunk_id": 167,
      "text": "Therefore, the sample\ncomplexity has a mild dependence on the size of H In the example of C++\nprograms mentioned before, the number of hypotheses is 210,000 but the sample\ncomplexity is only c(10, 000...",
      "word_count": 227,
      "source_page": 104,
      "vector_index": 166,
      "start_position": 37311,
      "end_position": 37537
    },
    {
      "chunk_id": 168,
      "text": "Formally, if we deﬁne a sequence of\nproblems (Zn, Hn, ℓn)∞\nn=1 such that log(|Hn|) = n, then the exhaustive search\napproach yields an exponential runtime In the example of C++ programs, if Hn\nis the s...",
      "word_count": 151,
      "source_page": 104,
      "vector_index": 167,
      "start_position": 37465,
      "end_position": 37615
    },
    {
      "chunk_id": 169,
      "text": "8.2 Implementing the ERM Rule\n105\n8.2.2\nAxis Aligned Rectangles\nLet Hn be the class of axis aligned rectangles in Rn, namely,\nHn = {h(a1,...,an,b1,...,bn) : ∀i, ai ≤bi}\nwhere\nh(a1,...,an,b1,...,bn)(x,...",
      "word_count": 223,
      "source_page": 105,
      "vector_index": 168,
      "start_position": 37616,
      "end_position": 37838
    },
    {
      "chunk_id": 170,
      "text": "It is easy to verify that the resulting rectangle has zero training error and that\nthe runtime of ﬁnding each ai and bi is O(m) Hence, the total runtime of this\nprocedure is O(nm) Not Eﬃciently Learna...",
      "word_count": 243,
      "source_page": 105,
      "vector_index": 169,
      "start_position": 37805,
      "end_position": 38047
    },
    {
      "chunk_id": 171,
      "text": "106\nThe Runtime of Learning\nthe rectangle with the minimal training error This procedure is guaranteed to\nﬁnd an ERM hypothesis, and the runtime of the procedure is mO(n) It follows\nthat if n is ﬁxed,...",
      "word_count": 226,
      "source_page": 106,
      "vector_index": 170,
      "start_position": 38120,
      "end_position": 38345
    },
    {
      "chunk_id": 172,
      "text": "The size of Hn\nC is\nat most 3n +1 (since in a conjunction formula, each element of x either appears,\nor appears with a negation sign, or does not appear at all, and we also have the\nall negative formu...",
      "word_count": 250,
      "source_page": 106,
      "vector_index": 171,
      "start_position": 38288,
      "end_position": 38537
    },
    {
      "chunk_id": 173,
      "text": "8.3 Eﬃciently Learnable, but Not by a Proper ERM\n107\nNot Eﬃciently Learnable in the Agnostic Case\nAs in the case of axis aligned rectangles, unless P = NP, there is no algorithm\nwhose running time is ...",
      "word_count": 228,
      "source_page": 107,
      "vector_index": 172,
      "start_position": 38594,
      "end_position": 38821
    },
    {
      "chunk_id": 174,
      "text": "However, from the computational perspective, this learning problem is hard It has been shown (see (Pitt & Valiant 1988, Kearns et al 1994)) that unless\nRP = NP, there is no polynomial time algorithm t...",
      "word_count": 241,
      "source_page": 107,
      "vector_index": 173,
      "start_position": 38800,
      "end_position": 39040
    },
    {
      "chunk_id": 175,
      "text": "108\nThe Runtime of Learning\nalgorithm might return a hypothesis that does not belong to the original hypoth-\nesis class; hence the name “representation independent” learning We emphasize\nthat in most ...",
      "word_count": 240,
      "source_page": 108,
      "vector_index": 174,
      "start_position": 39071,
      "end_position": 39310
    },
    {
      "chunk_id": 176,
      "text": "8.4 Hardness of Learning*\n109\nto some partial information about it On that high level intuitive sense, results\nabout the cryptographic security of some system translate into results about\nthe unlearna...",
      "word_count": 250,
      "source_page": 109,
      "vector_index": 175,
      "start_position": 39411,
      "end_position": 39660
    },
    {
      "chunk_id": 177,
      "text": "Roughly speaking, a one way\nfunction is a function f : {0, 1}n →{0, 1}n (more formally, it is a sequence of\nfunctions, one for each dimension n) that is easy to compute but is hard to in-\nvert More fo...",
      "word_count": 237,
      "source_page": 109,
      "vector_index": 176,
      "start_position": 39570,
      "end_position": 39806
    },
    {
      "chunk_id": 178,
      "text": "Now, let Fn be a family of trapdoor functions over {0, 1}n that can be calcu-\nlated by some polynomial time algorithm That is, we ﬁx an algorithm that given\na secret key (representing one function in ...",
      "word_count": 234,
      "source_page": 109,
      "vector_index": 177,
      "start_position": 39746,
      "end_position": 39979
    },
    {
      "chunk_id": 179,
      "text": "110\nThe Runtime of Learning\nthe class of functions that can be calculated by small Boolean circuits is not\neﬃciently learnable, even in the realizable case 8.5\nSummary\nThe runtime of learning algorith...",
      "word_count": 231,
      "source_page": 110,
      "vector_index": 178,
      "start_position": 39980,
      "end_position": 40210
    },
    {
      "chunk_id": 180,
      "text": "We have also shown another example, the class of 3-term DNF, where\nimplementing ERM is hard even in the realizable case, yet the class is eﬃciently\nlearnable by another algorithm Hardness of implement...",
      "word_count": 213,
      "source_page": 110,
      "vector_index": 179,
      "start_position": 40151,
      "end_position": 40363
    },
    {
      "chunk_id": 181,
      "text": "8.7 Exercises\n111\nassume that such a hypothesis can be calculated given these O(n) examples\nin time O(n), and that the empirical risk of each such hypothesis can be\nevaluated in time O(mn) For example...",
      "word_count": 205,
      "source_page": 111,
      "vector_index": 180,
      "start_position": 40364,
      "end_position": 40568
    },
    {
      "chunk_id": 182,
      "text": "Show that ERMH over the class H = HSn of linear predictors is compu-\ntationally hard More precisely, we consider the sequence of problems in\nwhich the dimension n grows linearly and the number of exam...",
      "word_count": 210,
      "source_page": 111,
      "vector_index": 181,
      "start_position": 40524,
      "end_position": 40733
    },
    {
      "chunk_id": 183,
      "text": "Show that any algorithm that ﬁnds an ERMHSn hypothesis for any training\nsample S ∈(Rn ×{+1, −1})m can be used to solve the Max FS problem of\nsize m, n Hint: Deﬁne a mapping that transforms linear ineq...",
      "word_count": 241,
      "source_page": 111,
      "vector_index": 182,
      "start_position": 40649,
      "end_position": 40889
    },
    {
      "chunk_id": 184,
      "text": "112\nThe Runtime of Learning\nWe wish to reduce the k-coloring problem to ERMHn\nk : that is, to prove\nthat if there is an algorithm that solves the ERMHn\nk problem in time\npolynomial in k, n, and the sa...",
      "word_count": 246,
      "source_page": 112,
      "vector_index": 183,
      "start_position": 40904,
      "end_position": 41149
    },
    {
      "chunk_id": 185,
      "text": "Hint: Given a coloring f of the vertices of G, we should come up with k\nhyperplanes, h1 hk whose intersection is a perfect classiﬁer for S(G) Let b = 0.6 for all of these hyperplanes and, for t ≤k let...",
      "word_count": 242,
      "source_page": 112,
      "vector_index": 184,
      "start_position": 41123,
      "end_position": 41364
    },
    {
      "chunk_id": 186,
      "text": "8.7 Exercises\n113\nlarger than RP In particular, it is believed that NP-hard problems cannot be\nsolved by a randomized polynomial time algorithm • Show that if a class H is properly PAC learnable by a ...",
      "word_count": 214,
      "source_page": 113,
      "vector_index": 185,
      "start_position": 41415,
      "end_position": 41628
    },
    {
      "chunk_id": 187,
      "text": "9\nLinear Predictors\nIn this chapter we will study the family of linear predictors, one of the most\nuseful families of hypothesis classes Many learning algorithms that are being\nwidely used in practice...",
      "word_count": 234,
      "source_page": 117,
      "vector_index": 186,
      "start_position": 41629,
      "end_position": 41862
    },
    {
      "chunk_id": 188,
      "text": "+ b It will be convenient also to use the notation\nLd = {x 7→⟨w, x⟩+ b : w ∈Rd, b ∈R},\nwhich reads as follows: Ld is a set of functions, where each function is parame-\nterized by w ∈Rd and b ∈R, and e...",
      "word_count": 189,
      "source_page": 117,
      "vector_index": 187,
      "start_position": 41800,
      "end_position": 41988
    },
    {
      "chunk_id": 189,
      "text": "118\nLinear Predictors\nRd+1 Therefore,\nhw,b(x) = ⟨w, x⟩+ b = ⟨w′, x′⟩ It follows that each aﬃne function in Rd can be rewritten as a homogenous linear\nfunction in Rd+1 applied over the transformation t...",
      "word_count": 242,
      "source_page": 118,
      "vector_index": 188,
      "start_position": 41989,
      "end_position": 42230
    },
    {
      "chunk_id": 190,
      "text": "9.1 Halfspaces\n119\nin the nonseparable case (i.e., the agnostic case) is known to be computationally\nhard (Ben-David & Simon 2001) There are several approaches to learning non-\nseparable data The most...",
      "word_count": 238,
      "source_page": 119,
      "vector_index": 189,
      "start_position": 42336,
      "end_position": 42573
    },
    {
      "chunk_id": 191,
      "text": "Let S = {(xi, yi)}m\ni=1 be a training set of size m Since we assume the\nrealizable case, an ERM predictor should have zero errors on the training set That is, we are looking for some vector w ∈Rd for ...",
      "word_count": 183,
      "source_page": 119,
      "vector_index": 190,
      "start_position": 42544,
      "end_position": 42726
    },
    {
      "chunk_id": 192,
      "text": "120\nLinear Predictors\nby yi That is, Ai,j = yi xi,j, where xi,j is the j’th element of the vector xi Let\nv be the vector (1, , 1) ∈Rm Then, Equation (9.1) can be rewritten as\nAw ≥v The LP form require...",
      "word_count": 229,
      "source_page": 120,
      "vector_index": 191,
      "start_position": 42727,
      "end_position": 42955
    },
    {
      "chunk_id": 193,
      "text": "9.1 Halfspaces\n121\nyi⟨w⋆, xi⟩≥1 for all i, and among all vectors that satisfy these constraints, w⋆\nis of minimal norm The idea of the proof is to show that after performing T iterations, the cosine\no...",
      "word_count": 230,
      "source_page": 121,
      "vector_index": 192,
      "start_position": 43095,
      "end_position": 43324
    },
    {
      "chunk_id": 194,
      "text": "122\nLinear Predictors\nRemark 9.1\nThe Perceptron is simple to implement and is guaranteed to con-\nverge However, the convergence rate depends on the parameter B, which in\nsome situations might be expon...",
      "word_count": 249,
      "source_page": 122,
      "vector_index": 193,
      "start_position": 43395,
      "end_position": 43643
    },
    {
      "chunk_id": 195,
      "text": "Let us\nﬁrst assume that both of them are nonempty Then,\nX\ni∈I\naixi =\nX\nj∈J\n|aj|xj Now, suppose that x1, , xd+1 are shattered by the class of homogenous classes Then, there must exist a vector w such t...",
      "word_count": 215,
      "source_page": 122,
      "vector_index": 194,
      "start_position": 43626,
      "end_position": 43840
    },
    {
      "chunk_id": 196,
      "text": "9.2 Linear Regression\n123\nr\nr\nr\nr\nr r\nr\nr\nr r\nr\nFigure 9.1 Linear regression for d = 1 For instance, the x-axis may denote the age of\nthe baby, and the y-axis her weight 9.2\nLinear Regression\nLinear r...",
      "word_count": 234,
      "source_page": 123,
      "vector_index": 195,
      "start_position": 43841,
      "end_position": 44074
    },
    {
      "chunk_id": 197,
      "text": "124\nLinear Predictors\nIn the next subsection, we will see how to implement the ERM rule for linear\nregression with respect to the squared loss Of course, there are a variety of other\nloss functions th...",
      "word_count": 244,
      "source_page": 124,
      "vector_index": 196,
      "start_position": 44133,
      "end_position": 44376
    },
    {
      "chunk_id": 198,
      "text": "9.2 Linear Regression\n125\nOr, in matrix form:\nA =\n\n\n\n x1 xm \n\n\n\n\n\n\n x1 xm \n\n\n\n⊤\n,\n(9.7)\nb =\n\n\n\n x1 xm \n\n\n\n\n\n\ny1 ym\n\n\n (9.8)\nIf A is invertible then the solution to the ER...",
      "word_count": 243,
      "source_page": 125,
      "vector_index": 197,
      "start_position": 44473,
      "end_position": 44715
    },
    {
      "chunk_id": 199,
      "text": "Let vi denote the i’th column of V Then, we have\nA ˆw = AA+b = V DV ⊤V D+V ⊤b = V DD+V ⊤b =\nX\ni:Di,i̸=0\nviv⊤\ni b That is, A ˆw is the projection of b onto the span of those vectors vi for which\nDi,i ̸...",
      "word_count": 172,
      "source_page": 125,
      "vector_index": 198,
      "start_position": 44685,
      "end_position": 44856
    },
    {
      "chunk_id": 200,
      "text": "126\nLinear Predictors\nWe will focus here on the class of one dimensional, n-degree, polynomial re-\ngression predictors, namely,\nHn\npoly = {x 7→p(x)},\nwhere p is a one dimensional polynomial of degree ...",
      "word_count": 244,
      "source_page": 126,
      "vector_index": 199,
      "start_position": 44857,
      "end_position": 45100
    },
    {
      "chunk_id": 201,
      "text": "9.3 Logistic Regression\n127\nThe hypothesis class is therefore (where for simplicity we are using homogenous\nlinear functions):\nHsig = φsig ◦Ld = {x 7→φsig(⟨w, x⟩) : w ∈Rd} Note that when ⟨w, x⟩is very...",
      "word_count": 244,
      "source_page": 127,
      "vector_index": 200,
      "start_position": 45118,
      "end_position": 45361
    },
    {
      "chunk_id": 202,
      "text": "Clearly,\nwe would like that hw(x) would be large if y = 1 and that 1 −hw(x) (i.e., the\nprobability of predicting −1) would be large if y = −1 Note that\n1 −hw(x) = 1 −\n1\n1 + exp(−⟨w, x⟩) =\nexp(−⟨w, x⟩)...",
      "word_count": 245,
      "source_page": 127,
      "vector_index": 201,
      "start_position": 45307,
      "end_position": 45551
    },
    {
      "chunk_id": 203,
      "text": "128\nLinear Predictors\n9.4\nSummary\nThe family of linear predictors is one of the most useful families of hypothesis\nclasses, and many learning algorithms that are being widely used in practice\nrely on ...",
      "word_count": 249,
      "source_page": 128,
      "vector_index": 202,
      "start_position": 45562,
      "end_position": 45810
    },
    {
      "chunk_id": 204,
      "text": "9.6 Exercises\n129\nThus, (BR)2 ≤m • When running the Perceptron on this sequence of examples it makes m\nupdates before converging Hint: Choose d = m and for every i choose xi = ei 4 (*) Given any numbe...",
      "word_count": 217,
      "source_page": 129,
      "vector_index": 203,
      "start_position": 45906,
      "end_position": 46122
    },
    {
      "chunk_id": 205,
      "text": "Prove that the modiﬁed Per-\nceptron will perform the same number of iterations as the vanilla Perceptron\nand will converge to a vector that points to the same direction as the output\nof the vanilla Pe...",
      "word_count": 155,
      "source_page": 129,
      "vector_index": 204,
      "start_position": 46086,
      "end_position": 46240
    },
    {
      "chunk_id": 206,
      "text": "10\nBoosting\nBoosting is an algorithmic paradigm that grew out of a theoretical question and\nbecame a very practical machine learning tool The boosting approach uses a\ngeneralization of linear predicto...",
      "word_count": 236,
      "source_page": 130,
      "vector_index": 205,
      "start_position": 46241,
      "end_position": 46476
    },
    {
      "chunk_id": 207,
      "text": "A boosting\nalgorithm ampliﬁes the accuracy of weak learners Intuitively, one can think of\na weak learner as an algorithm that uses a simple “rule of thumb” to output a\nhypothesis that comes from an ea...",
      "word_count": 240,
      "source_page": 130,
      "vector_index": 206,
      "start_position": 46430,
      "end_position": 46669
    },
    {
      "chunk_id": 208,
      "text": "10.1 Weak Learnability\n131\nby Kearns and Valiant in 1988 and solved in 1990 by Robert Schapire, then\na graduate student at MIT However, the proposed mechanism was not very\npractical In 1995, Robert Sc...",
      "word_count": 224,
      "source_page": 131,
      "vector_index": 207,
      "start_position": 46670,
      "end_position": 46893
    },
    {
      "chunk_id": 209,
      "text": "10.1\nWeak Learnability\nRecall the deﬁnition of PAC learning given in Chapter 3: A hypothesis class,\nH, is PAC learnable if there exist mH : (0, 1)2 →N and a learning algorithm\nwith the following prope...",
      "word_count": 224,
      "source_page": 131,
      "vector_index": 208,
      "start_position": 46791,
      "end_position": 47014
    },
    {
      "chunk_id": 210,
      "text": "132\nBoosting\nThis deﬁnition is almost identical to the deﬁnition of PAC learning, which\nhere we will call strong learning, with one crucial diﬀerence: Strong learnability\nimplies the ability to ﬁnd an...",
      "word_count": 226,
      "source_page": 132,
      "vector_index": 209,
      "start_position": 47125,
      "end_position": 47350
    },
    {
      "chunk_id": 211,
      "text": "This implies that from the statistical perspective (i.e., if we ignore computational\ncomplexity), weak learnability is also characterized by the VC dimension of H\nand therefore is just as hard as PAC ...",
      "word_count": 187,
      "source_page": 132,
      "vector_index": 210,
      "start_position": 47285,
      "end_position": 47471
    },
    {
      "chunk_id": 212,
      "text": "Then, the immediate question is whether we can boost an eﬃcient weak learner\ninto an eﬃcient strong learner In the next section we will show that this is\nindeed possible, but before that, let us show ...",
      "word_count": 168,
      "source_page": 132,
      "vector_index": 211,
      "start_position": 47417,
      "end_position": 47584
    },
    {
      "chunk_id": 213,
      "text": "10.1 Weak Learnability\n133\nTo see that, we ﬁrst show that for every distribution that is consistent with\nH, there exists a decision stump with LD(h) ≤1/3 Indeed, just note that\nevery classiﬁer in H co...",
      "word_count": 246,
      "source_page": 133,
      "vector_index": 212,
      "start_position": 47585,
      "end_position": 47830
    },
    {
      "chunk_id": 214,
      "text": "We next show how to implement\nthe ERM rule eﬃciently for decision stumps 10.1.1\nEﬃcient Implementation of ERM for Decision Stumps\nLet X = Rd and consider the base hypothesis class of decision stumps o...",
      "word_count": 243,
      "source_page": 133,
      "vector_index": 213,
      "start_position": 47780,
      "end_position": 48022
    },
    {
      "chunk_id": 215,
      "text": "134\nBoosting\nthreshold θ Therefore, instead of minimizing over θ ∈R we can minimize over\nθ ∈Θj This already gives us an eﬃcient procedure: Choose j ∈[d] and θ ∈Θj that\nminimize the objective value of ...",
      "word_count": 248,
      "source_page": 134,
      "vector_index": 214,
      "start_position": 48072,
      "end_position": 48319
    },
    {
      "chunk_id": 216,
      "text": ", (xm, ym)\ndistribution vector D\ngoal: Find j⋆, θ⋆that solve Equation (10.1)\ninitialize: F ⋆= ∞\nfor j = 1, , d\nsort S using the j’th coordinate, and denote\nx1,j ≤x2,j ≤· · · ≤xm,j ≤xm+1,j\ndef\n= xm,j +...",
      "word_count": 165,
      "source_page": 134,
      "vector_index": 215,
      "start_position": 48255,
      "end_position": 48419
    },
    {
      "chunk_id": 217,
      "text": "10.2 AdaBoost\n135\na distribution over the examples in S, denoted D(t) That is, D(t) ∈Rm\n+ and\nPm\ni=1 D(t)\ni\n= 1 Then, the booster passes the distribution D(t) and the sample S\nto the weak learner (Tha...",
      "word_count": 246,
      "source_page": 135,
      "vector_index": 216,
      "start_position": 48420,
      "end_position": 48665
    },
    {
      "chunk_id": 218,
      "text": ", 1\nm) for t = 1, , T:\ninvoke weak learner ht = WL(D(t), S)\ncompute ϵt = Pm\ni=1 D(t)\ni\n1[yi̸=ht(xi)]\nlet wt = 1\n2 log\n\u0010\n1\nϵt −1\n\u0011\nupdate D(t+1)\ni\n=\nD(t)\ni\nexp(−wtyiht(xi))\nPm\nj=1 D(t)\nj\nexp(−wtyjht(xj...",
      "word_count": 152,
      "source_page": 135,
      "vector_index": 217,
      "start_position": 48659,
      "end_position": 48810
    },
    {
      "chunk_id": 219,
      "text": "136\nBoosting\nis fT In addition, denote\nZt = 1\nm\nm\nX\ni=1\ne−yift(xi) Note that for any hypothesis we have that 1[h(x)̸=y] ≤e−yh(x) Therefore, LS(fT ) ≤\nZT , so it suﬃces to show that ZT ≤e−2γ2T To upper...",
      "word_count": 235,
      "source_page": 136,
      "vector_index": 218,
      "start_position": 48811,
      "end_position": 49045
    },
    {
      "chunk_id": 220,
      "text": "10.3 Linear Combinations of Base Hypotheses\n137\nFinally, using the inequality 1 −a ≤e−a we have that\np\n1 −4γ2 ≤e−4γ2/2 =\ne−2γ2 This shows that Equation (10.3) holds and thus concludes our proof Each i...",
      "word_count": 248,
      "source_page": 137,
      "vector_index": 219,
      "start_position": 49046,
      "end_position": 49293
    },
    {
      "chunk_id": 221,
      "text": "Furthermore, since the weak learner is only applied with\ndistributions over the training set, in many cases we can implement the weak\nlearner so that it will have a zero probability of failure (i.e., ...",
      "word_count": 232,
      "source_page": 137,
      "vector_index": 220,
      "start_position": 49231,
      "end_position": 49462
    },
    {
      "chunk_id": 222,
      "text": "138\nBoosting\n(h1(x), , hT (x)) ∈RT , and then applying the (homogenous) halfspace deﬁned\nby w on ψ(x) In this section we analyze the estimation error of L(B, T) by bounding the\nVC-dimension of L(B, T)...",
      "word_count": 242,
      "source_page": 138,
      "vector_index": 221,
      "start_position": 49550,
      "end_position": 49791
    },
    {
      "chunk_id": 223,
      "text": "Now, let H be the rather complex class (compared to halfspaces on the line)\nof piece-wise constant functions Let gr be a piece-wise constant function with at\nmost r pieces; that is, there exist thresh...",
      "word_count": 173,
      "source_page": 138,
      "vector_index": 222,
      "start_position": 49732,
      "end_position": 49904
    },
    {
      "chunk_id": 224,
      "text": "10.3 Linear Combinations of Base Hypotheses\n139\nFrom this example we obtain that L(HDS1, T) can shatter any set of T + 1\ninstances in R; hence the VC-dimension of L(HDS1, T) is at least T +1 Therefore...",
      "word_count": 243,
      "source_page": 139,
      "vector_index": 223,
      "start_position": 49905,
      "end_position": 50147
    },
    {
      "chunk_id": 225,
      "text": "By Sauer’s lemma, there are at most (em/d)d diﬀerent di-\nchotomies (i.e., labelings) induced by B over C Therefore, we need to choose\nT hypotheses, out of at most (em/d)d diﬀerent hypotheses There are...",
      "word_count": 175,
      "source_page": 139,
      "vector_index": 224,
      "start_position": 50116,
      "end_position": 50290
    },
    {
      "chunk_id": 226,
      "text": "140\nBoosting\nA\nB\nC\nD\nFigure 10.1 The four types of functions, g, used by the base hypotheses for face\nrecognition The value of g for type A or B is the diﬀerence between the sum of the\npixels within t...",
      "word_count": 241,
      "source_page": 140,
      "vector_index": 225,
      "start_position": 50291,
      "end_position": 50531
    },
    {
      "chunk_id": 227,
      "text": "Each hypothesis in the base class is of the form h(x) = f(g(x)), where f is a\ndecision stump hypothesis and g : R24,24 →R is a function that maps an image\nto a scalar Each function g is parameterized ...",
      "word_count": 189,
      "source_page": 140,
      "vector_index": 226,
      "start_position": 50485,
      "end_position": 50673
    },
    {
      "chunk_id": 228,
      "text": "10.5 Summary\n141\nFigure 10.2 The ﬁrst and second features selected by AdaBoost, as implemented by\nViola and Jones The two features are shown in the top row and then overlaid on a\ntypical training face...",
      "word_count": 229,
      "source_page": 141,
      "vector_index": 227,
      "start_position": 50674,
      "end_position": 50902
    },
    {
      "chunk_id": 229,
      "text": "142\nBoosting\nH is weakly learnable using B For example, Klivans & Sherstov (2006) have\nshown that PAC learning of the class of intersection of halfspaces is hard (even\nin the realizable case) This har...",
      "word_count": 222,
      "source_page": 142,
      "vector_index": 228,
      "start_position": 51017,
      "end_position": 51238
    },
    {
      "chunk_id": 230,
      "text": "A recent book by Schapire & Freund\n(2012) covers boosting from all points of view, and gives easy access to the wealth\nof research that this ﬁeld has produced 10.7\nExercises\n1 Boosting the Conﬁdence: ...",
      "word_count": 226,
      "source_page": 142,
      "vector_index": 229,
      "start_position": 51207,
      "end_position": 51432
    },
    {
      "chunk_id": 231,
      "text": "10.7 Exercises\n143\nShow that the error of ht w.r.t the distribution D(t+1) is exactly 1/2 That\nis, show that for every t ∈[T]\nm\nX\ni=1\nD(t+1)\ni\n1[yi̸=ht(xi)] = 1/2 4 In this exercise we discuss the VC-...",
      "word_count": 246,
      "source_page": 143,
      "vector_index": 230,
      "start_position": 51472,
      "end_position": 51717
    },
    {
      "chunk_id": 232,
      "text": "11\nModel Selection and Validation\nIn the previous chapter we have described the AdaBoost algorithm and have\nshown how the parameter T of AdaBoost controls the bias-complexity trade-\noﬀ But, how do we ...",
      "word_count": 244,
      "source_page": 144,
      "vector_index": 231,
      "start_position": 51823,
      "end_position": 52066
    },
    {
      "chunk_id": 233,
      "text": "11.1 Model Selection Using SRM\n145\nIn this chapter we will present two approaches for model selection The ﬁrst\napproach is based on the Structural Risk Minimization (SRM) paradigm we\nhave described an...",
      "word_count": 244,
      "source_page": 145,
      "vector_index": 232,
      "start_position": 52115,
      "end_position": 52358
    },
    {
      "chunk_id": 234,
      "text": "Consider a countable\nsequence of hypothesis classes H1, H2, H3, For example, in the problem of\npolynomial regression mentioned, we can take Hd to be the set of polynomials\nof degree at most d Another ...",
      "word_count": 230,
      "source_page": 145,
      "vector_index": 233,
      "start_position": 52325,
      "end_position": 52554
    },
    {
      "chunk_id": 235,
      "text": "146\nModel Selection and Validation\nand a complexity term that depends on d The SRM rule will search for d and\nh ∈Hd that minimize the right-hand side of Equation (11.2) Getting back to the example of ...",
      "word_count": 234,
      "source_page": 146,
      "vector_index": 234,
      "start_position": 52555,
      "end_position": 52788
    },
    {
      "chunk_id": 236,
      "text": "A more accurate estimation of the true risk can be obtained by using\nsome of the training data as a validation set, over which one can evalutate the\nsuccess of the algorithm’s output predictor This pr...",
      "word_count": 226,
      "source_page": 146,
      "vector_index": 235,
      "start_position": 52750,
      "end_position": 52975
    },
    {
      "chunk_id": 237,
      "text": "11.2 Validation\n147\nwith respect to a hypothesis class of VC-dimension d, over a training set of m\nexamples Then, from the fundamental theorem of learning (Theorem 6.8) we\nobtain the bound\nLD(h) ≤LS(h...",
      "word_count": 244,
      "source_page": 147,
      "vector_index": 236,
      "start_position": 53021,
      "end_position": 53264
    },
    {
      "chunk_id": 238,
      "text": ", hr} be the set of all output predictors of the\ndiﬀerent algorithms For example, in the case of training polynomial regressors,\nwe would have each hr be the output of polynomial regression of degree ...",
      "word_count": 215,
      "source_page": 147,
      "vector_index": 237,
      "start_position": 53229,
      "end_position": 53443
    },
    {
      "chunk_id": 239,
      "text": "148\nModel Selection and Validation\nThis theorem tells us that the error on the validation set approximates the\ntrue error as long as H is not too large However, if we try too many methods\n(resulting i...",
      "word_count": 185,
      "source_page": 148,
      "vector_index": 238,
      "start_position": 53444,
      "end_position": 53628
    },
    {
      "chunk_id": 240,
      "text": "11.2 Validation\n149\n2\n4\n6\n8\n10\n0\n0.1\n0.2\n0.3\n0.4\nd\nerror\ntrain\nvalidation\nAs can be shown, the training error is monotonically decreasing as we increase\nthe polynomial degree (which is the complexity ...",
      "word_count": 229,
      "source_page": 149,
      "vector_index": 239,
      "start_position": 53629,
      "end_position": 53857
    },
    {
      "chunk_id": 241,
      "text": "It is important to\nverify that we are in the relevant regime For example, in the polynomial ﬁtting\nproblem described, if we start searching degrees from the set of values {1, 10, 20}\nand do not employ...",
      "word_count": 179,
      "source_page": 149,
      "vector_index": 240,
      "start_position": 53804,
      "end_position": 53982
    },
    {
      "chunk_id": 242,
      "text": "150\nModel Selection and Validation\nestimate of the true error The special case k = m, where m is the number of\nexamples, is called leave-one-out (LOO) k-Fold cross validation is often used for model s...",
      "word_count": 233,
      "source_page": 150,
      "vector_index": 241,
      "start_position": 53983,
      "end_position": 54215
    },
    {
      "chunk_id": 243,
      "text": "However, it\nmight sometime fail, as the artiﬁcial example given in Exercise 1 shows Rig-\norously understanding the exact behavior of cross validation is still an open\nproblem Rogers and Wagner (Rogers...",
      "word_count": 162,
      "source_page": 150,
      "vector_index": 242,
      "start_position": 54188,
      "end_position": 54349
    },
    {
      "chunk_id": 244,
      "text": "11.3 What to Do If Learning Fails\n151\n11.3\nWhat to Do If Learning Fails\nConsider the following scenario: You were given a learning task and have ap-\nproached it with a choice of a hypothesis class, a ...",
      "word_count": 238,
      "source_page": 151,
      "vector_index": 243,
      "start_position": 54350,
      "end_position": 54587
    },
    {
      "chunk_id": 245,
      "text": "The\napproximation error is deﬁned to be LD(h⋆) for some h⋆∈argminh∈H LD(h),\nwhile the estimation error is deﬁned to be LD(hS) −LD(h⋆), where hS is the\nlearned predictor (which is based on the training...",
      "word_count": 247,
      "source_page": 151,
      "vector_index": 244,
      "start_position": 54533,
      "end_position": 54779
    },
    {
      "chunk_id": 246,
      "text": "152\nModel Selection and Validation\nInstead, we give a diﬀerent error decomposition, one that can be estimated from\nthe train and validation sets LD(hS) = (LD(hS) −LV (hS)) + (LV (hS) −LS(hS)) + LS(hS)...",
      "word_count": 247,
      "source_page": 152,
      "vector_index": 245,
      "start_position": 54793,
      "end_position": 55039
    },
    {
      "chunk_id": 247,
      "text": "In addition,\nsince h⋆does not depend on S, the term (LS(h⋆)−LD(h⋆)) can be bounded quite\ntightly (as in Theorem 11.1) The last term is the approximation error It follows\nthat if LS(hS) is large then s...",
      "word_count": 243,
      "source_page": 152,
      "vector_index": 246,
      "start_position": 55013,
      "end_position": 55255
    },
    {
      "chunk_id": 248,
      "text": "11.3 What to Do If Learning Fails\n153\nm\nerror\ntrain error\nvalidation error\nm\nerror\ntrain error\nvalidation error\nFigure 11.1 Examples of learning curves Left: This learning curve corresponds to the\nsce...",
      "word_count": 247,
      "source_page": 153,
      "vector_index": 247,
      "start_position": 55319,
      "end_position": 55565
    },
    {
      "chunk_id": 249,
      "text": "In the ﬁrst\nscenario we expect the validation error to be approximately 1/2 for all preﬁxes,\nas we didn’t really learn anything In the second scenario the validation error\nwill start as a constant but...",
      "word_count": 242,
      "source_page": 153,
      "vector_index": 248,
      "start_position": 55513,
      "end_position": 55754
    },
    {
      "chunk_id": 250,
      "text": "154\nModel Selection and Validation\nvalidation error is starting to decrease then the best solution is to increase the\nnumber of examples (if we can aﬀord to enlarge the data) Another reasonable\nsoluti...",
      "word_count": 247,
      "source_page": 154,
      "vector_index": 249,
      "start_position": 55755,
      "end_position": 56001
    },
    {
      "chunk_id": 251,
      "text": "If this is not possible, consider reducing the complexity of the hypothesis class 5 If the approximation error seems to be large as well, try to change the hy-\npothesis class or the feature representa...",
      "word_count": 234,
      "source_page": 154,
      "vector_index": 250,
      "start_position": 55988,
      "end_position": 56221
    },
    {
      "chunk_id": 252,
      "text": "12\nConvex Learning Problems\nIn this chapter we introduce convex learning problems Convex learning comprises\nan important family of learning problems, mainly because most of what we can\nlearn eﬃciently...",
      "word_count": 236,
      "source_page": 156,
      "vector_index": 251,
      "start_position": 56323,
      "end_position": 56558
    },
    {
      "chunk_id": 253,
      "text": "These claims will be proven in the next two chapters, in\nwhich we will present two learning paradigms that successfully learn all problems\nthat are either convex-Lipschitz-bounded or convex-smooth-bou...",
      "word_count": 197,
      "source_page": 156,
      "vector_index": 252,
      "start_position": 56502,
      "end_position": 56698
    },
    {
      "chunk_id": 254,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n157\nnon-convex\nconvex\nGiven α ∈[0, 1], the combination, αu + (1 −α)v of the points u, v is called a\nconvex combination definition 12.2 (Convex Function)\nL...",
      "word_count": 179,
      "source_page": 157,
      "vector_index": 253,
      "start_position": 56699,
      "end_position": 56877
    },
    {
      "chunk_id": 255,
      "text": "158\nConvex Learning Problems\nx\nf(x)\nAn important property of convex functions is that every local minimum of the\nfunction is also a global minimum Formally, let B(u, r) = {v : ∥v −u∥≤r} be\na ball of r...",
      "word_count": 241,
      "source_page": 158,
      "vector_index": 254,
      "start_position": 56878,
      "end_position": 57118
    },
    {
      "chunk_id": 256,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n159\nf(w)\nf(u)\nw\nu\nf(w) + ⟨u −w, ∇f(w)⟩\nIf f is a scalar diﬀerentiable function, there is an easy way to check if it is\nconvex lemma 12.3\nLet f : R →R be a...",
      "word_count": 210,
      "source_page": 159,
      "vector_index": 255,
      "start_position": 57141,
      "end_position": 57350
    },
    {
      "chunk_id": 257,
      "text": "160\nConvex Learning Problems\n• Given some x ∈Rd and y ∈R, let f : Rd →R be deﬁned as f(w) =\n(⟨w, x⟩−y)2 Then, f is a composition of the function g(a) = a2 onto a\nlinear function, and hence f is a conv...",
      "word_count": 249,
      "source_page": 160,
      "vector_index": 256,
      "start_position": 57400,
      "end_position": 57648
    },
    {
      "chunk_id": 258,
      "text": "12.1 Convexity, Lipschitzness, and Smoothness\n161\nIntuitively, a Lipschitz function cannot change too fast Note that if f : R →R\nis diﬀerentiable, then by the mean value theorem we have\nf(w1) −f(w2) =...",
      "word_count": 231,
      "source_page": 161,
      "vector_index": 257,
      "start_position": 57730,
      "end_position": 57960
    },
    {
      "chunk_id": 259,
      "text": "162\nConvex Learning Problems\n12.1.3\nSmoothness\nThe deﬁnition of a smooth function relies on the notion of gradient Recall that\nthe gradient of a diﬀerentiable function f : Rd →R at w, denoted ∇f(w), i...",
      "word_count": 247,
      "source_page": 162,
      "vector_index": 258,
      "start_position": 58059,
      "end_position": 58305
    },
    {
      "chunk_id": 260,
      "text": "12.2 Convex Learning Problems\n163\nProof\nBy the chain rule we have that ∇f(w) = g′(⟨w, x⟩+ b)x, where g′ is the\nderivative of g Using the smoothness of g and the Cauchy-Schwartz inequality\nwe therefore...",
      "word_count": 236,
      "source_page": 163,
      "vector_index": 259,
      "start_position": 58391,
      "end_position": 58626
    },
    {
      "chunk_id": 261,
      "text": "That is, every hypothesis is some real-valued vector We shall, therefore, denote\na hypothesis in H by w Now we can ﬁnally deﬁne convex learning problems:\ndefinition 12.10 (Convex Learning Problem)\nA l...",
      "word_count": 205,
      "source_page": 163,
      "vector_index": 260,
      "start_position": 58609,
      "end_position": 58813
    },
    {
      "chunk_id": 262,
      "text": "164\nConvex Learning Problems\nEach linear function is parameterized by a vector w ∈Rd Hence, we can deﬁne\nH to be the set of all such parameters, namely, H = Rd The set of examples is\nZ = X ×Y = Rd×R =...",
      "word_count": 244,
      "source_page": 164,
      "vector_index": 261,
      "start_position": 58814,
      "end_position": 59057
    },
    {
      "chunk_id": 263,
      "text": "In particular, in Chapter 14 we will present a very\nsimple algorithm for minimizing convex functions 12.2.1\nLearnability of Convex Learning Problems\nWe have argued that for many cases, implementing th...",
      "word_count": 225,
      "source_page": 164,
      "vector_index": 262,
      "start_position": 59017,
      "end_position": 59241
    },
    {
      "chunk_id": 264,
      "text": "12.2 Convex Learning Problems\n165\nhomogenous case) Let A be any deterministic algorithm.1 Assume, by way of\ncontradiction, that A is a successful PAC learner for this problem That is, there\nexists a f...",
      "word_count": 238,
      "source_page": 165,
      "vector_index": 263,
      "start_position": 59274,
      "end_position": 59511
    },
    {
      "chunk_id": 265,
      "text": "This is trivially true for\nD2, whereas for D1, the probability of this event is\n(1 −µ)m ≥e−2µm = 0.99 Since we assume that A is a deterministic algorithm, upon receiving a training\nset of m examples, ...",
      "word_count": 233,
      "source_page": 165,
      "vector_index": 264,
      "start_position": 59463,
      "end_position": 59695
    },
    {
      "chunk_id": 266,
      "text": "166\nConvex Learning Problems\nhypothesis class It is easy to verify that H is convex The argument will be\nthe same as in Example 12.8, except that now the two distributions, D1, D2 will\nbe supported on...",
      "word_count": 247,
      "source_page": 166,
      "vector_index": 265,
      "start_position": 59745,
      "end_position": 59991
    },
    {
      "chunk_id": 267,
      "text": "Example 12.10\nLet X = {x ∈Rd : ∥x∥≤ρ} and Y = R Let H = {w ∈Rd :\n∥w∥≤B} and let the loss function be ℓ(w, (x, y)) = |⟨w, x⟩−y| This corre-\nsponds to a regression problem with the absolute-value loss, ...",
      "word_count": 171,
      "source_page": 166,
      "vector_index": 266,
      "start_position": 59960,
      "end_position": 60130
    },
    {
      "chunk_id": 268,
      "text": "12.3 Surrogate Loss Functions\n167\nExample 12.11\nLet X = {x ∈Rd : ∥x∥≤β/2} and Y = R Let H = {w ∈\nRd : ∥w∥≤B} and let the loss function be ℓ(w, (x, y)) = (⟨w, x⟩−y)2 This\ncorresponds to a regression pr...",
      "word_count": 243,
      "source_page": 167,
      "vector_index": 267,
      "start_position": 60131,
      "end_position": 60373
    },
    {
      "chunk_id": 269,
      "text": "That is,\nℓ0−1(w, (x, y)) = 1[y̸=sign(⟨w,x⟩)] = 1[y⟨w,x⟩≤0] This loss function is not convex with respect to w and indeed, when trying to\nminimize the empirical risk with respect to this loss function ...",
      "word_count": 208,
      "source_page": 167,
      "vector_index": 268,
      "start_position": 60332,
      "end_position": 60539
    },
    {
      "chunk_id": 270,
      "text": "168\nConvex Learning Problems\ny⟨w, x⟩\nℓhinge\nℓ0−1\n1\n1\nOnce we have deﬁned the surrogate convex loss, we can learn the problem with\nrespect to it The generalization requirement from a hinge loss learner...",
      "word_count": 245,
      "source_page": 168,
      "vector_index": 269,
      "start_position": 60540,
      "end_position": 60784
    },
    {
      "chunk_id": 271,
      "text": "12.5 Bibliographic Remarks\n169\nlearning algorithms for these families We also introduced the notion of convex\nsurrogate loss function, which enables us also to utilize the convex machinery for\nnonconv...",
      "word_count": 245,
      "source_page": 169,
      "vector_index": 270,
      "start_position": 60849,
      "end_position": 61093
    },
    {
      "chunk_id": 272,
      "text": "Consider the learning problem of logistic regression: Let H = X = {x ∈\nRd : ∥x∥≤B}, for some scalar B > 0, let Y = {±1}, and let the loss\nfunction ℓbe deﬁned as ℓ(w, (x, y)) = log(1 + exp(−y⟨w, x⟩)) S...",
      "word_count": 236,
      "source_page": 169,
      "vector_index": 271,
      "start_position": 61039,
      "end_position": 61274
    },
    {
      "chunk_id": 273,
      "text": "13\nRegularization and Stability\nIn the previous chapter we introduced the families of convex-Lipschitz-bounded\nand convex-smooth-bounded learning problems In this section we show that all\nlearning pro...",
      "word_count": 248,
      "source_page": 171,
      "vector_index": 272,
      "start_position": 61382,
      "end_position": 61629
    },
    {
      "chunk_id": 274,
      "text": "172\nRegularization and Stability\ntion, and the algorithm balances between low empirical risk and “simpler,” or\n“less complex,” hypotheses There are many possible regularization functions one can use, ...",
      "word_count": 231,
      "source_page": 172,
      "vector_index": 273,
      "start_position": 61742,
      "end_position": 61972
    },
    {
      "chunk_id": 275,
      "text": "In the next section\nwe deﬁne the notion of stability and prove that stable learning rules do not\noverﬁt But ﬁrst, let us demonstrate the RLM rule for linear regression with the\nsquared loss 13.1.1\nRid...",
      "word_count": 178,
      "source_page": 172,
      "vector_index": 274,
      "start_position": 61939,
      "end_position": 62116
    },
    {
      "chunk_id": 276,
      "text": "13.2 Stable Rules Do Not Overﬁt\n173\nIn the next section we formally show how regularization stabilizes the algo-\nrithm and prevents overﬁtting In particular, the analysis presented in the next\nsection...",
      "word_count": 241,
      "source_page": 173,
      "vector_index": 275,
      "start_position": 62117,
      "end_position": 62357
    },
    {
      "chunk_id": 277,
      "text": "13.2\nStable Rules Do Not Overﬁt\nIntuitively, a learning algorithm is stable if a small change of the input to the\nalgorithm does not change the output of the algorithm much Of course, there\nare many w...",
      "word_count": 236,
      "source_page": 173,
      "vector_index": 276,
      "start_position": 62298,
      "end_position": 62533
    },
    {
      "chunk_id": 278,
      "text": "174\nRegularization and Stability\nlearning algorithm drastically changes its prediction on zi if it observes it in the\ntraining set This is formalized in the following theorem theorem 13.2\nLet D be a d...",
      "word_count": 244,
      "source_page": 174,
      "vector_index": 277,
      "start_position": 62623,
      "end_position": 62866
    },
    {
      "chunk_id": 279,
      "text": "Theorem 13.2 tells us that a learning algorithm does not overﬁt if and only\nif it is on-average-replace-one-stable Of course, a learning algorithm that does\nnot overﬁt is not necessarily a good learni...",
      "word_count": 197,
      "source_page": 174,
      "vector_index": 278,
      "start_position": 62820,
      "end_position": 63016
    },
    {
      "chunk_id": 280,
      "text": "13.3 Tikhonov Regularization as a Stabilizer\n175\ndefinition 13.4 (Strongly Convex Functions)\nA function f is λ-strongly con-\nvex if for all w, u and α ∈(0, 1) we have\nf(αw + (1 −α)u) ≤αf(w) + (1 −α)f(...",
      "word_count": 234,
      "source_page": 175,
      "vector_index": 279,
      "start_position": 63017,
      "end_position": 63250
    },
    {
      "chunk_id": 281,
      "text": "176\nRegularization and Stability\nDenote fS(w) = LS(w) + λ∥w∥2, and based on Lemma 13.5 we know that fS is\n(2λ)-strongly convex Relying on part 3 of the lemma, it follows that for any v,\nfS(v) −fS(A(S)...",
      "word_count": 249,
      "source_page": 176,
      "vector_index": 280,
      "start_position": 63340,
      "end_position": 63588
    },
    {
      "chunk_id": 282,
      "text": "13.3 Tikhonov Regularization as a Stabilizer\n177\ncorollary 13.6\nAssume that the loss function is convex and ρ-Lipschitz Then, the RLM rule with the regularizer λ∥w∥2 is on-average-replace-one-stable\nw...",
      "word_count": 207,
      "source_page": 177,
      "vector_index": 281,
      "start_position": 63600,
      "end_position": 63806
    },
    {
      "chunk_id": 283,
      "text": "178\nRegularization and Stability\nCombining the preceding with Equation (13.14) and again using the assumption\nβ ≤λm/2 yield\nℓ(A(S(i)), zi) −ℓ(A(S), zi)\n≤\np\n2βℓ(A(S), zi) ∥A(S(i)) −A(S)∥+ β\n2 ∥A(S(i)) ...",
      "word_count": 236,
      "source_page": 178,
      "vector_index": 282,
      "start_position": 63807,
      "end_position": 64042
    },
    {
      "chunk_id": 284,
      "text": "13.4 Controlling the Fitting-Stability Tradeoﬀ\n179\ntradeoﬀbetween ﬁtting and overﬁtting This tradeoﬀis quite similar to the bias-\ncomplexity tradeoﬀwe discussed previously in the book We now derive bo...",
      "word_count": 250,
      "source_page": 179,
      "vector_index": 283,
      "start_position": 64120,
      "end_position": 64369
    },
    {
      "chunk_id": 285,
      "text": "We therefore usually tune λ\non the basis of a validation set, as described in Chapter 11 We can also easily derive a PAC-like guarantee1 from Corollary 13.8 for convex-\nLipschitz-bounded learning prob...",
      "word_count": 157,
      "source_page": 179,
      "vector_index": 284,
      "start_position": 64322,
      "end_position": 64478
    },
    {
      "chunk_id": 286,
      "text": "180\nRegularization and Stability\ncorollary 13.10\nAssume that the loss function is convex, β-smooth, and\nnonnegative Then, the RLM rule with the regularization function λ∥w∥2, for\nλ ≥2β\nm , satisﬁes th...",
      "word_count": 226,
      "source_page": 180,
      "vector_index": 285,
      "start_position": 64479,
      "end_position": 64704
    },
    {
      "chunk_id": 287,
      "text": "Furthermore, for convex-Lipschitz-bounded or convex-smooth-bounded\nproblems, the RLM rule with Tikhonov regularization leads to a stable learning\nalgorithm We discussed how the regularization paramete...",
      "word_count": 179,
      "source_page": 180,
      "vector_index": 286,
      "start_position": 64673,
      "end_position": 64851
    },
    {
      "chunk_id": 288,
      "text": "13.7 Exercises\n181\nIn the context of modern learning theory, the use of stability can be traced back\nat least to the work of Rogers & Wagner (1978), which noted that the sensitiv-\nity of a learning al...",
      "word_count": 239,
      "source_page": 181,
      "vector_index": 287,
      "start_position": 64852,
      "end_position": 65090
    },
    {
      "chunk_id": 289,
      "text": "13.7\nExercises\n1 From Bounded Expected Risk to Agnostic PAC Learning: Let A be\nan algorithm that guarantees the following: If m ≥mH(ϵ) then for every\ndistribution D it holds that\nE\nS∼Dm[LD(A(S))] ≤min...",
      "word_count": 203,
      "source_page": 181,
      "vector_index": 288,
      "start_position": 65053,
      "end_position": 65255
    },
    {
      "chunk_id": 290,
      "text": "182\nRegularization and Stability\nRd, let H = B, let Z = B × {0, 1}d, and let ℓ: Z × H →R be deﬁned as\nfollows:\nℓ(w, (x, α)) =\nd\nX\ni=1\nαi(xi −wi)2 This problem corresponds to an unsupervised learning t...",
      "word_count": 222,
      "source_page": 182,
      "vector_index": 289,
      "start_position": 65256,
      "end_position": 65477
    },
    {
      "chunk_id": 291,
      "text": "Show that the rate of uniform convergence of this problem grows with\nd Hint: Let m be a training set size Show that if d ≫2m, then there is\na high probability of sampling a set of examples such that t...",
      "word_count": 217,
      "source_page": 182,
      "vector_index": 290,
      "start_position": 65457,
      "end_position": 65673
    },
    {
      "chunk_id": 292,
      "text": "13.7 Exercises\n183\n4 Strong Convexity with Respect to General Norms:\nThroughout the section we used the ℓ2 norm In this exercise we generalize\nsome of the results to general norms Let ∥·∥be some arbit...",
      "word_count": 222,
      "source_page": 183,
      "vector_index": 291,
      "start_position": 65674,
      "end_position": 65895
    },
    {
      "chunk_id": 293,
      "text": "14\nStochastic Gradient Descent\nRecall that the goal of learning is to minimize the risk function, LD(h) =\nEz∼D[ℓ(h, z)] We cannot directly minimize the risk function since it depends\non the unknown di...",
      "word_count": 233,
      "source_page": 184,
      "vector_index": 292,
      "start_position": 65896,
      "end_position": 66128
    },
    {
      "chunk_id": 294,
      "text": "In SGD, we try to minimize the risk function LD(w)\ndirectly using a gradient descent procedure Gradient descent is an iterative\noptimization procedure in which at each step we improve the solution by ...",
      "word_count": 242,
      "source_page": 184,
      "vector_index": 293,
      "start_position": 66077,
      "end_position": 66318
    },
    {
      "chunk_id": 295,
      "text": "14.1 Gradient Descent\n185\nthe Stochastic Gradient Descent algorithm, along with several useful variants We show that SGD enjoys an expected convergence rate similar to the rate\nof gradient descent Fin...",
      "word_count": 216,
      "source_page": 185,
      "vector_index": 294,
      "start_position": 66380,
      "end_position": 66595
    },
    {
      "chunk_id": 296,
      "text": "Intuitively, since the gradi-\nent points in the direction of the greatest rate of increase of f around w(t),\nthe algorithm makes a small step in the opposite direction, thus decreasing the\nvalue of th...",
      "word_count": 234,
      "source_page": 185,
      "vector_index": 295,
      "start_position": 66543,
      "end_position": 66776
    },
    {
      "chunk_id": 297,
      "text": "186\nStochastic Gradient Descent\nFigure 14.1 An illustration of the gradient descent algorithm The function to be\nminimized is 1.25(x1 + 6)2 + (x2 −8)2 14.1.1\nAnalysis of GD for Convex-Lipschitz Functi...",
      "word_count": 216,
      "source_page": 186,
      "vector_index": 296,
      "start_position": 66802,
      "end_position": 67017
    },
    {
      "chunk_id": 298,
      "text": "14.1 Gradient Descent\n187\nlemma 14.1\nLet v1, , vT be an arbitrary sequence of vectors Any algorithm\nwith an initialization w(1) = 0 and an update rule of the form\nw(t+1) = w(t) −ηvt\n(14.4)\nsatisﬁes\nT\n...",
      "word_count": 197,
      "source_page": 187,
      "vector_index": 297,
      "start_position": 67018,
      "end_position": 67214
    },
    {
      "chunk_id": 299,
      "text": "188\nStochastic Gradient Descent\nLemma 14.1 applies to the GD algorithm with vt = ∇f(w(t)) As we will\nshow later in Lemma 14.7, if f is ρ-Lipschitz, then ∥∇f(w(t))∥≤ρ We therefore\nsatisfy the lemma’s c...",
      "word_count": 243,
      "source_page": 188,
      "vector_index": 298,
      "start_position": 67307,
      "end_position": 67549
    },
    {
      "chunk_id": 300,
      "text": "The existence of a tangent that lies below f is an important property of convex\nfunctions, which is in fact an alternative characterization of convexity lemma 14.3\nLet S be an open convex set A functi...",
      "word_count": 165,
      "source_page": 188,
      "vector_index": 299,
      "start_position": 67516,
      "end_position": 67680
    },
    {
      "chunk_id": 301,
      "text": "14.2 Subgradients\n189\nf(w)\nf(u)\nw\nu\nf(w) + ⟨u −w, ∇f(w)⟩\nFigure 14.2 Left: The right-hand side of Equation (14.7) is the tangent of f at w For\na convex function, the tangent lower bounds f Right: Illu...",
      "word_count": 250,
      "source_page": 189,
      "vector_index": 300,
      "start_position": 67681,
      "end_position": 67930
    },
    {
      "chunk_id": 302,
      "text": "190\nStochastic Gradient Descent\nExample 14.2 (A Subgradient of the Hinge Loss)\nRecall the hinge loss function\nfrom Section 12.3, f(w) = max{0, 1 −y⟨w, x⟩} for some vector x and scalar y To calculate a...",
      "word_count": 247,
      "source_page": 190,
      "vector_index": 301,
      "start_position": 67978,
      "end_position": 68224
    },
    {
      "chunk_id": 303,
      "text": "14.3 Stochastic Gradient Descent (SGD)\n191\nFigure 14.3 An illustration of the gradient descent algorithm (left) and the stochastic\ngradient descent algorithm (right) The function to be minimized is\n1....",
      "word_count": 236,
      "source_page": 191,
      "vector_index": 302,
      "start_position": 68307,
      "end_position": 68542
    },
    {
      "chunk_id": 304,
      "text": "192\nStochastic Gradient Descent\nsubgradient of f at w(t), we can still derive a similar bound on the expected\noutput of stochastic gradient descent This is formalized in the following theorem theorem ...",
      "word_count": 228,
      "source_page": 192,
      "vector_index": 303,
      "start_position": 68572,
      "end_position": 68799
    },
    {
      "chunk_id": 305,
      "text": "By taking expectation of the bound in the lemma we have\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t) −w⋆, vt⟩\n#\n≤B ρ\n√\nT (14.9)\nIt is left to show that\nE\nv1:T\n\"\n1\nT\nT\nX\nt=1\n(f(w(t)) −f(w⋆))\n#\n≤E\nv1:T\n\"\n1\nT\nT\nX\nt=1\n⟨w(t...",
      "word_count": 175,
      "source_page": 192,
      "vector_index": 304,
      "start_position": 68736,
      "end_position": 68910
    },
    {
      "chunk_id": 306,
      "text": "14.4 Variants\n193\nSince w(t) only depends on v1:t−1 and SGD requires that Evt[vt | w(t)] ∈∂f(w(t))\nwe obtain that Evt[vt | v1:t−1] ∈∂f(w(t)) Thus,\nE\nv1: t−1⟨w(t) −w⋆, E\nvt[vt | v1: t−1]⟩≥\nE\nv1: t−1[f(...",
      "word_count": 242,
      "source_page": 193,
      "vector_index": 305,
      "start_position": 68911,
      "end_position": 69152
    },
    {
      "chunk_id": 307,
      "text": "194\nStochastic Gradient Descent\nThen, for every u ∈H,\n∥w −u∥2 −∥v −u∥2 ≥0 Proof\nBy the convexity of H, for every α ∈(0, 1) we have that v+α(u−v) ∈H Therefore, from the optimality of v we obtain\n∥v −w∥...",
      "word_count": 226,
      "source_page": 194,
      "vector_index": 306,
      "start_position": 69249,
      "end_position": 69474
    },
    {
      "chunk_id": 308,
      "text": "14.4 Variants\n195\n14.4.3\nOther Averaging Techniques\nWe have set the output vector to be ¯w =\n1\nT\nPT\nt=1 w(t) There are alternative\napproaches such as outputting w(t) for some random t ∈[t], or outputt...",
      "word_count": 209,
      "source_page": 195,
      "vector_index": 307,
      "start_position": 69502,
      "end_position": 69710
    },
    {
      "chunk_id": 309,
      "text": "196\nStochastic Gradient Descent\nSince w(t+1) is the projection of w(t+ 1\n2 ) onto H, and w⋆∈H we have that\n∥w(t+ 1\n2 ) −w⋆∥2 ≥∥w(t+1) −w⋆∥2 Therefore,\n∥w(t) −w⋆∥2 −∥w(t+1) −w⋆∥2 ≥∥w(t) −w⋆∥2 −∥w(t+ 1\n...",
      "word_count": 250,
      "source_page": 196,
      "vector_index": 308,
      "start_position": 69826,
      "end_position": 70075
    },
    {
      "chunk_id": 310,
      "text": "14.5 Learning with SGD\n197\nLD(w), that is, a random vector whose conditional expected value is ∇LD(w(t)) We shall now see how such an estimate can be easily constructed For simplicity, let us ﬁrst con...",
      "word_count": 235,
      "source_page": 197,
      "vector_index": 309,
      "start_position": 70167,
      "end_position": 70401
    },
    {
      "chunk_id": 311,
      "text": "198\nStochastic Gradient Descent\nLD(w) with a number of iterations (i.e., number of examples)\nT ≥B2ρ2\nϵ2\nand with η =\nq\nB2\nρ2 T , then the output of SGD satisﬁes\nE [LD( ¯w)] ≤min\nw∈H LD(w) + ϵ It is in...",
      "word_count": 240,
      "source_page": 198,
      "vector_index": 310,
      "start_position": 70498,
      "end_position": 70737
    },
    {
      "chunk_id": 312,
      "text": "14.5 Learning with SGD\n199\nto z1, , zT Clearly, E[ft(w⋆)] = LD(w⋆) In addition, using the same argument\nas in the proof of Theorem 14.8 we have that\nE\n\"\n1\nT\nT\nX\nt=1\nft(w(t))\n#\n= E\n\"\n1\nT\nT\nX\nt=1\nLD(w(t...",
      "word_count": 239,
      "source_page": 199,
      "vector_index": 311,
      "start_position": 70830,
      "end_position": 71068
    },
    {
      "chunk_id": 313,
      "text": "200\nStochastic Gradient Descent\nthat H = Rd and therefore the projection step does not matter) as follows\nw(t+1) = w(t) −1\nλ t\n\u0010\nλw(t) + vt\n\u0011\n=\n\u0012\n1 −1\nt\n\u0013\nw(t) −1\nλ tvt\n= t −1\nt\nw(t) −1\nλ tvt\n= t −1\nt...",
      "word_count": 238,
      "source_page": 200,
      "vector_index": 312,
      "start_position": 71148,
      "end_position": 71385
    },
    {
      "chunk_id": 314,
      "text": "14.8 Exercises\n201\nin the context of stochastic optimization See, for example, (Nemirovski & Yudin\n1978, Nesterov & Nesterov 2004, Nesterov 2005, Nemirovski, Juditsky, Lan &\nShapiro 2009, Shapiro, Den...",
      "word_count": 230,
      "source_page": 201,
      "vector_index": 313,
      "start_position": 71475,
      "end_position": 71704
    },
    {
      "chunk_id": 315,
      "text": "15\nSupport Vector Machines\nIn this chapter and the next we discuss a very useful machine learning tool: the\nsupport vector machine paradigm (SVM) for learning linear predictors in high\ndimensional fea...",
      "word_count": 231,
      "source_page": 202,
      "vector_index": 314,
      "start_position": 71705,
      "end_position": 71935
    },
    {
      "chunk_id": 316,
      "text": "15.1 Margin and Hard-SVM\n203\nx\nx\nWhile both the dashed-black and solid-green hyperplanes separate the four ex-\namples, our intuition would probably lead us to prefer the black hyperplane over\nthe gree...",
      "word_count": 226,
      "source_page": 203,
      "vector_index": 315,
      "start_position": 72017,
      "end_position": 72242
    },
    {
      "chunk_id": 317,
      "text": "claim 15.1\nThe distance between a point x and the hyperplane deﬁned by\n(w, b) where ∥w∥= 1 is |⟨w, x⟩+ b| Proof\nThe distance between a point x and the hyperplane is deﬁned as\nmin{∥x −v∥: ⟨w, v⟩+ b = 0...",
      "word_count": 152,
      "source_page": 203,
      "vector_index": 316,
      "start_position": 72201,
      "end_position": 72352
    },
    {
      "chunk_id": 318,
      "text": "204\nSupport Vector Machines\nbetween x and u is at least the distance between x and v, which concludes our\nproof On the basis of the preceding claim, the closest point in the training set to the\nsepara...",
      "word_count": 243,
      "source_page": 204,
      "vector_index": 317,
      "start_position": 72353,
      "end_position": 72595
    },
    {
      "chunk_id": 319,
      "text": "15.1 Margin and Hard-SVM\n205\nproblem given in Equation (15.2) Therefore, ∥w0∥≤∥w⋆\nγ⋆∥=\n1\nγ⋆ It follows that\nfor all i,\nyi(⟨ˆw, xi⟩+ ˆb) =\n1\n∥w0∥yi(⟨w0, xi⟩+ b0) ≥\n1\n∥w0∥≥γ⋆ Since ∥ˆw∥= 1 we obtain tha...",
      "word_count": 232,
      "source_page": 205,
      "vector_index": 318,
      "start_position": 72678,
      "end_position": 72909
    },
    {
      "chunk_id": 320,
      "text": "15.1.2\nThe Sample Complexity of Hard-SVM\nRecall that the VC-dimension of halfspaces in Rd is d + 1 It follows that the\nsample complexity of learning halfspaces grows with the dimensionality of the\npro...",
      "word_count": 241,
      "source_page": 205,
      "vector_index": 319,
      "start_position": 72876,
      "end_position": 73116
    },
    {
      "chunk_id": 321,
      "text": "206\nSupport Vector Machines\nS′ = (αx1, y1), , (αxm, ym) is separable with a margin of αγ That is, a sim-\nple scaling of the data can make it separable with an arbitrarily large margin It\nfollows that ...",
      "word_count": 249,
      "source_page": 206,
      "vector_index": 320,
      "start_position": 73117,
      "end_position": 73365
    },
    {
      "chunk_id": 322,
      "text": "In particular, Theorem 26.13 in Section 26.3 states the following:\ntheorem 15.4\nLet D be a distribution over Rd ×{±1} that satisﬁes the (γ, ρ)-\nseparability with margin assumption using a homogenous h...",
      "word_count": 240,
      "source_page": 206,
      "vector_index": 321,
      "start_position": 73295,
      "end_position": 73534
    },
    {
      "chunk_id": 323,
      "text": "15.2 Soft-SVM and Norm Regularization\n207\nterms is controlled by a parameter λ This leads to the Soft-SVM optimization\nproblem:\nSoft-SVM\ninput: (x1, y1), , (xm, ym)\nparameter: λ > 0\nsolve:\nmin\nw,b,ξ\n ...",
      "word_count": 250,
      "source_page": 207,
      "vector_index": 322,
      "start_position": 73592,
      "end_position": 73841
    },
    {
      "chunk_id": 324,
      "text": "208\nSupport Vector Machines\n15.2.1\nThe Sample Complexity of Soft-SVM\nWe now analyze the sample complexity of Soft-SVM for the case of homogenous\nhalfspaces (namely, the output of Equation (15.6)) In C...",
      "word_count": 238,
      "source_page": 208,
      "vector_index": 323,
      "start_position": 73896,
      "end_position": 74133
    },
    {
      "chunk_id": 325,
      "text": "Furthermore, since the hinge loss upper bounds the 0−1 loss we also have\nE\nS∼Dm[L0−1\nD\n(A(S))] ≤Lhinge\nD\n(u) + λ∥u∥2 + 2ρ2\nλ m Last, for every B > 0, if we set λ =\nq\n2ρ2\nB2m then\nE\nS∼Dm[L0−1\nD\n(A(S))]...",
      "word_count": 227,
      "source_page": 208,
      "vector_index": 324,
      "start_position": 74074,
      "end_position": 74300
    },
    {
      "chunk_id": 326,
      "text": "15.2 Soft-SVM and Norm Regularization\n209\nexamples, ρ, the norm of the halfspace B (or equivalently the margin parameter\nγ) and, in the nonseparable case, the bounds also depend on the minimum hinge\nl...",
      "word_count": 216,
      "source_page": 209,
      "vector_index": 325,
      "start_position": 74301,
      "end_position": 74516
    },
    {
      "chunk_id": 327,
      "text": "Therefore, for this\nproblem, the value of ρ2 will be the maximal number of distinct words in a given\ndocument A halfspace for this problem assigns weights to words It is natural to assume\nthat by assi...",
      "word_count": 234,
      "source_page": 209,
      "vector_index": 326,
      "start_position": 74488,
      "end_position": 74721
    },
    {
      "chunk_id": 328,
      "text": "While this induc-\ntive bias can signiﬁcantly decrease our estimation error, it can also enlarge the\napproximation error 15.2.3\nThe Ramp Loss*\nThe margin-based bounds we have derived in Corollary 15.7 ...",
      "word_count": 153,
      "source_page": 209,
      "vector_index": 327,
      "start_position": 74681,
      "end_position": 74833
    },
    {
      "chunk_id": 329,
      "text": "210\nSupport Vector Machines\ninsensitive, and therefore there is no meaning to the norm of w or its margin\nwhen we measure error with the 0−1 loss However, it is possible to deﬁne a loss\nfunction that ...",
      "word_count": 211,
      "source_page": 210,
      "vector_index": 328,
      "start_position": 74834,
      "end_position": 75044
    },
    {
      "chunk_id": 330,
      "text": "y⟨w, x⟩\nℓramp\nℓhinge\nℓ0−1\n1\n1\nThe reason SVM relies on the hinge loss and not on the ramp loss is that\nthe hinge loss is convex and, therefore, from the computational point of view,\nminimizing the hin...",
      "word_count": 174,
      "source_page": 210,
      "vector_index": 329,
      "start_position": 74989,
      "end_position": 75162
    },
    {
      "chunk_id": 331,
      "text": "15.4 Duality*\n211\nlemma 15.9 (Fritz John)\nSuppose that\nw⋆∈argmin\nw\nf(w)\ns.t ∀i ∈[m], gi(w) ≤0,\nwhere f, g1, , gm are diﬀerentiable Then, there exists α ∈Rm such that\n∇f(w⋆) + P\ni∈I αi∇gi(w⋆) = 0, wher...",
      "word_count": 245,
      "source_page": 211,
      "vector_index": 330,
      "start_position": 75163,
      "end_position": 75407
    },
    {
      "chunk_id": 332,
      "text": "212\nSupport Vector Machines\nproblem with respect to w is unconstrained and the objective is diﬀerentiable;\nthus, at the optimum, the gradient equals zero:\nw −\nm\nX\ni=1\nαiyixi = 0\n⇒\nw =\nm\nX\ni=1\nαiyixi T...",
      "word_count": 231,
      "source_page": 212,
      "vector_index": 331,
      "start_position": 75442,
      "end_position": 75672
    },
    {
      "chunk_id": 333,
      "text": "15.6 Summary\n213\nSGD for Solving Soft-SVM\ngoal: Solve Equation (15.12)\nparameter: T\ninitialize: θ(1) = 0\nfor t = 1, , T\nLet w(t) =\n1\nλ tθ(t)\nChoose i uniformly at random from [m]\nIf (yi⟨w(t), xi⟩< 1)\n...",
      "word_count": 243,
      "source_page": 213,
      "vector_index": 332,
      "start_position": 75757,
      "end_position": 75999
    },
    {
      "chunk_id": 334,
      "text": "214\nSupport Vector Machines\n15.8\nExercises\n1 Show that the hard-SVM rule, namely,\nargmax\n(w,b):∥w∥=1\nmin\ni∈[m] |⟨w, xi⟩+ b|\ns.t ∀i, yi(⟨w, xi⟩+ b) > 0,\nis equivalent to the following formulation:\nargm...",
      "word_count": 223,
      "source_page": 214,
      "vector_index": 333,
      "start_position": 76047,
      "end_position": 76269
    },
    {
      "chunk_id": 335,
      "text": "16\nKernel Methods\nIn the previous chapter we described the SVM paradigm for learning halfspaces\nin high dimensional feature spaces This enables us to enrich the expressive\npower of halfspaces by ﬁrst ...",
      "word_count": 228,
      "source_page": 215,
      "vector_index": 334,
      "start_position": 76270,
      "end_position": 76497
    },
    {
      "chunk_id": 336,
      "text": "We introduce the “kernel trick” that enables computationally\neﬃcient implementation of learning, without explicitly handling the high dimen-\nsional representation of the domain instances Kernel based ...",
      "word_count": 208,
      "source_page": 215,
      "vector_index": 335,
      "start_position": 76458,
      "end_position": 76665
    },
    {
      "chunk_id": 337,
      "text": "216\nKernel Methods\nﬁrst deﬁne a mapping ψ : R →R2 as follows:\nψ(x) = (x, x2) We use the term feature space to denote the range of ψ After applying ψ the\ndata can be easily explained using the halfspac...",
      "word_count": 219,
      "source_page": 216,
      "vector_index": 336,
      "start_position": 76666,
      "end_position": 76884
    },
    {
      "chunk_id": 338,
      "text": "Predict the label of a test point, x, to be h(ψ(x)) Note that, for every probability distribution D over X × Y, we can readily\ndeﬁne its image probability distribution Dψ over F × Y by setting, for ev...",
      "word_count": 250,
      "source_page": 216,
      "vector_index": 337,
      "start_position": 76813,
      "end_position": 77062
    },
    {
      "chunk_id": 339,
      "text": "16.2 The Kernel Trick\n217\nAs before, we can rewrite p(x) = ⟨w, ψ(x)⟩where now ψ : Rn →Rd is such\nthat for every J ∈[n]r, r ≤k, the coordinate of ψ(x) associated with J is the\nmonomial Qr\ni=1 xJi Natur...",
      "word_count": 220,
      "source_page": 217,
      "vector_index": 338,
      "start_position": 77152,
      "end_position": 77371
    },
    {
      "chunk_id": 340,
      "text": "The bottom line of this discussion is that we can enrich the class of halfspaces\nby ﬁrst applying a nonlinear mapping, ψ, that maps the instance space into some\nfeature space, and then learning a half...",
      "word_count": 242,
      "source_page": 217,
      "vector_index": 339,
      "start_position": 77317,
      "end_position": 77558
    },
    {
      "chunk_id": 341,
      "text": "The common solution to this concern is kernel based learning The term “kernels”\nis used in this context to describe inner products in the feature space Given\nan embedding ψ of some domain space X into...",
      "word_count": 192,
      "source_page": 217,
      "vector_index": 340,
      "start_position": 77533,
      "end_position": 77724
    },
    {
      "chunk_id": 342,
      "text": "218\nKernel Methods\nX into a space where these similarities are realized as inner products It turns\nout that many learning algorithms for halfspaces can be carried out just on the\nbasis of the values o...",
      "word_count": 242,
      "source_page": 218,
      "vector_index": 341,
      "start_position": 77725,
      "end_position": 77966
    },
    {
      "chunk_id": 343,
      "text": ", am) = 1\nm\nP\ni max{0, 1−yiai} Similarly, Hard-SVM for nonhomogenous\nhalfspaces (Equation (15.2)) can be derived from Equation (16.2) by letting\nR(a) = a2 and letting f(a1, , am) be 0 if there exists ...",
      "word_count": 230,
      "source_page": 218,
      "vector_index": 342,
      "start_position": 77937,
      "end_position": 78166
    },
    {
      "chunk_id": 344,
      "text": "16.2 The Kernel Trick\n219\nOn the basis of the representer theorem we can optimize Equation (16.2) with\nrespect to the coeﬃcients α instead of the coeﬃcients w as follows Writing\nw = Pm\nj=1 αjψ(xj) we ...",
      "word_count": 247,
      "source_page": 219,
      "vector_index": 343,
      "start_position": 78167,
      "end_position": 78413
    },
    {
      "chunk_id": 345,
      "text": "Gi,j = K(xi, xj), which is often called the\nGram matrix In particular, specifying the preceding to the Soft-SVM problem given in Equa-\ntion (15.6), we can rewrite the problem as\nmin\nα∈Rm\n \nλαT Gα + 1\n...",
      "word_count": 153,
      "source_page": 219,
      "vector_index": 344,
      "start_position": 78368,
      "end_position": 78520
    },
    {
      "chunk_id": 346,
      "text": "220\nKernel Methods\nis extremely large while implementing the kernel function is very simple A few\nexamples are given in the following Example 16.1 (Polynomial Kernels)\nThe k degree polynomial kernel i...",
      "word_count": 243,
      "source_page": 220,
      "vector_index": 345,
      "start_position": 78521,
      "end_position": 78763
    },
    {
      "chunk_id": 347,
      "text": "Since ψ contains all the monomials up to degree k, a halfspace over the range\nof ψ corresponds to a polynomial predictor of degree k over the original space Hence, learning a halfspace with a k degree...",
      "word_count": 159,
      "source_page": 220,
      "vector_index": 346,
      "start_position": 78712,
      "end_position": 78870
    },
    {
      "chunk_id": 348,
      "text": "16.2 The Kernel Trick\n221\nsimple More generally, given a scalar σ > 0, the Gaussian kernel is deﬁned to\nbe\nK(x, x′) = e−∥x−x′∥2\n2 σ Intuitively, the Gaussian kernel sets the inner product in the featu...",
      "word_count": 246,
      "source_page": 221,
      "vector_index": 347,
      "start_position": 78871,
      "end_position": 79116
    },
    {
      "chunk_id": 349,
      "text": "There is no contradiction, because the sample complexity\nrequired to learn with Gaussian kernels depends on the margin in the feature\nspace, which will be large if we are lucky, but can in general be ...",
      "word_count": 220,
      "source_page": 221,
      "vector_index": 348,
      "start_position": 79027,
      "end_position": 79246
    },
    {
      "chunk_id": 350,
      "text": "As a more realistic example, consider the task of learning to ﬁnd a sequence of\ncharacters (“signature”) in a ﬁle that indicates whether it contains a virus or not Formally, let Xd be the set of all s...",
      "word_count": 221,
      "source_page": 221,
      "vector_index": 349,
      "start_position": 79199,
      "end_position": 79419
    },
    {
      "chunk_id": 351,
      "text": "222\nKernel Methods\ncomplexity that is polynomial in d However, the dimension of the feature space\nis exponential in d so a direct implementation of SVM over the feature space is\nproblematic Luckily, i...",
      "word_count": 240,
      "source_page": 222,
      "vector_index": 350,
      "start_position": 79420,
      "end_position": 79659
    },
    {
      "chunk_id": 352,
      "text": ", xm, the Gram matrix, Gi,j = K(xi, xj), is a positive semideﬁnite\nmatrix Proof\nIt is trivial to see that if K implements an inner product in some Hilbert\nspace then the Gram matrix is positive semide...",
      "word_count": 195,
      "source_page": 222,
      "vector_index": 351,
      "start_position": 79622,
      "end_position": 79816
    },
    {
      "chunk_id": 353,
      "text": "16.3 Implementing Soft-SVM with Kernels\n223\ndirectly tackles the Soft-SVM optimization problem in the feature space,\nmin\nw\n \nλ\n2 ∥w∥2 + 1\nm\nm\nX\ni=1\nmax{0, 1 −y⟨w, ψ(xi)⟩} ,\n(16.5)\nwhile only using ker...",
      "word_count": 182,
      "source_page": 223,
      "vector_index": 352,
      "start_position": 79817,
      "end_position": 79998
    },
    {
      "chunk_id": 354,
      "text": "(16.7)\nThe vectors β and α are updated according to the following procedure SGD for Solving Soft-SVM with Kernels\nGoal: Solve Equation (16.5)\nparameter: T\nInitialize: β(1) = 0\nfor t = 1, , T\nLet α(t) ...",
      "word_count": 190,
      "source_page": 223,
      "vector_index": 353,
      "start_position": 79966,
      "end_position": 80155
    },
    {
      "chunk_id": 355,
      "text": "224\nKernel Methods\nspace By the deﬁnition of α(t) =\n1\nλ tβ(t) and w(t) =\n1\nλ tθ(t), this claim implies\nthat Equation (16.7) also holds, and the proof of our lemma will follow To prove\nthat Equation (1...",
      "word_count": 227,
      "source_page": 224,
      "vector_index": 354,
      "start_position": 80156,
      "end_position": 80382
    },
    {
      "chunk_id": 356,
      "text": "In Chapter 10, we discussed the AdaBoost algo-\nrithm, which faces these challenges by using a weak learner: Even though we’re\nin a very high dimensional space, we have an “oracle” that bestows on us a...",
      "word_count": 224,
      "source_page": 224,
      "vector_index": 355,
      "start_position": 80326,
      "end_position": 80549
    },
    {
      "chunk_id": 357,
      "text": "16.5 Bibliographic Remarks\n225\n16.5\nBibliographic Remarks\nIn the context of SVM, the kernel-trick has been introduced in Boser et al (1992) See also Aizerman, Braverman & Rozonoer (1964) The observati...",
      "word_count": 247,
      "source_page": 225,
      "vector_index": 356,
      "start_position": 80550,
      "end_position": 80796
    },
    {
      "chunk_id": 358,
      "text": "Kernel Ridge Regression: The ridge regression problem, with a feature\nmapping ψ, is the problem of ﬁnding a vector w that minimizes the function\nf(w) = λ ∥w∥2 + 1\n2m\nm\nX\ni=1\n(⟨w, ψ(xi)⟩−yi)2,\n(16.8)\na...",
      "word_count": 175,
      "source_page": 225,
      "vector_index": 357,
      "start_position": 80741,
      "end_position": 80915
    },
    {
      "chunk_id": 359,
      "text": "226\nKernel Methods\nProve that K is a valid kernel; namely, ﬁnd a mapping ψ : {1, , N} →H\nwhere H is some Hilbert space, such that\n∀x, x′ ∈{1, , N}, K(x, x′) = ⟨ψ(x), ψ(x′)⟩ 5 A supermarket manager wou...",
      "word_count": 245,
      "source_page": 226,
      "vector_index": 358,
      "start_position": 80916,
      "end_position": 81160
    },
    {
      "chunk_id": 360,
      "text": "17\nMulticlass, Ranking, and Complex\nPrediction Problems\nMulticlass categorization is the problem of classifying instances into one of several\npossible target classes That is, we are aiming at learning...",
      "word_count": 210,
      "source_page": 227,
      "vector_index": 359,
      "start_position": 81259,
      "end_position": 81468
    },
    {
      "chunk_id": 361,
      "text": "In Section 17.3 we show how to use the multiclass machinery for complex pre-\ndiction problems in which Y can be extremely large but has some structure on\nit This task is often called structured output...",
      "word_count": 234,
      "source_page": 227,
      "vector_index": 360,
      "start_position": 81432,
      "end_position": 81665
    },
    {
      "chunk_id": 362,
      "text": "228\nMulticlass, Ranking, and Complex Prediction Problems\nsiﬁers, each of which discriminates between one class and the rest of the classes That is, given a training set S = (x1, y1), , (xm, ym), where...",
      "word_count": 247,
      "source_page": 228,
      "vector_index": 361,
      "start_position": 81666,
      "end_position": 81912
    },
    {
      "chunk_id": 363,
      "text": "A pseudocode of the One-versus-All\napproach is given in the following One-versus-All\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nalgorithm for binary classiﬁcation A\nforeach i ∈Y\nlet Si = (x1, (−1)1[...",
      "word_count": 188,
      "source_page": 228,
      "vector_index": 362,
      "start_position": 81894,
      "end_position": 82081
    },
    {
      "chunk_id": 364,
      "text": "17.1 One-versus-All and All-Pairs\n229\nAll-Pairs\ninput:\ntraining set S = (x1, y1), , (xm, ym)\nalgorithm for binary classiﬁcation A\nforeach i, j ∈Y s.t i < j\ninitialize Si,j to be the empty sequence\nfor...",
      "word_count": 230,
      "source_page": 229,
      "vector_index": 363,
      "start_position": 82082,
      "end_position": 82311
    },
    {
      "chunk_id": 365,
      "text": "1\n2\n3\nSuppose that the probability masses of classes 1, 2, 3 are 40%, 20%, and 40%,\nrespectively Consider the application of One-versus-All to this problem, and as-\nsume that the binary classiﬁcation ...",
      "word_count": 169,
      "source_page": 229,
      "vector_index": 364,
      "start_position": 82264,
      "end_position": 82432
    },
    {
      "chunk_id": 366,
      "text": "230\nMulticlass, Ranking, and Complex Prediction Problems\nthat even though the approximation error of the class of predictors of the form\nh(x) = argmaxi⟨wi, x⟩is zero, the One-versus-All approach might...",
      "word_count": 229,
      "source_page": 230,
      "vector_index": 365,
      "start_position": 82433,
      "end_position": 82661
    },
    {
      "chunk_id": 367,
      "text": "We will elaborate on Ψ later on Given Ψ and a vector w ∈Rd, we can deﬁne a multiclass predictor, h : X →Y,\nas follows:\nh(x) = argmax\ny∈Y\n⟨w, Ψ(x, y)⟩ That is, the prediction of h for the input x is th...",
      "word_count": 203,
      "source_page": 230,
      "vector_index": 366,
      "start_position": 82629,
      "end_position": 82831
    },
    {
      "chunk_id": 368,
      "text": "17.2 Linear Multiclass Predictors\n231\nChapter 16 and as we will discuss in more detail in Chapter 25) Two examples\nof useful constructions are given in the following The Multivector Construction:\nLet ...",
      "word_count": 244,
      "source_page": 231,
      "vector_index": 367,
      "start_position": 82832,
      "end_position": 83075
    },
    {
      "chunk_id": 369,
      "text": "We next describe an example of a feature function Ψ that\ndoes incorporate prior knowledge Let X be a set of text documents and Y be a\nset of possible topics Let d be a size of a dictionary of words Fo...",
      "word_count": 163,
      "source_page": 231,
      "vector_index": 368,
      "start_position": 83045,
      "end_position": 83207
    },
    {
      "chunk_id": 370,
      "text": "232\nMulticlass, Ranking, and Complex Prediction Problems\nshort Intuitively, Ψj(x, y) should be large if the word corresponding to j ap-\npears a lot in the document x but does not appear at all in docu...",
      "word_count": 237,
      "source_page": 232,
      "vector_index": 369,
      "start_position": 83208,
      "end_position": 83444
    },
    {
      "chunk_id": 371,
      "text": "We assume\nthat ∆(y, y) = 0 Note that the zero-one loss can be easily modeled by setting\n∆(y′, y) = 1[y′̸=y] 17.2.3\nERM\nWe have deﬁned the hypothesis class HΨ,W and speciﬁed a loss function ∆ To\nlearn ...",
      "word_count": 236,
      "source_page": 232,
      "vector_index": 370,
      "start_position": 83423,
      "end_position": 83658
    },
    {
      "chunk_id": 372,
      "text": "17.2 Linear Multiclass Predictors\n233\nloss functions (see Section 12.3) In particular, we generalize the hinge loss to\nmulticlass problems 17.2.4\nGeneralized Hinge Loss\nRecall that in binary classiﬁca...",
      "word_count": 203,
      "source_page": 233,
      "vector_index": 371,
      "start_position": 83659,
      "end_position": 83861
    },
    {
      "chunk_id": 373,
      "text": "As\nwe have shown, ℓ(w, (x, y)) ≥∆(hw(x), y) Furthermore, equality holds when-\never the score of the correct label is larger than the score of any other label, y′,\nby at least ∆(y′, y), namely,\n∀y′ ∈Y ...",
      "word_count": 226,
      "source_page": 233,
      "vector_index": 372,
      "start_position": 83815,
      "end_position": 84040
    },
    {
      "chunk_id": 374,
      "text": "234\nMulticlass, Ranking, and Complex Prediction Problems\n• For each y′ ̸= y, the diﬀerence between ⟨w, Ψ(x, y)⟩and ⟨w, Ψ(x, y′)⟩is larger\nthan the loss of predicting y′ instead of y The diﬀerence ⟨w, ...",
      "word_count": 221,
      "source_page": 234,
      "vector_index": 373,
      "start_position": 84041,
      "end_position": 84261
    },
    {
      "chunk_id": 375,
      "text": "17.2 Linear Multiclass Predictors\n235\nConsider running Multiclass SVM with λ =\nq\n2ρ2\nB2m on a training set S ∼Dm\nand let hw be the output of Multiclass SVM Then,\nE\nS∼Dm[L∆\nD(hw)] ≤\nE\nS∼Dm[Lg−hinge\nD\n(...",
      "word_count": 184,
      "source_page": 235,
      "vector_index": 374,
      "start_position": 84324,
      "end_position": 84507
    },
    {
      "chunk_id": 376,
      "text": "In light of this claim, in order to ﬁnd a subgradient of the generalized\nhinge loss all we need to do is to ﬁnd y ∈Y that achieves the maximum in the\ndeﬁnition of the generalized hinge loss This yield...",
      "word_count": 225,
      "source_page": 235,
      "vector_index": 375,
      "start_position": 84426,
      "end_position": 84650
    },
    {
      "chunk_id": 377,
      "text": "236\nMulticlass, Ranking, and Complex Prediction Problems\n17.3\nStructured Output Prediction\nStructured output prediction problems are multiclass problems in which Y is\nvery large but is endowed with a ...",
      "word_count": 244,
      "source_page": 236,
      "vector_index": 376,
      "start_position": 84735,
      "end_position": 84978
    },
    {
      "chunk_id": 378,
      "text": "By “good” we mean a feature mapping that will lead to a low approximation\nerror for the class of linear predictors with respect to Ψ and ∆ Once we do this,\nwe can rely, for example, on the SGD learnin...",
      "word_count": 248,
      "source_page": 236,
      "vector_index": 377,
      "start_position": 84933,
      "end_position": 85180
    },
    {
      "chunk_id": 379,
      "text": "17.3 Structured Output Prediction\n237\nwords (i.e., sequences of letters) in Y We deﬁne the function ∆(y′, y) to be the\naverage number of letters that are diﬀerent in y′ and y, namely, 1\nr\nPr\ni=1 1[yi̸...",
      "word_count": 240,
      "source_page": 237,
      "vector_index": 378,
      "start_position": 85187,
      "end_position": 85426
    },
    {
      "chunk_id": 380,
      "text": "The second type of\nfeatures take the form\nΨi,j,2(x, y) = 1\nr\nr\nX\nt=2\n1[yt=i] 1[yt−1=j] That is, we sum the number of times the letter i follows the letter j Intuitively,\nthese features can capture rul...",
      "word_count": 250,
      "source_page": 237,
      "vector_index": 379,
      "start_position": 85394,
      "end_position": 85643
    },
    {
      "chunk_id": 381,
      "text": "238\nMulticlass, Ranking, and Complex Prediction Problems\nwhile the feature function Ψi,j,2 can be written in terms of\nφi,j,2(x, yt, yt−1) = 1[yt=i] 1[yt−1=j] Therefore, the prediction can be written a...",
      "word_count": 249,
      "source_page": 238,
      "vector_index": 380,
      "start_position": 85644,
      "end_position": 85892
    },
    {
      "chunk_id": 382,
      "text": "17.4 Ranking\n239\nX of arbitrary length A ranking hypothesis, h, is a function that receives a\nsequence of instances ¯x = (x1, , xr) ∈X ∗, and returns a permutation of [r] It is more convenient to let ...",
      "word_count": 249,
      "source_page": 239,
      "vector_index": 381,
      "start_position": 85922,
      "end_position": 86170
    },
    {
      "chunk_id": 383,
      "text": "In all the examples\nwe deﬁne ℓ(h, (¯x, y)) = ∆(h(¯x), y), for some function ∆: S∞\nr=1(Rr × Rr) →R+ • 0–1 Ranking loss: ∆(y′, y) is zero if y and y′ induce exactly the same\nranking and ∆(y′, y) = 1 oth...",
      "word_count": 238,
      "source_page": 239,
      "vector_index": 382,
      "start_position": 86127,
      "end_position": 86364
    },
    {
      "chunk_id": 384,
      "text": "240\nMulticlass, Ranking, and Complex Prediction Problems\nWe can easily see that ∆(y′, y) ∈[0, 1] and that ∆(y′, y) = 0 whenever\nπ(y′) = π(y) A typical way to deﬁne the discount function is by\nD(i) =\n(...",
      "word_count": 222,
      "source_page": 240,
      "vector_index": 383,
      "start_position": 86425,
      "end_position": 86646
    },
    {
      "chunk_id": 385,
      "text": ", xr)) = (⟨w, x1⟩, , ⟨w, xr⟩) (17.6)\nAs we discussed in Chapter 16, we can also apply a feature mapping that maps\ninstances into some feature space and then takes the inner products with w in the\nfeat...",
      "word_count": 212,
      "source_page": 240,
      "vector_index": 384,
      "start_position": 86639,
      "end_position": 86850
    },
    {
      "chunk_id": 386,
      "text": "17.4 Ranking\n241\nIn our case, y′\ni −y′\nj = ⟨w, xi −xj⟩ It follows that we can use the hinge loss upper\nbound as follows:\n1[sign(yi−yj)(y′\ni−y′\nj)≤0] ≤max {0, 1 −sign (yi −yj) ⟨w, xi −xj⟩} Taking the a...",
      "word_count": 229,
      "source_page": 241,
      "vector_index": 385,
      "start_position": 86851,
      "end_position": 87079
    },
    {
      "chunk_id": 387,
      "text": "Then (see Exercise 4),\nπ(y′) = argmax\nv∈V\nr\nX\ni=1\nvi y′\ni (17.7)\nLet us denote Ψ(¯x, v) = Pr\ni=1 vixi; it follows that\nπ(hw(¯x)) = argmax\nv∈V\nr\nX\ni=1\nvi⟨w, xi⟩\n= argmax\nv∈V\n*\nw,\nr\nX\ni=1\nvixi\n+\n= argma...",
      "word_count": 181,
      "source_page": 241,
      "vector_index": 386,
      "start_position": 87028,
      "end_position": 87208
    },
    {
      "chunk_id": 388,
      "text": "242\nMulticlass, Ranking, and Complex Prediction Problems\nequivalent to solving the problem\nargmin\nv∈V\nr\nX\ni=1\n(αivi + βi D(vi)),\nwhere αi = −⟨w, xi⟩and βi = yi/G(y, y) We can think of this problem a l...",
      "word_count": 248,
      "source_page": 242,
      "vector_index": 387,
      "start_position": 87209,
      "end_position": 87456
    },
    {
      "chunk_id": 389,
      "text": "This is because the constraints guarantee that\nthere is at most a single entry of each row that equals 1 and a single entry of each\ncolumn that equals 1 Therefore, the matrix B corresponds to the perm...",
      "word_count": 197,
      "source_page": 242,
      "vector_index": 388,
      "start_position": 87402,
      "end_position": 87598
    },
    {
      "chunk_id": 390,
      "text": "17.5 Bipartite Ranking and Multivariate Performance Measures\n243\nThe following claim states that every doubly stochastic matrix is a convex\ncombination of permutation matrices claim 17.3 ((Birkhoﬀ1946...",
      "word_count": 247,
      "source_page": 243,
      "vector_index": 389,
      "start_position": 87599,
      "end_position": 87845
    },
    {
      "chunk_id": 391,
      "text": "But, since for every other permutation matrix C we\nhave ⟨A, B⟩≤⟨A, C⟩we conclude that Ci is an optimal solution of both Equa-\ntion (17.9) and Equation (17.10) 17.5\nBipartite Ranking and Multivariate P...",
      "word_count": 249,
      "source_page": 243,
      "vector_index": 390,
      "start_position": 87801,
      "end_position": 88049
    },
    {
      "chunk_id": 392,
      "text": "244\nMulticlass, Ranking, and Complex Prediction Problems\nproblem stems from the inadequacy of the zero-one loss for what we are really\ninterested in A more adequate performance measure should take int...",
      "word_count": 244,
      "source_page": 244,
      "vector_index": 391,
      "start_position": 88097,
      "end_position": 88340
    },
    {
      "chunk_id": 393,
      "text": "However, as we will see, we\nsometimes set θ while taking into account additional constraints on the problem The loss functions we deﬁne in the following depend on the following 4 num-\nbers:\nTrue posit...",
      "word_count": 246,
      "source_page": 244,
      "vector_index": 392,
      "start_position": 88248,
      "end_position": 88493
    },
    {
      "chunk_id": 394,
      "text": "• Averaging sensitivity and speciﬁcity: This measure is the average of the\nsensitivity and speciﬁcity, namely, 1\n2\n\u0010\na\na+c +\nd\nd+b\n\u0011 This is also the accuracy\non positive examples averaged with the ac...",
      "word_count": 187,
      "source_page": 244,
      "vector_index": 393,
      "start_position": 88454,
      "end_position": 88640
    },
    {
      "chunk_id": 395,
      "text": "17.5 Bipartite Ranking and Multivariate Performance Measures\n245\nFβ =\n(1+β2)a\n(1+β2)a+b+β2c Again, we set θ = 0, and the loss function becomes\n∆(y′, y) = 1 −Fβ • Recall at k: We measure the recall whi...",
      "word_count": 241,
      "source_page": 245,
      "vector_index": 394,
      "start_position": 88641,
      "end_position": 88881
    },
    {
      "chunk_id": 396,
      "text": "As in the\nprevious section, a linear predictor for ranking is deﬁned to be\nhw(¯x) = (⟨w, x1⟩, , ⟨w, xr⟩) The corresponding loss function is one of the multivariate performance measures\ndescribed befor...",
      "word_count": 220,
      "source_page": 245,
      "vector_index": 395,
      "start_position": 88861,
      "end_position": 89080
    },
    {
      "chunk_id": 397,
      "text": "246\nMulticlass, Ranking, and Complex Prediction Problems\nOnce we have deﬁned b as in Equation (17.13), we can easily derive a convex\nsurrogate loss as follows Assuming that y ∈V , we have that\n∆(hw(¯x...",
      "word_count": 238,
      "source_page": 246,
      "vector_index": 396,
      "start_position": 89081,
      "end_position": 89318
    },
    {
      "chunk_id": 398,
      "text": "For any a, b ∈[r], let\n¯Ya,b = {v : |{i : vi = 1 ∧yi = 1}| = a ∧|{i : vi = 1 ∧yi = −1}| = b } Any vector v ∈V falls into ¯Ya,b for some a, b ∈[r] Furthermore, if ¯Ya,b ∩V\nis not empty for some a, b ∈[...",
      "word_count": 181,
      "source_page": 246,
      "vector_index": 397,
      "start_position": 89276,
      "end_position": 89456
    },
    {
      "chunk_id": 399,
      "text": "17.6 Summary\n247\nSolving Equation (17.14)\ninput:\n(x1, , xr), (y1, , yr), w, V, ∆\nassumptions:\n∆is a function of a, b, c, d\nV contains all vectors for which f(a, b) = 1 for some function f\ninitialize:\n...",
      "word_count": 250,
      "source_page": 247,
      "vector_index": 398,
      "start_position": 89457,
      "end_position": 89706
    },
    {
      "chunk_id": 400,
      "text": "248\nMulticlass, Ranking, and Complex Prediction Problems\nin (Daniely et al 2011, Daniely, Sabato & Shwartz 2012) See also Chapter 29,\nin which we analyze the sample complexity of multiclass learning D...",
      "word_count": 245,
      "source_page": 248,
      "vector_index": 399,
      "start_position": 89786,
      "end_position": 90030
    },
    {
      "chunk_id": 401,
      "text": "They also ob-\nserved that the maximization problem in the deﬁnition of the generalized hinge\nloss is equivalent to the assignment problem Agarwal & Roth (2005) analyzed the sample complexity of bipart...",
      "word_count": 202,
      "source_page": 248,
      "vector_index": 400,
      "start_position": 89998,
      "end_position": 90199
    },
    {
      "chunk_id": 402,
      "text": "17.8 Exercises\n249\nMulticlass Batch Perceptron\nInput:\nA training set (x1, y1), , (xm, ym)\nA class-sensitive feature mapping Ψ : X × Y →Rd\nInitialize: w(1) = (0, , 0) ∈Rd\nFor t = 1, 2, If (∃i and y ̸= ...",
      "word_count": 203,
      "source_page": 249,
      "vector_index": 401,
      "start_position": 90200,
      "end_position": 90402
    },
    {
      "chunk_id": 403,
      "text": "18\nDecision Trees\nA decision tree is a predictor, h : X →Y, that predicts the label associated with\nan instance x by traveling from a root node of a tree to a leaf For simplicity\nwe focus on the binar...",
      "word_count": 227,
      "source_page": 250,
      "vector_index": 402,
      "start_position": 90403,
      "end_position": 90629
    },
    {
      "chunk_id": 404,
      "text": "18.1 Sample Complexity\n251\n18.1\nSample Complexity\nA popular splitting rule at internal nodes of the tree is based on thresholding the\nvalue of a single feature That is, we move to the right or left ch...",
      "word_count": 220,
      "source_page": 251,
      "vector_index": 403,
      "start_position": 90687,
      "end_position": 90906
    },
    {
      "chunk_id": 405,
      "text": "In other words, each instance\nis a vector of d bits In that case, thresholding the value of a single feature\ncorresponds to a splitting rule of the form 1[xi=1] for some i = [d] For instance,\nwe can m...",
      "word_count": 245,
      "source_page": 251,
      "vector_index": 404,
      "start_position": 90872,
      "end_position": 91116
    },
    {
      "chunk_id": 406,
      "text": "252\nDecision Trees\nOverall, there are d + 3 options, hence we need log2(d + 3) bits to describe each\nblock Assuming each internal node has two children,1 it is not hard to show that\nthis is a preﬁx-fr...",
      "word_count": 233,
      "source_page": 252,
      "vector_index": 405,
      "start_position": 91233,
      "end_position": 91465
    },
    {
      "chunk_id": 407,
      "text": "Our bound indicates that such a tree will have low true\nrisk, LD(h) 18.2\nDecision Tree Algorithms\nThe bound on LD(h) given in Equation (18.1) suggests a learning rule for decision\ntrees – search for a...",
      "word_count": 231,
      "source_page": 252,
      "vector_index": 406,
      "start_position": 91421,
      "end_position": 91651
    },
    {
      "chunk_id": 408,
      "text": "18.2 Decision Tree Algorithms\n253\nand therefore all splitting rules are of the form 1[xi=1] for some feature i ∈[d] We discuss the case of real valued features in Section 18.2.3 The algorithm works by...",
      "word_count": 236,
      "source_page": 253,
      "vector_index": 407,
      "start_position": 91721,
      "end_position": 91956
    },
    {
      "chunk_id": 409,
      "text": "T2\nT1\n18.2.1\nImplementations of the Gain Measure\nDiﬀerent algorithms use diﬀerent implementations of Gain(S, i) Here we present\nthree We use the notation PS[F] to denote the probability that an event ...",
      "word_count": 155,
      "source_page": 253,
      "vector_index": 408,
      "start_position": 91937,
      "end_position": 92091
    },
    {
      "chunk_id": 410,
      "text": "254\nDecision Trees\nInformation Gain: Another popular gain measure that is used in the ID3\nand C4.5 algorithms of Quinlan (1993) is the information gain The information\ngain is the diﬀerence between th...",
      "word_count": 226,
      "source_page": 254,
      "vector_index": 409,
      "start_position": 92092,
      "end_position": 92317
    },
    {
      "chunk_id": 411,
      "text": "One solution is to limit the number of iterations of ID3,\nleading to a tree with a bounded number of nodes Another common solution is\nto prune the tree after it is built, hoping to reduce it to a much...",
      "word_count": 242,
      "source_page": 254,
      "vector_index": 410,
      "start_position": 92269,
      "end_position": 92510
    },
    {
      "chunk_id": 412,
      "text": "18.3 Random Forests\n255\n18.2.3\nThreshold-Based Splitting Rules for Real-Valued Features\nIn the previous section we have described an algorithm for growing a decision\ntree assuming that the features ar...",
      "word_count": 243,
      "source_page": 255,
      "vector_index": 411,
      "start_position": 92511,
      "end_position": 92753
    },
    {
      "chunk_id": 413,
      "text": "If the original number of real-valued features is d and the number of examples\nis m, then the number of constructed binary features becomes dm Calculating\nthe Gain of each feature might therefore take...",
      "word_count": 250,
      "source_page": 255,
      "vector_index": 412,
      "start_position": 92718,
      "end_position": 92967
    },
    {
      "chunk_id": 414,
      "text": "256\nDecision Trees\nthe algorithm A grows a decision tree (e.g., using the ID3 algorithm) based on\nthe sample S′, where at each splitting stage of the algorithm, the algorithm is\nrestricted to choosing...",
      "word_count": 237,
      "source_page": 256,
      "vector_index": 413,
      "start_position": 93007,
      "end_position": 93243
    },
    {
      "chunk_id": 415,
      "text": "19\nNearest Neighbor\nNearest Neighbor algorithms are among the simplest of all machine learning\nalgorithms The idea is to memorize the training set and then to predict the\nlabel of any new instance on ...",
      "word_count": 249,
      "source_page": 258,
      "vector_index": 414,
      "start_position": 93451,
      "end_position": 93699
    },
    {
      "chunk_id": 416,
      "text": "19.1\nk Nearest Neighbors\nThroughout the entire chapter we assume that our instance domain, X, is en-\ndowed with a metric function ρ That is, ρ : X ×X →R is a function that returns\nthe distance between...",
      "word_count": 156,
      "source_page": 258,
      "vector_index": 415,
      "start_position": 93657,
      "end_position": 93812
    },
    {
      "chunk_id": 417,
      "text": "19.2 Analysis\n259\nFigure 19.1 An illustration of the decision boundaries of the 1-NN rule The points\ndepicted are the sample points, and the predicted label of any new point will be the\nlabel of the s...",
      "word_count": 242,
      "source_page": 259,
      "vector_index": 416,
      "start_position": 93813,
      "end_position": 94054
    },
    {
      "chunk_id": 418,
      "text": "260\nNearest Neighbor\ngoes to inﬁnity, and the rate of convergence depends on the underlying distribu-\ntion As we have argued in Section 7.4, this type of analysis is not satisfactory One would like to...",
      "word_count": 222,
      "source_page": 260,
      "vector_index": 417,
      "start_position": 94093,
      "end_position": 94314
    },
    {
      "chunk_id": 419,
      "text": "We start by introducing some notation Let D be a distribution over X × Y Let DX denote the induced marginal distribution over X and let η : Rd →R be\nthe conditional probability1 over the labels, that ...",
      "word_count": 242,
      "source_page": 260,
      "vector_index": 418,
      "start_position": 94300,
      "end_position": 94541
    },
    {
      "chunk_id": 420,
      "text": "19.2 Analysis\n261\nProof\nSince LD(hS) = E(x,y)∼D[1[hS(x)̸=y]], we obtain that ES[LD(hS)] is the\nprobability to sample a training set S and an additional example (x, y), such\nthat the label of π1(x) is ...",
      "word_count": 250,
      "source_page": 261,
      "vector_index": 419,
      "start_position": 94542,
      "end_position": 94791
    },
    {
      "chunk_id": 421,
      "text": "262\nNearest Neighbor\nProof\nFrom the linearity of expectation, we can rewrite:\nE\nS\n\n\nX\ni:Ci∩S=∅\nP[Ci]\n\n=\nr\nX\ni=1\nP[Ci] E\nS\n\u0002\n1[Ci∩S=∅]\n\u0003 Next, for each i we have\nE\nS\n\u0002\n1[Ci∩S=∅]\n\u0003\n= P\nS[Ci ∩S = ∅] ...",
      "word_count": 239,
      "source_page": 262,
      "vector_index": 420,
      "start_position": 94865,
      "end_position": 95103
    },
    {
      "chunk_id": 422,
      "text": "19.2 Analysis\n263\nSince the number of boxes is r = (1/ϵ)d we get that\nE\nS,x[∥x −xπ1(x)∥] ≤\n√\nd\n\u0010\n2d ϵ−d\nm e\n+ ϵ\n\u0011 Combining the preceding with Lemma 19.1 we obtain that\nE\nS[LD(hS)] ≤2 LD(h⋆) + c\n√\nd\n\u0010...",
      "word_count": 239,
      "source_page": 263,
      "vector_index": 421,
      "start_position": 95204,
      "end_position": 95442
    },
    {
      "chunk_id": 423,
      "text": "In fact, it is easy\nto see that a necessary condition for the last term in Theorem 19.3 to be smaller\nthan ϵ is that m ≥(4 c\n√\nd/ϵ)d+1 That is, the size of the training set should\nincrease exponential...",
      "word_count": 230,
      "source_page": 263,
      "vector_index": 422,
      "start_position": 95399,
      "end_position": 95628
    },
    {
      "chunk_id": 424,
      "text": "264\nNearest Neighbor\nThe exponential dependence on the dimension is known as the curse of di-\nmensionality As we saw, the 1-NN rule might fail if the number of examples is\nsmaller than Ω((c+1)d) There...",
      "word_count": 240,
      "source_page": 264,
      "vector_index": 423,
      "start_position": 95679,
      "end_position": 95918
    },
    {
      "chunk_id": 425,
      "text": "Formally, an r-approximate search procedure is\nguaranteed to retrieve a point within distance of at most r times the distance\nto the nearest neighbor Three popular approximate algorithms for NN are th...",
      "word_count": 207,
      "source_page": 264,
      "vector_index": 424,
      "start_position": 95881,
      "end_position": 96087
    },
    {
      "chunk_id": 426,
      "text": "19.6 Exercises\n265\nis consistent (with respect to the hypothesis class of all functions from Rd to\n{0, 1}) A good presentation of the analysis is given in the book of Devroye et al (1996) Here, we giv...",
      "word_count": 248,
      "source_page": 265,
      "vector_index": 425,
      "start_position": 96088,
      "end_position": 96335
    },
    {
      "chunk_id": 427,
      "text": "266\nNearest Neighbor\n2 We use the notation y ∼p as a shorthand for “y is a Bernoulli random variable\nwith expected value p.” Prove the following lemma:\nlemma 19.7\nLet k ≥10 and let Z1, , Zk be indepen...",
      "word_count": 249,
      "source_page": 266,
      "vector_index": 426,
      "start_position": 96425,
      "end_position": 96673
    },
    {
      "chunk_id": 428,
      "text": "Conclude the proof of the theorem according to the following steps:\n• As in the proof of Theorem 19.3, six some ϵ > 0 and let C1, , Cr be the\ncover of the set X using boxes of length ϵ For each x, x′ ...",
      "word_count": 178,
      "source_page": 266,
      "vector_index": 427,
      "start_position": 96633,
      "end_position": 96810
    },
    {
      "chunk_id": 429,
      "text": "20\nNeural Networks\nAn artiﬁcial neural network is a model of computation inspired by the structure\nof neural networks in the brain In simpliﬁed models of the brain, it consists of\na large number of ba...",
      "word_count": 241,
      "source_page": 268,
      "vector_index": 428,
      "start_position": 96941,
      "end_position": 97181
    },
    {
      "chunk_id": 430,
      "text": "In the context of learning, we can deﬁne a hypothesis class consisting of neural\nnetwork predictors, where all the hypotheses share the underlying graph struc-\nture of the network and diﬀer in the wei...",
      "word_count": 245,
      "source_page": 268,
      "vector_index": 429,
      "start_position": 97102,
      "end_position": 97346
    },
    {
      "chunk_id": 431,
      "text": "20.1 Feedforward Neural Networks\n269\nhope it will ﬁnd a reasonable solution (as happens to be the case in several\npractical tasks) In Section 20.6 we describe how to implement SGD for neural\nnetworks ...",
      "word_count": 246,
      "source_page": 269,
      "vector_index": 430,
      "start_position": 97394,
      "end_position": 97639
    },
    {
      "chunk_id": 432,
      "text": "We will focus on three possible functions for σ: the sign\nfunction, σ(a) = sign(a), the threshold function, σ(a) = 1[a>0], and the sig-\nmoid function, σ(a) = 1/(1 + exp(−a)), which is a smooth approxi...",
      "word_count": 235,
      "source_page": 269,
      "vector_index": 431,
      "start_position": 97591,
      "end_position": 97825
    },
    {
      "chunk_id": 433,
      "text": "270\nNeural Networks\nand\not+1,j(x) = σ (at+1,j(x)) That is, the input to vt+1,j is a weighted sum of the outputs of the neurons in Vt\nthat are connected to vt+1,j, where weighting is according to w, an...",
      "word_count": 234,
      "source_page": 270,
      "vector_index": 432,
      "start_position": 97917,
      "end_position": 98150
    },
    {
      "chunk_id": 434,
      "text": "20.3 The Expressive Power of Neural Networks\n271\nThat is, the parameters specifying a hypothesis in the hypothesis class are the\nweights over the edges of the network We can now study the approximatio...",
      "word_count": 222,
      "source_page": 271,
      "vector_index": 433,
      "start_position": 98225,
      "end_position": 98446
    },
    {
      "chunk_id": 435,
      "text": "More concretely,\nwe will ﬁx some architecture, V, E, σ, and will study what functions hypotheses\nin HV,E,σ can implement, as a function of the size of V We start the discussion with studying which typ...",
      "word_count": 237,
      "source_page": 271,
      "vector_index": 434,
      "start_position": 98397,
      "end_position": 98633
    },
    {
      "chunk_id": 436,
      "text": "272\nNeural Networks\nthe functions gi(x), and therefore can be written as\nf(x) = sign\n k\nX\ni=1\ngi(x) + k −1 ,\nwhich concludes our proof The preceding claim shows that neural networks can implement any ...",
      "word_count": 246,
      "source_page": 272,
      "vector_index": 435,
      "start_position": 98712,
      "end_position": 98957
    },
    {
      "chunk_id": 437,
      "text": "This implies that |V | ≥Ω(2n/3), which concludes our\nproof for the case of networks with the sign activation function The proof for\nthe sigmoid case is analogous Remark 20.1\nIt is possible to derive a...",
      "word_count": 247,
      "source_page": 272,
      "vector_index": 436,
      "start_position": 98930,
      "end_position": 99176
    },
    {
      "chunk_id": 438,
      "text": "20.3 The Expressive Power of Neural Networks\n273\nimplement conjunctions, disjunctions, and negation of their inputs Circuit com-\nplexity measures the size of Boolean circuits required to calculate fun...",
      "word_count": 230,
      "source_page": 273,
      "vector_index": 437,
      "start_position": 99193,
      "end_position": 99422
    },
    {
      "chunk_id": 439,
      "text": "lemma 20.4\nSuppose that a neuron v, that implements the sign activation\nfunction, has k incoming edges, connecting it to neurons whose outputs are in\n{±1} Then, by adding one more edge, linking a “con...",
      "word_count": 234,
      "source_page": 273,
      "vector_index": 438,
      "start_position": 99361,
      "end_position": 99594
    },
    {
      "chunk_id": 440,
      "text": "274\nNeural Networks\nLet us start with a depth 2 network, namely, a network with a single hidden\nlayer Each neuron in the hidden layer implements a halfspace predictor Then,\nthe single neuron at the ou...",
      "word_count": 248,
      "source_page": 274,
      "vector_index": 439,
      "start_position": 99681,
      "end_position": 99928
    },
    {
      "chunk_id": 441,
      "text": "20.4 The Sample Complexity of Neural Networks\n275\nProof\nTo simplify the notation throughout the proof, let us denote the hy-\npothesis class by H Recall the deﬁnition of the growth function, τH(m), fro...",
      "word_count": 242,
      "source_page": 275,
      "vector_index": 440,
      "start_position": 99982,
      "end_position": 100223
    },
    {
      "chunk_id": 442,
      "text": "Therefore,\nτH(m) ≤\nT\nY\nt=1\nτH(t)(m) In addition, each H(t) can be written as a product of function classes, H(t) =\nH(t,1) × · · · × H(t,|Vt|), where each H(t,j) is all functions from layer t −1 to {±1...",
      "word_count": 230,
      "source_page": 275,
      "vector_index": 441,
      "start_position": 100174,
      "end_position": 100403
    },
    {
      "chunk_id": 443,
      "text": "276\nNeural Networks\nwe only consider networks in which the weights have a short representation as\nﬂoating point numbers with O(1) bits, by using the discretization trick we easily\nobtain that such net...",
      "word_count": 220,
      "source_page": 276,
      "vector_index": 442,
      "start_position": 100404,
      "end_position": 100623
    },
    {
      "chunk_id": 444,
      "text": "Then, it is NP hard to implement the\nERM rule with respect to HV,E,sign The proof relies on a reduction from the k-coloring problem and is left as\nExercise 6 One way around the preceding hardness resu...",
      "word_count": 233,
      "source_page": 276,
      "vector_index": 443,
      "start_position": 100594,
      "end_position": 100826
    },
    {
      "chunk_id": 445,
      "text": "20.6 SGD and Backpropagation\n277\nhope it will ﬁnd a reasonable solution (as happens to be the case in several\npractical tasks) 20.6\nSGD and Backpropagation\nThe problem of ﬁnding a hypothesis in HV,E,σ...",
      "word_count": 249,
      "source_page": 277,
      "vector_index": 444,
      "start_position": 100880,
      "end_position": 101128
    },
    {
      "chunk_id": 446,
      "text": "Recall the SGD algorithm for minimizing the risk function LD(w) We repeat\nthe pseudocode from Chapter 14 with a few modiﬁcations, which are relevant\nto the neural network application because of the no...",
      "word_count": 249,
      "source_page": 277,
      "vector_index": 445,
      "start_position": 101092,
      "end_position": 101340
    },
    {
      "chunk_id": 447,
      "text": "278\nNeural Networks\nSGD for Neural Networks\nparameters:\nnumber of iterations τ\nstep size sequence η1, η2, , ητ\nregularization parameter λ > 0\ninput:\nlayered graph (V, E)\ndiﬀerentiable activation funct...",
      "word_count": 212,
      "source_page": 278,
      "vector_index": 446,
      "start_position": 101341,
      "end_position": 101552
    },
    {
      "chunk_id": 448,
      "text": "20.6 SGD and Backpropagation\n279\nExplaining How Backpropagation Calculates the Gradient:\nWe next explain how the backpropagation algorithm calculates the gradient of\nthe loss function on an example (x...",
      "word_count": 229,
      "source_page": 279,
      "vector_index": 447,
      "start_position": 101553,
      "end_position": 101781
    },
    {
      "chunk_id": 449,
      "text": "• Let f(w) = Aw for A ∈Rm,n Then Jw(f) = A • For every n, we use the notation σ to denote the function from Rn to Rn\nwhich applies the sigmoid function element-wise That is, α = σ(θ) means\nthat for ev...",
      "word_count": 247,
      "source_page": 279,
      "vector_index": 448,
      "start_position": 101770,
      "end_position": 102016
    },
    {
      "chunk_id": 450,
      "text": "280\nNeural Networks\nNext, we discuss how to calculate the partial derivatives with respect to the\nedges from Vt−1 to Vt, namely, with respect to the elements in Wt−1 Since we\nﬁx all other weights of t...",
      "word_count": 249,
      "source_page": 280,
      "vector_index": 449,
      "start_position": 102087,
      "end_position": 102335
    },
    {
      "chunk_id": 451,
      "text": "0\n0\n· · ·\no⊤\nt−1\n\n\n\n\n\n\n (20.2)\nThen, Wt−1ot−1 = Ot−1wt−1, so we can also write\ngt(wt−1) = ℓt(σ(Ot−1 wt−1)) Therefore, applying the chain rule, we obtain that\nJwt−1(gt) = Jσ(Ot−1wt−1)(ℓt) diag(σ...",
      "word_count": 177,
      "source_page": 280,
      "vector_index": 450,
      "start_position": 102308,
      "end_position": 102484
    },
    {
      "chunk_id": 452,
      "text": "20.7 Summary\n281\nIn particular,\nδt = Jot(ℓt) = Jσ(Wtot)(ℓt+1)diag(σ′(Wtot))Wt\n= Jot+1(ℓt+1)diag(σ′(at+1))Wt\n= δt+1 diag(σ′(at+1))Wt In summary, we can ﬁrst calculate the vectors {at, ot} from the bott...",
      "word_count": 243,
      "source_page": 281,
      "vector_index": 451,
      "start_position": 102485,
      "end_position": 102727
    },
    {
      "chunk_id": 453,
      "text": "20.8\nBibliographic Remarks\nNeural networks were extensively studied in the 1980s and early 1990s, but with\nmixed empirical success In recent years, a combination of algorithmic advance-\nments, as well...",
      "word_count": 193,
      "source_page": 281,
      "vector_index": 452,
      "start_position": 102680,
      "end_position": 102872
    },
    {
      "chunk_id": 454,
      "text": "282\nNeural Networks\nKlivans & Sherstov (2006) have shown that for any c > 0, intersections of nc\nhalfspaces over {±1}n are not eﬃciently PAC learnable, even if we allow repre-\nsentation independent le...",
      "word_count": 244,
      "source_page": 282,
      "vector_index": 453,
      "start_position": 102873,
      "end_position": 103116
    },
    {
      "chunk_id": 455,
      "text": "Hint: For every f : {−1, 1}n →{−1, 1} construct a 1-Lipschitz function\ng : [−1, 1]n →[−1, 1] such that if you can approximate g then you can express\nf 3 Growth function of product: For i = 1, 2, let F...",
      "word_count": 238,
      "source_page": 282,
      "vector_index": 454,
      "start_position": 103085,
      "end_position": 103322
    },
    {
      "chunk_id": 456,
      "text": "20.9 Exercises\n283\nif we feed the network with the real number 0.x1x2 xn, then the output\nof the network will be x Hint: Denote α = 0.x1x2 xn and observe that 10kα −0.5 is at least 0.5\nif xk = 1 and i...",
      "word_count": 245,
      "source_page": 283,
      "vector_index": 455,
      "start_position": 103349,
      "end_position": 103593
    },
    {
      "chunk_id": 457,
      "text": "21\nOnline Learning\nIn this chapter we describe a diﬀerent model of learning, which is called online\nlearning Previously, we studied the PAC learning model, in which the learner\nﬁrst receives a batch o...",
      "word_count": 227,
      "source_page": 287,
      "vector_index": 456,
      "start_position": 103594,
      "end_position": 103820
    },
    {
      "chunk_id": 458,
      "text": "On each online round, the learner ﬁrst receives an instance (the learner buys\na papaya and knows its shape and color, which form the instance) Then, the\nlearner is required to predict a label (is the ...",
      "word_count": 208,
      "source_page": 287,
      "vector_index": 457,
      "start_position": 103783,
      "end_position": 103990
    },
    {
      "chunk_id": 459,
      "text": "288\nOnline Learning\n21.1\nOnline Classiﬁcation in the Realizable Case\nOnline learning is performed in a sequence of consecutive rounds, where at round\nt the learner is given an instance, xt, taken from...",
      "word_count": 247,
      "source_page": 288,
      "vector_index": 458,
      "start_position": 103991,
      "end_position": 104237
    },
    {
      "chunk_id": 460,
      "text": "To make nontrivial statements we must further restrict the problem The real-\nizability assumption is one possible natural restriction In the realizable case, we\nassume that all the labels are generate...",
      "word_count": 243,
      "source_page": 288,
      "vector_index": 459,
      "start_position": 104219,
      "end_position": 104461
    },
    {
      "chunk_id": 461,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n289\ntional aspect of learning, and do not restrict the algorithms to be eﬃcient In\nSection 21.3 and Section 21.4 we study eﬃcient online learning algor...",
      "word_count": 245,
      "source_page": 289,
      "vector_index": 460,
      "start_position": 104523,
      "end_position": 104767
    },
    {
      "chunk_id": 462,
      "text": "290\nOnline Learning\ntheorem 21.3\nLet H be a ﬁnite hypothesis class The Halving algorithm\nenjoys the mistake bound MHalving(H) ≤log2(|H|) Proof\nWe simply note that whenever the algorithm errs we have |...",
      "word_count": 246,
      "source_page": 290,
      "vector_index": 461,
      "start_position": 104884,
      "end_position": 105129
    },
    {
      "chunk_id": 463,
      "text": "On\nround t of the game, the environment picks an instance xt, the learner predicts a\nlabel pt ∈{0, 1}, and ﬁnally the environment outputs the true label, yt ∈{0, 1} Suppose that the environment wants ...",
      "word_count": 236,
      "source_page": 290,
      "vector_index": 462,
      "start_position": 105081,
      "end_position": 105316
    },
    {
      "chunk_id": 464,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n291\nv1\nv2\nv3\nh1\nh2\nh3\nh4\nv1\n0\n0\n1\n1\nv2\n0\n1\n∗\n∗\nv3\n∗\n∗\n0\n1\nFigure 21.1 An illustration of a shattered tree of depth 2 The dashed path\ncorresponds to the...",
      "word_count": 238,
      "source_page": 291,
      "vector_index": 463,
      "start_position": 105374,
      "end_position": 105611
    },
    {
      "chunk_id": 465,
      "text": ", yd) ∈{0, 1}d\nthere exists h ∈H such that for all t ∈[d] we have h(vit) = yt where it =\n2t−1 + Pt−1\nj=1 yj 2t−1−j An illustration of a shattered tree of depth 2 is given in Figure 21.1 definition 21....",
      "word_count": 235,
      "source_page": 291,
      "vector_index": 464,
      "start_position": 105570,
      "end_position": 105804
    },
    {
      "chunk_id": 466,
      "text": "292\nOnline Learning\nx = j Then, it is easy to show that Ldim(H) = 1 while |H| = d can be arbitrarily\nlarge Therefore, this example shows that Ldim(H) can be signiﬁcantly smaller\nthan log2(|H|) Example...",
      "word_count": 224,
      "source_page": 292,
      "vector_index": 465,
      "start_position": 105805,
      "end_position": 106028
    },
    {
      "chunk_id": 467,
      "text": "21.1 Online Classiﬁcation in the Realizable Case\n293\nlemma 21.7\nSOA enjoys the mistake bound MSOA(H) ≤Ldim(H) Proof\nIt suﬃces to prove that whenever the algorithm makes a prediction mis-\ntake we have ...",
      "word_count": 249,
      "source_page": 293,
      "vector_index": 466,
      "start_position": 106136,
      "end_position": 106384
    },
    {
      "chunk_id": 468,
      "text": "294\nOnline Learning\n21.2\nOnline Classiﬁcation in the Unrealizable Case\nIn the previous section we studied online learnability in the realizable case We\nnow consider the unrealizable case Similarly to ...",
      "word_count": 240,
      "source_page": 294,
      "vector_index": 467,
      "start_position": 106487,
      "end_position": 106726
    },
    {
      "chunk_id": 469,
      "text": "An interesting question is whether we can derive an algorithm with low regret,\nmeaning that RegretA(H, T) grows sublinearly with the number of rounds, T,\nwhich implies that the diﬀerence between the e...",
      "word_count": 244,
      "source_page": 294,
      "vector_index": 468,
      "start_position": 106655,
      "end_position": 106898
    },
    {
      "chunk_id": 470,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n295\non round t is\nP[ˆyt ̸= yt] = |pt −yt| Put another way, instead of having the predictions of the learner being in {0, 1}\nwe allow them to be in [0...",
      "word_count": 231,
      "source_page": 295,
      "vector_index": 469,
      "start_position": 107010,
      "end_position": 107240
    },
    {
      "chunk_id": 471,
      "text": "21.2.1\nWeighted-Majority\nWeighted-majority is an algorithm for the problem of prediction with expert ad-\nvice In this online learning problem, on round t the learner has to choose the\nadvice of d give...",
      "word_count": 156,
      "source_page": 295,
      "vector_index": 470,
      "start_position": 107207,
      "end_position": 107362
    },
    {
      "chunk_id": 472,
      "text": "296\nOnline Learning\nWeighted-Majority\ninput: number of experts, d ; number of rounds, T\nparameter: η =\np\n2 log(d)/T\ninitialize: ˜w(1) = (1, , 1)\nfor t = 1, 2, set w(t) = ˜w(t)/Zt where Zt = P\ni ˜w(t)\n...",
      "word_count": 204,
      "source_page": 296,
      "vector_index": 471,
      "start_position": 107363,
      "end_position": 107566
    },
    {
      "chunk_id": 473,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n297\nSumming this inequality over t we get\nlog(ZT +1) −log(Z1) =\nT\nX\nt=1\nlog Zt+1\nZt\n≤−η\nT\nX\nt=1\n⟨w(t), vt⟩+ T η2\n2 (21.3)\nNext, we lower bound ZT +1 ...",
      "word_count": 220,
      "source_page": 297,
      "vector_index": 472,
      "start_position": 107622,
      "end_position": 107841
    },
    {
      "chunk_id": 474,
      "text": ", hd} In this case, we can refer to\neach hypothesis, hi, as an expert, whose advice is to predict hi(xt), and whose\ncost is vt,i = |hi(xt) −yt| The prediction of the algorithm will therefore be\npt = P...",
      "word_count": 154,
      "source_page": 297,
      "vector_index": 473,
      "start_position": 107813,
      "end_position": 107966
    },
    {
      "chunk_id": 475,
      "text": "298\nOnline Learning\ncorollary 21.12\nLet H be a ﬁnite hypothesis class There exists an algorithm\nfor online classiﬁcation, whose predictions come from [0, 1], that enjoys the regret\nbound\nT\nX\nt=1\n|pt −...",
      "word_count": 231,
      "source_page": 298,
      "vector_index": 474,
      "start_position": 107967,
      "end_position": 108197
    },
    {
      "chunk_id": 476,
      "text": "The expert is deﬁned by the following algorithm Expert(i1, i2, , iL)\ninput A hypothesis class H ; Indices i1 < i2 < · · · < iL\ninitialize: V1 = H\nfor t = 1, 2, , T\nreceive xt\nfor r ∈{0, 1} let V (r)\nt...",
      "word_count": 182,
      "source_page": 298,
      "vector_index": 475,
      "start_position": 108188,
      "end_position": 108369
    },
    {
      "chunk_id": 477,
      "text": "21.2 Online Classiﬁcation in the Unrealizable Case\n299\nTheorem 21.11 tells us that the expected number of mistakes of Weighted-Majority\nis at most the number of mistakes of the best expert plus\np\n2 lo...",
      "word_count": 250,
      "source_page": 299,
      "vector_index": 476,
      "start_position": 108370,
      "end_position": 108619
    },
    {
      "chunk_id": 478,
      "text": "Now, consider the Expert(i1, i2, , iL) running on the sequence x1, x2, , xT By construction, the set Vt maintained by Expert(i1, i2, , iL) equals the set Vt\nmaintained by SOA when running on the seque...",
      "word_count": 236,
      "source_page": 299,
      "vector_index": 477,
      "start_position": 108607,
      "end_position": 108842
    },
    {
      "chunk_id": 479,
      "text": "300\nOnline Learning\n21.3\nOnline Convex Optimization\nIn Chapter 12 we studied convex learning problems and showed learnability\nresults for these problems in the agnostic PAC learning framework In this ...",
      "word_count": 235,
      "source_page": 300,
      "vector_index": 478,
      "start_position": 108843,
      "end_position": 109077
    },
    {
      "chunk_id": 480,
      "text": "21.4 The Online Perceptron Algorithm\n301\ntheorem 21.15\nThe Online Gradient Descent algorithm enjoys the following\nregret bound for every w⋆∈H,\nRegretA(w⋆, T) ≤∥w⋆∥2\n2η\n+ η\n2\nT\nX\nt=1\n∥vt∥2 If we furthe...",
      "word_count": 243,
      "source_page": 301,
      "vector_index": 479,
      "start_position": 109111,
      "end_position": 109353
    },
    {
      "chunk_id": 481,
      "text": "302\nOnline Learning\nw ∈Rd} In Section 9.1.2 we have presented the batch version of the Perceptron,\nwhich aims to solve the ERM problem with respect to H We now present an\nonline version of the Percept...",
      "word_count": 245,
      "source_page": 302,
      "vector_index": 480,
      "start_position": 109399,
      "end_position": 109643
    },
    {
      "chunk_id": 482,
      "text": "We conclude that indeed Ldim(H) = ∞ To sidestep this impossibility result, the Perceptron algorithm relies on the\ntechnique of surrogate convex losses (see Section 12.3) This is also closely related\nt...",
      "word_count": 250,
      "source_page": 302,
      "vector_index": 481,
      "start_position": 109618,
      "end_position": 109867
    },
    {
      "chunk_id": 483,
      "text": "21.4 The Online Perceptron Algorithm\n303\nfunction and we can take vt = 0 Otherwise, it is easy to verify that vt = −ytxt\nis in ∂ft(w(t)) We therefore obtain the update rule\nw(t+1) =\n(\nw(t)\nif yt⟨w(t),...",
      "word_count": 241,
      "source_page": 303,
      "vector_index": 482,
      "start_position": 109901,
      "end_position": 110141
    },
    {
      "chunk_id": 484,
      "text": "304\nOnline Learning\ntheorem 21.16\nSuppose that the Perceptron algorithm runs on a sequence\n(x1, y1), , (xT , yT ) and let R = maxt ∥xt∥ Let M be the rounds on which the\nPerceptron errs and let ft(w) =...",
      "word_count": 226,
      "source_page": 304,
      "vector_index": 483,
      "start_position": 110204,
      "end_position": 110429
    },
    {
      "chunk_id": 485,
      "text": "More speciﬁcally, the distance from xt to the\ndecision boundary is at least γ = 1/∥w⋆∥and the bound becomes (R/γ)2 When the separability assumption does not hold, the bound involves the term\n[1 −yt⟨w⋆...",
      "word_count": 242,
      "source_page": 304,
      "vector_index": 484,
      "start_position": 110383,
      "end_position": 110624
    },
    {
      "chunk_id": 486,
      "text": "21.6 Bibliographic Remarks\n305\n21.6\nBibliographic Remarks\nThe Standard Optimal Algorithm was derived by the seminal work of Lit-\ntlestone (1988) A generalization to the nonrealizable case, as well as ...",
      "word_count": 250,
      "source_page": 305,
      "vector_index": 485,
      "start_position": 110625,
      "end_position": 110874
    },
    {
      "chunk_id": 487,
      "text": "Let d ≥2, X = {1, , d} and let H = {hj : j ∈[d]}, where hj(x) = 1[x=j] Calculate MHalving(H) (i.e., derive lower and upper bounds on MHalving(H),\nand prove that they are equal) 4 The Doubling Trick:\nI...",
      "word_count": 167,
      "source_page": 305,
      "vector_index": 486,
      "start_position": 110855,
      "end_position": 111021
    },
    {
      "chunk_id": 488,
      "text": "306\nOnline Learning\nShow that if the regret of A on each period of 2m rounds is at most α\n√\n2m,\nthen the total regret is at most\n√\n2\n√\n2 −1 α\n√\nT 5 Online-to-batch Conversions: In this exercise we dem...",
      "word_count": 197,
      "source_page": 306,
      "vector_index": 487,
      "start_position": 111022,
      "end_position": 111218
    },
    {
      "chunk_id": 489,
      "text": "22\nClustering\nClustering is one of the most widely used techniques for exploratory data anal-\nysis Across all disciplines, from social sciences to biology to computer science,\npeople try to get a ﬁrst...",
      "word_count": 228,
      "source_page": 307,
      "vector_index": 488,
      "start_position": 111219,
      "end_position": 111446
    },
    {
      "chunk_id": 490,
      "text": "Mathematically speaking, similarity (or proximity) is not a transi-\ntive relation, while cluster sharing is an equivalence relation and, in particular,\nit is a transitive relation More concretely, it ...",
      "word_count": 202,
      "source_page": 307,
      "vector_index": 489,
      "start_position": 111405,
      "end_position": 111606
    },
    {
      "chunk_id": 491,
      "text": "308\nClustering\nIn contrast, a clustering method that emphasizes not having far-away points\nshare the same cluster (e.g., the 2-means algorithm that will be described in\nSection 22.1) will cluster the ...",
      "word_count": 239,
      "source_page": 308,
      "vector_index": 490,
      "start_position": 111607,
      "end_position": 111845
    },
    {
      "chunk_id": 492,
      "text": "Clustering\n309\nThis phenomenon is not just artiﬁcial but occurs in real applications A given\nset of objects can be clustered in various diﬀerent meaningful ways This may\nbe due to having diﬀerent impl...",
      "word_count": 244,
      "source_page": 309,
      "vector_index": 491,
      "start_position": 111846,
      "end_position": 112089
    },
    {
      "chunk_id": 493,
      "text": "Alternatively, the function\ncould be a similarity function s : X × X →[0, 1] that is symmetric\nand satisﬁes s(x, x) = 1 for all x ∈X Additionally, some clustering\nalgorithms also require an input para...",
      "word_count": 176,
      "source_page": 309,
      "vector_index": 492,
      "start_position": 112046,
      "end_position": 112221
    },
    {
      "chunk_id": 494,
      "text": "310\nClustering\nIn the following we survey some of the most popular clustering methods In\nthe last section of this chapter we return to the high level discussion of what is\nclustering 22.1\nLinkage-Base...",
      "word_count": 224,
      "source_page": 310,
      "vector_index": 493,
      "start_position": 112222,
      "end_position": 112445
    },
    {
      "chunk_id": 495,
      "text": "The most common\nways are\n1 Single Linkage clustering, in which the between-clusters distance is deﬁned\nby the minimum distance between members of the two clusters, namely,\nD(A, B)\ndef\n=\nmin{d(x, y) : ...",
      "word_count": 217,
      "source_page": 310,
      "vector_index": 494,
      "start_position": 112407,
      "end_position": 112623
    },
    {
      "chunk_id": 496,
      "text": "22.2 k-Means and Other Cost Minimization Clusterings\n311\nb\nc\nd\ne\na\n{a}\n{b}\n{c}\n{d}\n{e}\n{b, c}\n{d, e}\n{b, c, d, e}\n{a, b, c, d, e}\nThe single linkage algorithm is closely related to Kruskal’s algorithm...",
      "word_count": 211,
      "source_page": 311,
      "vector_index": 495,
      "start_position": 112624,
      "end_position": 112834
    },
    {
      "chunk_id": 497,
      "text": "Stop merging as soon as all the\nbetween-clusters distances are larger than r We can also set r to be\nα max{d(x, y) : x, y ∈X} for some α < 1 In that case the stopping\ncriterion is called “scaled dista...",
      "word_count": 237,
      "source_page": 311,
      "vector_index": 496,
      "start_position": 112803,
      "end_position": 113039
    },
    {
      "chunk_id": 498,
      "text": "312\nClustering\nparameter In practice, it is often up to the user of the clustering algorithm to\nchoose the parameter k that is most suitable for the given clustering problem In the following we descri...",
      "word_count": 240,
      "source_page": 312,
      "vector_index": 497,
      "start_position": 113040,
      "end_position": 113279
    },
    {
      "chunk_id": 499,
      "text": "(22.1)\nThe k-means objective function is relevant, for example, in digital com-\nmunication tasks, where the members of X may be viewed as a collection\nof signals that have to be transmitted While X ma...",
      "word_count": 220,
      "source_page": 312,
      "vector_index": 498,
      "start_position": 113222,
      "end_position": 113441
    },
    {
      "chunk_id": 500,
      "text": "22.2 k-Means and Other Cost Minimization Clusterings\n313\nAn example where such an objective makes sense is the facility location\nproblem Consider the task of locating k ﬁre stations in a city One can\n...",
      "word_count": 232,
      "source_page": 313,
      "vector_index": 499,
      "start_position": 113442,
      "end_position": 113673
    },
    {
      "chunk_id": 501,
      "text": "22.2.1\nThe k-Means Algorithm\nThe k-means objective function is quite popular in practical applications of clus-\ntering However, it turns out that ﬁnding the optimal k-means solution is of-\nten computa...",
      "word_count": 170,
      "source_page": 313,
      "vector_index": 500,
      "start_position": 113629,
      "end_position": 113798
    },
    {
      "chunk_id": 502,
      "text": "314\nClustering\nProof\nTo simplify the notation, let us use the shorthand G(C1, , Ck) for the\nk-means objective, namely,\nG(C1, , Ck) =\nmin\nµ1,...,µk∈Rn\nk\nX\ni=1\nX\nx∈Ci\n∥x −µi∥2 (22.2)\nIt is convenient to...",
      "word_count": 247,
      "source_page": 314,
      "vector_index": 501,
      "start_position": 113799,
      "end_position": 114045
    },
    {
      "chunk_id": 503,
      "text": ", C(t−1)\nk\n) Combining this with Equation (22.4) and Equation (22.5),\nwe obtain that G(C(t)\n1 , , C(t)\nk ) ≤G(C(t−1)\n1\n, , C(t−1)\nk\n), which concludes our\nproof While the preceding lemma tells us that...",
      "word_count": 150,
      "source_page": 314,
      "vector_index": 502,
      "start_position": 114028,
      "end_position": 114177
    },
    {
      "chunk_id": 504,
      "text": "22.3 Spectral Clustering\n315\n22.3\nSpectral Clustering\nOften, a convenient way to represent the relationships between points in a data\nset X = {x1, , xm} is by a similarity graph; each vertex represent...",
      "word_count": 239,
      "source_page": 315,
      "vector_index": 503,
      "start_position": 114178,
      "end_position": 114416
    },
    {
      "chunk_id": 505,
      "text": "For k = 2, the mincut problem can be solved eﬃciently However, in practice it\noften does not lead to satisfactory partitions The problem is that in many cases,\nthe solution of mincut simply separates ...",
      "word_count": 176,
      "source_page": 315,
      "vector_index": 504,
      "start_position": 114395,
      "end_position": 114570
    },
    {
      "chunk_id": 506,
      "text": "316\nClustering\ndefinition 22.2 (Unnormalized Graph Laplacian)\nThe unnormalized graph\nLaplacian is the m × m matrix L = D −W where D is a diagonal matrix with\nDi,i = Pm\nj=1 Wi,j The matrix D is called ...",
      "word_count": 243,
      "source_page": 316,
      "vector_index": 505,
      "start_position": 114571,
      "end_position": 114813
    },
    {
      "chunk_id": 507,
      "text": "22.4 Information Bottleneck*\n317\n22.3.3\nUnnormalized Spectral Clustering\nUnnormalized Spectral Clustering\nInput: W ∈Rm,m ; Number of clusters k\nInitialize: Compute the unnormalized graph Laplacian L\nL...",
      "word_count": 249,
      "source_page": 317,
      "vector_index": 506,
      "start_position": 114883,
      "end_position": 115131
    },
    {
      "chunk_id": 508,
      "text": "It relies on notions from information theory To\nillustrate the method, consider the problem of clustering text documents where\neach document is represented as a bag-of-words; namely, each document is ...",
      "word_count": 210,
      "source_page": 317,
      "vector_index": 507,
      "start_position": 115074,
      "end_position": 115283
    },
    {
      "chunk_id": 509,
      "text": "318\nClustering\nparameter, and the minimization is over all possible probabilistic assignments of\npoints to clusters Intuitively, we would like to achieve two contradictory goals On one hand, we would ...",
      "word_count": 241,
      "source_page": 318,
      "vector_index": 508,
      "start_position": 115284,
      "end_position": 115524
    },
    {
      "chunk_id": 510,
      "text": "Are there any\nbasic properties of clustering that are independent of any speciﬁc algorithm or\ntask One method for addressing such questions is via an axiomatic approach There\nhave been several attempt...",
      "word_count": 184,
      "source_page": 318,
      "vector_index": 509,
      "start_position": 115498,
      "end_position": 115681
    },
    {
      "chunk_id": 511,
      "text": "22.5 A High Level View of Clustering\n319\nConsistency (Co) If d and d′ are dissimilarity functions over X, such that\nfor every x, y ∈X, if x, y belong to the same cluster in F(X, d) then\nd′(x, y) ≤d(x,...",
      "word_count": 243,
      "source_page": 319,
      "vector_index": 510,
      "start_position": 115760,
      "end_position": 116002
    },
    {
      "chunk_id": 512,
      "text": "However, Kleinberg (2003) has shown the following “impossibility” result:\ntheorem 22.4\nThere exists no function, F, that satisﬁes all the three proper-\nties: Scale Invariance, Richness, and Consistenc...",
      "word_count": 222,
      "source_page": 319,
      "vector_index": 511,
      "start_position": 115961,
      "end_position": 116182
    },
    {
      "chunk_id": 513,
      "text": "It is important to note that there is no single “bad property” among the three\nproperties For every pair of the the three axioms, there exist natural clustering\nfunctions that satisfy the two properti...",
      "word_count": 154,
      "source_page": 319,
      "vector_index": 512,
      "start_position": 116128,
      "end_position": 116281
    },
    {
      "chunk_id": 514,
      "text": "320\nClustering\nAlternatively, one can relax the Consistency property For example, say that two\nclusterings C = (C1, Ck) and C′ = (C′\n1, C′\nl) are compatible if for every\nclusters Ci ∈C and C′\nj ∈C′, e...",
      "word_count": 236,
      "source_page": 320,
      "vector_index": 513,
      "start_position": 116282,
      "end_position": 116517
    },
    {
      "chunk_id": 515,
      "text": "There is no generic clustering solution, just as there is no clas-\nsiﬁcation algorithm that will learn every learnable task (as the No-Free-Lunch\ntheorem shows) Clustering, just like classiﬁcation pre...",
      "word_count": 186,
      "source_page": 320,
      "vector_index": 514,
      "start_position": 116475,
      "end_position": 116660
    },
    {
      "chunk_id": 516,
      "text": "22.8 Exercises\n321\n(might) ﬁnd a solution whose k-means objective is at least t · OPT, where\nOPT is the minimum k-means objective 2 k-Means Might Not Necessarily Converge to a Local Minimum:\nShow that...",
      "word_count": 248,
      "source_page": 321,
      "vector_index": 515,
      "start_position": 116661,
      "end_position": 116908
    },
    {
      "chunk_id": 517,
      "text": "322\nClustering\nProve that for every k > 1 the k-diam clustering function deﬁned in the\nprevious exercise is not a center-based clustering function Hint: Given a clustering input (X, d), with |X| > 2, ...",
      "word_count": 218,
      "source_page": 322,
      "vector_index": 516,
      "start_position": 117036,
      "end_position": 117253
    },
    {
      "chunk_id": 518,
      "text": "23\nDimensionality Reduction\nDimensionality reduction is the process of taking data in a high dimensional\nspace and mapping it into a new space whose dimensionality is much smaller This process is clos...",
      "word_count": 229,
      "source_page": 323,
      "vector_index": 517,
      "start_position": 117254,
      "end_position": 117482
    },
    {
      "chunk_id": 519,
      "text": "It is not hard to show that in\ngeneral, exact recovery of x from Wx is impossible (see Exercise 1) The ﬁrst method we describe is called Principal Component Analysis (PCA) In PCA, both the compression...",
      "word_count": 199,
      "source_page": 323,
      "vector_index": 518,
      "start_position": 117452,
      "end_position": 117650
    },
    {
      "chunk_id": 520,
      "text": "324\nDimensionality Reduction\n23.1\nPrincipal Component Analysis (PCA)\nLet x1, , xm be m vectors in Rd We would like to reduce the dimensional-\nity of these vectors using a linear transformation A matri...",
      "word_count": 239,
      "source_page": 324,
      "vector_index": 519,
      "start_position": 117651,
      "end_position": 117889
    },
    {
      "chunk_id": 521,
      "text": "Proof\nFix any U, W and consider the mapping x 7→UWx The range of this\nmapping, R = {UWx : x ∈Rd}, is an n dimensional linear subspace of Rd Let\nV ∈Rd,n be a matrix whose columns form an orthonormal ba...",
      "word_count": 236,
      "source_page": 324,
      "vector_index": 520,
      "start_position": 117860,
      "end_position": 118095
    },
    {
      "chunk_id": 522,
      "text": "23.1 Principal Component Analysis (PCA)\n325\nWe further simplify the optimization problem by using the following elementary\nalgebraic manipulations For every x ∈Rd and a matrix U ∈Rd,n such that\nU ⊤U =...",
      "word_count": 247,
      "source_page": 325,
      "vector_index": 521,
      "start_position": 118096,
      "end_position": 118342
    },
    {
      "chunk_id": 523,
      "text": ", xm be arbitrary vectors in Rd, let A = Pm\ni=1 xix⊤\ni ,\nand let u1, , un be n eigenvectors of the matrix A corresponding to the largest\nn eigenvalues of A Then, the solution to the PCA optimization p...",
      "word_count": 216,
      "source_page": 325,
      "vector_index": 522,
      "start_position": 118308,
      "end_position": 118523
    },
    {
      "chunk_id": 524,
      "text": "326\nDimensionality Reduction\nIt is not hard to verify (see Exercise 2) that the right-hand side equals to\nPn\nj=1 Dj,j We have therefore shown that for every matrix U ∈Rd,n with or-\nthonormal columns i...",
      "word_count": 238,
      "source_page": 326,
      "vector_index": 523,
      "start_position": 118524,
      "end_position": 118761
    },
    {
      "chunk_id": 525,
      "text": "23.1.1\nA More Eﬃcient Solution for the Case d ≫m\nIn some situations the original dimensionality of the data is much larger than\nthe number of examples m The computational complexity of calculating the...",
      "word_count": 237,
      "source_page": 326,
      "vector_index": 524,
      "start_position": 118709,
      "end_position": 118945
    },
    {
      "chunk_id": 526,
      "text": "23.1 Principal Component Analysis (PCA)\n327\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\nFigure 23.1 A set of vectors in R2 (blue x’s) and their reconstruction after\ndimensionality reduction to R...",
      "word_count": 241,
      "source_page": 327,
      "vector_index": 525,
      "start_position": 118983,
      "end_position": 119223
    },
    {
      "chunk_id": 527,
      "text": "328\nDimensionality Reduction\nx\nx\nx\nx x\nx\nx\no\no\no\no o\no\no\n*\n*\n*\n***\n*\n+ +\n+ + +\n+ +\nFigure 23.2 Images of faces extracted from the Yale data set Top-Left: the original\nimages in R50x50 Top-Right: the i...",
      "word_count": 165,
      "source_page": 328,
      "vector_index": 526,
      "start_position": 119266,
      "end_position": 119430
    },
    {
      "chunk_id": 528,
      "text": "23.2 Random Projections\n329\n23.2\nRandom Projections\nIn this section we show that reducing the dimension by using a random linear\ntransformation leads to a simple compression scheme with a surprisingly...",
      "word_count": 239,
      "source_page": 329,
      "vector_index": 527,
      "start_position": 119431,
      "end_position": 119669
    },
    {
      "chunk_id": 529,
      "text": "Then, for every ϵ ∈(0, 3)\nwe have\nP\n\" \f\f\f\f\f\n∥(1/√n)Wx∥2\n∥x∥2\n−1\n\f\f\f\f\f > ϵ\n#\n≤2 e−ϵ2n/6 Proof\nWithout loss of generality we can assume that ∥x∥2 = 1 Therefore, an\nequivalent inequality is\nP\n\u0002\n(1 −ϵ)n ≤...",
      "word_count": 168,
      "source_page": 329,
      "vector_index": 528,
      "start_position": 119640,
      "end_position": 119807
    },
    {
      "chunk_id": 530,
      "text": "330\nDimensionality Reduction\nThen, with probability of at least 1−δ over a choice of a random matrix W ∈Rn,d\nsuch that each element of W is distributed normally with zero mean and variance\nof 1/n we h...",
      "word_count": 241,
      "source_page": 330,
      "vector_index": 529,
      "start_position": 119808,
      "end_position": 120048
    },
    {
      "chunk_id": 531,
      "text": "Now, lets take one step forward and assume that x = Uα,\nwhere α is a sparse vector, ∥α∥0 ≤s, and U is a ﬁxed orthonormal matrix That\nis, x has a sparse representation in another basis It turns out tha...",
      "word_count": 183,
      "source_page": 330,
      "vector_index": 530,
      "start_position": 120012,
      "end_position": 120194
    },
    {
      "chunk_id": 532,
      "text": "23.3 Compressed Sensing\n331\nCompressed sensing is a technique that simultaneously acquires and com-\npresses the data The key result is that a random linear transformation can\ncompress x without losing...",
      "word_count": 221,
      "source_page": 331,
      "vector_index": 531,
      "start_position": 120195,
      "end_position": 120415
    },
    {
      "chunk_id": 533,
      "text": "Another important application of compressed sensing is medical\nimaging, in which requiring fewer measurements translates to less radiation for\nthe patient Informally, the main premise of compressed se...",
      "word_count": 224,
      "source_page": 331,
      "vector_index": 532,
      "start_position": 120380,
      "end_position": 120603
    },
    {
      "chunk_id": 534,
      "text": "332\nDimensionality Reduction\nProof\nWe assume, by way of contradiction, that ˜x ̸= x Since x satisﬁes the\nconstraints in the optimization problem that deﬁnes ˜x we clearly have that\n∥˜x∥0 ≤∥x∥0 ≤s Ther...",
      "word_count": 245,
      "source_page": 332,
      "vector_index": 533,
      "start_position": 120604,
      "end_position": 120848
    },
    {
      "chunk_id": 535,
      "text": "That is, xs is the vector which equals x on the s largest elements of x and equals\n0 elsewhere Let y = Wx be the compression of x and let\nx⋆∈argmin\nv:W v=y\n∥v∥1\nbe the reconstructed vector Then,\n∥x⋆−x...",
      "word_count": 193,
      "source_page": 332,
      "vector_index": 534,
      "start_position": 120810,
      "end_position": 121002
    },
    {
      "chunk_id": 536,
      "text": "23.3 Compressed Sensing\n333\ntheorem 23.9\nLet U be an arbitrary ﬁxed d × d orthonormal matrix, let ϵ, δ\nbe scalars in (0, 1), let s be an integer in [d], and let n be an integer that satisﬁes\nn ≥100 s ...",
      "word_count": 249,
      "source_page": 333,
      "vector_index": 535,
      "start_position": 121003,
      "end_position": 121251
    },
    {
      "chunk_id": 537,
      "text": "Next, T1 will be the s indices corresponding to the s largest elements in absolute\nvalue of hT c\n0 Let T0,1 = T0 ∪T1 and T c\n0,1 = [d]\\T0,1 Next, T2 will correspond to\nthe s largest elements in absolu...",
      "word_count": 217,
      "source_page": 333,
      "vector_index": 536,
      "start_position": 121221,
      "end_position": 121437
    },
    {
      "chunk_id": 538,
      "text": "334\nDimensionality Reduction\nCombining these two claims with Equation (23.5) we get that\n∥h∥2 ≤∥hT0,1∥2 + ∥hT c\n0,1∥2 ≤2∥hT0,1∥2 + 2s−1/2∥x −xs∥1\n≤2\n\u0010\n2ρ\n1−ρ + 1\n\u0011\ns−1/2∥x −xs∥1\n= 21 + ρ\n1 −ρ s−1/2∥x ...",
      "word_count": 231,
      "source_page": 334,
      "vector_index": 537,
      "start_position": 121438,
      "end_position": 121668
    },
    {
      "chunk_id": 539,
      "text": "Thus, using the triangle inequality we\nobtain that\n∥x∥1 ≥∥x+h∥1 =\nX\ni∈T0\n|xi+hi|+\nX\ni∈T c\n0\n|xi+hi| ≥∥xT0∥1−∥hT0∥1+∥hT c\n0 ∥1−∥xT c\n0 ∥1\n(23.7)\nand since ∥xT c\n0 ∥1 = ∥x −xs∥1 = ∥x∥1 −∥xT0∥1 we get th...",
      "word_count": 158,
      "source_page": 334,
      "vector_index": 538,
      "start_position": 121584,
      "end_position": 121741
    },
    {
      "chunk_id": 540,
      "text": "23.3 Compressed Sensing\n335\nSince ∥hT0∥2 + ∥hT1∥2 ≤\n√\n2∥hT0,1∥2 we therefore get that\n∥WhT0,1∥2\n2 ≤\n√\n2ϵ∥hT0,1∥2\nX\nj≥2\n∥hTj∥2 Combining this with Equation (23.6) and Equation (23.9) we obtain\n(1 −ϵ)∥h...",
      "word_count": 222,
      "source_page": 335,
      "vector_index": 539,
      "start_position": 121742,
      "end_position": 121963
    },
    {
      "chunk_id": 541,
      "text": "336\nDimensionality Reduction\nwhere in the last inequality we used Stirling’s approximation Overall we obtained\nthat\n|Q| ≤(2k + 1)d (π/e)d/2 (d/2)−d/2 2−d (23.10)\nNow lets specify k For each x ∈B2(1) l...",
      "word_count": 219,
      "source_page": 336,
      "vector_index": 540,
      "start_position": 122034,
      "end_position": 122252
    },
    {
      "chunk_id": 542,
      "text": "Let S be the span of {Ui : i ∈I}, where Ui is the ith\ncolumn of U Let δ ∈(0, 1), ϵ ∈(0, 1), and n ∈N such that\nn ≥24 log(2/δ) + s log(12/ϵ)\nϵ2 Then, with probability of at least 1−δ over a choice of a...",
      "word_count": 167,
      "source_page": 336,
      "vector_index": 541,
      "start_position": 122216,
      "end_position": 122382
    },
    {
      "chunk_id": 543,
      "text": "23.3 Compressed Sensing\n337\nthe condition given in the lemma, the following holds with probability of at least\n1 −δ:\nsup\nv∈Q\n\f\f\f\f\n∥WUIv∥2\n∥UIv∥2\n−1\n\f\f\f\f ≤ϵ/2,\nThis also implies that\nsup\nv∈Q\n\f\f\f\f\n∥WUIv...",
      "word_count": 205,
      "source_page": 337,
      "vector_index": 542,
      "start_position": 122383,
      "end_position": 122587
    },
    {
      "chunk_id": 544,
      "text": "338\nDimensionality Reduction\n23.4\nPCA or Compressed Sensing Suppose we would like to apply a dimensionality reduction technique to a given\nset of examples Which method should we use, PCA or compressed...",
      "word_count": 239,
      "source_page": 338,
      "vector_index": 543,
      "start_position": 122588,
      "end_position": 122826
    },
    {
      "chunk_id": 545,
      "text": "On the other hand, PCA\nwill lead to poor performance, since the data is far from being in an n dimensional\nsubspace, as long as n < d Indeed, it is easy ro verify that in such a case, the\naveraged rec...",
      "word_count": 244,
      "source_page": 338,
      "vector_index": 544,
      "start_position": 122761,
      "end_position": 123004
    },
    {
      "chunk_id": 546,
      "text": "23.6 Bibliographic Remarks\n339\n23.6\nBibliographic Remarks\nPCA is equivalent to best subspace approximation using singular value decom-\nposition (SVD) The SVD method is described in Appendix C SVD date...",
      "word_count": 241,
      "source_page": 339,
      "vector_index": 545,
      "start_position": 123005,
      "end_position": 123245
    },
    {
      "chunk_id": 547,
      "text": "Let i be the minimal\nindex for which βi < 1 If i = n+1 we are done Otherwise, show that we can\nincrease βi, while possibly decreasing βj for some j > i, and obtain a better\nsolution This will imply th...",
      "word_count": 236,
      "source_page": 339,
      "vector_index": 546,
      "start_position": 123228,
      "end_position": 123463
    },
    {
      "chunk_id": 548,
      "text": "340\nDimensionality Reduction\nif an eigenvalue decomposition of some matrix C is required, verify that this\ndecomposition can be computed 4 An Interpretation of PCA as Variance Maximization:\nLet x1, , ...",
      "word_count": 244,
      "source_page": 340,
      "vector_index": 547,
      "start_position": 123464,
      "end_position": 123707
    },
    {
      "chunk_id": 549,
      "text": "Since w is an eigenvector of A we have that the\nconstraint E[(⟨w1, x⟩)(⟨w, x⟩)] = 0 is equivalent to the constraint\n⟨w1, w⟩= 0 5 The Relation between SVD and PCA:\nUse the SVD theorem (Corol-\nlary C.6)...",
      "word_count": 160,
      "source_page": 340,
      "vector_index": 548,
      "start_position": 123682,
      "end_position": 123841
    },
    {
      "chunk_id": 550,
      "text": "24\nGenerative Models\nWe started this book with a distribution free learning framework; namely, we\ndid not impose any assumptions on the underlying distribution over the data Furthermore, we followed a...",
      "word_count": 236,
      "source_page": 342,
      "vector_index": 549,
      "start_position": 123960,
      "end_position": 124195
    },
    {
      "chunk_id": 551,
      "text": "However,\nin some situations, it is reasonable to adopt the generative learning approach For example, sometimes it is easier (computationally) to estimate the parameters\nof the model than to learn a di...",
      "word_count": 166,
      "source_page": 342,
      "vector_index": 550,
      "start_position": 124163,
      "end_position": 124328
    },
    {
      "chunk_id": 552,
      "text": "24.1 Maximum Likelihood Estimator\n343\n24.1\nMaximum Likelihood Estimator\nLet us start with a simple example A drug company developed a new drug to\ntreat some deadly disease We would like to estimate th...",
      "word_count": 250,
      "source_page": 343,
      "vector_index": 551,
      "start_position": 124329,
      "end_position": 124578
    },
    {
      "chunk_id": 553,
      "text": "344\nGenerative Models\n24.1.1\nMaximum Likelihood Estimation for Continuous Random Variables\nLet X be a continuous random variable Then, for most x ∈R we have P[X =\nx] = 0 and therefore the deﬁnition of...",
      "word_count": 207,
      "source_page": 344,
      "vector_index": 552,
      "start_position": 124695,
      "end_position": 124901
    },
    {
      "chunk_id": 554,
      "text": "24.1 Maximum Likelihood Estimator\n345\n24.1.2\nMaximum Likelihood and Empirical Risk Minimization\nThe maximum likelihood estimator shares some similarity with the Empirical\nRisk Minimization (ERM) princ...",
      "word_count": 178,
      "source_page": 345,
      "vector_index": 553,
      "start_position": 125041,
      "end_position": 125218
    },
    {
      "chunk_id": 555,
      "text": "On the basis of this deﬁnition it is immediate that the maximum\nlikelihood principle is equivalent to minimizing the empirical risk with respect\nto the loss function given in Equation (24.4) That is,\n...",
      "word_count": 222,
      "source_page": 345,
      "vector_index": 554,
      "start_position": 125173,
      "end_position": 125394
    },
    {
      "chunk_id": 556,
      "text": "346\nGenerative Models\nTo answer this question we need to deﬁne how we assess the quality of an approxi-\nmated solution of the density estimation problem Unlike discriminative learning,\nwhere there is ...",
      "word_count": 239,
      "source_page": 346,
      "vector_index": 555,
      "start_position": 125443,
      "end_position": 125681
    },
    {
      "chunk_id": 557,
      "text": "From this fact we\ncan derive bounds of the form: with probability of at least 1 −δ we have that\n|ˆµ −µ⋆| ≤ϵ where ϵ depends on σ⋆/m and on δ In some situations, the maximum likelihood estimator clearl...",
      "word_count": 225,
      "source_page": 346,
      "vector_index": 556,
      "start_position": 125642,
      "end_position": 125866
    },
    {
      "chunk_id": 558,
      "text": "24.2 Naive Bayes\n347\nviously in the book A simple regularization technique is outlined in Exercise\n2 24.2\nNaive Bayes\nThe Naive Bayes classiﬁer is a classical demonstration of how generative as-\nsumpt...",
      "word_count": 241,
      "source_page": 347,
      "vector_index": 557,
      "start_position": 125867,
      "end_position": 126107
    },
    {
      "chunk_id": 559,
      "text": "348\nGenerative Models\nvector of features x = (x1, , xd) But now the generative assumption is as\nfollows First, we assume that P[Y = 1] = P[Y = 0] = 1/2 Second, we assume\nthat the conditional probabili...",
      "word_count": 228,
      "source_page": 348,
      "vector_index": 558,
      "start_position": 126187,
      "end_position": 126414
    },
    {
      "chunk_id": 560,
      "text": "In our case, the log-likelihood ratio becomes\n1\n2(x −µ0)T Σ−1(x −µ0) −1\n2(x −µ1)T Σ−1(x −µ1)\nWe can rewrite this as ⟨w, x⟩+ b where\nw = (µ1 −µ0)T Σ−1\nand\nb = 1\n2\n\u0000µT\n0 Σ−1µ0 −µT\n1 Σ−1µ1\n\u0001 (24.8)\nAs a ...",
      "word_count": 239,
      "source_page": 348,
      "vector_index": 559,
      "start_position": 126347,
      "end_position": 126585
    },
    {
      "chunk_id": 561,
      "text": "24.4 Latent Variables and the EM Algorithm\n349\nTherefore, the density of X can be written as:\nP[X = x] =\nk\nX\ny=1\nP[Y = y]P[X = x|Y = y]\n=\nk\nX\ny=1\ncy\n1\n(2π)d/2|Σy|1/2 exp\n\u0012\n−1\n2(x −µy)T Σ−1\ny (x −µy)\n\u0013...",
      "word_count": 241,
      "source_page": 349,
      "vector_index": 560,
      "start_position": 126586,
      "end_position": 126826
    },
    {
      "chunk_id": 562,
      "text": "350\nGenerative Models\nIf each row of Q deﬁnes a probability over the ith latent variable given X = xi,\nthen we can interpret F(Q, θ) as the expected log-likelihood of a training set\n(x1, y1), , (xm, y...",
      "word_count": 224,
      "source_page": 350,
      "vector_index": 561,
      "start_position": 126929,
      "end_position": 127152
    },
    {
      "chunk_id": 563,
      "text": "where at iteration t, we construct (Q(t+1), θ(t+1)) by performing two steps • Expectation Step: Set\nQ(t+1)\ni,y\n= Pθ(t)[Y = y|X = xi] (24.10)\nThis step is called the Expectation step, because it yields...",
      "word_count": 159,
      "source_page": 350,
      "vector_index": 562,
      "start_position": 127129,
      "end_position": 127287
    },
    {
      "chunk_id": 564,
      "text": "24.4 Latent Variables and the EM Algorithm\n351\nThe second term is the sum of the entropies of the rows of Q Let\nQ =\n(\nQ ∈[0, 1]m,k : ∀i,\nk\nX\ny=1\nQi,y = 1\n)\nbe the set of matrices whose rows deﬁne prob...",
      "word_count": 201,
      "source_page": 351,
      "vector_index": 563,
      "start_position": 127288,
      "end_position": 127488
    },
    {
      "chunk_id": 565,
      "text": "352\nGenerative Models\nwhile for Qi,y = Pθ[Y = y|X = xi] we have\nG(Q, θ) =\nm\nX\ni=1\n k\nX\ny=1\nPθ[Y = y|X = xi] log\n\u0012Pθ[X = xi, Y = y]\nPθ[Y = y|X = xi]\n\u0013 =\nm\nX\ni=1\nk\nX\ny=1\nPθ[Y = y|X = xi] log (Pθ[X = xi]...",
      "word_count": 211,
      "source_page": 352,
      "vector_index": 564,
      "start_position": 127489,
      "end_position": 127699
    },
    {
      "chunk_id": 566,
      "text": "24.5 Bayesian Reasoning\n353\nwhich in our case amounts to maximizing the following expression w.r.t c\nand µ:\nm\nX\ni=1\nk\nX\ny=1\nPθ(t)[Y = y|X = xi]\n\u0012\nlog(cy) −1\n2∥xi −µy∥2\n\u0013 (24.13)\nComparing the derivati...",
      "word_count": 248,
      "source_page": 353,
      "vector_index": 565,
      "start_position": 127789,
      "end_position": 128036
    },
    {
      "chunk_id": 567,
      "text": "Then, we update the centers on the basis of a weighted sum over\nthe entire sample For this reason, the EM approach for k-means is sometimes\ncalled “soft k-means.”\n24.5\nBayesian Reasoning\nThe maximum l...",
      "word_count": 203,
      "source_page": 353,
      "vector_index": 566,
      "start_position": 127997,
      "end_position": 128199
    },
    {
      "chunk_id": 568,
      "text": "354\nGenerative Models\nAs before, given a speciﬁc value of θ, it is assumed that the conditional proba-\nbility, P[X = x|θ], is known In the drug company example, X takes values in\n{0, 1} and P[X = x|θ]...",
      "word_count": 245,
      "source_page": 354,
      "vector_index": 567,
      "start_position": 128200,
      "end_position": 128444
    },
    {
      "chunk_id": 569,
      "text": "A new point X and the previous\npoints in S are independent only conditioned on θ This is diﬀerent from the\nfrequentist philosophy in which θ is a parameter that we might not know, but\nsince it is just...",
      "word_count": 223,
      "source_page": 354,
      "vector_index": 568,
      "start_position": 128390,
      "end_position": 128612
    },
    {
      "chunk_id": 570,
      "text": "24.6 Summary\n355\nIt is interesting to note that when P[θ] is uniform we obtain that\nP[X = x|S] ∝\nZ\nθx+P\ni xi(1 −θ)1−x+P\ni(1−xi) dθ Solving the preceding integral (using integration by parts) we obtain...",
      "word_count": 237,
      "source_page": 355,
      "vector_index": 569,
      "start_position": 128613,
      "end_position": 128849
    },
    {
      "chunk_id": 571,
      "text": "24.6\nSummary\nIn the generative approach to machine learning we aim at modeling the distri-\nbution over the data In particular, in parametric density estimation we further\nassume that the underlying di...",
      "word_count": 162,
      "source_page": 355,
      "vector_index": 570,
      "start_position": 128798,
      "end_position": 128959
    },
    {
      "chunk_id": 572,
      "text": "356\nGenerative Models\n24.8\nExercises\n1 Prove that the maximum likelihood estimator of the variance of a Gaussian\nvariable is biased 2 Regularization for Maximum Likelihood: Consider the following regu...",
      "word_count": 242,
      "source_page": 356,
      "vector_index": 571,
      "start_position": 128960,
      "end_position": 129201
    },
    {
      "chunk_id": 573,
      "text": "25\nFeature Selection and Generation\nIn the beginning of the book, we discussed the abstract model of learning, in\nwhich the prior knowledge utilized by the learner is fully encoded by the choice\nof th...",
      "word_count": 250,
      "source_page": 357,
      "vector_index": 572,
      "start_position": 129202,
      "end_position": 129451
    },
    {
      "chunk_id": 574,
      "text": "It is important to\nunderstand that the way we encode real world objects as an instance space X is\nby itself prior knowledge about the problem Furthermore, even when we already have an instance space X...",
      "word_count": 248,
      "source_page": 357,
      "vector_index": 573,
      "start_position": 129385,
      "end_position": 129632
    },
    {
      "chunk_id": 575,
      "text": "358\nFeature Selection and Generation\nWe emphasize that while there are some common techniques for feature learn-\ning one may want to try, the No-Free-Lunch theorem implies that there is no ulti-\nmate ...",
      "word_count": 242,
      "source_page": 358,
      "vector_index": 574,
      "start_position": 129674,
      "end_position": 129915
    },
    {
      "chunk_id": 576,
      "text": "Intuitively, the ﬁrst feature should be preferred over the second feature\nas the target can be perfectly predicted based on the ﬁrst feature alone, while it\ncannot be perfectly predicted based on the ...",
      "word_count": 233,
      "source_page": 358,
      "vector_index": 575,
      "start_position": 129857,
      "end_position": 130089
    },
    {
      "chunk_id": 577,
      "text": "Furthermore, in applications\nsuch as medical diagnostics, obtaining each possible “feature” (e.g., test result)\ncan be costly; therefore, a predictor that uses only a small number of features\nis desir...",
      "word_count": 155,
      "source_page": 358,
      "vector_index": 576,
      "start_position": 130021,
      "end_position": 130175
    },
    {
      "chunk_id": 578,
      "text": "25.1 Feature Selection\n359\n25.1.1\nFilters\nMaybe the simplest approach for feature selection is the ﬁlter method, in which\nwe assess individual features, independently of other features, according to s...",
      "word_count": 235,
      "source_page": 359,
      "vector_index": 577,
      "start_position": 130176,
      "end_position": 130410
    },
    {
      "chunk_id": 579,
      "text": "The empirical squared loss of an\nERM linear predictor that uses only the jth feature would be\nmin\na,b∈R\n1\nm∥av + b −y∥2,\nwhere the meaning of adding a scalar b to a vector v is adding b to all coordin...",
      "word_count": 226,
      "source_page": 359,
      "vector_index": 578,
      "start_position": 130332,
      "end_position": 130557
    },
    {
      "chunk_id": 580,
      "text": "360\nFeature Selection and Generation\nIf Pearson’s coeﬃcient equals zero it means that the optimal linear function\nfrom v to y is the all-zeros function, which means that v alone is useless for\npredict...",
      "word_count": 241,
      "source_page": 360,
      "vector_index": 579,
      "start_position": 130638,
      "end_position": 130878
    },
    {
      "chunk_id": 581,
      "text": "All of these score functions\nsuﬀer from similar problems to the one illustrated previously We refer the reader\nto Guyon & Elisseeﬀ(2003) 25.1.2\nGreedy Selection Approaches\nGreedy selection is another ...",
      "word_count": 243,
      "source_page": 360,
      "vector_index": 580,
      "start_position": 130857,
      "end_position": 131099
    },
    {
      "chunk_id": 582,
      "text": "25.1 Feature Selection\n361\nThen, we update It = It−1 ∪{jt} We now describe a more eﬃcient implementation of the forward greedy selec-\ntion approach for linear regression which is called Orthogonal Mat...",
      "word_count": 227,
      "source_page": 361,
      "vector_index": 581,
      "start_position": 131129,
      "end_position": 131355
    },
    {
      "chunk_id": 583,
      "text": "Then,\nmin\nθ,α ∥Vt−1θ + αuj −y∥2\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, Vt−1θ −y⟩\n\u0003\n= min\nθ,α\n\u0002\n∥Vt−1θ −y∥2 + α2∥uj∥2 + 2α⟨uj, −y⟩\n\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θ...",
      "word_count": 168,
      "source_page": 361,
      "vector_index": 582,
      "start_position": 131277,
      "end_position": 131444
    },
    {
      "chunk_id": 584,
      "text": "362\nFeature Selection and Generation\nOrthogonal Matching Pursuit (OMP)\ninput:\ndata matrix X ∈Rm,d, labels vector y ∈Rm,\nbudget of features T\ninitialize: I1 = ∅\nfor t = 1, , T\nuse SVD to ﬁnd an orthono...",
      "word_count": 230,
      "source_page": 362,
      "vector_index": 583,
      "start_position": 131445,
      "end_position": 131674
    },
    {
      "chunk_id": 585,
      "text": "Therefore, for each j we need to solve an optimization problem\nover a single variable, which is a much easier task than optimizing over t An even simpler approach is to upper bound R(w) using a “simpl...",
      "word_count": 171,
      "source_page": 362,
      "vector_index": 584,
      "start_position": 131622,
      "end_position": 131792
    },
    {
      "chunk_id": 586,
      "text": "25.1 Feature Selection\n363\nselection procedure with respect to the function\nR(w) = log\n\n\nm\nX\ni=1\nexp\n\n−yi\nd\nX\nj=1\nwjhj(xi)\n\n\n\n (25.3)\nSee Exercise 3 Backward Elimination\nAnother popular greedy...",
      "word_count": 228,
      "source_page": 363,
      "vector_index": 585,
      "start_position": 131793,
      "end_position": 132020
    },
    {
      "chunk_id": 587,
      "text": "In other words, we want w to be sparse, which implies that we only need to\nmeasure the features corresponding to nonzero elements of w Solving this optimization problem is computationally hard (Natara...",
      "word_count": 179,
      "source_page": 363,
      "vector_index": 586,
      "start_position": 131982,
      "end_position": 132160
    },
    {
      "chunk_id": 588,
      "text": "364\nFeature Selection and Generation\nEquation (25.4) and Equation (25.5) lead to the same solution, the two problems\nare in some sense equivalent The ℓ1 regularization often induces sparse solutions T...",
      "word_count": 244,
      "source_page": 364,
      "vector_index": 587,
      "start_position": 132161,
      "end_position": 132404
    },
    {
      "chunk_id": 589,
      "text": "25.2 Feature Manipulation and Normalization\n365\nAdding ℓ1 regularization to a linear regression problem with the squared loss\nyields the LASSO algorithm, deﬁned as\nargmin\nw\n\u0012 1\n2m∥Xw −y∥2 + λ∥w∥1\n\u0013 (2...",
      "word_count": 249,
      "source_page": 365,
      "vector_index": 588,
      "start_position": 132463,
      "end_position": 132711
    },
    {
      "chunk_id": 590,
      "text": "Recall that ridge regression returns\nthe vector\nargmin\nw\n\u0014 1\nm∥Xw −y∥2 + λ∥w∥2\n\u0015\n= (2λmI + X⊤X)−1X⊤y Suppose that d = 2 and the underlying data distribution is as follows First we\nsample y uniformly a...",
      "word_count": 239,
      "source_page": 365,
      "vector_index": 589,
      "start_position": 132679,
      "end_position": 132917
    },
    {
      "chunk_id": 591,
      "text": "366\nFeature Selection and Generation\nwe will obtain that x1 = y+0.5α\n1.5\nand x2 = y Then, for λ ≤10−3 the solution of\nridge regression is quite close to w⋆ Moreover, the generalization bounds we have ...",
      "word_count": 216,
      "source_page": 366,
      "vector_index": 590,
      "start_position": 132918,
      "end_position": 133133
    },
    {
      "chunk_id": 592,
      "text": "Next, we demonstrate in the following how a simple transformation on features,\nsuch as clipping, can sometime decrease the approximation error of our hypoth-\nesis class Consider again linear regressio...",
      "word_count": 243,
      "source_page": 366,
      "vector_index": 591,
      "start_position": 133100,
      "end_position": 133342
    },
    {
      "chunk_id": 593,
      "text": "This simple ex-\nample shows that a simple transformation can have a signiﬁcant inﬂuence on the\napproximation error Of course, it is not hard to think of examples in which the same feature trans-\nforma...",
      "word_count": 153,
      "source_page": 366,
      "vector_index": 592,
      "start_position": 133300,
      "end_position": 133452
    },
    {
      "chunk_id": 594,
      "text": "25.2 Feature Manipulation and Normalization\n367\nshould rely on our prior assumptions on the problem In the aforementioned ex-\nample, a prior assumption that may lead us to use the “clipping” transform...",
      "word_count": 246,
      "source_page": 367,
      "vector_index": 593,
      "start_position": 133453,
      "end_position": 133698
    },
    {
      "chunk_id": 595,
      "text": "368\nFeature Selection and Generation\nLogarithmic Transformation:\nThe transformation is fi ←log(b+fi), where b is a user-speciﬁed parameter This\nis widely used when the feature is a “counting” feature ...",
      "word_count": 231,
      "source_page": 368,
      "vector_index": 594,
      "start_position": 133791,
      "end_position": 134021
    },
    {
      "chunk_id": 596,
      "text": "Then, we select\na subset of features (feature selection) or transform individual features (feature\ntransformation) In this section we describe feature learning, in which we start\nwith some instance sp...",
      "word_count": 233,
      "source_page": 368,
      "vector_index": 595,
      "start_position": 133968,
      "end_position": 134200
    },
    {
      "chunk_id": 597,
      "text": "25.3 Feature Learning\n369\nand given a document, (p1, , pd), where each pi is a word in the document,\nwe represent the document as a vector x ∈{0, 1}k, where xi is 1 if wi = pj for\nsome j ∈[d], and xi ...",
      "word_count": 228,
      "source_page": 369,
      "vector_index": 596,
      "start_position": 134245,
      "end_position": 134472
    },
    {
      "chunk_id": 598,
      "text": "Applying a linear predictor on the pixel-based rep-\nresentation of the image does not yield a good classiﬁer What we would like\nto have is a mapping ψ that would take the pixel-based representation of...",
      "word_count": 229,
      "source_page": 369,
      "vector_index": 597,
      "start_position": 134422,
      "end_position": 134650
    },
    {
      "chunk_id": 599,
      "text": "Then, we can think of the clusters as\n“words,” and of instances as “documents,” where a document x is mapped to\nthe vector ψ(x) ∈{0, 1}k, where ψ(x)i is 1 if and only if x belongs to the ith\ncluster N...",
      "word_count": 248,
      "source_page": 369,
      "vector_index": 598,
      "start_position": 134581,
      "end_position": 134828
    },
    {
      "chunk_id": 600,
      "text": "370\nFeature Selection and Generation\nin {0, 1}k that indicates the closest centroid to x, while φ takes as input an\nindicator vector and returns the centroid representing this vector An important prop...",
      "word_count": 227,
      "source_page": 370,
      "vector_index": 599,
      "start_position": 134829,
      "end_position": 135055
    },
    {
      "chunk_id": 601,
      "text": "Note that when s = 1 and we further restrict ∥v∥1 =\n1 then we obtain the k-means encoding function; that is, ψ(x) is the indicator\nvector of the centroid closest to x For larger values of s, the optim...",
      "word_count": 235,
      "source_page": 370,
      "vector_index": 600,
      "start_position": 135006,
      "end_position": 135240
    },
    {
      "chunk_id": 602,
      "text": "25.5 Bibliographic Remarks\n371\n25.5\nBibliographic Remarks\nGuyon & Elisseeﬀ(2003) surveyed several feature selection procedures, including\nmany types of ﬁlters Forward greedy selection procedures for m...",
      "word_count": 250,
      "source_page": 371,
      "vector_index": 601,
      "start_position": 135241,
      "end_position": 135490
    },
    {
      "chunk_id": 603,
      "text": "25.6\nExercises\n1 Prove the equality given in Equation (25.1) Hint: Let a∗, b∗be minimizers of\nthe left-hand side Find a, b such that the objective value of the right-hand\nside is smaller than that of ...",
      "word_count": 159,
      "source_page": 371,
      "vector_index": 602,
      "start_position": 135481,
      "end_position": 135639
    },
    {
      "chunk_id": 604,
      "text": "26\nRademacher Complexities\nIn Chapter 4 we have shown that uniform convergence is a suﬃcient condition\nfor learnability In this chapter we study the Rademacher complexity, which\nmeasures the rate of u...",
      "word_count": 247,
      "source_page": 375,
      "vector_index": 603,
      "start_position": 135729,
      "end_position": 135975
    },
    {
      "chunk_id": 605,
      "text": "376\nRademacher Complexities\nThis can be written more compactly by deﬁning σ = (σ1, , σm) ∈{±1}m to\nbe a vector such that S1 = {zi : σi = 1} and S2 = {zi : σi = −1} Then, if we\nfurther assume that |S1|...",
      "word_count": 246,
      "source_page": 376,
      "vector_index": 604,
      "start_position": 136006,
      "end_position": 136251
    },
    {
      "chunk_id": 606,
      "text": "26.1 The Rademacher Complexity\n377\nNext, we note that for each j, zj and z′\nj are i.i.d variables Therefore, we can\nreplace them without aﬀecting the expectation:\nE\nS,S′\n\nsup\nf∈F\n\n(f(z′\nj) −f(zj))...",
      "word_count": 175,
      "source_page": 377,
      "vector_index": 605,
      "start_position": 136360,
      "end_position": 136534
    },
    {
      "chunk_id": 607,
      "text": "of Equation (26.7))\n=\nE\nS,S′\n\nsup\nf∈F\n\n(f(z′\nj) −f(zj)) +\nX\ni̸=j\n(f(z′\ni) −f(zi))\n\n\n\n (26.8)\nRepeating this for all j we obtain that\nE\nS,S′\n\"\nsup\nf∈F\nm\nX\ni=1\n(f(z′\ni) −f(zi))\n#\n=\nE\nS,S′,σ\n\"\nsu...",
      "word_count": 181,
      "source_page": 377,
      "vector_index": 606,
      "start_position": 136478,
      "end_position": 136658
    },
    {
      "chunk_id": 608,
      "text": "378\nRademacher Complexities\nFurthermore, if h⋆= argminh LD(h) then for each δ ∈(0, 1) with probability of\nat least 1 −δ over the choice of S we have\nLD(ERMH(S)) −LD(h⋆) ≤2 ES′∼Dm R(ℓ◦H ◦S′)\nδ Proof\nTh...",
      "word_count": 246,
      "source_page": 378,
      "vector_index": 607,
      "start_position": 136659,
      "end_position": 136904
    },
    {
      "chunk_id": 609,
      "text": "26.1 The Rademacher Complexity\n379\nProof\nFirst note that the random variable RepD(F, S) = suph∈H (LD(h) −LS(h))\nsatisﬁes the bounded diﬀerences condition of Lemma 26.4 with a constant 2c/m Combining t...",
      "word_count": 240,
      "source_page": 379,
      "vector_index": 608,
      "start_position": 136994,
      "end_position": 137233
    },
    {
      "chunk_id": 610,
      "text": "(26.11)\nCombining this with the union bound we conclude our proof The preceding theorem tells us that if the quantity R(ℓ◦H◦S) is small then it\nis possible to learn the class H using the ERM rule It i...",
      "word_count": 168,
      "source_page": 379,
      "vector_index": 609,
      "start_position": 137198,
      "end_position": 137365
    },
    {
      "chunk_id": 611,
      "text": "380\nRademacher Complexities\nlemma 26.7\nLet A be a subset of Rm and let A′ = {PN\nj=1 αja(j) : N ∈\nN, ∀j, a(j) ∈A, αj ≥0, ∥α∥1 = 1} Then, R(A′) = R(A) Proof\nThe main idea follows from the fact that for ...",
      "word_count": 242,
      "source_page": 380,
      "vector_index": 610,
      "start_position": 137366,
      "end_position": 137607
    },
    {
      "chunk_id": 612,
      "text": "26.1 The Rademacher Complexity\n381\nand therefore\nmR(A′) ≤log\n X\na∈A′\nm\nY\ni=1\nexp\n\u0012a2\ni\n2\n\u0013 = log\n X\na∈A′\nexp\n\u0000∥a∥2/2\n\u0001 ≤log\n\u0012\n|A′| max\na∈A′ exp\n\u0000∥a∥2/2\n\u0001\u0013\n= log(|A′|) + max\na∈A′(∥a∥2/2) Since R(A) = 1...",
      "word_count": 236,
      "source_page": 381,
      "vector_index": 611,
      "start_position": 137656,
      "end_position": 137891
    },
    {
      "chunk_id": 613,
      "text": "Clearly, it suﬃces to prove that\nfor any set A and all i we have R(Ai) ≤R(A) Without loss of generality we will\nprove the latter claim for i = 1 and to simplify notation we omit the subscript\nfrom φ1 ...",
      "word_count": 168,
      "source_page": 381,
      "vector_index": 612,
      "start_position": 137851,
      "end_position": 138018
    },
    {
      "chunk_id": 614,
      "text": "382\nRademacher Complexities\nbe omitted since both a and a′ are from the same set A and the rest of the\nexpression in the supremum is not aﬀected by replacing a and a′ Therefore,\nmR(A1) ≤1\n2\nE\nσ2,...,σ...",
      "word_count": 225,
      "source_page": 382,
      "vector_index": 613,
      "start_position": 138019,
      "end_position": 138243
    },
    {
      "chunk_id": 615,
      "text": "26.3 Generalization Bounds for SVM\n383\nFinally, since the variables σ1, , σm are independent we have\nE\nσ\n\"\n∥\nm\nX\ni=1\nσixi∥2\n2\n#\n= E\nσ\n\nX\ni,j\nσiσj⟨xi, xj⟩\n\n\n=\nX\ni̸=j\n⟨xi, xj⟩E\nσ [σiσj] +\nm\nX\ni=1\n⟨x...",
      "word_count": 231,
      "source_page": 383,
      "vector_index": 614,
      "start_position": 138342,
      "end_position": 138572
    },
    {
      "chunk_id": 616,
      "text": "384\nRademacher Complexities\nWe shall consider the following general constraint-based formulation Let H =\n{w : ∥w∥2 ≤B} be our hypothesis class, and let Z = X × Y be the examples\ndomain Assume that the...",
      "word_count": 234,
      "source_page": 384,
      "vector_index": 615,
      "start_position": 138610,
      "end_position": 138843
    },
    {
      "chunk_id": 617,
      "text": "theorem 26.12\nSuppose that D is a distribution over X × Y such that with\nprobability 1 we have that ∥x∥2 ≤R Let H = {w : ∥w∥2 ≤B} and let\nℓ: H × Z →R be a loss function of the form given in Equation (...",
      "word_count": 249,
      "source_page": 384,
      "vector_index": 616,
      "start_position": 138777,
      "end_position": 139025
    },
    {
      "chunk_id": 618,
      "text": "26.3 Generalization Bounds for SVM\n385\nProof\nThroughout the proof, let the loss function be the ramp loss (see Sec-\ntion 15.2.3) Note that the range of the ramp loss is [0, 1] and that it is a\n1-Lipsc...",
      "word_count": 248,
      "source_page": 385,
      "vector_index": 617,
      "start_position": 139062,
      "end_position": 139309
    },
    {
      "chunk_id": 619,
      "text": "theorem 26.14\nAssume that the conditions of Theorem 26.13 hold Then,\nwith probability of at least 1 −δ over the choice of S ∼Dm, we have that\nP\n(x,y)∼D[y ̸= sign(⟨wS, x⟩)] ≤4R∥wS∥\n√m\n+\ns\nln( 4 log2(∥w...",
      "word_count": 170,
      "source_page": 385,
      "vector_index": 618,
      "start_position": 139268,
      "end_position": 139437
    },
    {
      "chunk_id": 620,
      "text": "386\nRademacher Complexities\nRemark 26.2\nNote that all the bounds we have derived do not depend on the\ndimension of w This property is utilized when learning SVM with kernels, where\nthe dimension of w ...",
      "word_count": 231,
      "source_page": 386,
      "vector_index": 619,
      "start_position": 139438,
      "end_position": 139668
    },
    {
      "chunk_id": 621,
      "text": "Let H = {w ∈Rd : ∥w∥1 ≤B} and\nlet ℓ: H × Z →R be a loss function of the form given in Equation (26.18)\nsuch that for all y ∈Y, a 7→φ(a, y) is an ρ-Lipschitz function and such that\nmaxa∈[−BR,BR] |φ(a, ...",
      "word_count": 240,
      "source_page": 386,
      "vector_index": 620,
      "start_position": 139604,
      "end_position": 139843
    },
    {
      "chunk_id": 622,
      "text": "27\nCovering Numbers\nIn this chapter we describe another way to measure the complexity of sets, which\nis called covering numbers 27.1\nCovering\ndefinition 27.1 (Covering)\nLet A ⊂Rm be a set of vectors W...",
      "word_count": 240,
      "source_page": 388,
      "vector_index": 621,
      "start_position": 139935,
      "end_position": 140174
    },
    {
      "chunk_id": 623,
      "text": "27.2 From Covering to Rademacher Complexity via Chaining\n389\nNext, we derive a contraction principle lemma 27.3\nFor each i ∈[m], let φi : R →R be a ρ-Lipschitz function;\nnamely, for all α, β ∈R we hav...",
      "word_count": 240,
      "source_page": 389,
      "vector_index": 622,
      "start_position": 140233,
      "end_position": 140472
    },
    {
      "chunk_id": 624,
      "text": "390\nCovering Numbers\nWe can now write\nR(A) = 1\nm E⟨σ, a∗⟩\n= 1\nm E\n\"\n⟨σ, a∗−b(M)⟩+\nM\nX\nk=1\n⟨σ, b(k) −b(k−1)⟩\n#\n≤1\nm E\nh\n∥σ∥∥a∗−b(M)∥\ni\n+\nM\nX\nk=1\n1\nm E\n\"\nsup\na∈ˆ\nBk\n⟨σ, a⟩\n# Since ∥σ∥= √m and ∥a∗−b(M)∥≤...",
      "word_count": 239,
      "source_page": 390,
      "vector_index": 623,
      "start_position": 140584,
      "end_position": 140822
    },
    {
      "chunk_id": 625,
      "text": "28\nProof of the Fundamental Theorem\nof Learning Theory\nIn this chapter we prove Theorem 6.8 from Chapter 6 We remind the reader\nthe conditions of the theorem, which will hold throughout this chapter: ...",
      "word_count": 248,
      "source_page": 392,
      "vector_index": 624,
      "start_position": 140897,
      "end_position": 141144
    },
    {
      "chunk_id": 626,
      "text": "28.2 The Lower Bound for the Agnostic Case\n393\nCombining this with Lemma 26.8 we obtain the following bound on the Rademacher\ncomplexity:\nR(A) ≤\nr\n2d log(em/d)\nm Using Theorem 26.5 we obtain that with...",
      "word_count": 238,
      "source_page": 393,
      "vector_index": 625,
      "start_position": 141218,
      "end_position": 141455
    },
    {
      "chunk_id": 627,
      "text": "394\nProof of the Fundamental Theorem of Learning Theory\nthat there are h+, h−∈H for which h+(c) = 1 and h−(c) = −1 Deﬁne two\ndistributions, D+ and D−, such that for b ∈{±1} we have\nDb({(x, y)}) =\n( 1+...",
      "word_count": 248,
      "source_page": 394,
      "vector_index": 626,
      "start_position": 141517,
      "end_position": 141764
    },
    {
      "chunk_id": 628,
      "text": "28.2 The Lower Bound for the Agnostic Case\n395\nTherefore,\nmax\nb∈{±1} P [LDb(A(y)) −LDb(hb) = ϵ]\n= max\nb∈{±1}\nX\ny\nPb[y]1[A(y)̸=b]\n≥1\n2\nX\ny\nP+[y]1[A(y)̸=+] + 1\n2\nX\ny\nP−[y]1[A(y)̸=−]\n= 1\n2\nX\ny∈N+\n(P+[y]1...",
      "word_count": 240,
      "source_page": 395,
      "vector_index": 627,
      "start_position": 141826,
      "end_position": 142065
    },
    {
      "chunk_id": 629,
      "text": "396\nProof of the Fundamental Theorem of Learning Theory\nh ∈H such that h(ci) = bi for all i ∈[d], and its error is 1−ρ\n2 In addition, for\nany other function f : X →{±1}, it is easy to verify that\nLDb(...",
      "word_count": 247,
      "source_page": 396,
      "vector_index": 628,
      "start_position": 142164,
      "end_position": 142410
    },
    {
      "chunk_id": 630,
      "text": "Therefore, the right-hand side of Equation (28.6) equals\nρ\nd\nd\nX\ni=1\nE\nj∼U([d])m\nE\nb∼U({±1}d)\nE\n∀r,yr∼bjr\n1[A(S)(ci)̸=bi] (28.7)\nWe now proceed in two steps First, we show that among all learning algo...",
      "word_count": 153,
      "source_page": 396,
      "vector_index": 629,
      "start_position": 142384,
      "end_position": 142536
    },
    {
      "chunk_id": 631,
      "text": "28.2 The Lower Bound for the Agnostic Case\n397\ny ∈{±1}m, let yI denote the elements of y corresponding to indices for which\njr = i and let y¬I be the rest of the elements of y We have\nE\nb∼U({±1}d)\nE\n∀...",
      "word_count": 240,
      "source_page": 397,
      "vector_index": 630,
      "start_position": 142537,
      "end_position": 142776
    },
    {
      "chunk_id": 632,
      "text": "398\nProof of the Fundamental Theorem of Learning Theory\nAs long as m <\nd\n8ρ2 , this term would be larger than ρ/4 In summary, we have shown that if m <\nd\n8ρ2 then for any algorithm there\nexists a dist...",
      "word_count": 229,
      "source_page": 398,
      "vector_index": 631,
      "start_position": 142832,
      "end_position": 143060
    },
    {
      "chunk_id": 633,
      "text": "28.3 The Upper Bound for the Realizable Case\n399\nClaim 1\nP[S ∈B] ≤2 P[(S, T) ∈B′] Proof of Claim 1: Since S and T are chosen independently we can write\nP[(S, T) ∈B′] =\nE\n(S,T )∼D2m\n\u0002\n1[(S,T )∈B′]\n\u0003\n=\n...",
      "word_count": 248,
      "source_page": 399,
      "vector_index": 632,
      "start_position": 143166,
      "end_position": 143413
    },
    {
      "chunk_id": 634,
      "text": "Combining all the preceding we conclude the proof of Claim 1 Claim 2 (Symmetrization):\nP[(S, T) ∈B′] ≤e−ϵm/4 τH(2m) Proof of Claim 2: To simplify notation, let α = mϵ/2 and for a sequence A =\n(x1, , x...",
      "word_count": 163,
      "source_page": 399,
      "vector_index": 633,
      "start_position": 143395,
      "end_position": 143557
    },
    {
      "chunk_id": 635,
      "text": "400\nProof of the Fundamental Theorem of Learning Theory\nEA∼D2m[f(A, Aj)] Since this holds for any j it also holds for the expectation of\nj chosen at random from J In particular, it holds for the funct...",
      "word_count": 237,
      "source_page": 400,
      "vector_index": 634,
      "start_position": 143558,
      "end_position": 143794
    },
    {
      "chunk_id": 636,
      "text": "28.3 The Upper Bound for the Realizable Case\n401\n28.3.1\nFrom ϵ-Nets to PAC Learnability\ntheorem 28.4\nLet H be a hypothesis class over X with VCdim(H) = d Let\nD be a distribution over X and let c ∈H be...",
      "word_count": 198,
      "source_page": 401,
      "vector_index": 635,
      "start_position": 143850,
      "end_position": 144047
    },
    {
      "chunk_id": 637,
      "text": "29\nMulticlass Learnability\nIn Chapter 17 we have introduced the problem of multiclass categorization, in\nwhich the goal is to learn a predictor h : X →[k] In this chapter we address PAC\nlearnability o...",
      "word_count": 246,
      "source_page": 402,
      "vector_index": 636,
      "start_position": 144048,
      "end_position": 144293
    },
    {
      "chunk_id": 638,
      "text": "In Chapter 13, Exercise 2, we have shown\nthat this equivalence breaks down for a certain convex learning problem The\nlast section of this chapter is devoted to showing that the equivalence between\nlea...",
      "word_count": 161,
      "source_page": 402,
      "vector_index": 637,
      "start_position": 144240,
      "end_position": 144400
    },
    {
      "chunk_id": 639,
      "text": "29.2 The Multiclass Fundamental Theorem\n403\nTo deﬁne the Natarajan dimension, we ﬁrst generalize the deﬁnition of shat-\ntering definition 29.1 (Shattering (Multiclass Version))\nWe say that a set C ⊂X\n...",
      "word_count": 250,
      "source_page": 403,
      "vector_index": 638,
      "start_position": 144401,
      "end_position": 144650
    },
    {
      "chunk_id": 640,
      "text": "404\nMulticlass Learnability\nlemma 29.4 (Natarajan)\n|H| ≤|X|Ndim(H) · k2Ndim(H) The proof of Natarajan’s lemma shares the same spirit of the proof of Sauer’s\nlemma and is left as an exercise (see Exerc...",
      "word_count": 249,
      "source_page": 404,
      "vector_index": 639,
      "start_position": 144768,
      "end_position": 145016
    },
    {
      "chunk_id": 641,
      "text": "29.3 Calculating the Natarajan Dimension\n405\nBy Sauer’s lemma, | (Hbin)C | ≤|C|d We conclude that\n2|C| ≤\n\f\f\f\n\u0010\nHOvA,k\nbin\n\u0011\nC\n\f\f\f ≤|C|dk The proof follows by taking the logarithm and applying Lemma A....",
      "word_count": 249,
      "source_page": 405,
      "vector_index": 640,
      "start_position": 145086,
      "end_position": 145334
    },
    {
      "chunk_id": 642,
      "text": "406\nMulticlass Learnability\ntheorem 29.7\nNdim(HΨ) ≤d Proof\nLet C ⊂X be a shattered set, and let f0, f1 : C →[k] be the two\nfunctions that witness the shattering We need to show that |C| ≤d For every\nx...",
      "word_count": 250,
      "source_page": 406,
      "vector_index": 641,
      "start_position": 145415,
      "end_position": 145664
    },
    {
      "chunk_id": 643,
      "text": "Since this holds for every\nB ⊆C we obtain that |C| = |ρ(C)| and |Hρ(C)| = 2|C|, which concludes our\nproof The theorem is tight in the sense that there are mappings Ψ for which Ndim(HΨ) =\nΩ(d) For exam...",
      "word_count": 185,
      "source_page": 406,
      "vector_index": 642,
      "start_position": 145627,
      "end_position": 145811
    },
    {
      "chunk_id": 644,
      "text": "29.4 On Good and Bad ERMs\n407\nlearnable by some ERM, but other ERMs will fail to learn it Clearly, this also\nimplies that the class is learnable but it does not have the uniform convergence\nproperty F...",
      "word_count": 234,
      "source_page": 407,
      "vector_index": 643,
      "start_position": 145812,
      "end_position": 146045
    },
    {
      "chunk_id": 645,
      "text": "Therefore, to specify an ERM, we should only specify\nthe hypothesis it returns upon receiving a sample of the form\nS = {(x1, ∗), , (xm, ∗)} We consider two ERMs: The ﬁrst, Agood, is deﬁned by\nAgood(S)...",
      "word_count": 234,
      "source_page": 407,
      "vector_index": 644,
      "start_position": 146019,
      "end_position": 146252
    },
    {
      "chunk_id": 646,
      "text": "408\nMulticlass Learnability\nProof\nLet D be a distribution over X and suppose that the correct labeling\nis hA For any sample, Agood returns either h∅or hA If it returns hA then its\ntrue error is zero T...",
      "word_count": 240,
      "source_page": 408,
      "vector_index": 645,
      "start_position": 146253,
      "end_position": 146492
    },
    {
      "chunk_id": 647,
      "text": "By Chernoﬀ’s bound, if m ≤d−1\n6ϵ ,\nthen with probability ≥e−1\n6 , the sample will include no more than d−1\n2\nexamples\nfrom X Thus the returned hypothesis will have error ≥ϵ The conclusion of the examp...",
      "word_count": 225,
      "source_page": 408,
      "vector_index": 646,
      "start_position": 146459,
      "end_position": 146683
    },
    {
      "chunk_id": 648,
      "text": "29.6 Exercises\n409\n29.6\nExercises\n1 Let d, k > 0 Show that there exists a binary hypothesis Hbin of VC dimension\nd such that Ndim(HOvA,k\nbin\n) = d 2 Prove Lemma 29.6 3 Prove Natarajan’s lemma Hint: Fi...",
      "word_count": 209,
      "source_page": 409,
      "vector_index": 647,
      "start_position": 146684,
      "end_position": 146892
    },
    {
      "chunk_id": 649,
      "text": "30\nCompression Bounds\nThroughout the book, we have tried to characterize the notion of learnability\nusing diﬀerent approaches At ﬁrst we have shown that the uniform conver-\ngence property of a hypothe...",
      "word_count": 246,
      "source_page": 410,
      "vector_index": 648,
      "start_position": 147015,
      "end_position": 147260
    },
    {
      "chunk_id": 650,
      "text": "Since V and T are independent, we\nimmediately get the following from Bernstein’s inequality (see Lemma B.10) lemma 30.1\nAssume that the range of the loss function is [0, 1] Then,\nP\n\"\nLD(hT ) −LV (hT )...",
      "word_count": 187,
      "source_page": 410,
      "vector_index": 649,
      "start_position": 147231,
      "end_position": 147417
    },
    {
      "chunk_id": 651,
      "text": "30.1 Compression Bounds\n411\ntheorem 30.2\nLet k be an integer and let B : Zk →H be a mapping from\nsequences of k examples to the hypothesis class Let m ≥2k be a training set\nsize and let A : Zm →H be a...",
      "word_count": 235,
      "source_page": 411,
      "vector_index": 650,
      "start_position": 147418,
      "end_position": 147652
    },
    {
      "chunk_id": 652,
      "text": "Denote δ′ = mkδ Using the assumption k ≤m/2, which implies that n =\nm −k ≥m/2, the above implies that with probability of at least 1 −δ′ we have\nthat\nLD(A(S)) ≤LV (A(S)) +\nr\nLV (A(S))4k log(m/δ′)\nm\n+ ...",
      "word_count": 211,
      "source_page": 411,
      "vector_index": 651,
      "start_position": 147604,
      "end_position": 147814
    },
    {
      "chunk_id": 653,
      "text": "412\nCompression Bounds\ndefinition 30.5\n(Compression Scheme for Unrealizable Sequences)\nLet H be a hypothesis class of functions from X to Y and let k be an integer We say that H has a compression sche...",
      "word_count": 233,
      "source_page": 412,
      "vector_index": 652,
      "start_position": 147815,
      "end_position": 148047
    },
    {
      "chunk_id": 654,
      "text": "Now, apply the realizable\ncompression scheme on the examples that have not been removed The output of\nthe realizable compression scheme, denoted h′, must be correct on the examples\nthat have not been ...",
      "word_count": 213,
      "source_page": 412,
      "vector_index": 653,
      "start_position": 148014,
      "end_position": 148226
    },
    {
      "chunk_id": 655,
      "text": "30.2 Examples\n413\nA Compression Scheme:\nW.l.o.g assume all labels are positive (otherwise, replace xi by yixi) The com-\npression scheme we propose is as follows First, A ﬁnds the vector w which is\nin ...",
      "word_count": 248,
      "source_page": 413,
      "vector_index": 654,
      "start_position": 148227,
      "end_position": 148474
    },
    {
      "chunk_id": 656,
      "text": "Then w′ is also in the convex hull and\n∥w′∥2 = (1 −α)2∥w∥2 + α2∥xi∥2 + 2α(1 −α)⟨w, xi⟩\n≤(1 −α)2∥w∥2 + α2∥xi∥2\n= ∥xi∥4∥w∥2 + ∥xi∥2∥w∥4\n(∥w∥2 + ∥xi∥2)2\n=\n∥xi∥2∥w∥2\n∥w∥2 + ∥xi∥2\n= ∥w∥2 ·\n1\n∥w∥2/∥xi∥2 + 1...",
      "word_count": 225,
      "source_page": 413,
      "vector_index": 655,
      "start_position": 148416,
      "end_position": 148640
    },
    {
      "chunk_id": 657,
      "text": "414\nCompression Bounds\nNote that p(x) can be rewritten as ⟨w, ψ(x)⟩where the elements of ψ(x) are all\nthe monomials of x up to degree r Therefore, the problem of constructing a com-\npression scheme fo...",
      "word_count": 204,
      "source_page": 414,
      "vector_index": 656,
      "start_position": 148641,
      "end_position": 148844
    },
    {
      "chunk_id": 658,
      "text": "31\nPAC-Bayes\nThe Minimum Description Length (MDL) and Occam’s razor principles allow a\npotentially very large hypothesis class but deﬁne a hierarchy over hypotheses and\nprefer to choose hypotheses tha...",
      "word_count": 224,
      "source_page": 415,
      "vector_index": 657,
      "start_position": 148845,
      "end_position": 149068
    },
    {
      "chunk_id": 659,
      "text": "Whenever we get a new instance x, we randomly pick\na hypothesis h ∈H according to Q and predict h(x) We deﬁne the loss of Q on\nan example z to be\nℓ(Q, z)\ndef\n=\nE\nh∼Q[ℓ(h, z)] By the linearity of expec...",
      "word_count": 185,
      "source_page": 415,
      "vector_index": 658,
      "start_position": 149030,
      "end_position": 149214
    },
    {
      "chunk_id": 660,
      "text": "416\nPAC-Bayes\nlater show how in some cases this idea leads to the regularized risk minimization\nprinciple theorem 31.1\nLet D be an arbitrary distribution over an example domain Z Let H be a hypothesis...",
      "word_count": 249,
      "source_page": 416,
      "vector_index": 659,
      "start_position": 149215,
      "end_position": 149463
    },
    {
      "chunk_id": 661,
      "text": "31.2 Bibliographic Remarks\n417\nNext, we claim that for all h we have ES[e2(m−1)∆(h)2] ≤m To do so, recall that\nHoeﬀding’s inequality tells us that\nP\nS[∆(h) ≥ϵ] ≤e−2mϵ2 This implies that ES[e2(m−1)∆(h)...",
      "word_count": 231,
      "source_page": 417,
      "vector_index": 660,
      "start_position": 149506,
      "end_position": 149736
    },
    {
      "chunk_id": 662,
      "text": "Appendix A Technical Lemmas\nlemma A.1\nLet a > 0 Then: x ≥2a log(a) ⇒x ≥a log(x) It follows that a\nnecessary condition for the inequality x < a log(x) to hold is that x < 2a log(a) Proof\nFirst note tha...",
      "word_count": 249,
      "source_page": 419,
      "vector_index": 661,
      "start_position": 149815,
      "end_position": 150063
    },
    {
      "chunk_id": 663,
      "text": "lemma A.3\nLet X be a random variable and x′ ∈R be a scalar and assume\nthat there exists a > 0 such that for all t ≥0 we have P[|X −x′| > t] ≤2e−t2/a2 Then, E[|X −x′|] ≤4 a Proof\nFor all i = 0, 1, 2, d...",
      "word_count": 193,
      "source_page": 419,
      "vector_index": 662,
      "start_position": 150024,
      "end_position": 150216
    },
    {
      "chunk_id": 664,
      "text": "420\nTechnical Lemmas\nProof\nFor all i = 0, 1, 2, denote ti = a (i+\np\nlog(b)) Since ti is monotonically\nincreasing we have that\nE[|X −x′|] ≤a\np\nlog(b) +\n∞\nX\ni=1\nti P[|X −x′| > ti−1] Using the assumption...",
      "word_count": 250,
      "source_page": 420,
      "vector_index": 663,
      "start_position": 150217,
      "end_position": 150466
    },
    {
      "chunk_id": 665,
      "text": "Technical Lemmas\n421\nUsing Stirling’s approximation we further have that\n≤\n\u0010e m\nd\n\u0011d\n \n1 +\n\u0012d\ne\n\u0013d\n(m −d)\n(d + 1)\n√\n2πd(d/e)d =\n\u0010e m\nd\n\u0011d \u0012\n1 +\nm −d\n√\n2πd(d + 1)\n\u0013\n=\n\u0010e m\nd\n\u0011d\n· d + 1 + (m −d)/\n√\n2πd\n...",
      "word_count": 238,
      "source_page": 421,
      "vector_index": 664,
      "start_position": 150467,
      "end_position": 150704
    },
    {
      "chunk_id": 666,
      "text": "Appendix B Measure Concentration\nLet Z1, , Zm be an i.i.d sequence of random variables and let µ be their mean The strong law of large numbers states that when m tends to inﬁnity, the em-\npirical aver...",
      "word_count": 237,
      "source_page": 422,
      "vector_index": 665,
      "start_position": 150705,
      "end_position": 150941
    },
    {
      "chunk_id": 667,
      "text": "B.2 Chebyshev’s Inequality\n423\nTherefore,\nP[Z > 1 −a] ≥1 −1 −µ\na\n= a + µ −1\na B.2\nChebyshev’s Inequality\nApplying Markov’s inequality on the random variable (Z −E[Z])2 we obtain\nChebyshev’s inequality...",
      "word_count": 233,
      "source_page": 423,
      "vector_index": 666,
      "start_position": 150995,
      "end_position": 151227
    },
    {
      "chunk_id": 668,
      "text": "424\nMeasure Concentration\nmonotonicity of the exponent function and Markov’s inequality, we have that for\nevery t > 0\nP[Z > (1 + δ)p] = P[etZ > et(1+δ)p] ≤E[etZ]\ne(1+δ)tp (B.5)\nNext,\nE[etZ] = E[et P\ni...",
      "word_count": 206,
      "source_page": 424,
      "vector_index": 667,
      "start_position": 151265,
      "end_position": 151470
    },
    {
      "chunk_id": 669,
      "text": "B.4 Hoeﬀding’s Inequality\n425\nand,\nE[e−tZ] = E[e−t P\ni Zi] = E[\nY\ni\ne−tZi]\n=\nY\ni\nE[e−tZi]\nby independence\n=\nY\ni\n\u00001 + pi(e−t −1)\n\u0001\n≤\nY\ni\nepi(e−t−1)\nusing 1 + x ≤ex\n= e(e−t−1)p Setting t = −log(1 −δ) yi...",
      "word_count": 239,
      "source_page": 425,
      "vector_index": 668,
      "start_position": 151471,
      "end_position": 151709
    },
    {
      "chunk_id": 670,
      "text": "426\nMeasure Concentration\nTherefore,\nP[ ¯X ≥ϵ] ≤e−λϵ Y\ni\ne\nλ2(b−a)2\n8m2\n= e−λϵ+ λ2(b−a)2\n8m Setting λ = 4mϵ/(b −a)2 we obtain\nP[ ¯X ≥ϵ] ≤e\n−2mϵ2\n(b−a)2 Applying the same arguments on the variable −¯X ...",
      "word_count": 240,
      "source_page": 426,
      "vector_index": 669,
      "start_position": 151710,
      "end_position": 151949
    },
    {
      "chunk_id": 671,
      "text": "B.5 Bennet’s and Bernstein’s Inequalities\n427\nThen for all ϵ > 0,\nP\n\" m\nX\ni=1\nZi > ϵ\n#\n≤e−mσ2h(\nϵ\nmσ2 ) where\nh(a) = (1 + a) log(1 + a) −a By using the inequality h(a) ≥a2/(2 + 2a/3) it is possible to...",
      "word_count": 218,
      "source_page": 427,
      "vector_index": 670,
      "start_position": 152025,
      "end_position": 152242
    },
    {
      "chunk_id": 672,
      "text": "428\nMeasure Concentration\nSolving for t yields\nt2/2\nm LD(h) + t/3 = log(1/δ)\n⇒\nt2/2 −log(1/δ)\n3\nt −log(1/δ) m LD(h) = 0\n⇒\nt = log(1/δ)\n3\n+\ns\nlog2(1/δ)\n32\n+ 2 log(1/δ) m LD(h)\n≤2 log(1/δ)\n3\n+\np\n2 log(1...",
      "word_count": 223,
      "source_page": 428,
      "vector_index": 671,
      "start_position": 152328,
      "end_position": 152550
    },
    {
      "chunk_id": 673,
      "text": "B.7 Concentration of χ2 Variables\n429\nFinally, for all ϵ ∈(0, 3),\nP [(1 −ϵ)k ≤Z ≤(1 + ϵ)k] ≥1 −2e−ϵ2k/6 Proof\nLet us write Z = Pk\ni=1 X2\ni where Xi ∼N(0, 1) To prove both bounds\nwe use Chernoﬀ’s bound...",
      "word_count": 250,
      "source_page": 429,
      "vector_index": 672,
      "start_position": 152646,
      "end_position": 152895
    },
    {
      "chunk_id": 674,
      "text": "Appendix C Linear Algebra\nC.1\nBasic Deﬁnitions\nIn this chapter we only deal with linear algebra over ﬁnite dimensional Euclidean\nspaces We refer to vectors as column vectors Given two d dimensional ve...",
      "word_count": 246,
      "source_page": 430,
      "vector_index": 673,
      "start_position": 152931,
      "end_position": 153176
    },
    {
      "chunk_id": 675,
      "text": "C.2 Eigenvalues and Eigenvectors\n431\nC.2\nEigenvalues and Eigenvectors\nLet A ∈Rd,d be a matrix A non-zero vector u is an eigenvector of A with a\ncorresponding eigenvalue λ if\nAu = λu theorem C.1 (Spect...",
      "word_count": 245,
      "source_page": 431,
      "vector_index": 674,
      "start_position": 153280,
      "end_position": 153524
    },
    {
      "chunk_id": 676,
      "text": "432\nLinear Algebra\nlemma C.3\nLet A ∈Rm,n be a matrix of rank r Assume that v1, , vr is an\northonormal set of right singular vectors of A, u1, , ur is an orthonormal set\nof corresponding left singular ...",
      "word_count": 223,
      "source_page": 432,
      "vector_index": 675,
      "start_position": 153655,
      "end_position": 153877
    },
    {
      "chunk_id": 677,
      "text": "For i ≤r we have\nBvi =\nr\nX\nj=1\nσjujv⊤\nj vi = σiui = Avi,\nwhere the last equality follows from the deﬁnition The next lemma relates the singular values of A to the eigenvalues of A⊤A\nand AA⊤ lemma C.4\n...",
      "word_count": 168,
      "source_page": 432,
      "vector_index": 676,
      "start_position": 153837,
      "end_position": 154004
    },
    {
      "chunk_id": 678,
      "text": "C.4 Singular Value Decomposition (SVD)\n433\nFinally, we show that if A has rank r then it has r orthonormal singular\nvectors lemma C.5\nLet A ∈Rm,n with rank r Deﬁne the following vectors:\nv1 =\nargmax\nv...",
      "word_count": 247,
      "source_page": 433,
      "vector_index": 677,
      "start_position": 154005,
      "end_position": 154251
    },
    {
      "chunk_id": 679,
      "text": "Therefore,\nmax\nv:∥v∥=1 ∥Av∥2 =\nmax\nx:∥x∥=1\nn\nX\ni=1\nD2\ni,ixi\n2 The solution of the right-hand side is to set x = (1, 0, , 0), which implies that\nv1 is the ﬁrst eigenvector of A⊤A Since ∥Av1∥> 0 it foll...",
      "word_count": 150,
      "source_page": 433,
      "vector_index": 678,
      "start_position": 154226,
      "end_position": 154375
    },
    {
      "chunk_id": 680,
      "text": "References\nAbernethy, J., Bartlett, P L., Rakhlin, A & Tewari, A (2008), Optimal strategies and\nminimax lower bounds for online convex games, in ‘Proceedings of the Nineteenth\nAnnual Conference on Com...",
      "word_count": 250,
      "source_page": 437,
      "vector_index": 679,
      "start_position": 154489,
      "end_position": 154738
    },
    {
      "chunk_id": 681,
      "text": "438\nReferences\nBartlett, P L & Mendelson, S (2002), ‘Rademacher and Gaussian complexities: Risk\nbounds and structural results’, Journal of Machine Learning Research 3, 463–482 Ben-David, S., Cesa-Bian...",
      "word_count": 250,
      "source_page": 438,
      "vector_index": 680,
      "start_position": 154848,
      "end_position": 155097
    },
    {
      "chunk_id": 682,
      "text": "Math Soc 21(1), 1–46 Blumer, A., Ehrenfeucht, A., Haussler, D & Warmuth, M K (1987), ‘Occam’s razor’,\nInformation Processing Letters 24(6), 377–380 Blumer, A., Ehrenfeucht, A., Haussler, D & Warmuth, ...",
      "word_count": 171,
      "source_page": 438,
      "vector_index": 681,
      "start_position": 155094,
      "end_position": 155264
    },
    {
      "chunk_id": 683,
      "text": "References\n439\nBreiman, L (1996), Bias, variance, and arcing classiﬁers, Technical Report 460, Statis-\ntics Department, University of California at Berkeley Breiman, L (2001), ‘Random forests’, Machin...",
      "word_count": 243,
      "source_page": 439,
      "vector_index": 682,
      "start_position": 155265,
      "end_position": 155507
    },
    {
      "chunk_id": 684,
      "text": "(1965), ‘Behavior of sequential predictors of binary sequences’, Trans 4th\nPrague Conf Information Theory Statistical Decision Functions, Random Processes\npp 263–272 Cover, T & Hart, P (1967), ‘Neares...",
      "word_count": 166,
      "source_page": 439,
      "vector_index": 683,
      "start_position": 155496,
      "end_position": 155661
    },
    {
      "chunk_id": 685,
      "text": "440\nReferences\nDietterich, T G & Bakiri, G (1995), ‘Solving multiclass learning problems via error-\ncorrecting output codes’, Journal of Artiﬁcial Intelligence Research 2, 263–286 Donoho, D L (2006), ...",
      "word_count": 247,
      "source_page": 440,
      "vector_index": 684,
      "start_position": 155662,
      "end_position": 155908
    },
    {
      "chunk_id": 686,
      "text": "Georghiades, A., Belhumeur, P & Kriegman, D (2001), ‘From few to many: Illumina-\ntion cone models for face recognition under variable lighting and pose’, IEEE Trans Pattern Anal Mach Intelligence 23(6...",
      "word_count": 188,
      "source_page": 440,
      "vector_index": 685,
      "start_position": 155902,
      "end_position": 156089
    },
    {
      "chunk_id": 687,
      "text": "References\n441\nHinton, G E., Osindero, S & Teh, Y.-W (2006), ‘A fast learning algorithm for deep\nbelief nets’, Neural Computation 18(7), 1527–1554 Hiriart-Urruty, J.-B & Lemar´echal, C (1996), Convex ...",
      "word_count": 249,
      "source_page": 441,
      "vector_index": 686,
      "start_position": 156090,
      "end_position": 156338
    },
    {
      "chunk_id": 688,
      "text": "(2003), ‘An impossibility theorem for clustering’, Advances in Neural\nInformation Processing Systems pp 463–470 Klivans, A R & Sherstov, A A (2006), Cryptographic hardness for learning intersec-\ntions...",
      "word_count": 198,
      "source_page": 441,
      "vector_index": 687,
      "start_position": 156325,
      "end_position": 156522
    },
    {
      "chunk_id": 689,
      "text": "442\nReferences\nLe, Q V., Ranzato, M.-A., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J &\nNg, A Y (2012), Building high-level features using large scale unsupervised learning,\nin ‘International ...",
      "word_count": 250,
      "source_page": 442,
      "vector_index": 688,
      "start_position": 156523,
      "end_position": 156772
    },
    {
      "chunk_id": 690,
      "text": "Minsky, M & Papert, S (1969), Perceptrons: An Introduction to Computational Ge-\nometry, The MIT Press Mukherjee, S., Niyogi, P., Poggio, T & Rifkin, R (2006), ‘Learning theory: stability is\nsuﬃcient f...",
      "word_count": 173,
      "source_page": 442,
      "vector_index": 689,
      "start_position": 156768,
      "end_position": 156940
    },
    {
      "chunk_id": 691,
      "text": "References\n443\nNesterov, Y & Nesterov, I (2004), Introductory lectures on convex optimization: A\nbasic course, Vol 87, Springer Netherlands Novikoﬀ, A B J (1962), On convergence proofs on perceptrons,...",
      "word_count": 249,
      "source_page": 443,
      "vector_index": 690,
      "start_position": 156941,
      "end_position": 157189
    },
    {
      "chunk_id": 692,
      "text": "(2010), Online learning: Random averages,\ncombinatorial parameters, and learnability, in ‘NIPS’ Rakhlin, S., Mukherjee, S & Poggio, T (2005), ‘Stability results in learning theory’,\nAnalysis and Appli...",
      "word_count": 221,
      "source_page": 443,
      "vector_index": 691,
      "start_position": 157175,
      "end_position": 157395
    },
    {
      "chunk_id": 693,
      "text": "444\nReferences\nSankaran, J K (1993), ‘A note on resolving infeasibility in linear programs by con-\nstraint relaxation’, Operations Research Letters 13(1), 19–20 Sauer, N (1972), ‘On the density of fam...",
      "word_count": 243,
      "source_page": 444,
      "vector_index": 692,
      "start_position": 157396,
      "end_position": 157638
    },
    {
      "chunk_id": 694,
      "text": "Shalev-Shwartz, S., Shamir, O & Sridharan, K (2010), Learning kernel-based halfs-\npaces with the zero-one loss, in ‘Conference on Learning Theory (COLT)’ Shalev-Shwartz, S., Shamir, O., Sridharan, K &...",
      "word_count": 194,
      "source_page": 444,
      "vector_index": 693,
      "start_position": 157632,
      "end_position": 157825
    },
    {
      "chunk_id": 695,
      "text": "References\n445\nShelah, S (1972), ‘A combinatorial problem; stability and order for models and theories\nin inﬁnitary languages’, Pac J Math 4, 247–261 Sipser, M (2006), Introduction to the Theory of Co...",
      "word_count": 248,
      "source_page": 445,
      "vector_index": 694,
      "start_position": 157826,
      "end_position": 158073
    },
    {
      "chunk_id": 696,
      "text": "Vapnik, V N (1998), Statistical Learning Theory, Wiley Vapnik, V N & Chervonenkis, A Y (1971), ‘On the uniform convergence of relative\nfrequencies of events to their probabilities’, Theory of Probabil...",
      "word_count": 164,
      "source_page": 445,
      "vector_index": 695,
      "start_position": 158071,
      "end_position": 158234
    },
    {
      "chunk_id": 697,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbac...",
      "word_count": 271,
      "source_page": 447,
      "vector_index": 696,
      "start_position": 158370,
      "end_position": 158640
    },
    {
      "chunk_id": 698,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbac...",
      "word_count": 274,
      "source_page": 447,
      "vector_index": 697,
      "start_position": 158370,
      "end_position": 158643
    },
    {
      "chunk_id": 699,
      "text": "Index\n3-term DNF, 107\nF1-score, 244\nℓ1 norm, 183, 332, 363, 386\naccuracy, 38, 43\nactivation function, 269\nAdaBoost, 130, 134, 362\nall-pairs, 228, 404\napproximation error, 61, 64\nauto-encoders, 368\nbac...",
      "word_count": 277,
      "source_page": 447,
      "vector_index": 698,
      "start_position": 158370,
      "end_position": 158646
    },
    {
      "chunk_id": 700,
      "text": "448\nIndex\nforward greedy selection, 360\nfrequentist, 353\ngain, 253\nGD, see gradient descent\ngeneralization error, 35\ngenerative models, 342\nGini index, 254\nGlivenko-Cantelli, 58\ngradient, 158\ngradient...",
      "word_count": 327,
      "source_page": 448,
      "vector_index": 699,
      "start_position": 158654,
      "end_position": 158980
    },
    {
      "chunk_id": 701,
      "text": "Index\n449\nNormalized Discounted Cumulative Gain,\nsee NDCG\nOccam’s razor, 91\nOMP, 360\none-vs-all, 227\none-vs-rest, see one-vs-all\none-vs.-all, 404\nonline convex optimization, 300\nonline gradient descen...",
      "word_count": 325,
      "source_page": 449,
      "vector_index": 700,
      "start_position": 158981,
      "end_position": 159305
    }
  ]
}